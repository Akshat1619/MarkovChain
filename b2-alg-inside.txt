Basic Algebra
Digital Second Editions
By Anthony W. Knapp
Basic Algebra
Advanced Algebra
Basic Real Analysis,
with an appendix “Elementary Complex Analysis”
Advanced Real Analysis
Anthony W. Knapp



Basic Algebra
Along with a Companion Volume Advanced Algebra




Digital Second Edition, 2016




Published by the Author
East Setauket, New York
Anthony W. Knapp
81 Upper Sheep Pasture Road
East Setauket, N.Y. 11733–1729, U.S.A.
Email to: aknapp@math.stonybrook.edu
Homepage: www.math.stonybrook.edu/∼aknapp

Title: Basic Algebra
Cover: Construction of a regular heptadecagon, the steps shown in color sequence; see page 505.
Mathematics Subject Classification (2010): 15–01, 20–01, 13–01, 12–01, 16–01, 08–01, 18A05,
68P30.
First Edition, ISBN-13 978-0-8176-3248-9
c
"2006   Anthony W. Knapp
Published by Birkhäuser Boston
Digital Second Edition, not to be sold, no ISBN
c
"2016   Anthony W. Knapp
Published by the Author
All rights reserved. This file is a digital second edition of the above named book. The text, images,
and other data contained in this file, which is in portable document format (PDF), are proprietary to
the author, and the author retains all rights, including copyright, in them. The use in this file of trade
names, trademarks, service marks, and similar items, even if they are not identified as such, is not
to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.
All rights to print media for the first edition of this book have been licensed to Birkhäuser Boston,
c/o Springer Science+Business Media Inc., 233 Spring Street, New York, NY 10013, USA, and
this organization and its successor licensees may have certain rights concerning print media for the
digital second edition. The author has retained all rights worldwide concerning digital media for
both the first edition and the digital second edition.
The file is made available for limited noncommercial use for purposes of education, scholarship, and
research, and for these purposes only, or for fair use as understood in the United States copyright law.
Users may freely download this file for their own use and may store it, post it online, and transmit it
digitally for purposes of education, scholarship, and research. They may not convert it from PDF to
any other format (e.g., EPUB), they may not edit it, and they may not do reverse engineering with it.
In transmitting the file to others or posting it online, users must charge no fee, nor may they include
the file in any collection of files for which a fee is charged. Any exception to these rules requires
written permission from the author.
Except as provided by fair use provisions of the United States copyright law, no extracts or quotations
from this file may be used that do not consist of whole pages unless permission has been granted by
the author (and by Birkhäuser Boston if appropriate).
The permission granted for use of the whole file and the prohibition against charging fees extend to
any partial file that contains only whole pages from this file, except that the copyright notice on this
page must be included in any partial file that does not consist exclusively of the front cover page.
Such a partial file shall not be included in any derivative work unless permission has been granted
by the author (and by Birkhäuser Boston if appropriate).
Inquiries concerning print copies of either edition should be directed to Springer Science+Business
Media Inc.

                                                   iv
                   To Susan

                      and

       To My Children, Sarah and William,

                      and

            To My Algebra Teachers:

   Ralph Fox, John Fraleigh, Robert Gunning,
John Kemeny, Bertram Kostant, Robert Langlands,
 Goro Shimura, Hale Trotter, Richard Williamson
                            CONTENTS




      Contents of Advanced Algebra                             x
      Preface to the Second Edition                           xi
      Preface to the First Edition                          xiii
      List of Figures                                       xvii
      Dependence Among Chapters                              xix
      Standard Notation                                       xx
      Guide for the Reader                                   xxi
I.    PRELIMINARIES ABOUT THE INTEGERS,
      POLYNOMIALS, AND MATRICES                               1
       1. Division and Euclidean Algorithms                   1
       2. Unique Factorization of Integers                    4
       3. Unique Factorization of Polynomials                 9
       4. Permutations and Their Signs                       15
       5. Row Reduction                                      19
       6. Matrix Operations                                  24
       7. Problems                                           30
II.  VECTOR SPACES OVER Q, R, AND C                          33
      1. Spanning, Linear Independence, and Bases            33
      2. Vector Spaces Defined by Matrices                   38
      3. Linear Maps                                         42
      4. Dual Spaces                                         50
      5. Quotients of Vector Spaces                          54
      6. Direct Sums and Direct Products of Vector Spaces    58
      7. Determinants                                        65
      8. Eigenvectors and Characteristic Polynomials         73
      9. Bases in the Infinite-Dimensional Case              78
     10. Problems                                            82
III. INNER-PRODUCT SPACES                                    89
      1. Inner Products and Orthonormal Sets                 89
      2. Adjoints                                            99
      3. Spectral Theorem                                   105
      4. Problems                                           112
                                   vii
viii                               Contents


IV.    GROUPS AND GROUP ACTIONS                                117
        1. Groups and Subgroups                                118
        2. Quotient Spaces and Homomorphisms                   129
        3. Direct Products and Direct Sums                     135
        4. Rings and Fields                                    141
        5. Polynomials and Vector Spaces                       148
        6. Group Actions and Examples                          159
        7. Semidirect Products                                 167
        8. Simple Groups and Composition Series                171
        9. Structure of Finitely Generated Abelian Groups      176
       10. Sylow Theorems                                      185
       11. Categories and Functors                             189
       12. Problems                                            200

V.     THEORY OF A SINGLE LINEAR TRANSFORMATION                211
        1. Introduction                                        211
        2. Determinants over Commutative Rings with Identity   215
        3. Characteristic and Minimal Polynomials              218
        4. Projection Operators                                226
        5. Primary Decomposition                               228
        6. Jordan Canonical Form                               231
        7. Computations with Jordan Form                       238
        8. Problems                                            241

VI.    MULTILINEAR ALGEBRA                                     248
        1. Bilinear Forms and Matrices                         249
        2. Symmetric Bilinear Forms                            253
        3. Alternating Bilinear Forms                          256
        4. Hermitian Forms                                     258
        5. Groups Leaving a Bilinear Form Invariant            260
        6. Tensor Product of Two Vector Spaces                 263
        7. Tensor Algebra                                      277
        8. Symmetric Algebra                                   283
        9. Exterior Algebra                                    291
       10. Problems                                            295

VII. ADVANCED GROUP THEORY                                     306
      1. Free Groups                                           306
      2. Subgroups of Free Groups                              317
      3. Free Products                                         322
      4. Group Representations                                 329
                                 Contents                              ix


VII. ADVANCED GROUP THEORY (Continued)
      5. Burnside’s Theorem                                           345
      6. Extensions of Groups                                         347
      7. Problems                                                     360

VIII. COMMUTATIVE RINGS AND THEIR MODULES                             370
       1. Examples of Rings and Modules                               370
       2. Integral Domains and Fields of Fractions                    381
       3. Prime and Maximal Ideals                                    384
       4. Unique Factorization                                        387
       5. Gauss’s Lemma                                               393
       6. Finitely Generated Modules                                  399
       7. Orientation for Algebraic Number Theory and
          Algebraic Geometry                                          411
       8. Noetherian Rings and the Hilbert Basis Theorem              417
       9. Integral Closure                                            420
      10. Localization and Local Rings                                428
      11. Dedekind Domains                                            437
      12. Problems                                                    443

IX.   FIELDS AND GALOIS THEORY                                        452
       1. Algebraic Elements                                          453
       2. Construction of Field Extensions                            457
       3. Finite Fields                                               461
       4. Algebraic Closure                                           464
       5. Geometric Constructions by Straightedge and Compass         468
       6. Separable Extensions                                        474
       7. Normal Extensions                                           481
       8. Fundamental Theorem of Galois Theory                        484
       9. Application to Constructibility of Regular Polygons         489
      10. Application to Proving the Fundamental Theorem of Algebra   492
      11. Application to Unsolvability of Polynomial Equations with
          Nonsolvable Galois Group                                    493
      12. Construction of Regular Polygons                            499
      13. Solution of Certain Polynomial Equations with Solvable
          Galois Group                                                506
      14. Proof That π Is Transcendental                              515
      15. Norm and Trace                                              519
      16. Splitting of Prime Ideals in Extensions                     526
      17. Two Tools for Computing Galois Groups                       532
      18. Problems                                                    539
x                                    Contents




X.      MODULES OVER NONCOMMUTATIVE RINGS           553
         1. Simple and Semisimple Modules           553
         2. Composition Series                      560
         3. Chain Conditions                        565
         4. Hom and End for Modules                 567
         5. Tensor Product for Modules              574
         6. Exact Sequences                         583
         7. Problems                                587

APPENDIX                                            593
    A1. Sets and Functions                          593
    A2. Equivalence Relations                       599
    A3. Real Numbers                                601
    A4. Complex Numbers                             604
    A5. Partial Orderings and Zorn’s Lemma          605
    A6. Cardinality                                 610

        Hints for Solutions of Problems             615
        Selected References                         715
        Index of Notation                           717
        Index                                       721



        CONTENTS OF ADVANCED ALGEBRA

I.      Transition to Modern Number Theory
II.     Wedderburn–Artin Ring Theory
III.    Brauer Group
IV.     Homological Algebra
V.      Three Theorems in Algebraic Number Theory
VI.     Reinterpretation with Adeles and Ideles
VII.    Infinite Field Extensions
VIII.   Background for Algebraic Geometry
IX.     The Number Theory of Algebraic Curves
X.      Methods of Algebraic Geometry
           PREFACE TO THE SECOND EDITION



In the years since publication of the first edition of Basic Algebra, many readers
have reacted to the book by sending comments, suggestions, and corrections.
People especially approved of the inclusion of some linear algebra before any
group theory, and they liked the ideas of proceeding from the particular to the
general and of giving examples of computational techniques right from the start.
They appreciated the overall comprehensive nature of the book, associating this
feature with the large number of problems that develop so many sidelights and
applications of the theory.
   Along with the general comments and specific suggestions were corrections,
and there were enough corrections, perhaps a hundred in all, so that a second
edition now seems to be in order. Many of the corrections were of minor matters,
yet readers should not have to cope with errors along with new material. Fortu-
nately no results in the first edition needed to be deleted or seriously modified,
and additional results and problems could be included without renumbering.
   For the first edition, the author granted a publishing license to Birkhäuser
Boston that was limited to print media, leaving the question of electronic publi-
cation unresolved. The main change with the second edition is that the question
of electronic publication has now been resolved, and a PDF file, called the “digital
second edition,” is being made freely available to everyone worldwide for personal
use. This file may be downloaded from the author’s own Web page and from
elsewhere.
   The main changes to the text of the first edition of Basic Algebra are as follows:
• The corrections sent by readers and by reviewers have been made. The most
    significant such correction was a revision to the proof of Zorn’s Lemma, the
    earlier proof having had a gap.
• A number of problems have been added at the ends of the chapters, most of
    them with partial or full solutions added to the section of Hints at the back of
    the book. Of particular note are problems on the following topics:
     (a) (Chapter II) the relationship in two and three dimensions between deter-
         minants and areas or volumes,
     (b) (Chapters V and IX) further aspects of canonical forms for matrices and
         linear mappings,
     (c) (Chapter VIII) amplification of uses of the Fundamental Theorem of
         Finitely Generated Modules over principal ideal domains,
                                         xi
xii                         Preface to the Second Edition

     (d) (Chapter IX) the interplay of extension of scalars and Galois theory,
     (e) (Chapter IX) properties and examples of ordered fields and real closed
          fields.
• Some revisions have been made to the chapter on field theory (Chapter IX).
    It was originally expected, and it continues to be expected, that a reader who
    wants a fuller treatment of fields will look also at the chapter on infinite
    field extensions in Advanced Algebra. However, the original placement of the
    break between volumes left some possible confusion about the role of “normal
    extensions” in field theory, and that matter has now been resolved.
• Characteristic polynomials initially have a variable ∏ as a reminder of how
    they arise from eigenvalues. But it soon becomes important to think of them
    as abstract polynomials, not as polynomial functions. The indeterminate
    had been left as ∏ throughout most of the book in the original edition, and
    some confusion resulted. The indeterminate is now called X rather than ∏
    from Chapter V on, and characteristic polynomials have been treated
    unambiguously thereafter as abstract polynomials.
• Occasional paragraphs have been added that point ahead to material in
    Advanced Algebra.
    The preface to the first edition mentioned three themes that recur throughout
and blend together at times: the analogy between integers and polynomials in
one variable over a field, the interplay between linear algebra and group theory,
and the relationship between number theory and geometry. A fourth is the gentle
mention of notions in category theory to tie together phenomena that occur in
different areas of algebra; an example of such a notion is “universal mapping
property.” Readers will benefit from looking for these and other such themes,
since recognizing them helps one get a view of the whole subject at once.
    It was Benjamin Levitt, Birkhäuser mathematics editor in New York, who
encouraged the writing of a second edition, who made a number of suggestions
about pursuing it, and who passed along comments from several anonymous
referees about the strengths and weaknesses of the book. I am especially grateful
to those readers who have sent me comments over the years. Many corrections and
suggestions were kindly pointed out to the author by Skip Garibaldi of Emory
University and Ario Contact of Shiraz, Iran. The long correction concerning
Zorn’s Lemma resulted from a discussion with Qiu Ruyue. The typesetting was
done by the program Textures using AMS-TEX, and the figures were drawn with
Mathematica.
    Just as with the first edition, I invite corrections and other comments from
readers. For as long as I am able, I plan to point to a list of known corrections
from my own Web page, www.math.stonybrook.edu/∼aknapp.
                                                                      A. W. KNAPP
                                                                     January 2016
             PREFACE TO THE FIRST EDITION




Basic Algebra and its companion volume Advanced Algebra systematically de-
velop concepts and tools in algebra that are vital to every mathematician, whether
pure or applied, aspiring or established. These two books together aim to give the
reader a global view of algebra, its use, and its role in mathematics as a whole.
The idea is to explain what the young mathematician needs to know about algebra
in order to communicate well with colleagues in all branches of mathematics.
   The books are written as textbooks, and their primary audience is students who
are learning the material for the first time and who are planning a career in which
they will use advanced mathematics professionally. Much of the material in the
books, particularly in Basic Algebra but also in some of the chapters of Advanced
Algebra, corresponds to normal course work. The books include further topics
that may be skipped in required courses but that the professional mathematician
will ultimately want to learn by self-study. The test of each topic for inclusion is
whether it is something that a plenary lecturer at a broad international or national
meeting is likely to take as known by the audience.
  The key topics and features of Basic Algebra are as follows:
• Linear algebra and group theory build on each other throughout the book.
  A small amount of linear algebra is introduced first, as the topic likely to be
  better known by the reader ahead of time, and then a little group theory is
  introduced, with linear algebra providing important examples.
• Chapters on linear algebra develop notions related to vector spaces, the
  theory of linear transformations, bilinear forms, classical linear groups, and
  multilinear algebra.
• Chapters on modern algebra treat groups, rings, fields, modules, and Galois
  groups, including many uses of Galois groups and methods of computation.
• Three prominent themes recur throughout and blend together at times: the
  analogy between integers and polynomials in one variable over a field, the in-
  terplay between linear algebra and group theory, and the relationship between
  number theory and geometry.
• The development proceeds from the particular to the general, often introducing
  examples well before a theory that incorporates them.
• More than 400 problems at the ends of chapters illuminate aspects of the
  text, develop related topics, and point to additional applications. A separate
                                        xiii
xiv                           Preface to the First Edition

    90-page section “Hints for Solutions of Problems” at the end of the book gives
    detailed hints for most of the problems, complete solutions for many.
• Applications such as the fast Fourier transform, the theory of linear error-
    correcting codes, the use of Jordan canonical form in solving linear systems
    of ordinary differential equations, and constructions of interest in mathematical
    physics arise naturally in sequences of problems at the ends of chapters and
    illustrate the power of the theory for use in science and engineering.
    Basic Algebra endeavors to show some of the interconnections between
different areas of mathematics, beyond those listed above. Here are examples:
Systems of orthogonal functions make an appearance with inner-product spaces.
Covering spaces naturally play a role in the examination of subgroups of free
groups. Cohomology of groups arises from considering group extensions. Use
of the power-series expansion of the exponential function combines with algebraic
numbers to prove that π is transcendental. Harmonic analysis on a cyclic group
explains the mysterious method of Lagrange resolvents in the theory of Galois
groups.
    Algebra plays a singular role in mathematics by having been developed so
extensively at such an early date. Indeed, the major discoveries of algebra even
from the days of Hilbert are well beyond the knowledge of most nonalgebraists
today. Correspondingly most of the subject matter of the present book is at
least 100 years old. What has changed over the intervening years concerning
algebra books at this level is not so much the mathematics as the point of
view toward the subject matter and the relative emphasis on and generality of
various topics. For example, in the 1920s Emmy Noether introduced vector
spaces and linear mappings to reinterpret coordinate spaces and matrices, and
she defined the ingredients of what was then called “modern algebra”—the
axiomatically defined rings, fields, and modules, and their homomorphisms. The
introduction of categories and functors in the 1940s shifted the emphasis even
more toward the homomorphisms and away from the objects themselves. The
creation of homological algebra in the 1950s gave a unity to algebraic topics
cutting across many fields of mathematics. Category theory underwent a period
of great expansion in the 1950s and 1960s, followed by a contraction and a return
more to a supporting role. The emphasis in topics shifted. Linear algebra had
earlier been viewed as a separate subject, with many applications, while group
theory and the other topics had been viewed as having few applications. Coding
theory, cryptography, and advances in physics and chemistry have changed all
that, and now linear algebra and group theory together permeate mathematics and
its applications. The other subjects build on them, and they too have extensive
applications in science and engineering, as well as in the rest of mathematics.
    Basic Algebra presents its subject matter in a forward-looking way that takes
this evolution into account. It is suitable as a text in a two-semester advanced
                              Preface to the First Edition                        xv

undergraduate or first-year graduate sequence in algebra. Depending on the grad-
uate school, it may be appropriate to include also some material from Advanced
Algebra. Briefly the topics in Basic Algebra are linear algebra and group theory,
rings, fields, and modules. A full list of the topics in Advanced Algebra appears
on page x; of these, the Wedderburn theory of semisimple algebras, homological
algebra, and foundational material for algebraic geometry are the ones that most
commonly appear in syllabi of first-year graduate courses.
   A chart on page xix tells the dependence among chapters and can help with
preparing a syllabus. Chapters I–VII treat linear algebra and group theory at
various levels, except that three sections of Chapter IV and one of Chapter V
introduce rings and fields, polynomials, categories and functors, and determinants
over commutative rings with identity. Chapter VIII concerns rings, with emphasis
on unique factorization; Chapter IX concerns field extensions and Galois theory,
with emphasis on applications of Galois theory; and Chapter X concerns modules
and constructions with modules.
   For a graduate-level sequence the syllabus is likely to include all of Chapters
I–V and parts of Chapters VIII and IX, at a minimum. Depending on the
knowledge of the students ahead of time, it may be possible to skim much of
the first three chapters and some of the beginning of the fourth; then time may
allow for some of Chapters VI and VII, or additional material from Chapters VIII
and IX, or some of the topics in Advanced Algebra. For many of the topics in
Advanced Algebra, parts of Chapter X of Basic Algebra are prerequisite.
   For an advanced undergraduate sequence the first semester can include Chap-
ters I through III except Section II.9, plus the first six sections of Chapter IV and
as much as reasonable from Chapter V; the notion of category does not appear
in this material. The second semester will involve categories very gently; the
course will perhaps treat the remainder of Chapter IV, the first five or six sections
of Chapter VIII, and at least Sections 1–3 and 5 of Chapter IX.
   More detailed information about how the book can be used with courses can
be deduced by using the chart on page xix in conjunction with the section “Guide
for the Reader” on pages xxi–xxiv. In my own graduate teaching, I have built one
course around Chapters I–III, Sections 1–6 of Chapter IV, all of Chapter V, and
about half of Chapter VI. A second course dealt with the remainder of Chapter
IV, a little of Chapter VII, Sections 1–6 of Chapter VIII, and Sections 1–11 of
Chapter IX.
   The problems at the ends of chapters are intended to play a more important
role than is normal for problems in a mathematics book. Almost all problems
are solved in the section of hints at the end of the book. This being so, some
blocks of problems form additional topics that could have been included in the
text but were not; these blocks may either be regarded as optional topics, or they
may be treated as challenges for the reader. The optional topics of this kind
xvi                           Preface to the First Edition

usually either carry out further development of the theory or introduce significant
applications. For example one block of problems at the end of Chapter VII
carries the theory of representations of finite groups a little further by developing
the Poisson summation formula and the fast Fourier transform. For a second
example blocks of problems at the ends of Chapters IV, VII, and IX introduce
linear error-correcting codes as an application of the theory in those chapters.
   Not all problems are of this kind, of course. Some of the problems are
really pure or applied theorems, some are examples showing the degree to which
hypotheses can be stretched, and a few are just exercises. The reader gets no
indication which problems are of which type, nor of which ones are relatively
easy. Each problem can be solved with tools developed up to that point in the
book, plus any additional prerequisites that are noted.
   Beyond a standard one-variable calculus course, the most important prereq-
uisite for using Basic Algebra is that the reader already know what a proof is,
how to read a proof, and how to write a proof. This knowledge typically is
obtained from honors calculus courses, or from a course in linear algebra, or
from a first junior–senior course in real variables. In addition, it is assumed that
the reader is comfortable with a small amount of linear algebra, including matrix
computations, row reduction of matrices, solutions of systems of linear equations,
and the associated geometry. Some prior exposure to groups is helpful but not
really necessary.
   The theorems, propositions, lemmas, and corollaries within each chapter are
indexed by a single number stream. Figures have their own number stream, and
one can find the page reference for each figure from the table on pages xvii–xviii.
Labels on displayed lines occur only within proofs and examples, and they are
local to the particular proof or example in progress. Some readers like to skim
or skip proofs on first reading; to facilitate this procedure, each occurrence of the
word “PROOF” or “PROOF” is matched by an occurrence at the right margin of the
symbol § to mark the end of that proof.
   I am grateful to Ann Kostant and Steven Krantz for encouraging this project
and for making many suggestions about pursuing it. I am especially indebted to
an anonymous referee, who made detailed comments about many aspects of a
preliminary version of the book, and to David Kramer, who did the copyediting.
The typesetting was by AMS-TEX, and the figures were drawn with Mathematica.
   I invite corrections and other comments from readers. I plan to maintain a list
of known corrections on my own Web page.
                                                                   A. W. KNAPP
                                                                    August 2006
                       LIST OF FIGURES



2.1. The vector space of lines v + U in R2 parallel to a given line U
      through the origin                                                  55
2.2. Factorization of linear maps via a quotient of vector spaces         56
2.3. Three 1-dimensional vector subspaces of R2 such that each pair
      has intersection 0                                                  62
2.4. Universal mapping property of a direct product of vector spaces      64
2.5. Universal mapping property of a direct sum of vector spaces          65
2.6. Area of a parallelogram as a difference of areas                     88
3.1. Geometric interpretation of the parallelogram law                    92
3.2. Resolution of a vector into a parallel component and an
      orthogonal component                                                94
4.1. Factorization of homomorphisms of groups via the quotient of a
      group by a normal subgroup                                          133
4.2. Universal mapping property of an external direct product of groups   137
4.3. Universal mapping property of a direct product of groups             137
4.4. Universal mapping property of an external direct sum of abelian
      groups                                                              139
4.5. Universal mapping property of a direct sum of abelian groups         140
4.6. Factorization of homomorphisms of rings via the quotient of a ring
      by an ideal                                                         147
4.7. Substitution homomorphism for polynomials in one indeterminate       151
4.8. Substitution homomorphism for polynomials in n indeterminates        157
4.9. A square diagram                                                     194
4.10. Diagrams obtained by applying a covariant functor and a
      contravariant functor                                               195
4.11. Universal mapping property of a product in a category               196
4.12. Universal mapping property of a coproduct in a category             198
5.1. Example of a nilpotent matrix in Jordan form                         234
5.2. Powers of the nilpotent matrix in Figure 5.1                         234
6.1. Universal mapping property of a tensor product                       264
6.2. Diagrams for uniqueness of a tensor product                          264
                                    xvii
xviii                              List of Figures




 6.3.    Commutative diagram of a natural transformation {TX }                268
 6.4.    Commutative diagram of a triple tensor product                       277
 6.5.    Universal mapping property of a tensor algebra                       282
 7.1.    Universal mapping property of a free group                           308
 7.2.    Universal mapping property of a free product                         323
 7.3.    An intertwining operator for two representations                     333
 7.4.    Equivalent group extensions                                          352
 8.1.    Universal mapping property of the integral group ring of G           374
 8.2.    Universal mapping property of a free left R module                   377
 8.3.    Factorization of R homomorphisms via a quotient of R modules         379
 8.4.    Universal mapping property of the group algebra RG                   381
 8.5.    Universal mapping property of the field of fractions of R            383
 8.6.    Real points of the curve y 2 = (x − 1)x(x + 1)                       412
 8.7.    Universal mapping property of the localization of R at S             431
 9.1.    Closure of positive constructible x coordinates under
         multiplication and division                                          470
 9.2.    Closure of positive constructible x coordinates under square roots   470
 9.3.    Construction of a regular pentagon                                   501
 9.4.    Construction of a regular 17-gon                                     505
 10.1.   Universal mapping property of a tensor product of a right R module
         and a left R module                                                  575
            DEPENDENCE AMONG CHAPTERS




Below is a chart of the main lines of dependence of chapters on prior chapters.
The dashed lines indicate helpful motivation but no logical dependence. Apart
from that, particular examples may make use of information from earlier chapters
that is not indicated by the chart.



                                 I, II


     III


                              IV.1–IV.6


               IV.7–IV.11                           V


                   VII           VI            VIII.1–VIII.6


                                  X             IX.1–IX.13     VIII.7–VIII.11


                                                      IX.14–IX.17




                                         xix
                               STANDARD NOTATION


See the Index of Notation, pp. 717–719, for symbols defined starting on page 1.
Item                                     Meaning
 #S or |S|                               number of elements in S
 ∅                                       empty set
 {x ∈ E | P}                             the set of x in E such that P holds
 Ec                                      complement of the set E
 E ∪ F, ET∩ F, E − F
 S                                       union, intersection, difference of sets
     α Eα ,     α Eα                     union, intersection of the sets E α
 E ⊆ F, E ⊇ F                            E is contained in F, E contains F
 E $ F, E % F                            E properly contained in F, properly contains F
 E × F, ×s∈S X s                         products of sets
(a1 , . . . , an ), {a1 , . . . , an }   ordered n-tuple, unordered n-tuple
  f : E → F, x Ø7→ f (x)                 function, effect of function
  f ◦ g or f g, f Ø E                    composition of g followed by f , restriction to E
  f ( · , y)                             the function x 7→ f (x, y)
  f (E), f −1 (E)                        direct and inverse image of a set
δ°i j¢                                   Kronecker delta: 1 if i = j, 0 if i 6= j
 n
 k                                       binomial coefficient
n positive, n negative                   n > 0, n < 0
Z, Q, R, C                               integers, rationals, reals, complex numbers
P (and
max    Q similarly min)                  maximum of a finite subset of a totally ordered set
    or                                   sum or product, possibly with a limit operation
countable                                finite or in one-one correspondence with Z
[x]                                      greatest integer ≤ x if x is real
Re z, Im z                               real and imaginary parts of complex z
z̄                                       complex conjugate of z
|z|                                      absolute value of z
1                                        multiplicative identity
1 or I                                   identity matrix or operator
1X                                       identity function on X
Qn , Rn , Cn                             spaces of column vectors
diag(a1 , . . . , an )                   diagonal matrix
∼
=                                        is isomorphic to, is equivalent to


                                                  xx
                    GUIDE FOR THE READER



This section is intended to help the reader find out what parts of each chapter are
most important and how the chapters are interrelated. Further information of this
kind is contained in the abstracts that begin each of the chapters.
   The book pays attention to at least three recurring themes in algebra, allowing
a person to see how these themes arise in increasingly sophisticated ways. These
are the analogy between integers and polynomials in one indeterminate over a
field, the interplay between linear algebra and group theory, and the relationship
between number theory and geometry. Keeping track of how these themes evolve
will help the reader understand the mathematics better and anticipate where it is
headed.
   In Chapter I the analogy between integers and polynomials in one indeterminate
over the rationals, reals, or complex numbers appears already in the first three
sections. The main results of these sections are theorems about unique factoriza-
tion in each of the two settings. The relevant parts of the underlying structures for
the two settings are the same, and unique factorization can therefore be proved in
both settings by the same argument. Many readers will already know this unique
factorization, but it is worth examining the parallel structure and proof at least
quickly before turning to the chapters that follow.
   Before proceeding very far into the book, it is worth looking also at the appendix
to see whether all its topics are familiar. Readers will find Section A1 useful
at least for its summary of set-theoretic notation and for its emphasis on the
distinction between range and image for a function. This distinction is usually
unimportant in analysis but becomes increasingly important as one studies more
advanced topics in algebra. Readers who have not specifically learned about
equivalence relations and partial orderings can learn about them from Sections
A2 and A5. Sections A3 and A4 concern the real and complex numbers; the
emphasis is on notation and the Intermediate Value Theorem, which plays a role
in proving the Fundamental Theorem of Algebra. Zorn’s Lemma and cardinality
in Sections A5 and A6 are usually unnecessary in an undergraduate course. They
arise most importantly in Sections II.9 and IX.4, which are normally omitted in
an undergraduate course, and in Proposition 8.8, which is invoked only in the last
few sections of Chapter VIII.
   The remainder of this section is an overview of individual chapters and pairs
of chapters.
                                         xxi
xxii                              Guide for the Reader

    Chapter I is in three parts. The first part, as mentioned above, establishes unique
factorization for the integers and for polynomials in one indeterminate over the
rationals, reals, or complex numbers. The second part defines permutations and
shows that they have signs such that the sign of any composition is the product of
the signs; this result is essential for defining general determinants in Section II.7.
The third part will likely be a review for all readers. It establishes notation for row
reduction of matrices and for operations on matrices, and it uses row reduction
to show that a one-sided inverse for a square matrix is a two-sided inverse.
    Chapters II–III treat the fundamentals of linear algebra. Whereas the matrix
computations in Chapter I were concrete, Chapters II–III are relatively abstract.
Much of this material is likely to be a review for graduate students. The geometric
interpretation of vectors spaces, subspaces, and linear mappings is not included in
the chapter, being taken as known previously. The fundamental idea that a newly
constructed object might be characterized by a “universal mapping property”
appears for the first time in Chapter II, and it appears more and more frequently
throughout the book. One aspect of this idea is that it is sometimes not so
important what certain constructed objects are, but what they do. A related idea
being emphasized is that the mappings associated with a newly constructed object
are likely to be as important as the object, if not more so; at the least, one needs to
stop and find what those mappings are. Section II.9 uses Zorn’s Lemma and can
be deferred until Chapter IX if one wants. Chapter III discusses special features
of real and complex vector spaces endowed with inner products. The main result
is the Spectral Theorem in Section 3. Many of the problems at the end of the
chapter make contact with real analysis. The subject of linear algebra continues
in Chapter V.
    Chapter IV is the primary chapter on group theory and may be viewed as in
three parts. Sections 1–6 form the first part, which is essential for all later chapters
in the book. Sections 1–3 introduce groups and some associated constructions,
along with a number of examples. Many of the examples will be seen to be
related to specific or general vector spaces, and thus the theme of the interplay
between group theory and linear algebra is appearing concretely for the first time.
In practice, many examples of groups arise in the context of group actions, and
abstract group actions are defined in Section 6. Of particular interest are group
representations, which are group actions on a vector space by linear mappings.
Sections 4–5 are a digression to define rings, fields, and ring homomorphisms,
and to extend the theories concerning polynomials and vector spaces as presented
in Chapters I–II. The immediate purpose of the digression is to make prime fields,
their associated multiplicative groups, and the notion of characteristic available
for the remainder of the chapter. The definition of vector space is extended
to allow scalars from any field. The definition of polynomial is extended to
allow coefficients from any commutative ring with identity, rather than just the
                                 Guide for the Reader                            xxiii

rationals or reals or complex numbers, and to allow more than one indeterminate.
Universal mapping properties for polynomial rings are proved. Sections 7–10
form the second part of the chapter and are a continuation of group theory. The
main result is the Fundamental Theorem of Finitely Generated Abelian Groups,
which is in Section 9. Section 11 forms the third part of the chapter. This section
is a gentle introduction to categories and functors, which are useful for working
with parallel structures in different settings within algebra. As S. Mac Lane says
in his book, “Category theory asks of every type of Mathematical object: ‘What
are the morphisms?’; it suggests that these morphisms should be described at the
same time as the objects. . . . This emphasis on (homo)morphisms is largely due to
Emmy Noether, who emphasized the use of homomorphisms of groups and rings.”
The simplest parallel structure reflected in categories is that of an isomorphism.
The section also discusses general notions of product and coproduct functors.
Examples of products are direct products in linear algebra and in group theory.
Examples of coproducts are direct sums in linear algebra and in abelian group
theory, as well as disjoint unions in set theory. The theory in this section helps in
unifying the mathematics that is to come in Chapters VI–VIII and X. The subject
of group theory in continued in Chapter VII, which assumes knowledge of the
material on category theory.
   Chapters V and VI continue the development of linear algebra. Chapter VI uses
categories, but Chapter V does not. Most of Chapter V concerns the analysis of a
linear transformation carrying a finite-dimensional vector space over a field into
itself. The questions are to find invariants of such transformations and to classify
the transformations up to similarity. Section 2 at the start extends the theory of
determinants so that the matrices are allowed to have entries in a commutative
ring with identity; this extension is necessary in order to be able to work easily
with characteristic polynomials. The extension of this theory is carried out by
an important principle known as the “permanence of identities.” Chapter VI
largely concerns bilinear forms and tensor products, again in the context that the
coefficients are from a field. This material is necessary in many applications to
geometry and physics, but it is not needed in Chapters VII–IX. Many objects in
the chapter are constructed in such a way that they are uniquely determined by
a universal mapping property. Problems 18–22 at the end of the chapter discuss
universal mapping properties in the general context of category theory, and they
show that a uniqueness theorem is automatic in all cases.
   Chapter VII continues the development of group theory, making use of category
theory. It is in two parts. Sections 1–3 concern free groups and the topic of
generators and relations; they are essential for abstract descriptions of groups
and for work in topology involving fundamental groups. Section 3 constructs a
notion of free product and shows that it is the coproduct functor for the category
of groups. Sections 4–6 continue the theme of the interplay of group theory and
xxiv                             Guide for the Reader

linear algebra. Section 4 analyzes group representations of a finite group when
the underlying field is the complex numbers, and Section 5 applies this theory
to obtain a conclusion about the structure of finite groups. Section 6 studies
extensions of groups and uses them to motivate the subject of cohomology of
groups.
    Chapter VIII introduces modules, giving many examples in Section 1, and
then goes on to discuss questions of unique factorization in integral domains.
Section 6 obtains a generalization for principal ideal domains of the Fundamental
Theorem of Finitely Generated Abelian Groups, once again illustrating the first
theme—similarities between the integers and certain polynomial rings. Section 7
introduces the third theme, the relationship between number theory and geometry,
as a more sophisticated version of the first theme. The section compares a certain
polynomial ring in two variables with a certain ring of algebraic integers that
extends the ordinary integers. Unique factorization of elements fails for both, but
the geometric setting has a more geometrically meaningful factorization in terms
of ideals that is evidently unique. This kind of unique factorization turns out to
work for the ring of algebraic integers as well. Sections 8–11 expand the examples
in Section 7 into a theory of unique factorization of ideals in any integrally closed
Noetherian domain whose nonzero prime ideals are all maximal.
    Chapter IX analyzes algebraic extensions of fields. The first 13 sections
make use only of Sections 1–6 in Chapter VIII. Sections 1–5 of Chapter IX
give the foundational theory, which is sufficient to exhibit all the finite fields and
to prove that certain classically proposed constructions in Euclidean geometry
are impossible. Sections 6–8 introduce Galois theory, but Theorem 9.28 and
its three corollaries may be skipped if Sections 14–17 are to be omitted. Sec-
tions 9–11 give a first round of applications of Galois theory: Gauss’s theorem
about which regular n-gons are in principle constructible with straightedge and
compass, the Fundamental Theorem of Algebra, and the Abel–Galois theorem
that solvability of a polynomial equation with rational coefficients in terms of
radicals implies solvability of the Galois group. Sections 12–13 give a second
round of applications: Gauss’s method in principle for actually constructing the
constructible regular n-gons and a converse to the Abel–Galois theorem. Sections
14–17 make use of Sections 7–11 of Chapter VIII, proving that π is transcendental
and obtaining two methods for computing Galois groups.
    Chapter X is a relatively short chapter developing further tools for dealing
with modules over a ring with identity. The main construction is that of the
tensor product over a ring of a unital right module and a unital left module, the
result being an abelian group. The chapter makes use of material from Chapters
VI and VIII, but not from Chapter IX.
Basic Algebra
                                       CHAPTER I

         Preliminaries about the Integers, Polynomials,
                         and Matrices



Abstract. This chapter is mostly a review, discussing unique factorization of positive integers,
unique factorization of polynomials whose coefficients are rational or real or complex, signs of
permutations, and matrix algebra.
    Sections 1–2 concern unique factorization of positive integers. Section 1 proves the division
and Euclidean algorithms, used to compute greatest common divisors. Section 2 establishes unique
factorization as a consequence and gives several number-theoretic consequences, including the
Chinese Remainder Theorem and the evaluation of the Euler ϕ function.
    Section 3 develops unique factorization of rational and real and complex polynomials in one inde-
terminate completely analogously, and it derives the complete factorization of complex polynomials
from the Fundamental Theorem of Algebra. The proof of the fundamental theorem is postponed to
Chapter IX.
    Section 4 discusses permutations of a finite set, establishing the decomposition of each permu-
tation as a disjoint product of cycles. The sign of a permutation is introduced, and it is proved that
the sign of a product is the product of the signs.
    Sections 5–6 concern matrix algebra. Section 5 reviews row reduction and its role in the solution
of simultaneous linear equations. Section 6 defines the arithmetic operations of addition, scalar
multiplication, and multiplication of matrices. The process of matrix inversion is related to the
method of row reduction, and it is shown that a square matrix with a one-sided inverse automatically
has a two-sided inverse that is computable via row reduction.



                        1. Division and Euclidean Algorithms

The first three sections give a careful proof of unique factorization for integers
and for polynomials with rational or real or complex coefficients, and they give
an indication of some first consequences of this factorization. For the moment
let us restrict attention to the set Z of integers. We take addition, subtraction,
and multiplication within Z as established, as well as the properties of the usual
ordering in Z.
   A factor of an integer n is a nonzero integer k such that n = kl for some
integer l. In this case we say also that k divides n, that k is a divisor of n, and
that n is a multiple of k. We write k | n for this relationship. If n is nonzero, any
product formula n = kl1 · · · lr is a factorization of n. A unit in Z is a divisor
                                                  1
2                     I. Preliminaries about the Integers, Polynomials, and Matrices

of 1, hence is either +1 or −1. The factorization n = kl of n 6= 0 is called
nontrivial if neither k nor l is a unit. An integer p > 1 is said to be prime if it
has no nontrivial factorization p = kl.
   The statement of unique factorization for positive integers, which will be given
precisely in Section 2, says roughly that each positive integer is the product of
primes and that this decomposition is unique apart from the order of the factors.1
Existence will follow by an easy induction. The difficulty is in the uniqueness. We
shall prove uniqueness by a sequence of steps based on the “Euclidean algorithm,”
which we discuss in a moment. In turn, the Euclidean algorithm relies on the
following.

   Proposition 1.1 (division algorithm). If a and b are integers with b 6= 0, then
there exist unique integers q and r such that a = bq + r and 0 ≤ r < |b|.
  PROOF. Possibly replacing q by −q, we may assume that b > 0. The integers
n with bn ≤ a are bounded above by |a|, and there exists such an n, namely
n = −|a|. Therefore there is a largest such integer, say n = q. Set r =
a − bq. Then 0 ≤ r and a = bq + r. If r ∏ b, then r − b ∏ 0 says that
a = b(q + 1) + (r − b) ∏ b(q + 1). The inequality q + 1 > q contradicts the
maximality of q, and we conclude that r < b. This proves existence.
  For uniqueness when b > 0, suppose a = bq1 + r1 = bq2 + r2 . Subtracting,
we obtain b(q1 − q2 ) = r2 − r1 with |r2 − r1 | < b, and this is a contradiction
unless r2 − r1 = 0.                                                           §

   Let a and b be integers not both 0. The greatest common divisor of a and
b is the largest integer d > 0 such that d | a and d | b. Let us see existence.
The integer 1 divides a and b. If b, for example, is nonzero, then any such d
has |d| ≤ |b|, and hence the greatest common divisor indeed exists. We write
d = GCD(a, b).
   Let us suppose that b 6= 0. The Euclidean algorithm consists of iterated ap-
plication of the division algorithm (Proposition 1.1) to a and b until the remainder
term r disappears:

                  a = bq1 + r1 ,               0 ≤ r1 < b,
                  b = r 1 q2 + r 2 ,           0 ≤ r2 < r1 ,
                 r 1 = r 2 q3 + r 3 ,          0 ≤ r3 < r2 ,
                     ..
                      .
               rn−2 = rn−1 qn + rn ,           0 ≤ rn < rn−1         (with rn 6= 0, say),
               rn−1 = rn qn+1 .
    1 It   is to be understood that the prime factorization of 1 is as the empty product.
                        1. Division and Euclidean Algorithms                     3

The process must stop with some remainder term rn+1 equal to 0 in this way since
b > r1 > r2 > · · · ∏ 0. The last nonzero remainder term, namely rn above, will
be of interest to us.

  EXAMPLE. For a = 13 and b = 5, the steps read

                               13 = 5 · 2 + 3,
                                5 = 3 · 1 + 2,
                                 3=2·1+ 1 ,
                                 2 = 1 · 2.

The last nonzero remainder term is written with a box around it.

  Proposition 1.2. Let a and b be integers with b 6= 0, and let d = GCD(a, b).
Then
   (a) the number rn in the Euclidean algorithm is exactly d,
   (b) any divisor d 0 of both a and b necessarily divides d,
   (c) there exist integers x and y such that ax + by = d.

  REMARK. Proposition 1.2c is sometimes called Bezout’s identity.
  EXAMPLE, CONTINUED. We rewrite the steps of the Euclidean algorithm, as
applied in the above example with a = 13 and b = 5, so as to yield successive
substitutions:

  13 = 5 · 2 + 3,       3 = 13 − 5 · 2,
   5 = 3 · 1 + 2,       2 = 5 − 3 · 1 = 5 − (13 − 5 · 2) · 1 = 5 · 3 − 13 · 1,
   3=2·1+ 1 ,           1 = 3 − 2 · 1 = (13 − 5 · 2) − (5 · 3 − 13 · 1) · 1
                          = 13 · 2 − 5 · 5.

Thus we see that 1 = 13x + 5y with x = 2 and y = −5. This shows for the
example that the number rn works in place of d in Proposition 1.2c, and the rest
of the proof of the proposition for this example is quite easy. Let us now adjust
this computation to obtain a complete proof of the proposition in general.
  PROOF OF PROPOSITION 1.2. Put r0 = b and r−1 = a, so that

                    rk−2 = rk−1 qk + rk        for 1 ≤ k ≤ n.                 (∗)

The argument proceeds in three steps.
4              I. Preliminaries about the Integers, Polynomials, and Matrices

    Step 1. We show that rn is a divisor of both a and b. In fact, from rn−1 =
rn qn+1 , we have rn | rn−1 . Let k ≤ n, and assume inductively that rn divides
rk−1 , . . . , rn−1 , rn . Then (∗) shows that rn divides rk−2 . Induction allows us to
conclude that rn divides r−1 , r0 , . . . , rn−1 . In particular, rn divides a and b.
    Step 2. We prove that ax + by = rn for suitable integers x and y. In fact,
we show by induction on k for k ≤ n that there exist integers x and y with
ax + by = rk . For k = −1 and k = 0, this conclusion is trivial. If k ∏ 1 is given
and if the result is known for k − 2 and k − 1, then we have
                                  ax2 + by2 = rk−2 ,
                                                                                  (∗∗)
                                  ax1 + by1 = rk−1

for suitable integers x2 , y2 , x1 , y1 . We multiply the second of the equalities of
(∗∗) by qk , subtract, and substitute into (∗). The result is

              rk = rk−2 − rk−1 qk = a(x2 − qk x1 ) + b(y2 − qk y1 ),

and the induction is complete. Thus ax + by = rn for suitable x and y.
    Step 3. Finally we deduce (a), (b), and (c). Step 1 shows that rn divides a and
b. If d 0 > 0 divides both a and b, the result of Step 2 shows that d 0 | rn . Thus
d 0 ≤ rn , and rn is the greatest common divisor. This is the conclusion of (a); (b)
follows from (a) since d 0 | rn , and (c) follows from (a) and Step 2.            §

  Corollary 1.3. Within Z, if c is a nonzero integer that divides a product mn
and if GCD(c, m) = 1, then c divides n.
   PROOF. Proposition 1.2c produces integers x and y with cx + my = 1.
Multiplying by n, we obtain cnx + mny = n. Since c divides mn and divides
itself, c divides both terms on the left side. Therefore it divides the right side,
which is n.                                                                      §

  Corollary 1.4. Within Z, if a and b are nonzero integers with GCD(a, b) = 1
and if both of them divide the integer m, then ab divides m.
   PROOF. Proposition 1.2c produces integers x and y with ax + by = 1.
Multiplying by m, we obtain amx + bmy = m, which we rewrite in integers
as ab(m/b)x + ab(m/a)y = m. Since ab divides each term on the left side, it
divides the right side, which is m.                                     §


                      2. Unique Factorization of Integers

We come now to the theorem asserting unique factorization for the integers. The
precise statement is as follows.
                            2. Unique Factorization of Integers                        5

   Theorem 1.5 (Fundamental Theorem of Arithmetic). Each positive integer
n can be written as a product of primes, n = p1 p2 · · · pr , with the integer 1
being written as an empty product. This factorization is unique in the following
sense: if n = q1 q2 · · · qs is another such factorization, then r = s and, after some
reordering of the factors, q j = p j for 1 ≤ j ≤ r.

   The main step is the following lemma, which relies on Corollary 1.3.

   Lemma 1.6. Within Z, if p is a prime and p divides a product ab, then p
divides a or p divides b.
   REMARK. Lemma 1.6 is sometimes known as Euclid’s Lemma.
  PROOF. Suppose that p does not divide a. Since p is prime, GCD(a, p) = 1.
Taking m = a, n = b, and c = p in Corollary 1.3, we see that p divides b. §

   PROOF OF EXISTENCE IN THEOREM 1.5. We induct on n, the case n = 1 being
handled by an empty product expansion. If the result holds for k = 1 through
k = n − 1, there are two cases: n is prime and n is not prime. If n is prime, then
n = n is the desired factorization. Otherwise we can write n = ab nontrivially
with a > 1 and b > 1. Then a ≤ n − 1 and b ≤ n − 1, so that a and b have
factorizations into primes by the inductive hypothesis. Putting them together
yields a factorization into primes for n = ab.                                  §
   PROOF OF UNIQUENESS IN THEOREM 1.5. Suppose that n = p1 p2 · · · pr =
q1 q2 · · · qs with all factors prime and with r ≤ s. We prove the uniqueness by
induction on r, the case r = 0 being trivial and the case r = 1 following from
the definition of “prime.” Inductively from Lemma 1.6 we have pr | qk for some
k. Since qk is prime, pr = qk . Thus we can cancel and obtain p1 p2 · · · pr−1 =
q1 q2 · · · qbk · · · qs , the hat indicating an omitted factor. By induction the factors
on the two sides here are the same except for order. Thus the same conclusion
is valid when comparing the two sides of the equality p1 p2 · · · pr = q1 q2 · · · qs .
The induction is complete, and the desired uniqueness follows.                         §

    In the product expansion of Theorem 1.5, it is customary to group factors that
are equal, thus writing the positive integer n as n = p1k1 · · · prkr with the primes
p j distinct and with the integers k j all ∏ 0. This kind of decomposition is unique
                              k
up to order if all factors p j j with k j = 0 are dropped, and we call it a prime
factorization of n.

   Corollary 1.7. If n = p1k1 · · · prkr is a prime factorization of a positive integer
n, then the positive divisors d of n are exactly all products d = p1l1 · · · prlr with
0 ≤ l j ≤ k j for all j.
6               I. Preliminaries about the Integers, Polynomials, and Matrices

  REMARK. A general divisor of n within Z is the product of a unit ±1 and a
positive divisor.
   PROOF. Certainly any such product divides n. Conversely if d divides n, write
n = dx for some positive integer x. Apply Theorem 1.5 to d and to x, form the
resulting prime factorizations, and multiply them together. Then we see from the
uniqueness for the prime factorization of n that the only primes that can occur in
the expansions of d and x are p1 , . . . , pr and that the sum of the exponents of p j
in the expansions of d and x is k j . The result follows.                          §

   If we want to compare prime factorizations for two positive integers, we can
insert 0th powers of primes as necessary and thereby assume that the same primes
appear in both expansions. Using this device, we obtain a formula for greatest
common divisors.

   Corollary 1.8. If two positive integers a and b have expansions as products
of powers of r distinct primes given by a = p1k1 · · · prkr and b = p1l1 · · · prlr , then

                        GCD(a, b) = p1min(k1 ,l1 ) · · · prmin(kr ,lr ) .

   PROOF. Let d 0 be the right side of the displayed equation. It is plain that d 0
is positive and that d 0 divides a and b. On the other hand, two applications of
Corollary 1.7 show that the greatest common divisor of a and b is a number d
of the form p1m 1 · · · prmr with the property that m j ≤ k j and m j ≤ l j for all j.
Therefore m j ≤ min(k j , l j ) for all j, and d ≤ d 0 . Since any positive divisor of
both a and b is ≤ d, we have d 0 ≤ d. Thus d 0 = d.                                 §

    In special cases Corollary 1.8 provides a useful way to compute GCD(a, b),
but the Euclidean algorithm is usually a more efficient procedure. Nevertheless,
Corollary 1.8 remains a handy tool for theoretical purposes. Here is an example:
Two nonzero integers a and b are said to be relatively prime if GCD(a, b) = 1.
It is immediate from Corollary 1.8 that two nonzero integers a and b are relatively
prime if and only if there is no prime p that divides both a and b.

   Corollary 1.9 (Chinese Remainder Theorem). Let a and b be positive rela-
tively prime integers. To each pair (r, s) of integers with 0 ≤ r < a and 0 ≤ s < b
corresponds a unique integer n such that 0 ≤ n < ab, a divides n − r, and b
divides n − s. Moreover, every integer n with 0 ≤ n < ab arises from some such
pair (r, s).
   REMARK. In notation for congruences that we introduce formally in Chapter IV,
the result says that if GCD(a, b) = 1, then the congruences n ≡ r mod a and
n ≡ s mod b have one and only one simultaneous solution n with 0 ≤ n < ab.
                           2. Unique Factorization of Integers                     7

   PROOF. Let us see that n exists as asserted. Since a and b are relatively
prime, Proposition 1.2c produces integers x 0 and y 0 such that ax 0 − by 0 = 1.
Multiplying by s − r, we obtain ax − by = s − r for suitable integers x and y.
Put t = ax + r = by + s, and write by the division algorithm (Proposition 1.1)
t = abq + n for some integer q and for some integer n with 0 ≤ n < ab. Then
n − r = t − abq − r = ax − abq is divisible by a, and similarly n − s is divisible
by b.
   Suppose that n and n 0 both have the asserted properties. Then a divides
n − n 0 = (n − r) − (n 0 − r), and b divides n − n 0 = (n − s) − (n 0 − s). Since
a and b are relatively prime, Corollary 1.4 shows that ab divides n − n 0 . But
|n − n 0 | < ab, and the only integer N with |N | < ab that is divisible by ab is
N = 0. Thus n − n 0 = 0 and n = n 0 . This proves uniqueness.
   Finally the argument just given defines a one-one function from a set of ab
pairs (r, s) to a set of ab elements n. Its image must therefore be all such integers
n. This proves the corollary.                                                      §

  If n is a positive integer, we define ϕ(n) to be the number of integers k with
0 ≤ k < n such that k and n are relatively prime. The function ϕ is called the
Euler ϕ function.

   Corollary 1.10. Let N > 1 be an integer, and let N = p1k1 · · · prkr be a prime
factorization of N . Then
                                      r
                                      Y       k −1
                            ϕ(N ) =         pj j     ( p j − 1).
                                      j=1

   REMARK. The conclusion is valid also for N = 1 if we interpret the right side
of the formula to be the empty product.
   PROOF. For positive integers a and b, let us check that

                   ϕ(ab) = ϕ(a)ϕ(b)                if GCD(a, b) = 1.             (∗)

In view of Corollary 1.9, it is enough to prove that the mapping (r, s) 7→ n given
in that corollary has the property that GCD(r, a) = GCD(s, b) = 1 if and only if
GCD(n, ab) = 1.
   To see this property, suppose that n satisfies 0 ≤ n < ab and GCD(n, ab) > 1.
Choose a prime p dividing both n and ab. By Lemma 1.6, p divides a or p divides
b. By symmetry we may assume that p divides a. If (r, s) is the pair corresponding
to n under Corollary 1.9, then the corollary says that a divides n − r. Since p
divides a, p divides n − r. Since p divides n, p divides r. Thus GCD(r, a) > 1.
   Conversely suppose that (r, s) is a pair with 0 ≤ r < a and 0 ≤ s < b such
that GCD(r, a) = GCD(s, b) = 1 is false. Without loss of generality, we may
8                I. Preliminaries about the Integers, Polynomials, and Matrices

assume that GCD(r, a) > 1. Choose a prime p dividing both r and a. If n is the
integer with 0 ≤ n < ab that corresponds to (r, s) under Corollary 1.9, then the
corollary says that a divides n − r. Since p divides a, p divides n − r. Since p
divides r, p divides n. Thus GCD(n, ab) > 1. This completes the proof of (∗).
   For a power pk of a prime p with k > 0, the integers n with 0 ≤ n < pk
such that GCD(n, pk ) > 1 are the multiples of p, namely 0, p, 2 p, . . . , pk − p.
There are pk−1 of them. Thus the number of integers n with 0 ≤ n < pk such
that GCD(n, pk ) = 1 is pk − pk−1 = pk−1 ( p − 1). In other words,

                 ϕ( pk ) = pk−1 ( p − 1)                 if p is prime and k ∏ 1.         (∗∗)

To prove the corollary, we induct on r, the case r = 1 being handled by (∗∗). If
the formula of the corollary is valid for r − 1, then (∗) allows us to combine that
result with the formula for ϕ( pkr ) given in (∗∗) to obtain the formula for ϕ(N ).
                                                                                 §

   We conclude this section by extending the notion of greatest common divisor to
apply to more than two integers. If a1 , . . . , at are integers not all 0, their greatest
common divisor is the largest integer d > 0 that divides all of a1 , . . . , at . This
exists, and we write d = GCD(a1 , . . . , at ) for it. It is immediate that d equals the
greatest common divisor of the nonzero members of the set {a1 , . . . , at }. Thus,
in deriving properties of greatest common divisors, we may assume that all the
integers are nonzero.

  Corollary 1.11. Let a1 , . . . , at be positive integers, and let d be their greatest
common divisor. Then
                                                 k        k
   (a) if for each j with 1 ≤ j ≤ t, a j = p11, j · · · pr r, j is an expansion of a j as
       a product of powers of r distinct primes p1 , . . . , pr , it follows that
                                   min1≤ j≤t {k1, j }          min1≤ j≤t {kr, j }
                           d = p1                       · · · pr                    ,

    (b) any divisor° d 0 of all of a1 , . . . , at necessarily
                                                   ¢           divides d,
    (c) d = GCD GCD(a1 , . . . , at−1 ), at if t > 1,
    (d) there exist integers x1 , . . . , xt such that a1 x1 + · · · + at xt = d.
   PROOF. Part (a) is proved in the same way as Corollary 1.8 except that Corollary
1.7 is to be applied r times rather than just twice. Further application of Corollary
1.7 shows that any positive divisor d 0 of a1 , . . . , at is of the form d 0 = p1m 1 · · · prmr
with m 1 ≤ k1, j for all j, . . . , and with m r ≤ kr, j for all j. Therefore m 1 ≤
min1≤ j≤r {k1, j }, . . . , and m r ≤ min1≤ j≤r {kr, j }, and it follows that d 0 divides
d. This proves (b). Conclusion (c) follows by using the formula in (a), and (d)
follows by combining (c), Proposition 1.2c, and induction.                                    §
                               3. Unique Factorization of Polynomials                                    9

                        3. Unique Factorization of Polynomials

This section establishes unique factorization for ordinary rational, real, and com-
plex polynomials. We write Q for the set of rational numbers, R for the set of
real numbers, and C for the set of complex numbers, each with its arithmetic
operations. The rational numbers are constructed from the integers by a process
reviewed in Section A3 of the appendix, the real numbers are defined from the
rational numbers by a process reviewed in that same section, and the complex
numbers are defined from the real numbers by a process reviewed in Section A4
of the appendix. Sections A3 and A4 of the appendix mention special properties
of R and C beyond those of the arithmetic operations, but we shall not make
serious use of these special properties here until nearly the end of the section—
after unique factorization of polynomials has been established. Let F denote any
of Q, R, or C. The members of F are called scalars.
    We work with ordinary polynomials with coefficients in F. Informally these
are expressions P(X) = an X n +· · ·+a1 X +a0 with an , . . . , a1 , a0 in F. Although
it is tempting to think of P(X) as a function with independent variable X, it is
better to identify P with the sequence (a0 , a1 , . . . , an , 0, 0, . . . ) of coefficients,
using expressions P(X) = an X n + · · · + a1 X + a0 only for conciseness and for
motivation of the definitions of various operations.
    The precise definition therefore is that a polynomial in one indeterminate
with coefficients in F is an infinite sequence of members of F such that all terms
of the sequence are 0 from some point on. The indexing of the sequence is to begin
with 0. We may refer to a polynomial P as P(X) if we want to emphasize that
the indeterminate is called X. Addition, subtraction, and scalar multiplication
are defined in coordinate-by-coordinate fashion:
 (a0 , a1 , . . . , an , 0, 0, . . . ) + (b0 ,b1 , . . . , bn , 0, 0, . . . )
                                               = (a0 + b0 , a1 + b1 , . . . , an + bn , 0, 0, . . . ),
 (a0 , a1 , . . . , an , 0, 0, . . . ) − (b0 ,b1 , . . . , bn , 0, 0, . . . )
                                               = (a0 − b0 , a1 − b1 , . . . , an − bn , 0, 0, . . . ),
        c(a0 , a1 , . . . , an , 0, 0, . . . ) = (ca0 , ca1 , . . . , can , 0, 0, . . . ).
Polynomial multiplication is defined so as to match multiplication of expressions
an X n + · · · + a1 X + a0 if the product is expanded out, powers of X are added,
and then terms containing like powers of X are collected:
      (a0 , a1 , . . . , 0, 0, . . . )(b0 , b1 , . . . , 0, 0, . . . ) = (c0 , c1 , . . . , 0, 0, . . . ),
                   PN
where c N =            k=0 ak b N −k . We take it as known that the usual associative,
commutative, and distributive laws are then valid. The set of all polynomials in
the indeterminate X is denoted by F[X].
10             I. Preliminaries about the Integers, Polynomials, and Matrices

   The polynomial with all entries 0 is denoted by 0 and is called the zero
polynomial. For all polynomials P = (a0 , . . . , an , 0, . . . ) other than 0, the
degree of P, denoted by deg P, is defined to be the largest index n such that
an 6= 0. The constant polynomials are by definition the zero polynomial and the
polynomials of degree 0. If P and Q are nonzero polynomials, then

          P+Q=0              or    deg(P + Q) ≤ max(deg P, deg Q),
                                deg(c P) = deg P,
                            deg(P Q) = deg P + deg Q.

In the formula for deg(P + Q), equality holds if deg P 6= deg Q. Implicit in the
formula for deg(P Q) is the fact that P Q cannot be 0 unless P = 0 or Q = 0. A
cancellation law for multiplication is an immediate consequence:

                P R = Q R with R 6= 0              implies        P = Q.

In fact, P R = Q R implies (P − Q)R = 0; since R 6= 0, P − Q must be 0.
   If P = (a0 , . . . , an , 0, . . . ) is a polynomial and r is in F, we can evaluate P
at r, obtaining as a result the number P(r) = an r n + · · · + a1r + a0 . Taking into
account all values of r, we obtain a mapping P 7→ P( · ) of F[X] into the set of
functions from F into F. Because of the way that the arithmetic operations on
polynomials have been defined, we have

                            (P + Q)(r) = P(r) + Q(r),
                            (P − Q)(r) = P(r) − Q(r),
                               (c P)(r) = c P(r),
                              (P Q)(r) = P(r)Q(r).

In other words, the mapping P 7→ P( · ) respects the arithmetic operations. We
say that r is a root of P if P(r) = 0.
   Now we turn to the question of unique factorization. The definitions and the
proof are completely analogous to those for the integers. A factor of a polynomial
A is a nonzero polynomial B such that A = B Q for some polynomial Q. In
this case we say also that B divides A, that B is a divisor of A, and that A is a
multiple of B. We write B | A for this relationship. If A is nonzero, any product
formula A = B Q 1 · · · Q r is a factorization of A. A unit in F[X] is a divisor of 1,
hence is any polynomial of degree 0; such a polynomial is a constant polynomial
A(X) = c with c equal to a nonzero scalar. The factorization A = B Q of
A 6= 0 is called nontrivial if neither B nor Q is a unit. A prime P in F[X] is a
nonzero polynomial that is not a unit and has no nontrivial factorization P = B Q.
Observe that the product of a prime and a unit is always a prime.
                        3. Unique Factorization of Polynomials                   11

  Proposition 1.12 (division algorithm). If A and B are polynomials in F[X]
and if B not the 0 polynomial, then there exist unique polynomials Q and R in
F[X] such that
   (a) A = B Q + R and
   (b) either R is the 0 polynomial or deg R < deg B.

   REMARK. This result codifies the usual method of dividing polynomials in
high-school algebra. That method writes A/B = Q + R/B, and then one obtains
the above result by multiplying by B. The polynomial Q is the quotient in the
division, and R is the remainder.
   PROOF OF UNIQUENESS. If A = B Q + R = B Q 1 + R1 , then B(Q − Q 1 ) =
R1 − R. Without loss of generality, R1 − R is not the 0 polynomial since otherwise
Q − Q 1 = 0 also. Then

   deg B + deg(Q − Q 1 ) = deg(R1 − R) ≤ max(deg R, deg R1 ) < deg B,

and we have a contradiction.                                                     §
   PROOF OF EXISTENCE. If A = 0 or deg A < deg B, we take Q = 0 and
R = A, and we are done. Otherwise we induct on deg A. Assume the result
for degree ≤ n − 1, and let deg A = n. Write A = an X n + A1 with A1 = 0
or deg A1 < deg A. Let B = bk X k + B1 with B1 = 0 or deg B1 < deg B. Put
Q 1 = an bk−1 X n−k . Then

   A − B Q 1 = an X n + A1 − an X n − an bk−1 X n−k B1 = A1 − an bk−1 X n−k B1

with the right side equal to 0 or of degree < deg A. Then the right side, by
induction, is of the form B Q 2 + R, and A = B(Q 1 + Q 2 ) + R is the required
decomposition.                                                              §

  Corollary 1.13 (Factor Theorem). If r is in F and if P is a polynomial in
F[X], then X − r divides P if and only if P(r) = 0.
   PROOF. If P = (X − r)Q, then P(r) = (r − r)Q(r) = 0. Conversely let
P(r) = 0. Taking B(X) = X − r in the division algorithm (Proposition 1.12),
we obtain P = (X − r)Q + R with R = 0 or deg R < deg(X − r) = 1.
Thus R is a constant polynomial, possibly 0. In any case we have 0 = P(r) =
(r − r)Q(r) + R(r), and thus R(r) = 0. Since R is constant, we must have
R = 0, and then P = (X − r)Q.                                             §

  Corollary 1.14. If P is a nonzero polynomial with coefficients in F and if
deg P = n, then P has at most n distinct roots.
12                I. Preliminaries about the Integers, Polynomials, and Matrices

   REMARKS. Since there are infinitely many scalars in any of Q and R and
C, the corollary implies that the function from F to F associated to P, namely
r 7→ P(r), cannot be identically 0 if P 6= 0. Starting in Chapter IV, we shall
allow other F’s besides Q and R and C, and then this implication can fail. For
example, when F is the two-element “field” F = {0, 1} with 1 + 1 = 0 and with
otherwise the expected addition and multiplication, then P(X) = X 2 + X is not
the zero polynomial but P(r) = 0 for r = 0 and r = 1. It is thus important to
distinguish polynomials in one indeterminate from their associated functions of
one variable.

   PROOF. Let r1 , . . . , rn+1 be distinct roots of P(X). By the Factor Theorem
(Corollary 1.13), X − r1 is a factor of P(X). We prove inductively on k that
the product (X − r1 )(X − r2 ) · · · (X − rk ) is a factor of P(X). Assume that this
assertion holds for k, so that P(X) = (X − r1 ) · · · (X − rk )Q(X) and

                  0 = P(rk+1 ) = (rk+1 − r1 ) · · · (rk+1 − rk )Q(rk+1 ).

Since the r j ’s are distinct, we must have Q(rk+1 ) = 0. By the Factor Theorem,
we can write Q(X) = (X − rk+1 )R(X) for some polynomial R(X). Substitution
gives P(X) = (X − r1 ) · · · (X − rk )(X − rk+1 )R(X), and (X − r1 ) · · · (X − rk+1 )
is exhibited as a factor of P(X). This completes the induction. Consequently

                          P(X) = (X − r1 ) · · · (X − rn+1 )S(X)

for some polynomial S(X). Comparing the degrees of the two sides, we find that
deg S = −1, and we have a contradiction.                                    §


   We can use the division algorithm in the same way as with the integers in
Sections 1–2 to obtain unique factorization. Within the set of integers, we defined
greatest common divisors so as to be positive, but their negatives would have
worked equally well. That flexibility persists with polynomials; the essential
feature of any greatest common divisor of polynomials is shared by any product
of that polynomial by a unit. A greatest common divisor of polynomials A and
B with B 6= 0 is any polynomial D of maximum degree such that D divides A
and D divides B. We shall see that D is indeed unique up to multiplication by a
nonzero scalar.2

   2 For some purposes it is helpful to isolate one particular greatest common divisor by taking the

coefficient of the highest power of X to be 1.
                         3. Unique Factorization of Polynomials                 13

   The Euclidean algorithm is the iterative process that makes use of the division
algorithm in the form

             A=     B Q 1 + R1 ,          R1 = 0 or deg R1 < deg B,
             B=     R1 Q 2 + R2 ,         R2 = 0 or deg R2 < deg R1 ,
            R1 =    R2 Q 3 + R3 ,         R3 = 0 or deg R3 < deg R2 ,
               ..
                .
          Rn−2 =    Rn−1 Q n + Rn ,       Rn = 0 or deg Rn < deg Rn−1 ,
          Rn−1 =    Rn Q n+1 .
In the above computation the integer n is defined by the conditions that Rn 6= 0
and that Rn+1 = 0. Such an n must exist since deg B > deg R1 > · · · ∏ 0. We
can now obtain an analog for F[X] of the result for Z given as Proposition 1.2.

   Proposition 1.15. Let A and B be polynomials in F[X] with B 6= 0, and let
R1 , . . . , Rn be the remainders generated by the Euclidean algorithm when applied
to A and B. Then
     (a) Rn is a greatest common divisor of A and B,
    (b) any D1 that divides both A and B necessarily divides Rn ,
     (c) the greatest common divisor of A and B is unique up to multiplication
            by a nonzero scalar,
    (d) any greatest common divisor D has the property that there exist polyno-
            mials P and Q with A P + B Q = D.
   PROOF. Conclusions (a) and (b) are proved in the same way that parts (a) and
(b) of Proposition 1.2 are proved, and conclusion (d) is proved with D = Rn in
the same way that Proposition 1.2c is proved.
   If D is a greatest common divisor of A and B, it follows from (a) and (b) that
D divides Rn and that deg D = deg Rn . This proves (c).                        §

   Using Proposition 1.15, we can prove analogs for F[X] of the two corollaries
of Proposition 1.2. But let us instead skip directly to what is needed to obtain an
analog for F[X] of unique factorization as in Theorem 1.5.

   Lemma 1.16. If A and B are nonzero polynomials with coefficients in F and
if P is a prime polynomial such that P divides AB, then P divides A or P divides
B.
   PROOF. If P does not divide A, then 1 is a greatest common divisor of A and
P, and Proposition 1.15d produces polynomials S and T such that AS + P T = 1.
Multiplication by B gives AB S + P T B = B. Then P divides AB S because it
divides AB, and P divides P T B because it divides P. Hence P divides B. §
14             I. Preliminaries about the Integers, Polynomials, and Matrices

   Theorem 1.17 (unique factorization). Every member of F[X] of degree ∏ 1 is a
product of primes. This factorization is unique up to order and up to multiplication
of each prime factor by a unit, i.e., by a nonzero scalar.
   PROOF. The existence follows in the same way as the existence in Theorem
1.5; induction on the integers is to be replaced by induction on the degree. The
uniqueness follows from Lemma 1.16 in the same way that the uniqueness in
Theorem 1.5 follows from Lemma 1.6.                                           §

   We turn to a consideration of properties of polynomials that take into account
special features of R and C. If F is R, then X 2 + 1 is prime. The reason is that
a nontrivial factorization of X 2 + 1 would have to involve two first-degree real
polynomials and then r 2 +1 would have to be 0 for some real r, namely for r equal
to the root of either of the first-degree polynomials. On the other hand, X 2 + 1
is not prime when F = C since X 2 + 1 = (X + i)(X − i). The Fundamental
Theorem of Algebra, stated below, implies that every prime polynomial over C is
of degree 1. It is possible to prove the Fundamental Theorem of Algebra within
complex analysis as a consequence of Liouville’s Theorem or within real analysis
as a consequence of the Heine–Borel Theorem and other facts about compactness.
This text gives a proof of the Fundamental Theorem of Algebra in Chapter IX
using modern algebra, specifically Sylow theory as in Chapter IV and Galois
theory as in Chapter IX. One further fact is needed; this fact uses elementary
calculus and is proved below as Proposition 1.20.

  Theorem 1.18 (Fundamental Theorem of Algebra). Any polynomial in C[X]
with degree ∏ 1 has at least one root.

   Corollary 1.19. Let P be a nonzero polynomial of degree n in C[X],
and let r1 , . . . , rk be the distinct roots. Then there exist unique integers m j > 0
                                                                Q
for 1 ≤ j ≤ k such that P(X) is a scalar multiple of kj=1 (X − r j )m j . The
                        P
numbers m j have kj=1 m j = n.
    PROOF. We may assume that deg P > 0. We apply unique factorization
(Theorem 1.17) to P(X). It follows from the Fundamental Theorem of Algebra
(Theorem 1.18) and the Factor Theorem (Corollary 1.13) that each prime polyno-
                          C has degree 1. Thus the unique factorization of P(X)
mial with coefficients in Q
                           n
has to be of the form c l=1   (X − zl ) for some c 6= 0 and for some complex
numbers zl that are unique up to order. The zl ’s are roots, and every root is a zl by
the Factor Theorem. Grouping like factors proves the desired factorization and
                                       P
its uniqueness. The numbers m j have kj=1 m j = n by a count of degrees. §

  The integers m j in the corollary are called the multiplicities of the roots of the
polynomial P(X).
                              4. Permutations and Their Signs                            15

   We conclude this section by proving the result from calculus that will enter
the proof of the Fundamental Theorem of Algebra in Chapter IX.

   Proposition 1.20. Any polynomial in R[X] with odd degree has at least one
root.
   PROOF. Without loss of generality, we may take the leading coefficient to
be 1. Thus let the polynomial be P(X) = X 2n+1 + a2n X 2n + · · · + a1 X + a0 =
X 2n+1 + R(X). Since limx→±∞ P(x)/x 2n+1 = 1, there is some positive r0 such
that P(−r0 ) < 0 and P(r0 ) > 0. By the Intermediate Value Theorem, given in
Section A3 of the appendix, P(r) = 0 for some r with −r0 ≤ r ≤ r0 .           §


                         4. Permutations and Their Signs

Let S be a finite nonempty set of n elements. A permutation of S is a one-one
function from S onto S. The elements might be listed as a1 , a2 , . . . , an , but it
will simplify the notation to view them simply as 1, 2, . . . , n. We use ordinary
function notation for describing the effect of permutations. Thus the value of a
permutation σ at j is σ ( j), and the composition of τ followed by σ is σ ◦ τ or
simply σ τ , with (σ τ )( j) = σ (τ ( j)). Composition is automatically associative,
i.e., (ρσ )τ = ρ(σ τ ), because the effect of both sides on j, when we expand
things out, is ρ(σ (τ ( j))). The composition of two permutations is also called
their product.
    The identity permutation will be denoted by 1. Any permutation σ , being
a one-one onto function, has a well-defined inverse permutation σ −1 with the
property that σ σ −1 = σ −1 σ = 1. One way of describing concisely the effect
of a permutation is to list its domain   µ values∂and to put the corresponding range
                                            12345
values beneath them. Thus σ =                          is the permutation of {1, 2, 3, 4, 5}
                                            43512
with σ (1) = 4, σ (2) = 3, σ (3) = 5, σ (4) = 1, and σ (5) =µ2. The inverse            ∂
                                                                              43512
permutation is obtained by interchanging the two rows to obtain                          and
                                                                              12345
then adjusting
         µ        the∂entries in the rows so that the first row is in the usual order:
  −1       12345
σ =                    .
           45213
    If 2 ≤ k ≤ n, a k-cycle is a permutation σ that fixes each element in some
subset of n − k elements and moves the remaining elements c1 , . . . , ck according
to σ (c1 ) = c2 , σ (c2 ) = c3 , . . . , σ (ck−1 ) = ck , σ (ck ) = c1 . Such a cycle may be
denoted by (c1 c2 · · · ck−1 ck ) to stress its structure. For exampleµtake n = ∂         5;
                                                                                  12345
then σ = (2 3 5) is the 3-cycle given in our earlier notation by                           .
                                                                                  13542
16              I. Preliminaries about the Integers, Polynomials, and Matrices

The cycle (2 3 5) is the same as the cycle (3 5 2) and the cycle (5 2 3). It is
sometimes helpful to speak of the identity permutation 1 as the unique 1-cycle.
   A system of cycles is said to be disjoint if the sets that each of them moves
are disjoint in pairs. Thus (2 3 5) and (1 4) are disjoint, but (2 3 5) and (1 3)
are not. Any two disjoint cycles σ and τ commute in the sense that σ τ = τ σ .

   Proposition 1.21. Any permutation σ of {1, 2, . . . , n} is a product of disjoint
cycles. The individual cycles in the decomposition are unique in the sense of
being determined by σ .
             µ          ∂
               12345
   EXAMPLE.               = (2 3 5)(1 4).
               43512
    PROOF. Let us prove existence. Working with {1, 2, . . . , n}, we show that any
σ is the disjoint product of cycles in such a way that no cycle moves an element
 j unless σ moves j. We do so for all σ simultaneously by induction downward
on the number of elements fixed by σ . The starting case of the induction is that
σ fixes all n elements. Then σ is the identity, and we are regarding the identity
as a 1-cycle.
    For the inductive step suppose σ fixes the elements in a subset T of r el-
ements of {1, 2, . . . , n} with r < n. Let j be an element not in T , so that
σ ( j) 6= j. Choose k as small as possible so that some element is repeated
among j, σ ( j), σ 2 ( j), . . . , σ k ( j). This condition means that σ l ( j) = σ k ( j) for
some l with 0 ≤ l < k. Then σ k−l ( j) = j, and we obtain a contradiction to
the minimality of k unless k − l = k, i.e., l = 0. In other words, we have
σ k ( j) = j. We may thus form the k-cycle ∞ = ( j σ ( j) σ 2 ( j) σ k−1 ( j)). The
permutation ∞ −1 σ then fixes the r + k elements of T ∪ U , where U is the set of
elements j, σ ( j), σ 2 ( j), . . . , σ k−1 ( j). By the inductive hypothesis, ∞ −1 σ is the
product τ1 · · · τ p of disjoint cycles that move only elements not in T ∪ U . Since
∞ moves only the elements in U , ∞ is disjoint from each of τ1 , . . . , τ p . Therefore
σ = ∞ τ1 · · · τ p provides the required decomposition of σ .
    For uniqueness we observe from the proof of existence that each element
 j generates a k-cycle C j for some k ∏ 1 depending on j. If we have two
decompositions as in the proposition, then the cycle within each decomposition
that contains j must be C j . Hence the cycles in the two decompositions must
match.                                                                                      §

  A 2-cycle is often called a transposition. The proposition allows us to see
quickly that any permutation is a product of transpositions.

   Corollary 1.22. Any k-cycle σ permuting {1, 2, . . . , n} is a product of k − 1
transpositions if k > 1. Therefore any permutation σ of {1, 2, . . . , n} is a product
of transpositions.
                             4. Permutations and Their Signs                          17

   PROOF. For the first statement, we observe that (c1 c2 · · · ck−1 ck ) =
(c1 ck )(c1 ck−1 ) · · · (c1 c3 )(c1 c2 ). The second statement follows by combining
this fact with Proposition 1.21.                                                  §

  Our final tasks for this section are to attach a sign to each permutation and to
examine the properties of these signs. We begin with the special case that our
underlying set S is {1, . . . , n}. If σ is a permutation of {1, . . . , n}, consider the
numerical products
            Y                                        Y
                  |σ (k) − σ ( j)|        and              (σ (k) − σ ( j)).
           1≤ j<k≤n                                 1≤ j<k≤n

If (r, s) is any pair of integers with 1 ≤ r < s ≤ n, then the expression s − r
appears once and only once as a factor inQthe first product. Therefore the first
product is independent of σ and equals 1≤ j<k≤n (k − j). Meanwhile, each
factor of the second product is ±1 times the corresponding factor of the first
product. Therefore we have
                   Y                                 Y
                         (σ (k) − σ ( j)) = (sgn σ )     (k − j),
                1≤ j<k≤n                              1≤ j<k≤n

where sgn σ is +1 or −1, depending on σ . This sign is called the sign of the
permutation σ .

                                      ° {1, . . . ,¢ n}, let (a b) be a transposition,
  Lemma 1.23. Let σ be a permutation of
and form the product σ (a b). Then sgn σ (a b) = − sgn σ .
   PROOF. For the pairs ( j, k) with j < k, we are to compare σ (k) − σ ( j) with
σ (a b)(k) − σ (a b)( j). There are five cases. Without loss of generality, we
may assume that a < b.
   Case 1. If neither j nor k equals a or b, then σ (a b)(k) − σ (a b)( j) =
σ (k) − σ ( j). Thus such pairs ( j, k) make the same contribution to the product
for σ (a b) as to the product for σ , and they can be ignored.
   Case 2. If one of j and k equals one of a and b while the other does not, there
are three situations of interest. For each we compare the contributions of two such
pairs together. The first situation is that of pairs (a, t) and (t, b) with a < t < b.
These together contribute the factors (σ (t) − σ (a)) and (σ (b) − σ (t)) to the
product for σ , and they contribute the factors (σ (t) − σ (b)) and (σ (a) − σ (t))
to the product for σ (a b). Since

          (σ (t) − σ (a))(σ (b) − σ (t)) = (σ (t) − σ (b))(σ (a) − σ (t)),

the pairs together make the same contribution to the product for σ (a b) as to the
product for σ , and they can be ignored.
18             I. Preliminaries about the Integers, Polynomials, and Matrices

   Case 3. Continuing with matters as in Case 2, we next consider pairs (a, t) and
(b, t) with a < b < t. These together contribute the factors (σ (t) − σ (a)) and
(σ (t) − σ (b)) to the product for σ , and they contribute the factors (σ (t) − σ (b))
and (σ (t) − σ (a)) to the product for σ (a b). Since

          (σ (t) − σ (a))(σ (t) − σ (b)) = (σ (t) − σ (b))(σ (t) − σ (a)),

the pairs together make the same contribution to the product for σ (a b) as to the
product for σ , and they can be ignored.
   Case 4. Still with matters as in Case 2, we consider pairs (t, a) and (t, b) with
t < a < b. Arguing as in Case 3, we are led to an equality

          (σ (a) − σ (t))(σ (b) − σ (t)) = (σ (b) − σ (t))(σ (a) − σ (t)),

and these pairs can be ignored.
   Case 5. Finally we consider the pair (a, b) itself. It contributes σ (b) − σ (a)
to the product for σ , and it contributes σ (a) − σ (b) to the product for σ (a b).
These are negatives of one another, and we get a net contribution of one minus
sign in comparing our two product formulas. The lemma follows.                   §

   Proposition 1.24. The signs of permutations of {1, 2, . . . , n} have the follow-
ing properties:
    (a) sgn 1 = +1,
    (b) sgn σ = (−1)k if σ can be written as the product of k transpositions,
    (c) sgn(σ τ ) = (sgn σ )(sgn τ ),
    (d) sgn(σ −1 ) = sgn σ .

   PROOF. Conclusion (a) is immediate from the definition. For (b), let σ =
τ1 · · · τk with each τ j equal to a transposition. We apply Lemma 1.23 recursively,
using (a) at the end:

         sgn(τ1 · · · τk ) = (−1) sgn(τ1 · · · τk−1 ) = (−1)2 sgn(τ1 · · · τk−2 )
                        = · · · = (−1)k−1 sgn τ1 = (−1)k sgn 1 = (−1)k .

For (c), Corollary 1.22 shows that any permutation is the product of transpositions.
If σ is the product of k transpositions and τ is the product of l transpositions, then
σ τ is manifestly the product of k + l transpositions. Thus (c) follows from (b).
Finally (d) follows from (c) and (a) by taking τ = σ −1 .                           §

  Our discussion of signs has so far attached signs only to permutations of
S = {1, 2, . . . , n}. If we are given some other set S 0 of n elements and we want to
adapt our discussion of signs so that it applies to permutations of S 0 , we need
                                     5. Row Reduction                                   19

to identify S with S 0 , say by a one-one onto function ϕ : S → S 0 . If σ is a
permutation of S 0 , then ϕ −1 σ ϕ is a permutation of S, and we can define sgnϕ (σ ) =
sgn(ϕ −1 σ ϕ). The question is whether this definition is independent of ϕ.
   Fortunately the answer is yes, and the proof is easy. Suppose that √ : S → S 0
is a second one-one onto function, so that sgn√ (σ ) = sgn(√ −1 σ √). Then
ϕ −1 √ = τ is a permutation of {1, 2, . . . , n}, and (c) and (d) in Proposition 1.24
give

sgn√ (σ ) = sgn(√ −1 σ √) = sgn(√ −1 ϕϕ −1 σ ϕϕ −1 √)
           = sgn(τ −1 ) sgn(ϕ −1 σ ϕ) sgn(τ ) = sgn(τ ) sgnϕ (σ ) sgn(τ ) = sgnϕ (σ ).

   Consequently the definition of signs of permutations of {1, 2, . . . , n} can be
carried over to give a definition of signs of permutations of any finite nonempty set
of n elements, and the resulting signs are independent of the way we enumerate
the set. The conclusions of Proposition 1.24 are valid for this extended definition
of signs of permutations.


                                  5. Row Reduction

This section and the next review row reduction and matrix algebra for rational,
real, and complex matrices. As in Section 3 let F denote Q or R or C. The
members of F are called scalars.
   The term “row reduction” refers to the main part of the algorithm used for
solving simultaneous systems of algebraic linear equations with coefficients in
F. Such a system is of the form

                         a11 x1 + a12 x2 + · · · + a1n xn = b1 ,
                                                          ..
                                                           .
                         ak1 x1 + ak2 x2 + · · · + akn xn = bk ,

where the ai j and bi are known scalars and the x j are the unknowns, or variables.
The algorithm makes repeated use of three operations on the equations, each of
which preserves the set of solutions (x1 , . . . , xn ) because its inverse is an operation
of the same kind:
     (i) interchange two equations,
    (ii) multiply an equation by a nonzero scalar,
   (iii) replace an equation by the sum of it and a multiple of some other equation.
20                    I. Preliminaries about the Integers, Polynomials, and Matrices

The repeated writing of the variables in carrying out these steps is tedious and
unnecessary, since the steps affect only the known coefficients. Instead, we can
simply work with an array of the form
                                                       
                         a11 a12 · · · a1n          b1
                                     ..             .. 
                                         .            .    .
                         ak1 ak2 · · · akn          bk

The individual scalars appearing in the array are called entries. The above
operations on equations correspond exactly to operations on the rows3 of the
array, and they become
     (i) interchange two rows,
    (ii) multiply a row by a nonzero scalar,
   (iii) replace a row by the sum of it and a multiple of some other row.
Any operation of these types is called an elementary row operation. The vertical
line in the array is handy from one point of view in that it separates the left sides
of the equations from the right sides; if we have more than one set of right sides,
we can include all of them to the right of the vertical line and thereby solve all
the systems at the same time. But from another point of view, the vertical line is
unnecessary since it does not affect which operation we perform at a particular
time. Let us therefore drop it, abbreviating the system as
                        √ a11 a12 · · · a1n b1 !
                                         ..           ..
                                            .          .    .
                                   ak1     ak2      ···     akn     bk
   The main step in solving the system is to apply the three operations in succes-
sion to the array to reduce it to a particularly simple form. An array with k rows
and m columns4 is in reduced row-echelon form if it meets several conditions:
      • Each member of the first l of the rows, for some l with 0 ≤ l ≤ k, has at
         least one nonzero entry, and the other rows have all entries 0.
      • Each of the nonzero rows has 1 as its first nonzero entry; let us say that
         the i th nonzero row has this 1 in its j (i)th entry.
      • The integers j (i) are to be strictly increasing as a function of i, and the
         only entry in the j (i)th column that is nonzero is to be the one in the i th
         row.

   Proposition 1.25. Any array with k rows and m columns can be transformed
into reduced row-echelon form by a succession of steps of types (i), (ii), (iii).
     3 “Rows”     are understood to be horizontal, while “columns” are vertical.
     4 In   the above displayed matrix, the array has m = n + 1 columns.
                                   5. Row Reduction                                 21

    In fact, the transformation in the proposition is carried out by an algorithm
known as the method of row reduction of the array. Let us begin with an
example, indicating the particular operation at each stage by a label over an arrow
7→. To keep the example from being unwieldy, we consolidate steps of type (iii)
 into a single step when the “other row” is the same.
      EXAMPLE. In this example, k = m = 4. Row reduction gives
                                                                         
            0  0     2 7              1 −1        1 1            1 −1    1 1
         1 −1       1 1  (i)  0          0     2 7  (iii)  0    0   2 7
                           7→                         7→                 
          −1   1 −4 5               −1      1 −4 5               0   0 −3 6
          −2   2 −5 4               −2      2 −5 4               0   0 −3 6
                                                5         1 −1 0 − 5 
           1 −1     1 1            1 −1 0 − 2                              2
                        7                          7
   (ii)  0   0     1 2  (iii)  0      0 1      2
                                                       (ii)  0
                                                                  0 1     7
                                                                           2
                                                                              
 7→                       7→                    33  7→                   
           0  0 −3 6               0      0 0      2           0   0   0   1
           0  0 −3 6                               33                      33
                                   0      0 0      2
                                                               0   0 0     2
                       
           1 −1 0 0
   (iii)  0  0 1 0
  7→                   .
           0  0 0 1
           0  0 0 0
The final matrix here is in reduced row-echelon form. In the notation of the
definition, the number of nonzero rows in the reduced row-echelon form is l = 3,
and the integers j (i) are j (1) = 1, j (2) = 3, and j (3) = 4.
    The example makes clear what the algorithm is that proves Proposition 1.25.
We find the first nonzero column, apply an interchange (an operation of type (i))
if necessary to make the first entry in the column nonzero, multiply by a nonzero
scalar to make the first entry 1 (an operation of type (ii)), and apply operations of
type (iii) to eliminate the other nonzero entries in the column. Then we look for
the next column with a nonzero entry in entries 2 and later, interchange to get the
nonzero entry into entry 2 of the column, multiply to make the entry 1, and apply
operations of type (iii) to eliminate the other entries in the column. Continuing
in this way, we arrive at reduced row-echelon form.
    In the general case, as soon as our array, which contains both sides of our system
of equations, has been transformed into reduced row-echelon form, we can read
off exactly what the solutions are. It will be handy to distinguish two kinds of
variables among x1 , . . . , xn without including any added variables xn+1 , . . . , xm
in either of the classes. The corner variables are those x j ’s for which j is ≤ n and
is some j (i) in the definition of “reduced row-echelon form,” and the other x j ’s
with j ≤ n will be called independent variables. Let us describe the last steps
of the solution technique in the setting of an example. We restore the vertical line
that separated the data on the two sides of the equations.
22             I. Preliminaries about the Integers, Polynomials, and Matrices

   EXAMPLE. We consider what might happen to a certain system of 4 equations
in 4 unknowns. Putting the data in place for the right side makes the array have 4
rows and 5 columns. We transform the array into reduced row-echelon form and
suppose that it comes out to be
                                                             
                          1       −1      0    0         1
                         0        0      1    0         2    
                                                             .
                          0        0      0    1         3
                          0        0      0    0       1 or 0

If the lower right entry is 1, there are no solutions. In fact, the last row corresponds
to an equation 0 = 1, which announces a contradiction. More generally, if any
row of 0’s to the left of the vertical line is equal to something nonzero, there are
no solutions. In other words, there are no solutions to a system if the reduced
row-echelon form of the entire array has more nonzero rows than the reduced
row-echelon form of the part of the array to the left of the vertical line.
    On the other hand, if the lower right entry is 0, then there are solutions. To see
this, we restore the reduced array to a system of equations:

                                x1 − x2               = 1,
                                              x3      = 2,
                                                   x4 = 3;

we move the independent variables (namely x2 here) to the right side to obtain

                                       x1 = 1 + x2 ,
                                       x3 = 2,
                                       x4 = 3;

and we collect everything in a tidy fashion as
                                        
                              x1     1      1
                             x2   0    1
                              =   + x2   .
                              x3     2      0
                              x4     3      0
The independent variables are allowed to take on arbitrary values, and we have
succeeded in giving a formula for the solution that corresponds to an arbitrary set
of values for the independent variables.
   The method in the above example works completely generally. We obtain
solutions whenever each row of 0’s to the left of the vertical line is matched by
a 0 on the right side, and we obtain no solutions otherwise. In the case that we are
                                   5. Row Reduction                                 23

solving several systems with the same left sides, solutions exist for each of the
systems if the reduced row-echelon form of the entire array has the same number
of nonzero rows as the reduced row-echelon form of the part of the array to the
left of the vertical line.
   Let us record some observations about the method for solving systems of linear
equations and then some observations about the method of row reduction itself.

   Proposition 1.26. In the solution process for a system of k linear equations in
n variables with the vertical line in place,
    (a) the sum of the number of corner variables and the number of independent
        variables is n,
    (b) the number of corner variables equals the number of nonzero rows on the
        left side of the vertical line and hence is ≤ k,
    (c) when solutions exist, they are of the form

                       independent                    independent
          column +                 × column + · · · +             × column
                         variable                       variable

        in such a way that each independent variable x j is a free parameter in F,
        the column multiplying x j has a 1 in its j th entry, and the other columns
        have a 0 in that entry,
    (d) a homogeneous system, i.e., one with all right sides equal to 0, has
        a nonzero solution if the number k of equations is < the number n of
        variables,
    (e) the solutions of an inhomogeneous system, i.e., one in which the right
        sides are not necessarily all 0, are all given by the sum of any one particular
        solution and an arbitrary solution of the corresponding homogeneous
        system.
   PROOF. Conclusions (a), (b), and (c) follow immediately by inspection of
the solution method. For (d), we observe that no contradictory equation can
arise when the right sides are 0 and, in addition, that there must be at least one
independent variable by (a) since (b) shows that the number of corner variables
is ≤ k < n. Conclusion (e) is apparent from (c), since the first column in the
solution written in (c) is a column of 0’s in the homogeneous case.                  §

   Proposition 1.27. For an array with k rows and n columns in reduced row-
echelon form,
    (a) the sum of the number of corner variables and the number of independent
        variables is n,
    (b) the number of corner variables equals the number of nonzero rows and
        hence is ≤ k,
24             I. Preliminaries about the Integers, Polynomials, and Matrices

     (c) when k = n, either the array is of the form
                                                   
                               1 0 0 ··· 0
                             0 1 0 ··· 0
                                                   
                             0 0 1 ··· 0
                                            ..     
                                               .   
                                  0 0 0 ···            1
        or else it has a row of 0’s.
   PROOF. Conclusions (a) and (b) are immediate by inspection. In (c), failure of
the reduced row-echelon form to be as indicated forces there to be some noncorner
variable, so that the number of corner variables is < n. By (b), the number of
nonzero rows is < n, and hence there is a row of 0’s.                          §

   One final comment: For the special case of n equations in n variables, some
readers may be familiar with a formula known as “Cramer’s rule” for using
determinants to solve the system when the determinant of the array of coefficients
on the left side of the vertical line is nonzero. Determinants, including their
evaluation, and Cramer’s rule will be discussed in Chapter II. The point to make
for current purposes is that the use of Cramer’s rule for computation is, for n
large, normally a more lengthy process than the method of row reduction. In fact,
Problem 13 at the end of this chapter shows that the number of steps for solving
the system via row reduction is at most a certain multiple of n 3 . On the other
hand, the typical number of steps for solving the system by rote application of
Cramer’s rule is approximately a multiple of n 4 .


                               6. Matrix Operations

A rectangular array of scalars (i.e., members of F) with k rows and n columns
is called a k-by-n matrix. More precisely a k-by-n matrix over F is a function
from {1, . . . , k} × {1, . . . , n} to F. The expression “k-by-n” is called the size of
the matrix. The value of the function at the ordered pair (i, j) is often indicated
with subscript notation, such as ai j , rather than with the usual function notation
a(i, j). It is called the (i, j)th entry. Two matrices are equal if they are the
same function on ordered pairs; this means that they have the same size and their
corresponding entries are equal. A matrix is called square if its number of rows
equals its number of columns. A square matrix with all entries 0 for i 6= j is
called diagonal, and the entries with i = j are the diagonal entries.
   As the reader likely already knows, it is customary to write matrices in rectan-
gular patterns. By convention the first index always tells the number of the row
and the second index tells the number of the column. Thus a typical 2-by-3 matrix
                                       6. Matrix Operations                                        25
     µ                 ∂
      a11 a12 a13
is                        . In the indication of the size of the matrix, here 2-by-3, the 2
      a21 a22 a23
refers to the number of rows and the 3 refers to the number of columns.
    An n-dimensional row vector is a 1-by-n matrix, while a k-dimensional
column vector is a k-by-1 matrix. The set of all k-dimensional column vectors
is denoted by Fk . The set Fk is to be regarded as the space of all ordinary garden-
variety vectors. For economy of space, books often write such vectors horizontally
with entries separated by commas, for example as (c1 , c2 , c3 ), and it is extremely
important to treat such vectors as column vectors, not as row vectors, in order
to get matrix operations and the effect of linear transformations to correspond
nicely.5 Thus in this book,√(c1 , !   c2 , c3 ) is to be regarded as a space-saving way of
                                   c1
writing the column vector c2 .
                                   c3
    If a matrix is denoted by some letter like A, its (i, j)th entry will typically be
denoted by Ai j . In the reverse direction, sometimes a matrix is assembled from
its individual entries, which may be expressions depending on i and j. If some
such expression ai j is given for each pair (i, j), then we denote the corresponding
matrix by [ai j ] i=1,...,k , or simply by [ai j ] if there is no possibility of confusion.
                   j=1,...,n
    Various operations are defined on matrices. Specifically let Mkn (F) be the
set of k-by-n matrices with entries in F, so that Mk1 (F) is the same thing as Fk .
Addition of matrices is defined whenever two matrices have the same size, and it
is defined entry by entry; thus if A and B are in Mkn (F), then A + B is the member
of Mkn (F) with (A + B)i j = Ai j + Bi j . Scalar multiplication on matrices is
defined entry by entry as well; thus if A is in Mkn (F) and c is in F, then c A is
the member of Mkn (F) with (c A)i j = c Ai j . The matrix (−1)A is denoted by
−A. The k-by-n matrix with 0 in each entry is called a zero matrix. Ordinarily
it is denoted simply by 0; if some confusion is possible in a particular situation,
more precise notation will be introduced at the time. With these operations the
set Mkn (F) has the following properties:
 (i) the operation of addition satisfies
      (a) A + (B + C) = (A + B) + C for all A, B, C in Mkn (F) (associative
          law),
      (b) A + 0 = 0 + A = A for all A in Mkn (F),
      (c) A + (−A) = (−A) + A = 0 for all A in Mkn (F),
      (d) A + B = B + A for all A and B in Mkn (F) (commutative law);

     5 The
         alternatives are unpleasant. Either one is forced to write certain functions in the unnatural
notation x 7→ (x)f , or the correspondence is forced to involve transpose operations on frequent
occasions. Unhappily, books following either of these alternative conventions may be found.
26              I. Preliminaries about the Integers, Polynomials, and Matrices

(ii) the operation of scalar multiplication satisfies
     (a) (cd)A = c(d A) for all A in Mkn (F) and all scalars c and d,
     (b) 1A = A for all A in Mkn (F) and for the scalar 1;
(iii) the two operations are related by the distributive laws
     (a) c(A + B) = c A + cB for all A and B in Mkn (F) and for all scalars c,
     (b) (c + d)A = c A + d A for all A in Mkn (F) and all scalars c and d.
Since addition and scalar multiplication are defined entry by entry, all of these
identities follow from the corresponding identities for members of F.
   Multiplication of matrices is defined in such a way that the kind of system
of linear equations discussed in the previous section can be written as a matrix
equation in the form AX = B, where
                                                                           
                  a11   ···    a1n             x1                           b1
                        ..                       .                            .
         A=               .         ,   X =  ..  ,           and   B =  ..  .
                  ak1   ···    akn             xn                           bk

More precisely if A is a k-by-m matrix and B is an m-by-n matrix, then the
product C = AB is the k-by-n matrix defined by

                                              m
                                              X
                                     Ci j =         Ail Bl j .
                                              l=1


The (i, j)th entry of C is therefore the product of the i th row of A and the j th
column of B.
   Let us emphasize that the condition for a product AB to be defined is that
the number of columns of A should equal the number of rows of B. With this
definition the system of equations mentioned above is indeed of the form AX = B.

     Proposition 1.28. Matrix multiplication has the properties that
     (a) it is associative in the sense that (AB)C = A(BC), provided that the
         sizes match correctly, i.e., A is in Mkm (F), B is in Mmn (F), and C is in
         Mnp (F),
     (b) it is distributive over addition in the sense that A(B + C) = AB + AC
         and (B + C)D = B D + C D if the sizes match correctly.

                             ≥ is
   REMARK. Matrix multiplication               ≥ ¥ commutative,
                                  ¥ ≥not ¥necessarily    ≥ ¥ ≥even¥ for
                               10     01
square matrices. For example, 0 0     0 0
                                                 01
                                            = 0 0 , while 00 10 10
                                                                00
                                                                     =
≥ ¥
  00
  00
      .
                                 6. Matrix Operations                              27

  PROOF. For (a), we have
                        P                P    P
          ((AB)C)i j = nt=1 (AB)it Ct j = nt=1 m
                                               s=1 Ais Bst C t j
                       Pm                Pm Pn
and      (A(BC))i j = s=1 Ais (BC)s j = s=1 t=1 Ais Bst Ct j ,

and these are equal. For the first identity in (b), we have
                           P                      P
         (A(B + C))i j = l Ail (B + C)l j = l Ail (Bl j + Cl j )
                           P              P
                       = l Ail Bl j + l Ail Cl j = (AB)i j + (AC)i j ,

and the second identity is proved similarly.                                       §

   We have already defined the zero matrix 0 of a given size to be the matrix
having 0 in each entry. This matrix has the property that 0A = 0 and B0 = 0 if the
sizes match properly. The n-by-n identity matrix, denoted by I or sometimes 1,
is defined to be the matrix with Ii j = δi j , where δi j is the Kronecker delta
defined by                          Ω
                                      1      if i = j,
                             δi j =
                                      0      if i 6= j.
In other words, the identity matrix is the square matrix of the form
                                     1 0 0 ··· 0 
                                    0 1 0 ··· 0 
                                I =
                                   
                                     0 0 1 ··· 0  .
                                           .. 
                                             .
                                        0 0 0 ··· 1

It has the property that I A = A and B I = I whenever the sizes match properly.
    Let A be an n-by-n matrix. We say that A is invertible and has the n-by-n
matrix B as inverse if AB = B A = I . If B and C are n-by-n matrices with
AB = I and C A = I , then associativity of multiplication (Proposition 1.28a)
implies that B = I B = (C A)B = C(AB) = C I = C. Hence an inverse for A
is unique if it exists. We write A−1 for this inverse if it exists. Inverses of n-by-n
matrices have the property that if A and D are invertible, then AD is invertible
and (AD)−1 = D −1 A−1 ; moreover, if A is invertible, then A−1 is invertible and
its inverse is A.
    The method of row reduction in the previous section suggests a way of com-
puting the inverse of a matrix. Suppose that A is a square matrix to be inverted
and we are seeking its inverse B. Then AB = I . Examining the definition of
matrix multiplication, we see that this matrix equation means that the product of
A and the first column of B equals the first column of I , the product of A and the
second column of B equals the second column of I , and so on. We can thus think
28             I. Preliminaries about the Integers, Polynomials, and Matrices

of a column of B as the unknowns in a system of linear equations, the known
right sides being the entries of the column of the identity matrix. As the column
index varies, the left sides of these equations do not change, since they are always
given by A. So we can attempt to solve all of the systems µ (one ∂for each column)
                                                                  12 3
simultaneously. For example, to attempt to invert A =             45 6     , we set up
                                                                  7 8 10
                                                              
                          1       2    3         1     0     0
                         4       5    6         0     1     0 .
                          7       8    10        0     0     1

Imagine doing the row reduction. We can hope that the result will be of the form
                                                   
                         1 0 0           -   - -
                       0 1 0            -   - - ,
                         0 0 1           -   - -

with the identity matrix on the left side of the vertical line. If this is indeed the
result, then the computation shows that the matrix on the right side of the vertical
line is the only possibility for A−1 . But does A−1 in fact exist?
   Actually, another question arises as well. According to Proposition 1.27c, the
other possibility in applying row reduction is that the left side has a row of 0’s.
In this case, can we deduce that A−1 does not exist? Or, to put it another way,
can we be sure that some row of the reduced row-echelon form has all 0’s on the
left side of the vertical line and something nonzero on the right side?
   All of the answers to these questions are yes, and we prove them in a mo-
ment. First we need to see that elementary row operations are given by matrix
multiplications.

   Proposition 1.29. Each elementary row operation is given by left multiplica-
tion by an invertible matrix. The inverse matrix is the matrix of another elementary
row operation.
   REMARK. The square matrices giving these left multiplications are called
elementary matrices.
   PROOF. For the interchange of rows i and j, the part of the elementary matrix
in the rows and columns with i or j as index is
                                     i j
                                  µ       ∂
                                i 0 1
                                            ,
                                j 1 0
and otherwise the matrix is the identity. This matrix is its own inverse.
                                 6. Matrix Operations                               29

   For the multiplication of the i th row by a nonzero scalar c, the matrix is diagonal
with c in the i th diagonal entry and with 1 in all other diagonal entries. The inverse
matrix is of this form with c−1 in place of c.
   For the replacement of the i th row by the sum of the i th row and the product
of a times the j th row, the part of the elementary matrix in the rows and columns
with i or j as index is
                                         i j
                                       µ       ∂
                                     i 1 a
                                                  ,
                                     j 0 1
and otherwise the matrix is the identity. The inverse of this matrix is the same
except that a is replaced by −a.                                               §

  Theorem 1.30. The following conditions on an n-by-n square matrix A are
equivalent:
   (a) the reduced row-echelon form of A is the identity,
   (b) A is the product of elementary matrices,
   (c) A has an inverse,                           √ x1 !
                                                      .
   (d) the system of equations AX = 0 with X = .. has only the solution
                                                           xn
         X = 0.
   PROOF. If (a) holds, choose a sequence of elementary row operations that
reduce A to the identity, and let E 1 , . . . , Er be the corresponding elementary
matrices given by Proposition 1.29. Then we have Er · · · E 1 A = I , and hence
A = E 1−1 · · · Er−1 . The proposition says that each E j−1 is an elementary matrix,
and thus (b) holds.
   If (b) holds, then (c) holds because the elementary matrices are invertible and
the product of invertible matrices is invertible.
   If (c) holds and if AX = 0, then X = I X = (A−1 A)X = A−1 (AX) =
  −1
A 0 = 0. Hence (d) holds.
   If (d) holds, then the number of independent variables in the row reduction of
A is 0. Proposition 1.26a shows that the number of corner variables is n, and
parts (b) and (c) of Proposition 1.27 show that the reduced row-echelon form of
A is I . Thus (a) holds.                                                          §

   Corollary 1.31. If the solution procedure for finding the inverse of a square
matrix A leads from (A | I ) to (I | X), then A is invertible and its inverse is X.
Conversely if the solution procedure leads to (R | Y ) and R has a row of 0’s, then
A is not invertible.
  REMARK. Proposition 1.27c shows that this corollary addresses the only
possible outcomes of the solution procedure.
30              I. Preliminaries about the Integers, Polynomials, and Matrices

   PROOF. We apply the equivalence of (a) and (c) in Theorem 1.30 to settle the
existence or nonexistence of A−1 . In the case that A−1 exists, we know that the
solution procedure has to yield the inverse.                                  §

   Corollary 1.32. Let A be a square matrix. If B is a square matrix such that
B A = I , then A is invertible and B is its inverse. If C is a square matrix such
that AC = I , then A is invertible with inverse C.

   PROOF. Suppose B A = I . Let X be a column vector with AX = 0. Then
X = I X = (B A)X = B(AX) = B0 = 0. Since (d) implies (c) in Theorem
1.30, A is invertible.
   Suppose AC = I . Applying the result of the previous paragraph to C, we
conclude that C is invertible with inverse A. Therefore A is invertible with
inverse C.                                                                §



                                      7. Problems

1.   What is the greatest common divisor of 9894 and 11058?
2.   (a) Find integers x and y such that 11x + 7y = 1.
     (b) How are all pairs (x, y) of integers satisfying 11x + 7y = 1 related to the
         pair you found in (a)?
3.   Let {an }n∏1 be a sequence of positive integers, and let d be the largest integer
     dividing all an . Prove that d is the greatest common divisor of finitely many of
     the an .
4.   Determine the integers n for which there exist integers x and y such that n divides
     x + y − 2 and 2x − 3y − 3.
5.   Let P(X) and Q(X) be the polynomials P(X) = X 4 + X 3 + 2X 2 + X + 1 and
     Q(X) = X 5 + 2X 3 + X in R[X].
     (a) Find a greatest common divisor D(X) of P(X) and Q(X).
     (b) Find polynomials A and B such that A P + B Q = D.
6.   Let P(X) and Q(X) be polynomials in R[X]. Prove that if D(X) is a greatest
     common divisor of P(X) and Q(X) in C[X], then there exists a nonzero complex
     number c such that cD(X) is in R[X].
7.   (a) Let P(X) be in R[X], and regard it as in C[X]. Applying the Fundamental
         Theorem of Algebra and its corollary to P, prove that if z j is a root of P,
         then so is z̄ j , and z j and z̄ j have the same multiplicity.
     (b) Deduce that any prime polynomial in R[X] has degree at most 2.
                                      7. Problems                                    31

8.   (a) Suppose that a polynomial A(X) of degree > 0 in Q[X] has integer coef-
         ficients and leading coefficient 1. Show that if p/q is a root of A(X) with
         p and q integers such that GCD( p, q) = 1, then p/q is an integer n and n
         divides the constant term of A(X).
     (b) Deduce that X 2 − 2 and X 3 + X 2 + 1 are prime in Q[X].
9.   Reduce the fraction 8645/10465 to lowest terms.
10. How many different patterns are there of disjoint cycle structures for permutations
    of {1, 2, 3, 4}? Give examples of each, telling how many permutations there are
    of each kind and what the signs are of each.
11. Prove for n ∏ 2 that the number of permutations of {1, . . . , n} with sign −1
    equals the number with sign +1.
                                                        µ1 2 3∂
12. Find all solutions X of the system AX = B when A = 4 5 6 and B is given
                                                                 789
     by     µ0∂                           µ5∂                               µ3∂
     (a) B = 0 ,                   (b) B = 3 ,                    (c) B =    2 .
               0                              2                              1
13. Suppose that a single step in the row reduction process means a single arithmetic
    operation or a single interchange of two entries. Prove that there exists a constant
    C such that any square matrix can be transformed into reduced row-echelon form
    in ≤ Cn 3 steps, the matrix being of size n-by-n.
                                       ≥ ¥             ≥      ¥
                                                         −4 8
14. Compute A + B and AB if A = 24 35 and B = −1 3 .

15. Prove that if A and B are square matrices with AB = B A, then (A + B)n is
                                                  P    ° ¢                ° ¢
    given by the Binomial Theorem: (A + B)n = nk=0 nk An−k B k , where nk is
    the binomial coefficient n!/((n − k)!k!).
                                       µ1 1 0∂
                            th
16. Find a formula for the n power of 0 1 1 , n being a positive integer.
                                            001
17. Let D be an n-by-n diagonal matrix with diagonal entries d1 , . . . , dn , and let A
    be an n-by-n matrix. Compute AD and D A, and give a condition for the equality
    AD = D A to hold.
18. Fix n, and let E i j denote the n-by-n matrix that is 1 in the (i, j)th entry and
    is 0 elsewhere. Compute the product E kl E pq , expressing the result in terms of
    matrices E i j and instances of the Kronecker delta.
                                    ≥ ¥−1                  ≥       ¥
                                                              d −b
19. Verify that if ad−bc 6= 0, then ac db    = (ad−bc)−1 −c a and that the sys-
         ≥ ¥≥x ¥ ≥ p¥                                 ≥x¥                   ≥       ¥
    tem ac db                                                           −1 dp−bq .
                   y = q has the unique solution y = (ad − bc)                aq−cp
32               I. Preliminaries about the Integers, Polynomials, and Matrices

20. Which of the following matrices A is invertible? For the invertible ones, find
    A−1 . µ       ∂                   µ1 2 3 ∂                     µ7 4 1∂
             123
    (a) A = 4 5 6 ,          (b) A = 4 5 6 ,              (c) A = 6 4 1 .
                 789                             7 8 10                           431
21. Can a square matrix with a row of 0’s be invertible? Why or why not?
22. Prove that if the product AB of two n-by-n matrices is invertible, then A and B
    are invertible.
23. Let A be a square matrix such that Ak = 0 for some positive integer n. Prove
    that I + A is invertible.
24. Give an example of a set S and functions f : S → S and g : S → S such that
    the composition g ◦ f is the identity function but neither f nor g has an inverse
    function.
25. Give an example of two matrices, A of size 1-by-2 and B of size 2-by-1, such
    that AB = I , I being the 1-by-1 identity matrix. Verify that B A is not the 2-by-2
    identity matrix. Give a proof for these sizes that B A can never be the identity
    matrix.
Problems 26–29 concern least common multiples. Let a and b be positive integers.
A common multiple of a and b is an integer N such that a and b both divide N . The
least common multiple of a and b is the smallest positive common multiple of a and
b. It is denoted by LCM(a, b).
26. Prove that a and b have a least common multiple.
27. If a has a prime factorization given by a = p1k1 · · · prkr , prove that any positive
    multiple M of a has a prime factorization given by a = p1m 1 · · · prm r q1n 1 · · · qsn s ,
    where q1 , . . . , qs are primes not in the list p1 , . . . , pr , where m j ∏ k j for all j,
    and where n j ∏ 0 for all j.
28. (a) Prove that if a = p1k1 · · · prkr and b = p1l1 · · · prlr are expansions of a and b
        as products of powers of r distinct primes p1 , . . . , pr , then LCM(a, b) =
        p1max(k1 ,l1 ) · · · prmax(kr ,lr ) .
    (b) Prove that if N is any common multiple of a and b, then LCM(a, b) divides
        N.
    (c) Deduce that ab = GCD(a, b) LCM(a, b).
29. If a1 , . . . , at are positive integers, define their least common multiple to be the
    smallest positive integer M such that each a j divides M. Give a formula for this
    M in terms of expansions of a1 , . . . , at as products of powers of distinct primes.
                                        CHAPTER II

                       Vector Spaces over Q, R, and C




Abstract. This chapter introduces vector spaces and linear maps between them, and it goes on
to develop certain constructions of new vector spaces out of old, as well as various properties of
determinants.
     Sections 1–2 define vector spaces, spanning, linear independence, bases, and dimension. The
sections make use of row reduction to establish dimension formulas for certain vector spaces
associated with matrices. They conclude by stressing methods of calculation that have quietly
been developed in proofs.
     Section 3 relates matrices and linear maps to each other, first in the case that the linear map carries
column vectors to column vectors and then in the general finite-dimensional case. Techniques are
developed for working with the matrix of a linear map relative to specified bases and for changing
bases. The section concludes with a discussion of isomorphisms of vector spaces.
     Sections 4–6 take up constructions of new vector spaces out of old ones, together with corre-
sponding constructions for linear maps. The four constructions of vector spaces in these sections
are those of the dual of a vector space, the quotient of two vector spaces, and the direct sum and
direct product of two or more vector spaces.
     Section 7 introduces determinants of square matrices, together with their calculation and prop-
erties. Some of the results that are established are expansion in cofactors, Cramer’s rule, and the
value of the determinant of a Vandermonde matrix. It is shown that the determinant function is well
defined on any linear map from a finite-dimensional vector space to itself.
     Section 8 introduces eigenvectors and eigenvalues for matrices, along with their computation.
Also, in this section the characteristic polynomial and the trace of a square matrix are defined, and
all these notions are reinterpreted in terms of linear maps.
     Section 9 proves the existence of bases for infinite-dimensional vector spaces and discusses the
extent to which the material of the first eight sections extends from the finite-dimensional case to be
valid in the infinite-dimensional case.




                    1. Spanning, Linear Independence, and Bases

This chapter develops a theory of rational, real, and complex vector spaces. Many
readers will already be familiar with some aspects of this theory, particularly in
the case of the vector spaces Qn , Rn , and Cn of column vectors, where the tools
developed from row reduction allow one to introduce geometric notions and to
view geometrically the set of solutions to a set of linear equations. Thus we shall
                                                    33
34                                II. Vector Spaces over Q, R, and C

be brief about many of these matters, concentrating on the algebraic aspects of
the theory. Let F denote any of Q, R, or C. Members of F are called scalars.1
   A vector space over F is a set V with two operations, addition carrying V × V
into V and scalar multiplication carrying F × V into V , with the following
properties:
  (i) the operation of addition, written +, satisfies
     (a) v1 + (v2 + v3 ) = (v1 + v2 ) + v3 for all v1 , v2 , v3 in V (associative law),
     (b) there exists an element 0 in V with v + 0 = 0 + v = v for all v in V ,
     (c) to each v in V corresponds an element −v in V such that v + (−v) =
          (−v) + v = 0,
     (d) v1 + v2 = v2 + v1 for all v1 and v2 in V (commutative law);
 (ii) the operation of scalar multiplication, written without a sign, satisfies
     (a) a(bv) = (ab)v for all v in V and all scalars a and b,
     (b) 1v = v for all v in V and for the scalar 1;
(iii) the two operations are related by the distributive laws
     (a) a(v1 + v2 ) = av1 + av2 for all v1 and v2 in V and for all scalars a,
     (b) (a + b)v = av + bv for all v in V and all scalars a and b.
It is immediate from these properties that
       • 0 is unique (since 00 = 00 + 0 = 0),
       • −v is unique (since (−v)0 = (−v)0 + 0 = (−v)0 + (v + (−v)) =
          ((−v)0 + v) + (−v) = 0 + (−v) = (−v)),
       • 0v = 0 (since 0v = (0 + 0)v = 0v + 0v),
       • (−1)v = −v (since 0 = 0v = (1+(−1))v = 1v+(−1)v = v+(−1)v),
       • a0 = 0 (since a0 = a(0 + 0) = a0 + a0).
Members of V are called vectors.

     EXAMPLES.
    (1) V = Mkn (F), the space of all k-by-n matrices. The above properties of a
vector space over F were already observed in Section I.6. The vector space Fk of
all k-dimensional column vectors is the special case n = 1, and the vector space
F of scalars is the special case k = n = 1.
   (2) Let S be any nonempty set, and let V be the set of all functions from S into
F. Define operations by ( f + g)(s) = f (s) + g(s) and (c f )(s) = c( f (s)). The
operations on the right sides of these equations are those in F, and the properties
of a vector space follow from the fact that they hold in F at each s.
     1 All the material of this chapter will ultimately be seen to work when F is replaced by any “field.”

This point will not be important for us at this stage, and we postpone considering it further until
Chapter IV.
                          1. Spanning, Linear Independence, and Bases                             35

   (3) More generally than in Example 2, let S be any nonempty set, let U be a
vector space over F, and let V be the set of all functions from S into U . Define
the operations as in Example 2, but interpret the operations on the right sides of
the defining equations as those in U . Then the properties of a vector space follow
from the fact that they hold in U at each s.
    (4) Let V be any vector space over C, and restrict scalar multiplication to an
operation R × V → V . Then V becomes a vector space over R. In particular, C
is a vector space over R.
  (5) Let V = F[X] be the set of all polynomials in one indeterminate with
coefficients in F, and define addition and scalar multiplication as in Section I.3.
Then V is a vector space.
    (6) Let V be any vector space over F, and let U be any nonempty subset closed
under addition and scalar multiplication. Then U is a vector space over F. Such a
subset U is called a vector subspace of V ; sometimes one says simply subspace
if the context is unambiguous.2
   (7) Let V be any vector space over F, and let U = {vα } be any subset of
V . A finite linear combination of the members of U is any vector of the form
cα1 vα1 + · · · + cαn vαn with each cαj in F, each vαj in U , and n ∏ 0. The linear
span of U is the set of all finite linear combinations of members of U . It is a
vector subspace of V and is denoted by span{vα }. By convention, span ∅ = 0.
     (8) Many vector subspaces arise in the context of some branch of mathematics
after some additional structure is imposed. For example let V be the vector
space of all functions from R3 into R, an instance of Example 2. The subset
U of continuous members of V is a vector subspace; the closure under addition
and scalar multiplication comes down to knowing that addition is a continuous
function from R3 × R3 into R3 and that scalar multiplication from R × R3
into R3 is continuous as well. Another example is the subset of twice continu-
ously differentiable members f of V satisfying the partial differential equation
@2 f       2        2

@x2
      + @@ xf2 + @@ xf2 + f = 0 on R3 .
  1       2       3


   The associative and commutative laws in the definition of “vector space” imply
certain more complicated formulas of which the stated laws are special cases.
With associativity of addition, if n vectors v1 , . . . , vn are given, then any way of
inserting parentheses into the expression v1 +v2 +· · ·+vn leads to the same result,
and a similar conclusion applies to the associativity-like formula a(bv) = (ab)v
for scalar multiplication. In the presence of associativity, the commutative law
for addition implies that v1 + v2 + · · · + vn = vσ (1) + vσ (2) + · · · + vσ (n) for any
   2 The word “subspace” arises also in the context of metric spaces and more general topological

spaces, and the metric-topological notion of subspace is distinct from the vector notion of subspace.
36                             II. Vector Spaces over Q, R, and C

permutation of {1, . . . , n}. All these facts are proved by inductive arguments, and
the details are addressed in Problems 2–3 at the end of the chapter.
   Let V be a vector space over F. A subset {vα } of V spans V or is a spanning
set for V if the linear span of {vα }, in the sense of Example 7 above, is all of V .
A subset {vα } is linearly independent if whenever a finite linear combination
cα1 vα1 + · · · + cαn vαn equals the 0 vector, then all the coefficients must be 0:
cα1 = · · · = cαn = 0. By subtraction we see that in this case any equality of two
finite linear combinations
                     cα1 vα1 + · · · + cαn vαn = dα1 vα1 + · · · + dαn vαn
implies that the respective coefficients are equal: cαj = dαj for 1 ≤ j ≤ n.
   A subset {vα } is a basis if it spans V and is linearly independent. In this case
each member of V has one and only one expansion as a finite linear combination
of the members of {vα }.

     EXAMPLE. In Fn , the vectors
        1                0                      0                            0
          0                 1                 0                             0
     e1 =  0
           ..  ,       e2 =  0
                               ..  ,       e3 =  1
                                                   ..  ,          ... ,    en =  0
                                                                                   .. 
             .                   .                   .                               .
             0                    0                    0                              1

form a basis of F called the standard basis of F .
                     n                                     n


   Proposition 2.1. Let V be a vector space over F.
   (a) If {vα } is a linearly independent subset of V that is maximal with respect to
the property of being linearly independent (i.e., has the property of being strictly
contained in no linearly independent set), then {vα } is a basis of V .
   (b) If {vα } is a spanning set for V that is minimal with respect to the property
of spanning (i.e., has the property of strictly containing no spanning set), then
{vα } is a basis of V .
    PROOF. For (a), let v be given. We are to show that v is in the span of {vα }.
Without loss of generality, we may assume that v is not in the set {vα } itself.
By the assumed maximality, {vα } ∪ {v} is not linearly independent, and hence
cv + cα1 vα1 + · · · + cαn vαn = 0 for some scalars c, cα1 , . . . , cαn not all 0. Here
c 6= 0 since {vα } is linearly independent. Then v = −c−1 cα1 vα1 −· · ·−c−1 cαn vαn ,
and v is exhibited as in the linear span of {vα }.
    For (b), suppose that cα1 vα1 + · · · + cαn vαn = 0 with cα1 , . . . , cαn not all 0. Say
cα1 6= 0. Then we can solve for vα1 and see that vα1 is a finite linear combination of
vα2 , . . . , vαn . Substitution shows that any finite linear combination of the vα ’s is a
finite linear combination of the vα ’s other than vα1 , and we obtain a contradiction
to the assumed minimality of the spanning set.                                             §
                       1. Spanning, Linear Independence, and Bases                 37

   Proposition 2.2. Let V be a vector space over F. If V has a finite spanning
set {v1 , . . . , vm }, then any linearly independent set in V has ≤ m elements.
   PROOF. It is enough to show that no subset of m + 1 vectors can be linearly
independent. Arguing by contradiction, suppose that {u 1 , . . . , u n } is a linearly
independent set with n = m + 1. Write
                        u 1 = c11 v1 + c21 v2 + · · · + cm1 vm ,
                            ..
                             .
                        u n = c1n v1 + c2n v2 + · · · + cmn vm .
The system of linear equations
                               c11 x1 + · · · + c1n xn = 0,
                                                       ..
                                                        .
                              cm1 x1 + · · · + cmn xn = 0,
is a homogeneous system of linear equations with more unknowns than equations,
and Proposition 1.26d shows that it has a nonzero solution (x1 , . . . , xn ). Then
we have

            x1 u 1 + · · · + xn u n = c11 x1 v1 + c21 x1 v2 + · · · + cm1 x1 vm
                                         +            +                   +
                                         ···         ···                 ···
                                         +            +                   +
                                      c1n xn v1 + c2n xn v2 + · · · + cmn xn vm
                                    = 0,
in contradiction to the assumed linear independence of {u 1 , . . . , u n }.       §

   Corollary 2.3. If the vector space V has a finite spanning set {v1 , . . . , vm },
then
    (a) {v1 , . . . , vm } has a subset that is a basis,
    (b) any linearly independent set in V can be extended to a basis,
    (c) V has a basis,
    (d) any two bases have the same finite number of elements, necessarily ≤ m.
   REMARKS. In this case we say that V is finite-dimensional, and the number
of elements in a basis is called the dimension of V , written dim V . If V has no
finite spanning set, we say that V is infinite-dimensional. A suitable analog of
the conclusion in Corollary 2.3 is valid in the infinite-dimensional case, but the
proof is more complicated. We take up the infinite-dimensional case in Section 9.
38                         II. Vector Spaces over Q, R, and C

   PROOF. By discarding elements of the set {v1 , . . . , vm } one at a time if nec-
essary and by applying Proposition 2.1b, we obtain (a). For (b), we see from
Proposition 2.2 that the given linearly independent set has ≤ m elements. If we
adjoin elements to it one at a time so as to obtain larger linearly independent sets,
Proposition 2.2 shows that there must be a stage at which we can proceed no
further without violating linear independence. Proposition 2.1a then says that we
have a basis. For (c), we observe that (a) has already produced a basis. Any two
bases have the same number of elements, by two applications of Proposition 2.2,
and this proves (d).                                                              §

  EXAMPLES. The vector space Mkn (F) of k-by-n matrices has dimension kn.
The vector space of all polynomials in one indeterminate is infinite-dimensional
because the subspace consisting of 0 and of all polynomials of degree ≤ n has
dimension n + 1.

   Corollary 2.4. If V is a finite-dimensional vector space with dim V = n, then
any spanning set of n elements is a basis of V , and any linearly independent set
of n elements is a basis of V . Consequently any n-dimensional vector subspace
U of V coincides with V .
   PROOF. These conclusions are immediate from parts (a) and (b) of Corollary
2.3 if we take part (d) into account.                                      §

  Corollary 2.5. If V is a finite-dimensional vector space and U is a vector
subspace of V , then U is finite-dimensional, and dim U ≤ dim V .
   PROOF. Let {v1 , . . . , vm } be a basis of V . According to Proposition 2.2, any
linearly independent set in U has ≤ m elements, being linearly independent in
V . We can thus choose a maximal linearly independent subset of U with ≤ m
elements, and Proposition 2.1a shows that the result is a basis of U .            §


                    2. Vector Spaces Defined by Matrices

Let A be a member of Mkn (F), thus a k-by-n matrix. The row space of A is the
linear span of the rows of A, regarded as a vector subspace of the vector space of
all n-dimensional row vectors. The column space of A is the linear span of the
columns, regarded as a vector subspace of k-dimensional column vectors. The
null space of A is the vector subspace of n-dimensional column vectors v for
which Av = 0, where Av is the matrix product. The fact that this last space
is a vector subspace follows from the properties A(v1 + v2 ) = Av1 + Av2 and
A(cv) = c(Av) of matrix multiplication.
                             2. Vector Spaces Defined by Matrices                           39

   We can use matrix multiplication to view the matrix A as defining a function
v 7→ Av of Fn to Fk . This function satisfies the properties just listed,

              A(v1 + v2 ) = Av1 + Av2              and       A(cv) = c(Av),

and we shall consider further functions with these two properties starting in the
next section. In terms of this function, the null space of A is the set in the domain
Fn mapped to 0. Because of these same properties and because the product Ae j
of A and the j th standard basis vector e j in Fn is the j th column of A, the column
space of A is the image of the function v 7→ Av as a subset of the range Fk .

   Theorem 2.6. If A is in Mkn (F), then

     dim(column space(A)) + dim(null space(A)) = #(columns of A) = n.

   PROOF. Corollary 2.5 says that the null space is finite-dimensional, being a
vector subspace of Fn , and Corollary 2.3c shows that the null space has a basis,
say {v1 , . . . , vr }. By Corollary 2.3b we can adjoin vectors vr+1 , . . . , vn so that
{v1 , . . . , vn } is a basis of Fn . If v is in Fn , we can expand v in terms of this basis
as v = c1 v1 + · · · + cn vn . Application of A gives

Av = A(c1 v1 + · · · + cn vn ) = c1 Av1 + · · · + cr Avr + cr+1 Avr+1 + · · · + cn Avn
                               = cr+1 Avr+1 + · · · + cn Avn .

Therefore the vectors Avr+1 , . . . , Avn span the column space.
   Let us see that they form a basis for the column space. Thus suppose that
cr+1 Avr+1 + · · · + cn Avn = 0. Then A(cr+1 vr+1 + · · · + cn vn ) = 0, and
cr+1 vr+1 + · · · + cn vn is in the null space. Since {v1 , . . . , vr } is a basis of the null
space, we have

                     cr+1 vr+1 + · · · + cn vn = a1 v1 + · · · + ar vr

for suitable scalars a1 , . . . , ar . Therefore

              (−a1 )v1 + · · · + (−ar )vr + cr+1 vr+1 + · · · + cn vn = 0.

Since v1 , . . . , vn are linearly independent, all the c j are 0. We conclude that
Avr+1 , . . . , Avn are linearly independent and therefore form a basis of the column
space.
   As a result, we have established in the identity r + (n − r) = n that n − r
can be interpreted as dim(column space(A)) and that r can be interpreted as
dim(null space(A)). The theorem follows.                                            §
40                          II. Vector Spaces over Q, R, and C

   Proposition 2.7. If A is in Mkn (F), then each elementary row operation on A
preserves the row space of A.
   PROOF. Let the rows of A be r1 , . . . , rk . Their span is unchanged if we
interchange two of them or multiply one of them by a nonzero scalar. If we
replace the row ri by ri + cr j with j 6= i, then the span is unchanged since
                     ai ri + a j r j = ai (ri + cr j ) + (a j − ai c)r j
shows that any finite linear combination of the old rows is a finite linear combi-
nation of the new rows and since
                     bi (ri + cr j ) + b j r j = bi ri + (bi c + b j )r j
shows the reverse.                                                                   §

     Theorem 2.8. If A in Mkn (F) has reduced row-echelon form R, then
      dim(row space(A)) = dim(row space(R))
                        = #(nonzero rows of R) = #(corner variables of R)
and
 dim(null space(A)) = dim(null space(R)) = #(independent variables of R).
   PROOF. The first equality in the first conclusion is immediate from Proposition
2.7, and the last equality of that conclusion is known from the method of row
reduction. To see the middle equality, we need to see that the nonzero rows of R
are linearly independent. Let these rows be r1 , . . . , rt . For each i with 1 ≤ i ≤ t,
the index of the first nonzero entry of ri was denoted by j (i) in Section I.5. That
entry has to be 1, and the other rows have to be 0 in that entry, by definition of
reduced row-echelon form. If a finite linear combination c1r1 + · · · + ct rt is 0,
then inspection of the j (i)th entry yields the equality ci = 0, and thus we conclude
that all the coefficients are 0. This proves the desired linear independence.
   The first equality in the second conclusion is by the solution procedure for ho-
mogeneous systems of equations in Section I.5; the set of solutions is unchanged
by each row operation. To see the second equality, we recall that the form of the
solution is as a finite linear combination of specific vectors, the coefficients being
the independent variables. What the second equality is asserting is that these
vectors form a basis of the space of solutions. We are thus to prove that they are
linearly independent. Let the independent variables be certain x j ’s, and let the
corresponding vectors be v j ’s. Then we know that the vector v j has j th entry 1
and that all the other vectors have j th entry 0. If a finite linear combination of the
vectors is 0, then examination of the j th entry shows that the j th coefficient is 0.
The result follows.                                                                  §
                         2. Vector Spaces Defined by Matrices                   41

  Corollary 2.9. If A is in Mkn (F), then

      dim(row space(A)) + dim(null space(A)) = #(columns of A) = n.

  PROOF. We add the two formulas in Theorem 2.8 and see that

                   dim(row space(A)) + dim(null space(A))

equals the sum #(corner variables of R) + #(independent variables of R). Since
all variables are corner variables or independent variables, this sum is n, and the
result follows.                                                                  §

  Corollary 2.10. If A is in Mkn (F), then

                 dim(row space(A)) = dim(column space(A)).

   REMARK. The common value of the dimension of the row space of A and the
dimension of the column space of A is called the rank of A. Some authors use
the separate terms “row rank” and “column rank” for the two sides, and then the
result is that these integers are equal.
  PROOF. This follows by comparing Theorem 2.6 and Corollary 2.9.               §

   Although the above results may seem to have an abstract sound at first, methods
of calculation for all the objects in question have quietly been carried along in
the proofs, with everything rooted in the method of row reduction. All the proofs
have in effect already been given that these methods of calculation do what they
are supposed to do. If A is in Mkn (F), the transpose of A, denoted by At , is the
member of Mnk (F) with entries (At )i j = A ji . In particular, the transpose of a
row vector is a column vector, and vice versa.

   METHODS OF CALCULATION.
   (1) Basis of the row space of A. Row reduce A, and use the nonzero rows of
the reduced row-echelon form.
   (2) Basis of the column space of A. Transpose A, compute a basis of the row
space of At by Method 1, and transpose the resulting row vectors into column
vectors.
   (3) Basis of the null space of A. Use the solution procedure for Av = 0 given
in Section I.5. The set of solutions is given as all finite linear combinations of
certain column vectors, the coefficients being the independent variables. The
column vectors that are obtained form a basis of the null space.
   (4) Basis of the linear span of the column vectors v1 , . . . , vn . Arrange the
columns into a matrix A. Then the linear span is the column space of A, and a
basis can be determined by Method 2.
42                              II. Vector Spaces over Q, R, and C

    (5) Extension of a linearly independent set {v1 , . . . , vr } of column vectors in
Fn to a basis of Fn . Arrange the columns into a matrix, transpose, and row reduce.
Adjoin additional row vectors, one for each independent variable, as follows: if
x j is an independent variable, then the row vector corresponding to x j is to be 1
in the j th entry and 0 elsewhere. Transpose these additional row vectors so that
they become column vectors, and these are vectors that may be adjoined to obtain
a basis.
    (6) Shrinking of a set {v1 , . . . , vr } of column vectors to a subset that is a
basis for the linear span of {v1 , . . . , vr }. For each i with 0 ≤ i ≤ r, compute
di = dim(span{v1 , . . . , vr }). Retain vi for i ∏ 0 if di−1 < di , and discard vi
otherwise.


                                       3. Linear Maps

In this section we discuss linear maps, first in the setting of functions from Fn to
Fk and then in the setting of functions between two vector spaces over F. Much of
the discussion will center on making computations for such functions by means
of matrices.
   We have seen that any k-by-n matrix A defines a function L : Fn to Fk by
L(v) = Av and that this function satisfies
                                 L(u + v) = L(u) + L(v),
                                    L(cv) = cL(v),
for all u and v in Fn and all scalars c. A function L : Fn → Fk satisfying these
two conditions is said to be linear, or F linear if the scalars need emphasizing.
Traditional names for such functions are linear maps, linear mappings, and
linear transformations.3 Thus matrices yield linear maps. Here is a converse.

   Proposition 2.11. If L : Fn → Fk is a linear map, then there exists a unique
k-by-n matrix A such that L(v) = Av for all v in Fn .
     REMARK. The proof will show how to obtain the matrix A.
   PROOF. For 1 ≤ j ≤ n, let e j be the j th standard basis vector of Fn , having 1 in
its j th entry and 0’s elsewhere, and let the j th column of A be the k-dimensional
column vector L(e j ). If v is the column vector (c1 , c2 , . . . , cn ), then
                           ° Pn          ¢ Pn
                 L(v) = L     j=1 c j e j =   j=1 L(c j e j )
                         Pn                 Pn
                      = j=1 c j L(e j ) = j=1 c j ( j th column of A).
     3 Theterm linear function is particularly appropriate when the emphasis is on the fact that a
certain function is linear. The term linear operator is used also, particularly when the context has
something to do with analysis.
                                    3. Linear Maps                                   43

If L(v)i denotes the i th entry of the column vector L(v), this equality says that
                                         P
                                L(v)i = nj=1 c j Ai j .
The right side is the i th entry of Av, and hence L(v) = Av. This proves existence.
For uniqueness we observe from the formula L(e j ) = Ae j that the j th column of
A has to be L(e j ) for each j, and therefore A is unique.                       §

  In the special case of linear maps from Fn to Fk , the proof shows that two linear
maps that agree on the members of the standard basis are equal on all vectors.
We shall give a generalization of this fact as Proposition 2.13 below.

   EXAMPLE 1. Let L : R2 → R2 be rotation about the origin counterclockwise
through the angle θ. Taking L to be defined geometrically, one finds from the
       ≥ ¥ rule
parallelogram  ≥ for    ¥ addition of≥vectors
                                           ¥    ≥that L ¥is linear. Computation shows
                                                  − sin θ
that L 10 = cos       θ
                  sin θ
                          and   that   L 0
                                         1
                                             =      cos θ
                                                           . Applying Proposition 2.11
                       ≥ forming the
and the prescription for               ¥ matrix A given in the proof of the proposition,
                         cos θ − sin θ
we see that L(v) = sin θ cos θ v for all v in R2 .

   We can add two linear maps L : Fn → Fk and M : Fn → Fk by adding their
values at corresponding points: (L + M)(v) = L(v) + M(v). In addition, we
can multiply a linear map by a scalar by multiplying its values. Then L + M
and cL are linear, and it follows that the set of linear maps from Fn to Fk is a
vector subspace of the vector space of all functions from Fn to Fk , hence is itself
a vector space. The customary notation for this vector space is HomF (Fn , Fk );
the symbol Hom refers to the validity of the rule L(u + v) = L(u) + L(v), and
the subscript F refers to the validity of the additional rule L(cv) = cL(v) for all
c in F.
   If L corresponds to the matrix A and M corresponds to the matrix B, then
L + M corresponds to A + B and cL corresponds to c A. The next proposition
shows that composition of linear maps corresponds to multiplication of matrices.

    Proposition 2.12. Let L : Fn → Fm be the linear map corresponding to an
m-by-n matrix A, and let M : Fm → Fk be the linear map corresponding to a
k-by-m matrix B. Then the composite function M ◦ L : Fn → Fk is linear, and
it corresponds to the k-by-n matrix B A.
   PROOF. The function M ◦ L satisfies (M ◦ L)(u + v) = M(L(u + v)) =
M(Lu + Lv) = M(Lu) + M(Lv) = (M ◦ L)(u) + (M ◦ L)(v), and similarly it
satisfies (M ◦ L)(cv) = c(M ◦ L)(v). Therefore it is linear. The correspondence
of linear maps to matrices and the associativity of matrix multiplication together
give (M ◦ L)(v) = M(L(v)) = (B)(Lv) = B(Av) = (B A)v, and therefore
M ◦ L corresponds to B A.                                                       §
44                                II. Vector Spaces over Q, R, and C

   Now let us enlarge the setting for our discussion, treating arbitrary linear maps
L : U → V between vector spaces over F. We say that L : U → V is linear, or
F linear, if
                                   L(u + v) = L(u) + L(v),
                                      L(cv) = cL(v),
for all u and v in U and all scalars c. As with the special case that U = Fn and
V = Fk , linear functions are called linear maps, linear mappings, and linear
transformations. The set of all linear maps L : U → V is a vector space over F
and is denoted by HomF (U, V ). The following result is fundamental in working
with linear maps.

   Proposition 2.13. Let U and V be vector spaces over F, and let 0 be a basis
of U . Then to each function ` : 0 → V corresponds
                                            Ø        one and only one linear
map L : U → V whose restriction to 0 has L Ø0 = `.
     REMARK. We refer to L as the linear extension of `.
   PROOF. Suppose that ` : 0 → V is given. Since 0 is a basis of U , each
element of U has a unique expansion as a finite linear combination of members
of 0. Say that u = cα1 u α1 + · · · + cαr u αr . Then the requirement of linearity
on L forces L(u) = L(cα1 u α1 + · · · + cαr u αr ) = cα1 L(u α1 ) + · · · + cαr L(u αr ),
and therefore L is uniquely determined. For existence, define L by this formula.
Expanding u and v in this way, we readily see that L(u + v) = L(u) + L(v) and
L(cu) = cL(u). Therefore ` has a linear extension.                                    §

   The definition of linearity and the proposition just proved make sense even if
U and V are infinite-dimensional, but our objective for now will be to understand
linear maps in terms of matrices. Thus, until further notice at a point later in this
section, we shall assume that U and V are finite-dimensional. Remarks about the
infinite-dimensional case appear in Section 9.
   Since U and V are arbitrary finite-dimensional vector spaces, we no longer
have standard bases at hand, and thus we have no immediate way to associate a
matrix to a linear map L : U → V . What we therefore do is fix arbitrary bases
of U and V and work with them. It will be important to have an enumeration of
each of these bases, and we therefore let
                                         0 = (u 1 , . . . , u n )
and                                      1 = (v1 , . . . , vk )
be ordered bases of U and V , respectively.4 If a member u of U may be expanded
     4 Thenotation (u 1 , . . . , u n ) for an ordered basis, with each u j equal to a vector, is not to be
confused with the condensed notation (c1 , . . . , cn ) for a single column vector, with each c j equal to
a scalar.
                                       3. Linear Maps                                       45

in terms of 0 as u = c1 u 1 + · · · + cn u n , we write
                                                
                                  µ ∂            c1
                                     u           .. 
                                     0
                                          =        . ,
                                                 cn
calling this the column vector expressing u in the ordered
                                                      µ       ∂ 0. Using our
                                                             basis
                                                         L
linear map L : U → V , let us define a k-by-n matrix             by requiring that
                                                        10
the                              µ    ∂             µ         ∂
                                    L                 L(u j )
                  j th column of           be                   .
                                   10                   1
The positions in which the ordered bases 1 and 0 are listed in the notation is
important here; the range basis is to the left of the domain basis.5

    EXAMPLE 2. Let V be the space of all complex-valued solutions on R of the
differential equation y 00 (t) = y(t). Then V is a vector subspace of functions,
hence is a vector space in its own right. It is known that V is 2-dimensional with
solutions c1 et + c2 e−t . If y(t) is a solution, then differentiation of the equation
shows that y 0 (t) is another solution. In other words, the derivative operator d/dt is
a linear map from V to itself. One ordered basis of V is 0 = (et , e−t ), and another
is 1 µ= (cosh∂t, sinh t), where cosh t = 12 (et + e−t ) and sinh t = 12 (et − e−t ). To
       d/dt
find            , we need to express (d/dt)(et ) and (d/dt)(e−t ) in terms of cosh t
        10
and sinh t. We have
          µ               ∂ µ t∂ µ                          ∂ µ ∂
            (d/dt)(et )          e          cosh t + sinh t        1
                            =         =                      =
                  1              1                 1               1
        µ             −t
                          ∂ µ −t ∂ µ                             ∂ µ         ∂
          (d/dt)(e )             −e            − cosh t + sinh t         −1
and                         =             =                         =          .
                 1                 1                   1                   1
            µ         ∂ µ            ∂
              d/dt          1 −1
Therefore                =            .
                10          1      1

   Theorem 2.14. If L : U → V is a linear map between finite-dimensional
vector spaces over F and if 0 and 1 are ordered bases of U and V , respectively,
then                       µ      ∂ µ        ∂µ ∂
                             L(u)         L      u
                                    =
                              1          10      0
for all u in U .
   5 This order occurs in a number of analogous situations in mathematics and has the effect of

keeping the notation reasonably consistent with the notation for composition of functions.
46                          II. Vector Spaces over Q, R, and C

   PROOF. The two sides of the identity in question are linear in u, and Proposition
2.13 shows that it is enough to prove the identity for the members u of some
ordered basis of U . We choose 0 as this ordered basis. For the basis           µ vector
                                                                                    ∂
                                                                                 uj
u equal to the j th member u j of 0, use of the definition shows that                 is
                                                                                  0
µ column
the    ∂       vector e j that is 1 µin the∂j th entry and is 0 elsewhere.
                                                                       µ The ∂product
    L                                   L                                L(u j )
         e j is the j th column of           , which was defined to be            . Thus
  10                                   10                                  1
the identity in question is valid for u j , and the theorem follows.                  §

   If we take into account Proposition 2.13, saying that linear maps on U arise
uniquely from arbitrary functions on a basis of U , then Theorem 2.14 supplies
a one-one correspondence of linear maps L from U to V with matrices A of
the appropriate size, onceµ we ∂ fix ordered bases in the domain and range. The
                             L
correspondence is L ↔              .
                            10
   As in the special case with linear maps between spaces of column vectors,
this correspondence respects addition and scalar multiplication. Theorem 2.14
implies that under this correspondence, the image of L corresponds to the column
space of A. It implies also that the vector subspace of the domain U with L(u) =
0, which is called the kernel of L and is sometimes denoted by ker L, corresponds
to the null space of A. The kernel of L has the important property that

               the linear map L is one-one if and only if ker L = 0.

Another important property comes from this association of kernel with null space
and of image with column space. Namely, we apply Theorem 2.6, and we obtain
the following corollary.

  Corollary 2.15. If L : U → V is a linear map between finite-dimensional
vector spaces over F, then

             dim(domain(L)) = dim(kernel(L)) + dim(image(L)).

  The next result says that composition corresponds to matrix multiplication
under the correspondence of Theorem 2.14.

   Theorem 2.16. Let L : U → V and M : V → W be linear maps between
finite-dimensional vector spaces, and let 0, 1, and ƒ be ordered bases of U ,
V , and W . Then the composition M L is linear, and the corresponding matrix is
given by                 µ     ∂ µ         ∂µ      ∂
                           ML           M        L
                                  =                   .
                           ƒ0          ƒ1      10
                                    3. Linear Maps                                   47

   PROOF. If u is in U , three applications of Theorem 2.14 and one application
of associativity of matrix multiplication give
  µ        ∂µ       ∂       µ      ∂ µ    ∂µ      ∂
      ML        u           M L(u)      M    L(u)
                        =           =
      ƒ0        0             ƒ        ƒ1     1
                          µ    ∂∑µ     ∂µ ∂∏ ∑µ        ∂µ    ∂∏µ ∂
                            M        L    u         M      L    u
                        =                     =                    .
                            ƒ1      10    0         ƒ1    10    0

                   th
Taking
   µ u to∂ be the j member of 0, weµsee from
                                          ∂ µ this equation
                                                   ∂        that the j th column
     ML                               M         L
of          equals the j th column of                . Since j is arbitrary, the
     ƒ0                               ƒ1      10
theorem follows.                                                               §

   A computational device that appears at first to be only of theoretical interest
and then, when combined with other things, becomes of practical interest, is to
                                                    µ of∂
change one of the ordered bases in computing the matrix   a linear map. A handy
                                                       I
device for this purpose is a change-of-basis matrix          since Theorem 2.16
      µ     ∂ µ        ∂µ       ∂                     10
         L           I        L
gives          =                  .
        10          10       00

   EXAMPLE 2, CONTINUED. Let L be d/dt as a linear map carrying the space of
solutions of y 00 (t)µ = y(t)
                            ∂ to itself,
                                 µ       with∂ 0 = (et , e−t ) and 1 = (cosh t, sinh t)
                       d/dt         1      0
as before. Then               =                . Since et = cosh t + sinh t and e−t =
                   µ    00
                         ∂ µ        0    −1∂                                 µ     ∂
                       I         1       1                                      L
cosh t − sinh t,            =                by inspection. The product is            =
µ      ∂µ          ∂ 10µ         1∂ −1                                         10
    I      d/dt           1 −1
                      =             , a result we found before with a little more effort
  10        00            1     1
by computing matters directly.

   Often in practical applications the domain and the range are the same vector
space, the domain’s ordered basis equals the range’s ordered basis, and the matrix
of a linear map is known in this ordered basis. The problem is to determine the
matrix when the ordered basis is changed in both domain and range—changed in
                                              ∂ andµrange∂are the same. This time
such a way that the ordered bases in theµdomain
                                           I           I
we use two change-of-basis matrices             and         , but these are related.
       µ     ∂µ       ∂ µ        ∂       10           01
          I        I          I
Since                   =           = I , the two matrices are the inverses of one
         01      10          00
48                         II. Vector Spaces over Q, R, and C


       ∂ Thus,
another.
µ                    ∂ for matrix algebra, the problem is to compute just one of
              µ except
    I             I
          and          .
   01            10
   Normally one of these two matrices can be written down by inspection. For
example, if we are working with a linear map from a space of column vectors
to itself, one ordered basis of interest is the standard ordered basis 6. Another
ordered basis 1 might be determined by special features of the linear map. In
                    µ of∂1 are given as column vectors, hence are expressed in
this case the members
                         I
terms of 6. Thus             can be written by inspection. We shall encounter this
                       61
situation later in this chapter when we use “eigenvectors” in order to understand
linear maps better. Here is an example, but without eigenvectors.

   EXAMPLE 1, CONTINUED. We saw that rotation L counterclockwise≥ ≥ ¥ ≥ ¥about
                                                                             ¥
                                                                    1     0
the origin in R2 is given in the standard ordered basis 6 =         0
                                                                       ,  1
                                                                               by
µ      ∂ ≥                  ¥
   L          cos θ − sin θ
         = sin θ cos θ . Let us compute the matrix of L in the ordered basis 1 =
  66                                                       µ       ∂ µ         ∂
≥≥ ¥ ≥ ¥¥
    1     1                                                    I          1 1
       , 1 . The easy change-of-basis matrix to form is               =          .
    0                                                        61           0 1
Hence
 µ     ∂ µ          ∂µ        ∂µ    ∂ µ       ∂−1 µ                   ∂µ      ∂
    L           I           L     I     1 1         cos θ − sin θ        1 1
         =                           =                                          ,
   11         16         66      61     0 1         sin θ      cos θ     0 1
and the problem is reduced to one of matrix algebra.

   Our computations have proved the following proposition, which, as we shall
see later, motivates
               µ     much
                     ∂    of Chapter V. The matrix C in the statement of the
                  I
proposition is        .
                 01

   Proposition 2.17. Let L : V → V be a linear map on a finite-dimensional
vector space, and let A be the matrix of L relative to an ordered basis 0 (in domain
and range). Then the matrix of L in any other ordered basis 1 is of the form
C −1 AC for some invertible matrix C depending on 1.
   REMARK. If A is a square matrix, any square matrix of the form C −1 AC is said
to be similar to A. It is immediate that “is similar to” is an equivalence relation.

    Now let us return to the setting in which our vector spaces are allowed to be
infinite-dimensional. Two vector spaces U and V are said to be isomorphic if
there is a one-one linear map of U onto V . In this case, the linear map in question
is called an isomorphism, and one often writes U ∼   = V.
                                    3. Linear Maps                                  49

   Here is a finite-dimensional example: If U is n-dimensional with an ordered
basis 0 and V is k-dimensional with an ordered basis 1, then HomF (U, V ) is
isomorphic to Mnk (F)µby the∂ linear map that carries a member L of HomF (U, V )
                         L
to the k-by-n matrix          .
                        10
   The relation “is isomorphic to” is an equivalence relation. In fact, it is reflexive
since the identity map exhibits U as isomorphic to itself. It is transitive since
Theorem 2.16 shows that the composition M L of two linear maps L : U → V
and M : V → W is linear and since the composition of one-one onto functions
is one-one onto. To see that it is symmetric, we need to observe that the inverse
function L −1 of a one-one°onto linear map L ¢: U → V is linear. To see this
linearity, we observe that L° L −1 (v1 ) + L −1
                                              ¢ (v2 ) = L(L −1 (v1 )) + L(L −1 (v2 )) =
v1 + v2 = I (v1 + v2 ) = L L −1 (v1 + v2 ) . Since L is one-one,
                       L −1 (v1 ) + L −1 (v2 ) = L −1 (v1 + v2 ).
Similarly the facts that L(L −1 (cv)) = cv = cL(L −1 v) = L(c(L −1 (v))) and that
L is one-one imply that
                               L −1 (cv) = c(L −1 (v)),
and hence L −1 is linear. Thus “is isomorphic to” is indeed an equivalence relation.
    The vector spaces over F are partitioned, according to the basic result about
equivalence relations in Section A2 of the appendix, into equivalence classes.
Each member of an equivalence class is isomorphic to all other members of that
class and to no member of any other class.
    An isomorphism preserves all the vector-space structure of a vector space.
Spanning sets are mapped to spanning sets, linearly independent sets are mapped
to linearly independent sets, vector subspaces are mapped to vector subspaces,
dimensions of subspaces are preserved, and so on. In other words, for all purposes
of abstract vector-space theory, isomorphic vector spaces may be regarded as the
same. Let us give a condition for isomorphism that might at first seem to trivialize
all vector-space theory, reducing it to a count of dimensions, but then let us return
to say why this result is not to be considered as so important.
    Proposition 2.18. Two finite-dimensional vector spaces over F are isomorphic
if and only if they have the same dimension.
   PROOF. If a vector space U is isomorphic to a vector space V , then the
isomorphism carries any basis of U to a basis of V , and hence U and V have the
same dimension. Conversely if they have the same dimension, let (u 1 , . . . , u n )
be an ordered basis of U , and let (v1 , . . . , vn ) be an ordered basis of V . Define
`(u j ) = v j for 1 ≤ j ≤ n, and let L : U → V be the linear extension of `
given by Proposition 2.13. Then L is linear, one-one, and onto, and hence U is
isomorphic to V .                                                                    §
50                         II. Vector Spaces over Q, R, and C

   The proposition does not mean that one should necessarily be eager to make the
identification of two vector spaces that are isomorphic. An important distinction is
the one between “isomorphic” and “isomorphic via a canonically constructed µ lin- ∂
                                                                               L
ear map.” The isomorphism of linear maps with matrices given by L 7→
                                                                             10
is canonical since no choices are involved once 0 and 1 have been specified.
This is a useful isomorphism because we can track matters down and use the
isomorphism to make computations. On the other hand, it is not very useful to
say merely that HomF (U, V ) and Mkn (F) are isomorphic because they have the
same dimension.
   What tends to happen in practice is that vector spaces in applications come
equipped with additional structure—some rigid geometry, or a multiplication
operation, or something else. A general vector-space isomorphism has little
chance of having any connection to the additional structure and thereby of being
very helpful. On the other hand, a concrete isomorphism that is built by taking
this additional structure into account may indeed be useful.
   In the next section we shall encounter an example of an additional structure
that involves neither a rigid geometry nor a multiplication operation. We shall
introduce the “dual” V 0 of a vector space V , and we shall see that V and V 0 have
the same dimension if V is finite-dimensional. But no particular isomorphism
of V with V 0 is singled out as better than other ones, and it is wise not to try
to identify these spaces. By contrast, the double dual V 00 of V , which too will
be constructed in the next section, will be seen to be isomorphic to V in the
finite-dimensional case via a linear map ∂ : V → V 00 that we define explicitly.
The function ∂ is an example of a canonical isomorphism that we might want to
exploit.


                                  4. Dual Spaces

Let V be a vector space over F. A linear functional on V is a linear map from
V into F. The space of all such linear maps, as we saw in Section 3, is a vector
space. We denote it by V 0 and call it the dual space of V .
    The development of Section 3 tells us right away how to compute the dual
space of the space of column vectors Fn . If 6 is the standard ordered basis of Fn
and if 1 denotes the basis of F consisting of the scalar 1, then we can associate to
a linear functional v 0 on Fn its matrix
                   µ 0 ∂
                        v
                             = ( v 0 (e1 ) v 0 (e2 ) · · · v 0 (en ) ) ,
                       16

which is an n-dimensional row vector. The operation of v 0 on a column vector
                                             4. Dual Spaces                                              51
   √ x1 !
      .
v = .. is given by Theorem 2.14. Namely, v 0 (v) is a multiple of the scalar 1,
          xn
and the theorem tells us how to compute this multiple:
                                                                                              
    µ 0 ∂ µ 0 ∂ x1                                                                           x1
      v (v)         v      ...  = ( v 0 (e1 ) v 0 (e2 ) · · ·                                .
              =                                                                 v 0 (en ) )  ..  .
         1         16
                           xn                                                                xn
Thus the space of all linear functionals on Fn may be identified with the space of
all n-dimensional row vectors, and the effect of the row vector on a column vector
is given by matrix multiplication. Since the standard ordered basis of Fn and the
basis 1 of F are singled out as special, this identification is actually canonical,
and it is thus customary to make this identification without further comment.
    For a more general vector space V , no natural way of writing down elements
of V 0 comes to mind. Indeed, if a concrete V is given, it can help considerably
in understanding V to have an identification of V 0 that does not involve choices.
For example, in real analysis one proves in a suitable infinite-dimensional setting
that a (continuous) linear functional on the space of integrable functions is given
by integration with a bounded function, and that fact simplifies the handling of
the space of integrable functions.
    In any event, the canonical identification of linear functionals that we found
for Fn does not work once we pass to a more general finite-dimensional vector
space V . To make such an identification in the absence of additional structure,
we first fix an ordered basis (v1 , . . . , vn ) of V . If we do so, then V 0 is indeed
identified with the space of n-dimensional row vectors. The members of V 0 that
correspond to the standard basis of row vectors, i.e., the row vectors that are 1
in one entry and are 0 elsewhere, are of special interest. These are the linear
functionals vi0 such that
                                    vi0 (v j ) = δi j ,
where δi j is the Kronecker delta. Since these standard row vectors form a basis of
the space of row vectors, (v10 , . . . , vn0 ) is an ordered basis of V 0 . If the members
of the ordered basis (v1 , . . . , vn ) are permuted in some way, the members of
(v10 , . . . , vn0 ) are permuted in the same way. Thus the basis {v10 , . . . , vn0 } depends
only on the basis {v1 , . . . , vn }, not on the enumeration.6 The basis {v10 , . . . , vn0 }
is called the dual basis of V relative to {v1 , . . . , vn }. A consequence of this
discussion is the following result.

  Proposition 2.19. If V is a finite-dimensional vector space with dual V 0 , then
  0
V is finite-dimensional with dim V 0 = dim V .
      6 Althoughthe enumeration is not important, more structure is present here than simply an
association of an unordered basis of V 0 to an unordered basis of V . Each member of {v10 , . . . , vn0 } is
matched to a particular member of {v1 , . . . , vn }, namely the one on which it takes the value 1.
52                            II. Vector Spaces over Q, R, and C

    Linear functionals play an important role in working with a vector space. To
understand this role, it is helpful to think somewhat geometrically. Imagine the
problem of describing a vector subspace of a given vector space. One way of
describing it is from the inside, so to speak, by giving a spanning set. In this
case we end up by describing the subspace in terms of parameters, the parameters
being the scalar coefficients when we say that the subspace is the set of all finite
linear combinations of members of the spanning set. Another way of describing
the subspace is from the outside, cutting it down by conditions imposed on its
elements. These conditions tend to be linear equations, saying that certain linear
maps on the elements of the subspace give 0. Typically the subspace is then
described as the intersection of the kernels of some set of linear maps. Frequently
these linear maps will be scalar-valued, and then we are in a situation of describing
the subspace by a set of linear functionals.
    We know that every vector subspace of a finite-dimensional vector space V
can be described from the inside in this way; we merely give all its members. A
statement with more content is that we can describe it with finitely many members;
we can do so because we know that every vector subspace of V has a basis.
    For linear functionals really to be useful, we would like to know a correspond-
ing fact about describing subspaces from the outside—that every vector subspace
U of a finite-dimensional V can be described as the intersection of the kernels of
a finite set of linear functionals. To do so is easy. We take a basis of the vector
subspace U , say {v1 , . . . , vr }, extend it to a basis of V by adjoining vectors
vr+1 , . . . , vn , and form the dual basis {v10 , . . . , vn0 } of V 0 . The subspace U is then
described as the set of all vectors v in V such that v j0 (v) = 0 for r + 1 ≤ j ≤ n.
The following proposition expresses this fact in ways that are independent of the
choice of a basis. It uses the terminology annihilator of U , denoted by Ann(U ),
for the vector subspace of all members v 0 of V 0 with v 0 (u) = 0 for all u in U .

  Proposition 2.20. Let V be a finite-dimensional vector space, and let U be a
vector subspace of V . Then
     (a) dim U + dim Ann(U ) = dim V ,
     (b) every linear functional on U extends to a linear functional on V ,
     (c) whenever v0 is a member of V that is not in U , there exists a linear
         functional on V that is 0 on U and is 1 on v0 .

    PROOF. We retain the notation above, writing {v1 , . . . , vr } for a basis of U ,
vr+1 , . . . , vn for vectors that are adjoined to form a basis of V , and {v10 , . . . , vn0 }
                                                     0
for the dual basis of V 0 . For (a), we check that {vr+1 , . . . , vn0 } is a basis of Ann(U ).
It is enough to see that they span Ann(U ). These linear functionals are 0 on every
member of the basis {v1 , . . . , vr } of U and hence are in Ann(U ). On the other
hand, if v 0 is a member of Ann(U ), we can certainly write v 0 = c1 v10 + · · · + cn vn0
                                            4. Dual Spaces                                             53

for some scalars c1 , . . . , cn . Since v 0 is 0 on U , we must have v 0 (vi ) = 0 for
i ≤ r. Since v 0 (vi ) = ci , we obtain ci = 0 for i ≤ r. Therefore v 0 is a linear
                    0
combination of vr+1      , . . . , vn0 , and (a) is proved. Ø            Ø
    For (b), let us observe that the restrictions v10 ØU , . . . , vr0 ØU form the dual basis
of U 0 relative
           Ø to the basis
                                                              0       0
                              Ø {v1 , . . . , vr } of U . If u is in U , we0 can therefore write
          0Ø                 0Ø
u = c1 v1 U +· · ·+cr vr U for some scalars c1 , . . . , cr . Then v = c1 v10 +· · ·+cr vr0
  0

is the required extension of u 0 to all of V .
    For (c), we use a special choice of basis of V in the argument above. Namely,
we still take {v1 , . . . , vr } to be a basis of U , and then we let vr+1 = v0 . Finally
                                                                                     0
we adjoin vr+2 , . . . , vn to obtain a basis {v1 , . . . , vn } of V . Then vr+1        has the
required property.                                                                            §

   If L : U → V is a linear map between finite-dimensional vector spaces, then
the formula
                   (L t (v 0 ))(u) = v 0 (L(u))          for u ∈ U and v 0 ∈ V 0
defines a linear map L t : V 0 → U 0 . The linear map L t is called the contragre-
dient of L. The matrix of the contragredient of L is the transpose of the matrix
of L in the following sense.7

   Proposition 2.21. Let L : U → V be a linear map between finite-dimensional
vector spaces, let L t : V 0 → U 0 be its contragredient, let 0 and 1 be respective
ordered bases of U and V , and let 0 0 and 10 be their dual ordered bases. Then
                               µ t ∂ µ             ∂
                                   L            L
                                          =          .
                                 0 0 10        10
   PROOF. Let 0 = (u 1 , . . . , u n ), 1 = (v1 , . . . , vk ), 0 0 = (u 01 , . . . , u 0n ), and
1 = (v10 , . . . , vk0 ). Write B and A for the respective matrices in the formula
  0
                                                P                                P
in question. The equations L(u j ) = ik0 =1 Ai 0 j vi 0 and L t (vi0 ) = nj0 =1 Bj 0 i u 0j 0
imply that
                                              ° Pk                     ¢
                          vi0 (L(u j )) = vi0      i 0 =1 Ai 0 j vi 0 = Ai j
                                            P
and                       L t (vi0 )(u j ) = nj0 =1 Bj 0 i u 0j 0 (u j ) = Bji .

Therefore Bji = L t (vi0 )(u j ) = vi0 (L(u j )) = Ai j , as required.                                 §
    7 A general principle is involved in the definition of contragredient once we have a definition of

dual vector space, and we shall see further examples of this principle in the next two sections and in
later chapters: whenever a new systematic construction appears for the objects under study, it is well
to look for a corresponding construction with the functions relating these new objects. In language
to be introduced near the end of Chapter IV, the context for the construction will be a “category,” and
the principle says that it is well to see whether the construction is that of a “functor” on the category.
54                          II. Vector Spaces over Q, R, and C

   With V finite-dimensional, now consider V 00 = (V 0 )0 , the double dual. In the
case that V = Fn , we saw that V 0 could be viewed as the space of row vectors,
and it is reasonable to expect V 00 to involve a second transpose and again be the
space of column vectors. If so, then V gets identified with V 00 . In fact, this is true
in all cases, and we argue as follows. If v is in V , we can define a member ∂(v)
of V 00 by
                   ∂(v)(v 0 ) = v 0 (v)   for v ∈ V and v 0 ∈ V 0 .
This definition makes sense whether or not V is finite-dimensional. The function
∂ is a linear map from V into V 00 called the canonical map of V into V 00 . It is
independent of any choice of basis.

   Proposition 2.22. If V is any finite-dimensional vector space over F, then the
canonical map ∂ : V → V 00 is one-one onto.
    REMARKS. In the infinite-dimensional case the canonical map is one-one but
it is not onto. The proof that it is one-one uses the fact that V has a basis, but
we have deferred the proof of this fact about infinite-dimensional vector spaces
to Section 9. Problem 14 at the end of the chapter will give an example of an
infinite-dimensional V for which ∂ does not carry V onto V 00 . When combined
with the first corollary in Section A6 of the appendix, this example shows that ∂
never carries V onto V 00 in the infinite-dimensional case.
   PROOF. We saw in Section 3 that a linear map ∂ is one-one if and only if
ker ∂ = 0. Thus suppose ∂(v) = 0. Then 0 = ∂(v)(v 0 ) = v 0 (v) for all v 0 . Arguing
by contradiction, suppose v 6= 0. Then we can extend {v} to a basis of V , and the
linear functional v 0 that is 1 on v and is 0 on the other members of the basis will
have v 0 (v) 6= 0, contradiction. We conclude that ∂ is one-one. By Proposition
2.19 we have
                              dim V = dim V 0 = dim V 00 .                        (∗)
Since ∂ is one-one, it carries any basis of V to a linearly independent set in V 00 .
This linearly independent set has to be a basis, by Corollary 2.4 and the dimension
formula (∗).                                                                     §


                          5. Quotients of Vector Spaces

This section constructs a vector space V /U out of a vector space V and a vector
subspace U . We begin with the example illustrated in Figure 2.1. In the vector
space V = R2 , let U be a line through the origin. The lines parallel to U are
of the form v + U = {v + u | u ∈ U }, and we make the set of these lines
into a vector space by defining (v1 + U ) + (v2 + U ) = (v1 + v2 ) + U and
                             5. Quotients of Vector Spaces                         55

c(v + U ) = cv + U . The figure suggests that if we were to take any other line
W through the origin, then W would meet all the lines v + U , and the notion of
addition of lines v + U would correspond exactly to addition in W . Indeed we
can successfully make such a correspondence, but the advantage of introducing
the vector space of all lines v + U is that it is canonical, independent of the kind
of choice we have to make in selecting W . One example of the utility of having a
canonical construction is the ease with which we obtain correspondence of linear
maps stated in Proposition 2.25 below. Other examples will appear later.


                                                         U




               FIGURE 2.1. The vector space of lines v + U in R2
                  parallel to a given line U through the origin.

   Proposition 2.23. Let V be a vector space over F, and let U be a vector
subspace. The relation defined by saying that v1 ∼ v2 if v1 − v2 is in U is an
equivalence relation, and the equivalence classes are all sets of the form v + U
with v ∈ V . The set of equivalence classes V /U is a vector space under the
definitions

                     (v1 + U ) + (v2 + U ) = (v1 + v2 ) + U,
                                 c(v + U ) = cv + U,

and the function q(v) = v + U is linear from V onto V /U with kernel U .
  REMARKS. We say that V /U is the quotient space of V by U . The linear map
q(v) = v + U is called the quotient map of V onto V /U .
   PROOF. The properties of an equivalence relation are established as follows:

               v1 ∼ v1                      because 0 is in U,
       v1 ∼ v2 implies v2 ∼ v1              because U is closed under negatives,
    v1 ∼ v2 and v2 ∼ v3
            together imply v1 ∼ v3          because U is closed under addition.

Thus we have equivalence classes. The class of v1 consists of all vectors v2 such
that v2 − v1 is in U , hence consists of all vectors in v1 + U . Thus the equivalence
classes are indeed the sets v + U .
56                          II. Vector Spaces over Q, R, and C

   Let us check that addition and scalar multiplication, as given in the statement
of the proposition, are well defined. For addition let v1 ∼ w1 and v2 ∼ w2 .
Then v1 − w1 and v2 − w2 are in U . Since U is a vector subspace, the sum
(v1 − w1 ) + (v2 − w2 ) = (v1 + v2 ) − (w1 + w2 ) is in U . Thus v1 + v2 ∼ w1 + w2 ,
and addition is well defined. For scalar multiplication let v ∼ w, and let a scalar
c be given. Then v − w is in U , and c(v − w) = cv − cw is in U since U is a
vector subspace. Hence cv ∼ cw, and scalar multiplication is well defined.
   The vector-space properties of V /U are consequences of the properties for V .
To illustrate, consider associativity of addition. The argument in this case is that
     ((v1 + U ) + (v2 + U )) + (v3 + U ) = ((v1 + v2 ) + U ) + (v3 + U )
          = ((v1 + v2 ) + v3 ) + U = (v1 + (v2 + v3 )) + U
          = (v1 + U ) + ((v2 + v3 ) + U ) = (v1 + U ) + ((v2 + U ) + (v3 + U )).
   Finally the quotient map q : V → V /U given by q(v) = v + U is certainly
linear. Its kernel is {v | v + U = 0 + U }, and this equals {v | v ∈ U }, as asserted.
The map q is onto V /U since v + U = q(v).                                          §
     Corollary 2.24. If V is a vector space over F and U is a vector subspace, then
      (a) dim V = dim U + dim(V /U ),
     (b) the subspace U is the kernel of some linear map defined on V .
   REMARK. The first conclusion is valid even when all the spaces are not finite-
dimensional. For current purposes it is sufficient to regard dim V as +∞ if V is
infinite-dimensional; the sum of +∞ and any dimension as +∞.
   PROOF. Let q be the quotient map. The linear map q meets the conditions of
(b). For (a), take a basis of U and extend to a basis of V . Then the images under
q of the additional vectors form a basis of V /U .                              §
   Quotients of vector spaces allow for the factorization of certain linear maps,
as indicated in Proposition 2.25 and Figure 2.2.
   Proposition 2.25. Let L : V → W be a linear map between vector
spaces over F, let U0 = ker L, let U be a vector subspace of V contained in
U0 , and let q : V → V /U be the quotient map. Then there exists a linear
map L : V /U → W such that L = Lq. It has the same image as L, and
ker L = {u 0 + U | u 0 ∈ U0 }.
                                              L
                                    V     −−−→ W
                                    
                                    
                                   qy             L


                                 V /U
      FIGURE 2.2. Factorization of linear maps via a quotient of vector spaces.
                             5. Quotients of Vector Spaces                        57

   REMARK. One says that L factors through V /U or descends to V /U .
   PROOF. The definition of L has to be L(v + U ) = L(v). This forces Lq = L,
and L will have to be linear. What needs proof is that L is well defined. Thus
suppose v1 ∼ v2 . We are to prove that L(v1 + U ) = L(v2 + U ), i.e., that
L(v1 ) = L(v2 ). Now v1 − v2 is in U ⊆ U0 , and hence L(v1 − v2 ) = 0. Then
L(v1 ) = L(v1 − v2 ) + L(v2 ) = L(v2 ), as required. This proves that L is well
defined, and the conclusions about the image and the kernel of L are immediate
from the definition.                                                        §

   Corollary 2.26. Let L : V → W be a linear map between vector spaces over
F, and suppose that L is onto W and has kernel U . Then V /U is canonically
isomorphic to W .
   PROOF. Take U = U0 in Proposition 2.25, and form L : V /U → W with
L = Lq. The proposition shows that L is onto W and has trivial kernel, i.e., the 0
element of V /U . Having trivial kernel, L is one-one.                          §

   Theorem 2.27 (First Isomorphism Theorem). Let L : V → W be a linear
map between vector spaces over F, and suppose that L is onto W and has kernel
U . Then the map S 7→ L(S) gives a one-one correspondence between
    (a) the vector subspaces S of V containing U and
    (b) the vector subspaces of W .

   REMARK. As in Section A1 of the appendix, we write L(S) and L −1 (T ) to
indicate the direct and inverse images of S and T , respectively.
   PROOF. The passage from (a) to (b) is by direct image under L, and the passage
from (b) to (a) will be by inverse image under L −1 . Certainly the direct image
of a vector subspace as in (a) is a vector subspace as in (b). We are to show that
the inverse image of a vector subspace as in (b) is a vector subspace as in (a) and
that these two procedures invert one another.
   For any vector subspace T of W , L −1 (T ) is a vector subspace of V . In fact, if
v1 and v2 are in L −1 (T ), we can write L(v1 ) = t1 and L(v2 ) = t2 with t1 and t2
in T . Then the equations L(v1 + v2 ) = t1 + t2 and L(cv1 ) = cL(v1 ) = ct1 show
that v1 + v2 and cv1 are in L −1 (T ).
   Moreover, the vector subspace L −1 (T ) contains L −1 (0) = U . Therefore the
inverse image under L of a vector subspace as in (b) is a vector subspace as in
(a). Since L is a function, we have L(L −1 (T )) = T . Thus passing from (b) to
(a) and back recovers the vector subspace of W .
   If S is a vector subspace of V containing U , we still need to see that S =
L −1 (L(S)). Certainly S ⊆ L −1 (L(S)). In the reverse direction let v be in
L −1 (L(S)). Then L(v) is in L(S), i.e., L(v) = L(s) for some s in S. Since L
58                           II. Vector Spaces over Q, R, and C

is linear, L(v − s) = 0. Thus v − s is in ker L = U , which is contained in S
by assumption. Then s and v − s are in S, and hence v is in S. We conclude
that L −1 (L(S)) ⊆ S, and thus passing from (a) to (b) and then back recovers the
vector subspace of V containing U .                                            §

   If V is a vector space and V1 and V2 are vector subspaces, then we write
V1 + V2 for the set V1 + V2 of all sums v1 + v2 with v1 ∈ V1 and v2 ∈ V2 . This
is again a vector subspace of V and is called the sum of V1 and V2 . If we have
vector subspaces V1 , . . . , Vn , we abbreviate ((· · · (V1 + V2 ) + V3 ) + · · · + Vn ) as
V1 + · · · + Vn .

   Theorem 2.28 (Second Isomorphism Theorem). Let M and N be vector
subspaces of a vector space V over F. Then the map n + (M ∩ N ) 7→ n + M is
a well-defined canonical vector-space isomorphism

                             N /(M ∩ N ) ∼
                                         = (M + N )/M.

   PROOF. The function L(n +(M ∩ N )) = n + M is well defined since M ∩ N ⊆
M, and L is linear. The domain of L is {n + (M ∩ N ) | n ∈ N }, and the kernel is
the subset of this where n lies in M as well as N . For this to happen, n must be in
M ∩ N , and thus the kernel is the 0 element of N /(M ∩ N ). Hence L is one-one.
   To see that L is onto (M + N )/M, let (m +n)+ M be given. Then n +(M ∩ N )
maps to n + M, which equals (m + n) + M. Hence L is onto.                         §

  Corollary 2.29. Let M and N be finite-dimensional vector subspaces of a
vector space V over F. Then

                 dim(M + N ) + dim(M ∩ N ) = dim M + dim N .

     PROOF. Theorem 2.28 and two applications of Corollary 2.24a yield

      dim(M + N ) − dim M = dim((M + N )/M)
                          = dim(N /(M ∩ N )) = dim N − dim(M ∩ N ),

and the result follows.                                                                  §


            6. Direct Sums and Direct Products of Vector Spaces

In this section we introduce the direct sum and direct product of two or more
vector spaces over F. When there are only finitely many such subspaces, these
constructions come to the same thing, and we call it “direct sum.” We begin with
the case that two vector spaces are given.
                    6. Direct Sums and Direct Products of Vector Spaces                     59

   We define two kinds of direct sums. The external direct sum of two vector
spaces V1 and V2 over F, written V1 ⊕ V2 , is a vector space obtained as follows.
The underlying set is the set-theoretic product, i.e., the set V1 × V2 of ordered
pairs (v1 , v2 ) with v1 ∈ V1 and v2 ∈ V2 . The operations of addition and scalar
multiplication are defined coordinate by coordinate:

                      (u 1 , u 2 ) + (v1 , v2 ) = (u 1 + v1 , u 2 + v2 ),
                                    c(v1 , v2 ) = (cv1 , cv2 ),

and it is immediate that V1 ⊕ V2 satisfies the defining properties of a vector space.
   If {ai } is a basis of V1 and {b j } is a basis of V2 , then it follows from the formula
(v1 , v2 ) = (v1 , 0) + (0, v2 ) that {(ai , 0)} ∪ {(0, b j )} is a basis of V1 ⊕ V2 . Con-
sequently if V1 and V2 are finite-dimensional, then V1 ⊕ V2 is finite-dimensional
with
                           dim(V1 ⊕ V2 ) = dim V1 + dim V2 .
   Associated to the construction of the external direct sum of two vector spaces
are four linear maps of interest:

      two “projections,”         p1 : V1 ⊕ V2 → V1             with p1 (v1 , v2 ) = v1 ,
                                 p2 : V1 ⊕ V2 → V2             with p2 (v1 , v2 ) = v2 ,
      two “injections,”          i 1 : V1 → V1 ⊕ V2            with i 1 (v1 ) = (v1 , 0),
                                 i 2 : V2 → V1 ⊕ V2            with i 2 (v2 ) = (0, v2 ).

These have the properties that
                                           Ω
                                               I    on Vs if r = s,
                                pr i s =
                                               0    on Vs if r 6= s,
                       i 1 p1 + i 2 p2 = I          on V1 ⊕ V2 .

   The second notion of direct sum captures the idea of recognizing a situation as
canonically isomorphic to an external direct sum. This is based on the following
proposition.

  Proposition 2.30. Let V be a vector space over F, and let V1 and V2 be vector
subspaces of V . Then the following conditions are equivalent:
   (a) every member v of V decomposes uniquely as v = v1 + v2 with v1 ∈ V1
       and v2 ∈ V2 ,
   (b) V1 + V2 = V and V1 ∩ V2 = 0,
   (c) the function from the external direct sum V1 ⊕V2 to V given by (v1 , v2 ) 7→
       v1 + v2 is an isomorphism of vector spaces.
60                          II. Vector Spaces over Q, R, and C

   REMARKS.
   (1) If V is a vector space with vector subspaces V1 and V2 satisfying the
equivalent conditions of Proposition 2.30, then we say that V is the internal
direct sum of V1 and V2 . It is customary to write V = V1 ⊕ V2 in this case even
though what we have is a canonical isomorphism of the two sides, not an equality.
   (2) The dimension formula

                         dim(V1 ⊕ V2 ) = dim V1 + dim V2

for an internal direct sum follows, on the one hand, from the corresponding
formula for external direct sums; it follows, on the other hand, by using (b) and
Corollary 2.29.
   (3) In the proposition it is possible to establish a fourth equivalent condition as
follows: there exist linear maps p1 : V → V , p2 : V → V , i 1 : image p1 → V ,
and i 2 : image p2 → V such that
       • pr i s ps equals pr if r = s and equals 0 if r 6= s,
       • i 1 p1 + i 2 p2 = I , and
       • V1 = image i 1 p1 and V2 = image i 2 p2 .

   PROOF. If (a) holds, then the existence of the decomposition v = v1 + v2
shows that V1 + V2 = V . If v is in V1 ∩ V2 , then 0 = v + (−v) is a decomposition
of the kind in (a), and the uniqueness forces v = 0. Therefore V1 ∩ V2 = 0. This
proves (b).
   The function in (c) is certainly linear. If (b) holds and v is given in V , then
the identity V1 + V2 = V allows us to decompose v as v = v1 + v2 . This
proves that the linear map in (c) is onto. To see that it is one-one, suppose that
v1 + v2 = 0. Then v1 = −v2 shows that v1 is in V1 ∩ V2 . By (b), this intersection
is 0. Therefore v1 = v2 = 0, and the linear map in (c) is one-one.
   If (c) holds, then the fact that the linear map in (c) is onto V proves the existence
of the decomposition in (a). For uniqueness, suppose that v1 + v2 = u 1 + u 2
with u 1 and v1 in V1 and with u 2 and v2 in V2 . Then (u 1 , u 2 ) and (v1 , v2 ) have
the same image under the linear map in (c). Since the function in (c) is assumed
one-one, we conclude that (u 1 , u 2 ) = (v1 , v2 ). This proves the uniqueness of the
decomposition in (a).                                                                 §

    If V = V1 ⊕ V2 is a direct sum, then we can use the above projections and
injections to pass back and forth between linear maps with V1 and V2 as domain
or range and linear maps with V as domain or range. This passage back and forth
is called the universal mapping property of V1 ⊕ V2 and will be seen later in this
section to characterize V1 ⊕ V2 up to canonical isomorphism. Let us be specific
about how this property works.
                    6. Direct Sums and Direct Products of Vector Spaces                61

   To arrange for V to be the range, suppose that U is a vector space over F and
that L 1 : U → V1 and L 2 : U → V2 are linear maps. Then we can define a linear
map L : U → V by L = i 1 L 1 + i 2 L 2 , i.e., by

                   L(u) = (i 1 L 1 + i 2 L 2 )(u) = (L 1 (u), L 2 (u)),

and we can recover L 1 and L 2 from L by L 1 = p1 L and L 2 = p2 L.
    To arrange for V to be the domain, suppose that W is a vector space over F
and that M1 : V1 → W and M2 : V2 → W are linear maps. Then we can define
a linear map M : V → W by M = M1 p1 + M2 p2 , i.e., by

                           M(v1 , v2 ) = M1 (v1 ) + M2 (v2 ),

and we can recover M1 and M2 from M by M1 = Mi 1 and M2 = Mi 2 .
   The notion of direct sum readily extends to the direct sum of n vector spaces
over F. The external direct sum V1 ⊕ · · · ⊕ Vn is the set of ordered pairs
(v1 , . . . , vn ) with each v j in Vj and with addition and scalar multiplication defined
coordinate by coordinate. In the finite-dimensional case we have

                  dim(V1 ⊕ · · · ⊕ Vn ) = dim V1 + · · · + dim Vn .

   If V1 , . . . , Vn are given as vector subspaces of a vector space V , then we say
that V is the internal direct sum of V1 , . . . , Vn if the equivalent conditions of
Proposition 2.31 below are satisfied. In this case we write V = V1 ⊕ · · · ⊕ Vn
even though once again we really have a canonical isomorphism rather than an
equality.

  Proposition 2.31. Let V be a vector space over F, and let V1 , . . . , Vn be vector
subspaces of V . Then the following conditions are equivalent:
   (a) every member v of V decomposes uniquely as v = v1 + · · · + vn with
       v j ∈ Vj for 1 ≤ j ≤ n,
   (b) V1 + · · · + Vn = V and also Vj ∩ (V1 + · · · + Vj−1 + Vj+1 + · · · + Vn ) = 0
       for each j with 1 ≤ j ≤ n,
   (c) the function from the external direct sum V1 ⊕ · · · ⊕ Vn to V given by
       (v1 , . . . , vn ) 7→ v1 + · · · + vn is an isomorphism of vector spaces.

    Proposition 2.31 is proved in the same way as Proposition 2.30, and the
expected analog of Remark 3 with that proposition is valid as well. Notice
that the second condition in (b) is stronger than the condition that Vi ∩ Vj = 0 for
all i 6= j. Figure 2.3 illustrates how the condition Vi ∩ Vj = 0 for all i 6= j can
be satisfied even though (b) is not satisfied and even though the vector subspaces
do not therefore form a direct sum.
62                          II. Vector Spaces over Q, R, and C




            FIGURE 2.3. Three 1-dimensional vector subspaces of R2
                     such that each pair has intersection 0.

   If V = V1 ⊕· · ·⊕ Vn is a direct sum, then we can define projections p1 , . . . , pn
and injections i 1 , . . . , i n in the expected way, and we again get a universal mapping
property. That is, we can pass back and forth between linear maps with V1 , . . . , Vn
as domain or range and linear maps with V as domain or range. The argument
given above for n = 2 is easily adjusted to handle general n, and we omit the
details.
   To generalize the above notions to infinitely many vector spaces, there are two
quite different ways of proceeding. Let us treat first the external constructions.
Let a nonempty collection of        Lvector spaces Vα over F be given, one for each α ∈ A.
The external direct sum α∈A Vα is the set of all tuples {vα } in the Cartesian
product ×α∈A Vα with all but finitely many vα equal to 0 and with addition and
scalar multiplication defined coordinate by coordinate. For this construction we
obtain a basis as the union of embedded bases of the constituent spaces. The
                                    Q
external direct product α∈A Vα is the set of all tuples {vα } in ×α∈A Vα ,
again with addition and scalar multiplication defined coordinate by coordinate.
When there are only finitely many factors V1 , . . . , Vn , the external direct product,
which manifestly coincides with the external direct sum, is sometimes denoted
by V1 × · · · × Vn . For the external direct product when there are infinitely many
factors, there is no evident way to obtain a basis of the product from bases of the
constituents.
   The projections and injections that we defined in the case of finitely many
vector spaces are still meaningful here. The universal mapping property is still
valid as well, but it splinters into one form for direct sums and another form for
direct products. The formulas given above for using linear maps with the Vα ’s
as domain or range to define linear maps with the direct sum or direct product
as domain or range may involve sums with infinitely many nonzero terms, and
they are not directly usable. Instead, the formulas that continue to make sense
are the ones for recovering linear maps with the Vα ’s as domain or range from
linear maps with the direct sum or direct product as domain or range. These turn
out to determine the formulas uniquely for the linear maps with the direct sum
or direct product as domain or range. In other words, the appropriate universal
mapping property uniquely determines the direct sum or direct product up to an
                     6. Direct Sums and Direct Products of Vector Spaces                     63

isomorphism that respects the relevant projections and injections.
                                                           Q            L
   Let us see to the details. We denote typical members of α∈A Vα and α∈A Vα
by {vα }α∈A , with the understanding that only finitely many vα can be nonzero in
the case of the direct sum. The formulas are
        Y                           °        ¢
   pβ :     Vα → Vβ          with pβ {vα }α∈A = vβ ,
       α∈A
               M                                                      Ω
                                                                           vβ   if α = β,
  i β : Vβ →         Vα      with i β (vβ ) = {wα }α∈A and wα =
               α∈A                                                         0    if α 6= β.
   If U is a vector space over F and if a linear map
                                                  Q L β : U → Vβ is given for each
β ∈ A, we can obtain a linear map L : U → α∈A Vα that satisfies pβ L = L β
for all β. The definition that makes perfectly good sense is
                          L(u) = {L(u)α }α∈A = {L α (u)}α∈A .
What does not make sense is to try to express     P the right side in terms of the
injections i α ; we cannot write the right side as α∈A i α (L α (u)) because infinitely
many terms might be nonzero.
   If W is a vector space and L a linear map Mβ : Vβ → W is given for each β, we
can obtain a linear map M : α∈A Vα → W that satisfies Mi β = Mβ for all β;
the definition that makes perfectly good sense is
                               °         ¢ X
                            M {vα }α∈A =          Mα (vα ).
                                                α∈A

                                               P many vα can be nonzero. It can
The right side is meaningful since only finitely
be misleading to write the formula as M = α∈A Mα pα because infinitely many
of the linear maps Mα pα can be nonzero functions.
    In any event, we have a universal mapping property in both cases—for the direct
product with the projections in place and for the direct sum with the injections
in place. Let us see that these universal mapping properties characterize direct
products and direct sums up to an isomorphism respecting the projections and
injections, and that they allow us to define and recognize “internal” direct products
and direct sums.
    A direct product of a set of vector spaces Vα over F for α ∈ A consists of
a vector space V and a system of linear maps pα : V → Vα with the following
universal mapping property: whenever U is a vector space and {L α } is a system
of linear maps L α : U → Vα , then there exists a unique linear map L : U → V
such that pα L = L α for all α. See Figure 2.4. The external direct product
establishes existence of a direct product, and Proposition 2.32 below establishes
its uniqueness up to an isomorphism of the V ’s that respects the pα ’s. A direct
product is said to beØinternal if each Vα is a vector subspace of V and if for each
α, the restriction pα ØVα is the identity map on Vα . Because of the uniqueness, this
64                         II. Vector Spaces over Q, R, and C

definition of internal direct product is consistent with the earlier one when there
are only finitely Vα ’s.
                                           Lα
                                   Vα √−−− U
                                     x
                                     
                                  pα     L


                                   V
 FIGURE 2.4. Universal mapping property of a direct product of vector spaces.

   Proposition 2.32. Let A be a nonempty set of vector spaces over F, and let
Vα be the vector space corresponding to the member α of A. If (V, { pα }) and
(V ∗ , { pα∗ }) are two direct products of the Vα ’s, then the linear maps pα : V → Vα
and pα∗ : V ∗ → Vα are onto Vα , there exists a unique linear map L : V ∗ → V
such that pα∗ = pα L for all α ∈ A, and L is invertible.
    PROOF. In Figure 2.4 let U = V ∗ and L α = pα∗ . If L : V ∗ → V is the linear
map produced by the fact that V is a direct product, then we have pα L = pα∗ for
all α. Reversing the roles of V and V ∗ , we obtain a linear map L ∗ : V → V ∗
with pα∗ L ∗ = pα for all α. Therefore pα (L L ∗ ) = ( pα L)L ∗ = pα∗ L ∗ = pα .
    In Figure 2.4 we next let U = V and L α = pα for all α. Then the identity
1V on V has the same property pα 1V = pα relative to all pα that L L ∗ has, and
the uniqueness says that L L ∗ = 1V . Reversing the roles of V and V ∗ , we obtain
L ∗ L = 1V ∗ . Therefore L is invertible.
    For uniqueness suppose that 8 : V ∗ → V is another linear map with pα∗ =
pα 8 for all α ∈ A. Then the argument of the previous paragraph shows that
L ∗ 8 = 1V ∗ . Applying L on the left gives 8 = (L L ∗ )8 = L(L ∗ 8) = L1V ∗ =
L. Thus 8 = L.
    Finally we have to show that the α th map of a direct product is onto Vα . It
                           ∗
is
Q enough to show that pα is onto Vα . Taking V as the external direct product
   α∈A Vα with pα equal to the coordinate mapping, form the invertible linear map
L ∗ : V → V ∗ that has just been proved to exist. This satisfies pα = pα∗ L ∗ for all
α ∈ A. Since pα is onto Vα , pα∗ must be onto Vα .                                §

   A direct sum of a set of vector spaces Vα over F for α ∈ A consists of a vector
space V and a system of linear maps i α : Vα → V with the following universal
mapping property: whenever W is a vector space and {Mα } is a system of linear
maps Mα : Vα → W , then there exists a unique linear map M : V → W such
that Mi α = Mα for all α. See Figure 2.5. The external direct sum establishes
existence of a direct sum, and Proposition 2.33 below establishes its uniqueness
up to isomorphism of the V ’s that respects the i α ’s. A direct sum is said to be
internal if each Vα is a vector subspace of V and if for each α, the map i α is the
                                    7. Determinants                                 65

inclusion map of Vα into V . Because of the uniqueness, this definition of internal
direct sum is consistent with the earlier one when there are only finitely Vα ’s.
                                          Mα
                                   Vα −−−→ W
                                     
                                     
                                  iα y    M


                                   V
   FIGURE 2.5. Universal mapping property of a direct sum of vector spaces.


     Proposition 2.33. Let A be a nonempty set of vector spaces over F, and let
Vα be the vector space corresponding to the member α of A. If (V, {i α }) and
(V ∗ , {i α∗ }) are two direct sums of the Vα ’s, then the linear maps i α : Vα → V and
i α∗ : Vα → V ∗ are one-one, there exists a unique linear map M : V → V ∗ such
that i α∗ = Mi α for all α ∈ A, and M is invertible.

   PROOF. In Figure 2.5 let W = V ∗ and Mα = i α∗ . If M : V → V ∗ is the linear
map produced by the fact that V is a direct sum, then we have Mi α = i α∗ for all
α. Reversing the roles of V and V ∗ , we obtain a linear map M ∗ : V ∗ → V with
M ∗ i α∗ = i α for all α. Therefore (M ∗ M)i α = M ∗ i α∗ = i α .
   In Figure 2.5 we next let W = V and Mα = i α for all α. Then the identity 1V
on V has the same property 1V i α = i α relative to all i α that M ∗ M has, and the
uniqueness says that M ∗ M = 1V . Reversing the roles of V and V ∗ , we obtain
M M ∗ = 1V ∗ . Therefore M is invertible.
   For uniqueness suppose that 8 : V → V ∗ is another linear map with i α∗ = 8i α
for all α ∈ A. Then the argument of the previous paragraph shows that M ∗ 8 =
1V . Applying M on the left gives 8 = (M M ∗ )8 = M(M ∗ 8) = M1V = M.
Thus 8 = M.
   Finally we have to show that the α th map of a direct sum is one-one on Vα . It
                            ∗
is
Lenough to show that i α is one-one on Vα . Taking V as the external direct sum
   s∈S Vα with i α equal to the embedding mapping, form the invertible linear map
M ∗ : V ∗ → V that has just been proved to exist. This satisfies i α = M ∗ i α∗ for all
α ∈ A. Since i α is one-one, i α∗ must be one-one.                                  §



                                 7. Determinants

A “determinant” is a certain scalar attached initially to any square matrix and
ultimately to any linear map from a finite-dimensional vector space into itself.
66                        II. Vector Spaces over Q, R, and C

The definition is presumably known from high-school algebra in the case of
2-by-2 and 3-by-3 matrices:
                  µ      ∂
                    a b
              det           = ad − bc,
                    c d
             √           !
                a b c
         det d e f = aei + b f g + cdh − a f h − bdi − ceg.
                g h i

For n-by-n square matrices the determinant function will have the following
important properties:
     (i) det(AB) = det A det B,
    (ii) det I = 1,
   (iii) det A = 0 if and only if A has no inverse.
   Once we have constructed the determinant function with these properties, we
can then extend the function to be defined on all linear maps L : V → V with V
finite-dimensional.
              µ    ∂ To do so, we let 0 be any ordered basis of V , and we define
                 L
det L = det          . If 1 is another ordered basis, then
                00
                  µ      ∂        µ       ∂     µ      ∂       µ    ∂
                   L                I              L            I
              det            = det            det          det          ,
                  11               10             00           01
                   µ       ∂              µ    ∂      µ     ∂
                         L                   I           I
and this equals det          by (i) since         and         are inverses of each
                        00                  10          01
other and since their determinants, by (i) and (ii), are reciprocals. Hence the
definition of det L is independent of the choice of ordered basis, and determinant
is well defined on the linear map L : V → V . It is then immediate that the
determinant function on linear maps from V into V satisfies (i), (ii), and (iii)
above.
   Thus it is enough to establish the determinant function on n-by-n matrices.
Setting matters up in a useful way involves at least one subtle step, but much of
this step has fortunately already been carried out in the discussion of signs of
permutations in Section I.4. To proceed, we view det on n-by-n matrices over
F as a function of the n rows of the matrix, rather than the matrix itself. We
write V for the vector space M1n (F) of all n-dimensional row vectors. A function
 f : V × · · · × V → F defined on ordered k-tuples of members of V is called a
k-multilinear functional or k-linear functional if it depends linearly on each of
the k vector variables when the other k − 1 vector variables are held fixed. For
example,
                   f (( a b ) , ( c d )) = ac + b(c + d) + 12 ad
                                        7. Determinants                                        67

is a 2-linear functional on M12 (F) × M12 (F). A little more generally and more
suggestively,
      g(( a    b),(c       d )) = `1 ( a     b ) `2 ( c   d ) + `3 ( a     b ) `4 ( c   d)
is a 2-linear functional on M12 (F) × M12 (F) whenever `1 , . . . , `4 are linear
functionals on M12 (F).
      Let {v1 , . . . , vn } be a basis of V . Then a k-multilinear functional as above
is determined by its value on all k-tuples of basis vectors (vi1 , . . . , vik ). (Here
i 1 , . . . , i k are integers between 1 and n.) The reason is that we can fix all but
the first variable and expand out the expression by linearity so that only a basis
vector remains in each term for the first variable; for each resulting term we can
fix all but the second variable and expand out the expression by linearity; and so
on. Conversely if we specify arbitrary scalars for the values on each such k-tuple,
then we can define a k-multilinear functional assuming those values on the tuples
of basis vectors.
      A k-multilinear functional f on k-tuples from M1n (F) is said to be alternating
if f is 0 whenever two of the variables are equal.
                                                  ©                                 ™
      EXAMPLE. For k = 2 and n = 2, we use v1 = ( 1 0 ) , v2 = ( 0 1 ) as ba-
sis. Then a 2-linear multilinear functional f is determined by f (v1 , v1 ), f (v1 , v2 ),
 f (v2 , v1 ), and f (v2 , v2 ). If f is alternating, then f (v1 , v1 ) = f (v2 , v2 ) = 0.
But also f (v1 + v2 , v1 + v2 ) = 0, and expansion via 2-multilinearity gives
                 f (v1 , v1 ) + f (v1 , v2 ) + f (v2 , v1 ) + f (v2 , v2 ) = 0.
We have already seen that the first and last terms on the left side are 0, and thus
f (v2 , v1 ) = − f (v1 , v2 ). Therefore f is completely determined by f (v1 , v2 ).

    The principle involved in the computation within the example is valid more
generally: whenever a multilinear functional f is alternating and two of its
arguments are interchanged, then the value of f is multiplied by −1. In fact,
let us suppress all variables except for the i th and the j th . Then we have
      0 = f (v + w, v + w) = f (v + w, v) + f (v + w, w)
        = f (v, v) + f (w, v) + f (v, w) + f (w, w) = f (w, v) + f (v, w).
     Theorem 2.34. For M1n (F), the vector space of alternating n-multilinear
functionals has dimension 1, and a nonzero such functional has nonzero value on
(e1t , . . . , ent ), where {e1 , . . . , en } is the standard basis of Fn . Let f 0 be the unique
such alternating n-multilinear functional taking the value 1 on (e1t , . . . , ent ). If a
function det : Mnn (F) → F is defined by
                                 det A = f 0 (A1· , . . . , An· )
68                              II. Vector Spaces over Q, R, and C

when A has rows A1· , . . . , An· , then det has the properties that
   (a) det(AB) = det A det B,
   (b) det I = 1,
   (c) det A = 0Pif and only if A has no inverse,
   (d) det A = σ (sgn σ )A1σ (1) A2σ (2) · · · Anσ (n) , the sum being taken over all
       permutations σ of {1, . . . , n}.
    PROOF OF UNIQUENESS. Let f be an alternating n-multilinear functional, and
let {u 1 , . . . , u n } be the basis of the space of row vectors defined by u i = eit . Since
 f is multilinear, f is determined by its values on all n-tuples (u k1 , . . . , u kn ).
Since f is alternating, f (u k1 , . . . , u kn ) = 0 unless the u ki are distinct, i.e.,
unless (u k1 , . . . , u kn ) is of the form (u σ (1) , . . . , u σ (n) ) for some permutation
σ . We have seen that the value of f on an n-tuple of rows is multiplied
by −1 if two of the rows are interchanged. Corollary 1.22 and Proposition
1.24b consequently together imply that the value of f on an n-tuple is multi-
plied by sgn σ if the members of the n-tuple are permuted by σ . Therefore
 f (u σ (1) , . . . , u σ (n) ) = (sgn σ ) f (u 1 , . . . , u n ), and f is completely determined
by its value on (u 1 , . . . , u n ). We conclude that the vector space of alternating
n-multilinear functionals has dimension at most 1.                                             §
   PROOF OF EXISTENCE. Define det A, and therefore also f 0 , by (d). Each term
in this definition is the product of n linear functionals, the k th linear functional
being applied to the k th argument of f 0 , and f 0 is consequently n-multilinear.
To see that f 0 is alternating, suppose that the i th and j th rows are equal with
i 6= j. If τ is the transposition of i and j, then A1σ τ (1) A2σ τ (2) · · · Anσ τ (n) =
A1σ (1) A2σ (2) · · · Anσ (n) , and Lemma 1.23 hence shows that

     (sgn σ τ )A1σ τ (1) A2σ τ (2) · · · Anσ τ (n) + (sgn σ )A1σ (1) A2σ (2) · · · Anσ (n) = 0.

Thus if we compute the sum in (d) by grouping pairs of terms, the one for σ τ and
the one for σ if sgn σ = +1, we see that the whole sum is 0. Thus f 0 is alternating.
Finally when A is the identity matrix I , we see that A1σ (1) A2σ (2) · · · Anσ (n) = 0
unless σ is the identity permutation, and then the product is 1. Since sgn 1 = +1,
det I = +1. We conclude that the vector space of alternating n-multilinear
functionals has dimension exactly 1.                                                 §
   PROOF OF PROPERTIES OF det. Fix an n-by-n matrix B. Since f 0 is alternating
n-multilinear, so is (v1 , . . . , vn ) 7→ f 0 (v1 B, . . . , vn B). The vector space of
alternating n-multilinear functionals has been proved to be of dimension 1, and
therefore f 0 (v1 B, . . . , vn B) = c(B) f 0 (v1 , . . . , vn ) for some scalar c(B). In the
notation with det, this equation reads det(AB) = c(B) det A. Putting A = I , we
obtain det B = c(B) det I . Thus c(B) = det B, and (a) follows. We have already
proved (b), and (d) was the definition of det A. We are left with (c). If A−1
                                    7. Determinants                                69

exists, then (a) and (b) give det(A−1 ) det A = det I = 1, and hence det A 6= 0.
If A−1 does not exist, then Theorem 1.30 and Proposition 1.27c show that the
reduced row-echelon form R of A has a row of 0’s. We combine Proposition 1.29,
conclusion (a), the invertibility of elementary matrices, and the fact that invertible
matrices have nonzero determinant, and we see that det A is the product of det R
and a nonzero scalar. Since det is linear as a function of each row and since R
has a row of 0’s, det R = 0. Therefore det A = 0. This completes the proof of
the theorem.                                                                        §

   The fast procedure for evaluating determinants is to use row reduction, keeping
track of what happens. The effect of each kind of row operation on a determinant
and the reasons the function det behaves in this way are as follows:
     (i) Interchange two rows. This operation multiplies the determinant by −1
         because of the alternating property.
    (ii) Multiply a row by a nonzero scalar c. This operation multiplies the
         determinant by c because of the linearity of determinant as a function of
         that row.
   (iii) Replace the i th row by the sum of it and a multiple of the j th row with
          j 6= i. This operation leaves the determinant unchanged. In fact, the
         matrix whose i th row is replaced by the j th row has determinant 0 by the
         alternating property, and the rest follows by linearity in the i th row.
As with row reduction the number of steps required to compute a determinant
this way is ≤ Cn 3 in the n-by-n case.
   A certain savings of computation is possible as compared with full-fledged
row reduction. Namely, we have only to arrange for the reduced matrix to be 0
below the main diagonal, and then the determinant of the reduced matrix will
be the product of the diagonal entries, by inspection of the formula in Theorem
2.34d.
                               √             !
                                 1 2 3
   EXAMPLE. For the matrix 4 5 6 , we have
                                 7 8 10
       √            !           √         !
           1 2 3               1    2   3
                        (iii)
 det       4 5 6         = det 0 −3 −6
           7 8 10              0 −6 −11
                                √           !          √       !
                                  1   2   3              1 2 3
                        (ii)                  (iii)
                        = −3 det 0    1   2    = −3 det 0 1 2 = −3.
                                  0 −6 −11               0 0 1

   We conclude this section with a number of formulas for determinants.
70                               II. Vector Spaces over Q, R, and C

     Proposition 2.35. If A is an n-by-n square matrix, then det At = det A.
   PROOF. Corollary 2.9 says that the row space and the column space of A have
the same dimension, and A is invertible if and only if the row space has dimension
n. Thus A is invertible if and only if At is invertible, and Theorem 2.34c thus
shows that det A = 0 if and only if det At = 0. Now suppose that det A and det At
are nonzero. Then we can write A = E 1 · · · Er with each E j an elementary
                                                                   Q           matrix
of one of the three types. Theorem 2.34a shows that det A = rj=1 det E j and
          Q
det At = rj=1 det E jt , and hence it is enough to prove that det E j = det E jt for
each j. For E j of either of the first two types, E j = E jt and there is nothing to
prove. For E j of the third type, we have det E j = det E jt = 1. The result follows.
                                                                                   §

    Proposition 2.36 (expansion in cofactors). Let A be an n-by-n matrix, and let
ci j be the square matrix of size n − 1 obtained by deleting the i th row and the j th
A
column. Then
                            Pn
     (a) for any j, det A = i=1    (−1)i+ j Ai j det A
                                                     ci j , i.e., det A may be calculated
                                                    th
         by “expansion in cofactors”
                           P          about the j column,
     (b) for any i, det A = nj=1 (−1)i+ j Ai j det A ci j , i.e., det A may be calculated
         by “expansion in cofactors” about the i th row.
   REMARKS. If this formula is iterated, we obtain a procedure for evaluating a
determinant in about Cn! steps. This procedure amounts to using the formula for
det A in Theorem 2.34d and is ordinarily not of practical use. However, it is of
theoretical use, and Corollary 2.37 will provide a simple example of a theoretical
application.
   PROOF. It is enough to prove (a) since (b) then follows by combining (a) and
Proposition 2.35. In (a), the right side is 1 when A = I , and it is enough by
Theorem 2.34 to prove that the right side is alternating and n-multilinear. Each
term on the right side is n-multilinear, and hence so is the whole expression. To
see that the right side is alternating, suppose that the k th and l th rows are equal
with k < l. The k th and l th rows are both present in Aci j if i is not equal to k or l,
                   c
and thus each det Ai j is 0 for i not equal to k or l. We are left with showing that

                                       Ak j + (−1)l+ j Al j det A
                     (−1)k+ j Ak j det d                        cl j = 0.

The two matrices d        cl j have the same rows but in a different order. The
                 Ak j and A
order is

       1, . . . , k − 1, k + 1, . . . , l − 1, l, l + 1, . . . , n    in the case of d
                                                                                     Ak j ,
       1, . . . , k − 1, k, k + 1, . . . , l − 1, l + 1, . . . , n                   cl j .
                                                                      in the case of A
                                          7. Determinants                                        71

We can transform the first matrix into the second by transposing the index for
row l to the left one step at a time until it gets to the k th position. The number of
steps is l − k − 1, and therefore det Acl j = (−1)l−k−1 det d   Ak j . Consequently

                    Ak j + (−1)l+ j Al j det A
  (−1)k+ j Ak j det d                        cl j
                                       °                               ¢
                                   = (−1)k+ j Ak j + (−1)2l−k−1+ j Al j det d
                                                                            Ak j .

The right side is 0 since Ak j = Al j , and the proof is complete.                               §

   Corollary 2.37 (Vandermonde matrix and determinant). If r1 , . . . , rn are
scalars, then
                                                             
                            1       1       ···         1
                          r1      r2       ···        rn      Y
                                                             
                  det     r12     r22      ···        rn2    =    (r j − ri ).
                           ..      ..      ..          ..    
                                              .               j>i
                             .       .                   .
                          r1n−1   r2n−1     ···       rnn−1

   PROOF. We show that the determinant is
                                                                          
                                          1                    ···     1
                      Y                  r2                   ···     rn 
                    =   (r j − r1 ) det 
                                         ...                  ..       ..  ,
                          j>1                                     .      . 
                                                     r2n−2     ···    rnn−2

and then the result follows by induction. In the given matrix, replace the n th row
by the sum of it and −r1 times the (n − 1)st row, then the (n − 1)st row by the
sum of it and −r1 times the (n − 2)nd row, and so on. The resulting determinant
is
                                                   
         1          1         ···          1
       0        r2 − r1      ···      rn − r1      
       .            .         .           .        
   det 
       .
          .          ..          ..        ..       
                                                    
        0 r n−2 − r r n−3 · · · r n−2 − r r n−3 
              2           1 2                n           1 n
        0 r2n−1 − r1r2n−2          ···      rnn−1 − r1rnn−2
                                                          
               r2 − r1            ···          rn − r1
                 ..              ..              ..       
                   .                 .             .                      by Proposition 2.36a
   = det 
          r n−2 − r r n−3
                                                           
            2        1 2          ···      rnn−2 − r1rnn−3               applied with j = 1
           r2n−1 − r1r2n−2        ···      rnn−1 − r1rnn−2
72                              II. Vector Spaces over Q, R, and C
                                                                   
                                         1            ···       1
                                        r2           ···       rn 
     = (r2 − r1 ) · · · (rn − r1 ) det 
                                        ...          ..         ..  ,
                                                         .        . 
                                             r2n−2    ···      rnn−2
the last step following by multilinearity of the determinant in the columns (as a
consequence of Proposition 2.35 and multilinearity in the rows).               §

   The classical adjoint of the square matrix A, denoted by Aadj , is the matrix with
         adj
entries Ai j = (−1)i+ j det A
                            cji with Ackl defined as in the statement of Proposition
                                     th
2.36: Akl is the matrix A with the k row and l th column deleted.
       c
                                   µ       ∂adj     µ             ∂
                                     a b                 d −b
   In the 2-by-2 case, we have                   =                 . Thus we have
                                     c d               −c       a
A Aadj = Aadj A = (det A)I in the 2-by-2 case. Cramer’s rule for solving simul-
taneous linear equations results from the n-by-n generalization of this formula.

   Proposition 2.38 (Cramer’s rule). If A is an n-by-n matrix, then A Aadj =
 adj
A A = (det A)I , and thus det A 6= 0 implies A−1 = (det A)−1 Aadj . Conse-
                                                of thesimultaneous
quently if det A 6= 0, then the unique solution                  system
                                                                       Ax = b
                                                   x1              b1
                                                    .               .
of n equations in n unknowns, in which x =  ..  and b =  .. , has
                                                   xn              bn
                                                     det Bj
                                         xj =
                                                     det A
with Bj equal to the n-by-n matrix obtained from A by replacing the j th column
of A by b.
     REMARKS. If we think of the calculation of the determinant of an n-by-n matrix
as requiring about n 3 steps, then application of Cramer’s rule, at least if done in
an unthinking fashion, suggests that solving an invertible system requires about
n 3 (n + 1) steps, i.e., n + 1 determinants are involved in the explicit solution. Use
of row reduction directly to solve the system is more efficient than proceeding this
way. Thus Cramer’s rule is more important for its theoretical applications than it
is for making computations. One simple theoretical application is the observation
that each entry of the inverse of a matrix is the quotient of a polynomial function
of the entries divided by the determinant.
     PROOF. The (i, j)th entry of Aadj A is
                                n
                                X                     n
                                                      X
                                       adj
                (Aadj A)i j =         Aik Ak j =             (−1)i+k (det A
                                                                          c ki )Ak j .
                                k=1                   k=1
                     8. Eigenvectors and Characteristic Polynomials                          73

If i = j, then expansion in cofactors about the j th column (Proposition 2.36a)
identifies the right side as det A. If i 6= j, consider the matrix B obtained from A
by replacing the i th column of A by the j th column. Then the i th and j th columns
of B are equal, and hence det B = 0. Expanding det B in cofactors about the i th
column (Proposition 2.36a), we obtain

                     n
                     X                                     n
                                                           X
       0 = det B =         (−1)i+k (det B
                                        cki )Bki =               (−1)i+k (det A
                                                                              c ki )Ak j .
                     k=1                                   k=1


Thus A Aadj = (det A)I . A similar argument proves that Aadj A = (det A)I .
  For the application to Ax = b, we multiply both sides on the left by Aadj and
obtain (det A)x = Aadj b. Hence

                             n
                             X                       n
                                                     X
              (det A)x j =         (Aadj ) ji bi =         (−1)i+ j bi det A
                                                                           ci j ,
                             i=1                     i=1


and the right side equals det Bj by expansion in cofactors of det Bj about the j th
column (Proposition 2.36a).                                                     §



              8. Eigenvectors and Characteristic Polynomials

A vector v 6= 0 in Fn is an eigenvector of the n-by-n matrix A if Av = ∏v
for some scalar ∏. We call ∏ the eigenvalue associated with v. When ∏ is an
eigenvalue, the vector space of all v with Av = ∏v, i.e., the set consisting of the
eigenvectors and the 0 vector, is called the eigenspace for ∏.
   If we think of A as giving a linear map L from Fn to itself, an eigenvector takes
on geometric significance as a vector mapped to a multiple of itself by L. Another
geometric way of viewing matters is that the eigenvector yields a 1-dimensional
subspace U = Fv that is invariant, or stable, under L in the sense of satisfying
L(U ) ⊆ U .

    Proposition 2.39. An n-by-n matrix A has an eigenvector with eigenvalue ∏
if and only if det(∏I − A) = 0. In this case the eigenspace for ∏ is the kernel of
∏I − A.

   PROOF. We have Av = ∏v if and only if (∏I − A)v = 0, if and only if v is in
ker(∏I − A). This kernel is nonzero if and only if det(∏I − A) = 0.         §
74                               II. Vector Spaces over Q, R, and C

  With A fixed, the expression det(∏I − A) is a polynomial in ∏ of degree n
and is called the characteristic polynomial8 of A. To see that it is at least a
polynomial function of ∏, let us expand det(∏I − A) as
                                                        
                         ∏ − A11    −A12     ···   −A1n
                        −A21      ∏ − A22 · · ·   −A2n 
                   det 
                           .
                            ..        .
                                      ..     . ..     .. 
                                                         
                                                       .
                                 −An1         −An2        ···    ∏ − Ann
                             X
                        =        (sgn σ )term1,σ (1) · · · termn,σ (n) .
                             σ

The
Qn term for the permutation σ = 1 has σ (k) = k for every k and gives
  j=1 (∏ − A j j ). All other σ ’s have σ (k) = k for at most n − 2 values of k,
and ∏ therefore occurs at most n − 2 times. Thus the above expression is
             n
             Y                      n                       o
                                     other terms with powers
         =         (∏ − A j j ) +
             j=1
                                     of ∏ at most n − 2
                     ≥X
                      n           ¥       n                    o
                                           terms with powers of
         = ∏n −              A j j ∏n−1 +                        + (−1)n det A.
                       j=1
                                           ∏ from n − 2 to 1

The constant term is (−1)n det A as indicated because it is the value of the poly-
nomial at ∏ = 0, which is det(−A). In any event, we now see that characteristic
polynomials are polynomial functions. Starting in Chapter V, we shall treat them
as polynomials in one indeterminate in the sense9 of Section I.3; for now, we are
calling the indeterminate ∏, but later as our point of view evolves, we shall start
                                                    n−1
                      Pn of the coefficient of ∏
calling it X. The negative                               is the trace of A, denoted
by Tr A. Thus Tr A = j=1 A j j . Trace is a linear functional on the vector space
Mnn (F) of n-by-n matrices.
                         µ          ∂
                              4 1
   EXAMPLE 1. For A =                 , the characteristic polynomial is
                            −2 1
                        µ                  ∂
                           ∏ − 4 −1
      det(∏I − A) = det
                             2      ∏−1
                        = (∏ − 4)(∏ − 1) + 2 = ∏2 − 5∏ + 6 = (∏ − 2)(∏ − 3).
    8 Some authors call det(A − ∏I ) the characteristic polynomial. This is the same polynomial as

det(∏I − A) if n is even and is the negative of it if n is odd. The choice made here has the slight
advantage of always having leading coefficient 1, which is a handy property in some situations.
    9 In Chapter V we will allow determinants of matrices whose entries are from any “commutative

ring with identity,” C[∏] being an example. Then we can think of det(∏I − A) directly as involving
an indeterminate ∏ and not initially as a function of a scalar ∏.
                     8. Eigenvectors and Characteristic Polynomials                        75

The roots, and hence the eigenvalues, are ∏ = 2 and ∏ = 3. The eigenvectors for
∏ = 2 are computed by solving (2I − A)v = 0. The method of row reduction
gives
  µ                          ∂       µ                    ∂        µ       1
                                                                                   ∂
      2 − 4 −1           0               −2 −1        0                1   2
                                                                               0
                                 =                            7→                       .
        2   2−1          0                2  1        0                0   0   0

Thus we have x1 + 12 x2 = 0 and x1 µ  = −∂12 x2 . So µthe eigenvectors
                                                            ∂           for ∏ = 2
                                                          1
                                         x1             −2
are the nonzero vectors of the form            = x2           . Similarly we find
                                         x2               1
the eigenvectors for ∏ = 3 by starting from (3I − A)v = 0 and solving. The
result
µ ∂is thatµthe eigenvectors
                  ∂             for ∏ = 3 are the nonzero vectors of the form
  x1          −1
       = x2         . For this example, there is a basis of eigenvectors.
  x2            1

   Corollary 2.40. An n-by-n matrix A has at most n eigenvalues.
  PROOF. Since det(∏I − A) is a polynomial of degree n, this follows from
Proposition 2.39 and Corollary 1.14.                                    §

   It will later be of interest that certain matrices A have a basis of eigenvectors.
Such a basis exists for A as in Example 1 but not in general. One thing that
can prevent a matrix from having a basis of eigenvectors is the failure of the
characteristic
       µ        polynomial
                  ∂          to factor into first-degree factors. Thus, for example,
            0 1
A =                 has characteristic polynomial ∏2 + 1, which does not factor
         −1 0
into first-degree factors when F = R. Even when we do have a factorization
into first-degree factors, we can still fail to have a basis of eigenvectors, as the
following example shows.
                              µ       ∂
                              1 −1
   EXAMPLE 2. For A =                   , the characteristic polynomial is given
                        µ 0         1      ∂
                          ∏−1         1
by det(∏I − A) = det                         = (∏ − 1)2 . When we solve for
                     µ       0     ∏ −∂ 1                     µ ∂         µ ∂
                        0 1        0                            x1          1
eigenvectors, we get                     , and x2 = 0. Thus         = x1       ,
                        0 0        0                            x2          0
and we do not have a basis of eigenvectors.

   What happens is that the presence of a factor (∏ − c)k in the characteristic
polynomial ensures the existence of an r-parameter family of eigenvectors for
eigenvalue c, with 1 ≤ r ≤ k, but not necessarily with r = k. Example 2 shows
that r can be strictly less than k. For purposes of deciding whether there is a basis
76                          II. Vector Spaces over Q, R, and C

of eigenvectors, the positive result is that the different roots of the characteristic
polynomial do not interfere with each other; this is a consequence of the following
proposition.

   Proposition 2.41. If A is an n-by-n matrix, then eigenvectors for distinct
eigenvalues are linearly independent.
   REMARK. It follows that if the characteristic polynomial of A has n distinct
eigenvalues, then it has a basis of eigenvectors.
  PROOF. Let Av1 = ∏1 v1 , . . . , Avk = ∏k vk with ∏1 , . . . , ∏k distinct, and
suppose that
                       c1 v1 + · · · + ck vk = 0.
Applying A repeatedly gives

                                c1 ∏1 v1 + · · · + ck ∏k vk = 0,
                                c1 ∏21 v1 + · · · + ck ∏2k vk = 0,
                                                              ..
                                                               .
                           c1 ∏k−1                k−1
                               1 v1 + · · · + ck ∏k vk = 0.

                                         ( j)
If the j th entry of vi is denoted by vi , this system of vector equations says that
                               
           1        ···     1           ( j)     
                                    c1 v1           0
          ∏1       ···     ∏k        .            .. 
          .        ..          
                             ..      .       =   .               for 1 ≤ j ≤ n.
          ..          .      .        .
                                         ( j)       0
             ∏k−1   ···    ∏k−1     c  v
                                     k k
              1             k

The square matrix on the left side is a Vandermonde matrix, which is invertible
                                                                    ( j)
by Corollary 2.37 since ∏1 , . . . , ∏k are distinct. Therefore ci vi = 0 for all i
                                             ( j)
and j. Each vi is nonzero in some entry vi with j perhaps depending on i, and
hence ci = 0. Since all the coefficients ci have to be 0, v1 , . . . , vk are linearly
independent.                                                                        §

   The theory of eigenvectors and eigenvalues for square matrices allows us to
develop a corresponding theory for linear maps L : V → V , where V is an
n-dimensional vector space over F. If L is such a function, a vector v 6= 0
in V is an eigenvector of L if L(v) = ∏v for some scalar ∏. We call ∏ the
eigenvalue. When ∏ is an eigenvalue, the vector space of all v with L(v) = ∏v
is called the eigenspace for ∏ under L. We can compute the eigenvalues and
eigenvectors of L by working in any ordered basis 0 of V . The equation L(v) =
                     8. Eigenvectors and Characteristic Polynomials              77
              µ      ∂µ ∂         µ ∂
                  L      v           v
∏v becomes                    =∏         and is satisfied if and only if the column
        µ ∂      00      0           0                  µ      ∂
          v                                                L
vector          is an eigenvector of the matrix A =              with eigenvalue ∏.
          0                                               00
Applying Proposition 2.39 and remembering that determinants are well defined
on linear maps L : V → V , we see that L has an eigenvector with eigenvalue ∏
if and only if det(∏I − L) = 0 and that in this case the eigenspace is the kernel
of ∏I − L.
    What happens if we make these computations in a different
                                                         µ      ∂ ordered basis
                                                                            µ 1?  ∂
                                                              L                L
We know from Proposition 2.17 that the matrices A =               and B =
                                                        µ 00∂                 11
                                                            I
are similar, related by B = C −1 AC, where C =                   . Computing with
                   µ ∂                                     01
                     v
A leads to u =           as eigenvector for the eigenvalue ∏. The corresponding
                     0
                           −1          −1      −1         −1             −1
                    µ B(C
result forµB is ∂that   ∂ µu) =  ∂ C ACC u = C Au = ∏C u. Thus
              I       v        v
C −1 u =                  =         is an eigenvector of B with eigenvalue ∏, just
            10        0        1
as it should be.
    These considerations about eigenvalues suggest some facts about similar ma-
trices that we can observe more directly without first passing from matrices to
linear maps: One is that similar matrices have the same characteristic polynomial.
To see this, suppose that B = C −1 AC; then

        det(∏I − B) = det(∏I − C −1 AC) = det(C −1 (∏I − A)C)
                     = (det C −1 ) det(∏I − A)(det C −1 )
                     = (det C −1 )(det C −1 ) det(∏I − A) = det(∏I − A).

A second fact is that similar matrices have the same trace. In fact, the trace is
the negative of the coefficient of ∏n−1 in the characteristic polynomial, and the
characteristic polynomials are the same.
   Because of these considerations we are free in the future to speak of the char-
acteristic polynomial, the eigenvalues, and the trace of a linear map from a finite-
dimensional vector space to itself, as well as the determinant, and these notions
do not depend on any choice of ordered basis. We can speak unambiguously also
of the eigenvectors of such a linear map. For this notion the realization of the
eigenvectors in an ordered basis as column vectors depends on the ordered basis,
the dependence being given by the formulas two paragraphs before the present
one.
   One final remark is in order. When the scalars are taken to be the complex
numbers C, the Fundamental Theorem of Algebra (Theorem 1.18) is applicable:
78                          II. Vector Spaces over Q, R, and C

every polynomial of degree ∏ 1 has at least one root. When applied to the char-
acteristic polynomial of a square matrix or a linear map from a finite-dimensional
vector space to itself, this theorem tells us that the matrix or linear map always
has at least one eigenvalue, hence an eigenvector. We shall make serious use of
this fact in Chapter III.


                    9. Bases in the Infinite-Dimensional Case

So far in this chapter, the use of bases has been limited largely to vector spaces
having a finite spanning set. In this case we know from Corollary 2.3 that the
finite spanning set has a subset that is a basis, any linearly independent set can be
extended to a basis, and any two bases have the same finite number of elements.
We called such spaces finite-dimensional and defined the dimension of the vector
space to be the number of elements in a basis.
   The first objective in this section is to prove analogs of these results in the
infinite-dimensional case. We shall make use of Zorn’s Lemma as in Section A5
of the appendix, as well as the notion of cardinality discussed in Section A6 of the
appendix. Once these analogs are in place, we shall examine the various results
that we proved about finite-dimensional spaces to see the extent to which they
remain valid for infinite-dimensional spaces.

     Theorem 2.42. If V is any vector space over F, then
     (a)   any spanning set in V has a subset that is a basis,
     (b)   any linearly independent set in V can be extended to a basis,
     (c)   V has a basis,
     (d)   any two bases have the same cardinality.

   REMARKS. The common cardinality mentioned in (d) is called the dimension
of the vector space V . In many applications it is enough to use +∞ in place of
each infinite cardinal in dimension formulas. This was the attitude conveyed in
the remark with Corollary 2.24.
    PROOF. For (b), let E be the given linearly independent set, and let S be the
collection of all linearly independent subsets of V that contain E. Partially order
S by inclusion upward. The set S is nonempty because E is in S. Let T be a
chain in S, and let A be the union of the members of T . We show that A is in
S, and then A is certainly an upper bound of T . Because of its definition, A
contains E, and we are to prove that A is linearly independent. For A to fail to
be linearly independent would mean that there are vectors v1 , . . . , vn in A with
c1 v1 + · · · + cn vn = 0 for some system of scalars not all 0. Let v j be in the
                          9. Bases in the Infinite-Dimensional Case                     79

member A j of the chain T . Since A1 ⊆ A2 or A2 ⊆ A1 , v1 and v2 are both in
A1 or both in A2 . To keep the notation neutral, say they are both in A02 . Since
A02 ⊆ A3 or A3 ⊆ A02 , all of v1 , v2 , v3 are in A02 or they are all in A3 . Say they
are all in A03 . Continuing in this way, we arrive at one of the sets A1 , . . . , An ,
say A0n , such that all of v1 , . . . , vn are all in A0n . The members of A0n are linearly
independent by assumption, and we obtain the contradiction c1 = · · · = cn = 0.
We conclude that A is linearly independent. Thus the chain T has an upper bound
in S. By Zorn’s Lemma, S has a maximal element, say M. By Proposition 2.1a,
M is a basis of V containing E.
    For (a), let E be the given spanning set, and let S be the collection of all
linearly independent subsets of V that are contained in E. Partially order S by
inclusion upward. The set S is nonempty because ∅ is in S. Let T be a chain in
S, and let A be the union of the members of T . We show that A is in S, and then
A is certainly an upper bound of T . Because of its definition, A is contained in
E, and the same argument as in the previous paragraph shows that A is linearly
independent. Thus the chain T has an upper bound in S. By Zorn’s Lemma, S
has a maximal element, say M. Proposition 2.1a is not applicable, but its proof is
easily adjusted to apply here to show that M spans V and hence is a basis: Given
v in V , we are to prove that v lies is the linear span of M. First suppose that v
is in E. If v is in M, there is nothing to prove. Since M ∪ {v} is contained in
E, the assumed maximality implies that M ∪ {v} is not linearly independent, and
hence cv + c1 v1 + · · · + cn vn = 0 for some scalars c, c1 , . . . , cn not all 0 and
for some vectors v1 , . . . , vn in M. The scalar c cannot be 0 since M is linearly
independent. Thus v = −c−1 c1 v1 − · · · − c−1 cn vn , and v is exhibited as in the
linear span of M. Consequently every member of E lies in the linear span of M.
Now suppose that v is not in E. Since every member of V lies in the linear span
of E, every member of V lies in the linear span of M.
    Conclusion (c) follows from (a) by taking the spanning set to be V ; alternatively
it follows from (b) by taking the linearly independent set to be ∅.
    For (d), let A = {vα } and B = {wβ } be two bases of V . Each member a of A
can be written as a = c1 wβ1 + · · · + cn wβn uniquely with the scalars c1 , . . . , cn
nonzero and with each wβj in B. Let Ba be the finite subset {wβ1 , . . . , wβn }. Then
S have associated to each member of A a finite subset Ba of B. Let us see that
we
   a∈A Ba = B. If b is in B, then the linear span of B − {b} is not all of V . Thus
some v in V is not in this span. Expand v in terms of A as v = d1 vα1 +· · ·+dm vαm
with all d j 6= 0. Since v is not in the linear span of B − {b}, some a0 = vαj0
with 1 ≤ S  j0 ≤ m is not in this linear span. Then b is in Ba0 , and we conclude
that B = a∈A Ba . By the corollary near the end of Section A6 of the appendix,
card B ≤ card A. Reversing the roles of A and B, we obtain card A ≤ card B.
By the Schroeder–Bernstein Theorem, A and B have the same cardinality. This
proves (d).                                                                              §
80                         II. Vector Spaces over Q, R, and C

   Now let us go through the results of the chapter and see how many of them
extend to the infinite-dimensional case and why. It is possible but not very useful
in the infinite-dimensional case to associate an infinite “matrix” to a linear map
when bases or ordered bases are specified for the domain and range. Because this
association is not very useful, we shall not attempt to extend any of the results
concerning matrices. The facts concerning extensions of results just dealing with
dimensions and linear maps are as follows:

  COROLLARY 2.5. If V is any vector space and U is a vector subspace, then
dim U ≤ dim V .
  In fact, take a basis of U and extend it to a basis of V ; a basis of U is then
exhibited as a subset of a basis of V , and the conclusion about cardinal-number
dimensions follows.
   PROPOSITION 2.13. Let U and V be vector spaces over F, and let 0 be a basis
of U . Then to each functionØ ` : 0 → V corresponds one and only one linear
map L : U → V such that L Ø0 = `.
  In fact, the proof given in Section 3 is valid with no assumption about finite
dimensionality.
   COROLLARY 2.15. If L : U → V is a linear map between vector spaces over
F, then
          dim(domain(L)) = dim(kernel(L)) + dim(image(L)).
   In fact, this formula remains valid, but the earlier proof via matrices has to be
replaced. Instead, take a basis {vα | α ∈ A} of the kernel and extend it to a basis
{vα | α ∈ S} of the domain. It is routine to check that {L(vα ) | α ∈ S − A} is a
basis of the image of L.
     THEOREM 2.16 (part). The composition of two linear maps is linear.
  In fact, the proof in Section 3 remains valid with no assumption about finite
dimensionality.
   PROPOSITION 2.18. Two vector spaces over F are isomorphic if and only if
they have the same cardinal-number dimension.
   In fact, this result follows from Proposition 2.13 just as it did in the finite-
dimensional case; the only changes that are needed in the argument in Section 3
are small adjustments of the notation. Of course, one must not overinterpret this
result on the basis of the remark with Theorem 2.42: two vector spaces with
dimension +∞ need not be isomorphic. Despite the apparent definitive sound of
Proposition 2.18, one must not attach too much significance to it; vector spaces
that arise in practice tend to have some additional structure, and an isomorphism
based merely on equality of dimensions need not preserve the additional structure.
                          9. Bases in the Infinite-Dimensional Case                     81

  PROPOSITION 2.19. If V is a vector space and V 0 is its dual, then dim V ≤
dim V 0 . (In the infinite-dimensional case we do not have equality.)
   In fact, take a basis {vα } of V . If for each α we define vα0 (vβ ) = δαβ and use
Proposition 2.13 to form the linear extension vα0 , then the set {vα0 } is a linearly
independent subset of V 0 that is in one-one correspondence with the basis of V .
Extending {vα0 } to a basis of V 0 , we obtain the result.
   PROPOSITION 2.20. Let V be a vector space, and let U be a vector subspace of
V . Then
    (b) every linear functional on U extends to a linear functional on V ,
    (c) whenever v0 is a member of V that is not in U , there exists a linear
        functional on V that is 0 on U and is 1 on v0 .
Conclusion (a) of the original Proposition 2.20, which concerns annihilators, does
not extend to the infinite-dimensional case.
    To prove (b) without the finite dimensionality, let u 0 be a given linear functional
on U , let {u α } be a basis of U , and let {vβ } be a subset of V such that {u α } ∪ {vβ }
is a basis of V . Define v 0 (u α ) = u 0 (u α ) for each α and v 0 (vβ ) = 0 for each β.
Using Proposition 2.13, let v 0 be the linear extension to a linear functional on V .
Then v 0 has the required properties.
    To prove (c) without the finite dimensionality, we take a basis {u α } of U and
extend {u α } ∪ {v0 } to a basis of V . Define v 0 to equal 0 on each u α , to equal 1 on
v0 , and to equal 0 on the remaining members of the basis of V . Then the linear
extension of v 0 to V is the required linear functional.
   PROPOSITION 2.22. If V is any vector space over F, then the canonical map
∂ : V → V 00 is one-one. The canonical map is not onto V 00 if V is infinite-
dimensional.
  The proof that it is one-one given in Section 4 is applicable in the infinite-
dimensional case since we know from Theorem 2.42 that any linearly independent
subset of V can be extended to a basis. For the second conclusion when V has a
countably infinite basis, see Problem 31 at the end of the chapter.
   PROPOSITION 2.23 THROUGH COROLLARY 2.29. For these results about quo-
tients, the only place that finite dimensionality played a role was in the dimension
formulas, Corollaries 2.24 and 2.29. We restate these two results separately.
   COROLLARY 2.24. If V is a vector space over F and U is a vector subspace,
then
    (a) dim V = dim U + dim(V /U ),
    (b) the subspace U is the kernel of some linear map defined on V .
82                          II. Vector Spaces over Q, R, and C

   The proof in Section 5 requires no changes: Let q be the quotient map. The
linear map q meets the conditions of (b). For (a), take a basis of U and extend to
a basis of V . Then the images under q of the additional vectors form a basis of
V /U .
   COROLLARY 2.29. Let M and N be vector subspaces of a vector space V over
F. Then
            dim(M + N ) + dim(M ∩ N ) = dim M + dim N .
  In fact, Corollary 2.24a gives us dim(M + N ) = dim((M + N )/M) + dim M.
Substituting dim((M + N )/M) = dim(N /(M ∩ N )) from Theorem 2.28 and
adding dim(M ∩ N ) to both sides, we obtain dim(M + N ) + dim(M ∩ N ) =
dim(M ∩ N ) + dim(N /(M ∩ N )) + dim M. The first two terms on the right side
add to dim N by Corollary 2.24a, and the result follows.
   PROPOSITIONS 2.30 THROUGH 2.33. These results about direct products and
direct sums did not assume any finite dimensionality.
   The determinants of Sections 7–8 have no infinite-dimensional generalization,
and Proposition 2.41 is the only result in those two sections with a valid infinite-
dimensional analog. The valid analog in the infinite-dimensional case is that
eigenvectors for distinct eigenvalues under a linear map are linearly independent.
The proof given for Proposition 2.41 in Section 8 adapts to handle this analog,
                                      ( j)
provided we interpret components vi of a vector vi as the coefficients needed
to expand vi in a basis of the underlying vector space.


                                    10. Problems

1.   Determine bases of the following subsets of R3 :
     (a) the plane 3x − 2y + 5z = 0,
                  Ω            æ
                      x = 2t
     (b) the line y = −t , where −∞ < t < ∞.
                      z = 4t
2.   This problem shows that the associativity law in the definition of “vector space”
     implies certain more complicated formulas of which the stated law is a special
     case. Let v1 , . . . , vn be vectors in a vector space V . The only vector-space
     properties that are to be used in this problem are associativity of addition and the
     existence of the 0 element.
     (a) Define v(k) inductively upward by v(0) = 0 and v(k) = v(k−1) + vk , and
          define v (l) inductively downward by v (n+1) = 0 and v (l) = vl + v (l+1) .
          Prove that v(k) + v (k+1) is always the same element for 0 ≤ k ≤ n.
     (b) Prove that the same element of V results from any way of inserting paren-
          theses in the sum v1 + · · · + vn so that each step requires the addition of
          only two members of V .
                                      10. Problems                                    83

3.   This problem shows that the commutative and associative laws in the definition
     of “vector space” together imply certain more complicated formulas of which the
     stated commutative law is a special case. Let v1 , . . . , vn be vectors in a vector
     space V . The only vector-space properties that are to be used in this problem are
     commutativity of addition and the properties in the previous problem. Because
     of the previous problem, v1 + · · · + vn is a well-defined element of V , and it is
     not necessary to insert any parentheses in it. Prove that v1 + v2 + · · · + vn =
     vσ (1) + vσ (2) + · · · + vσ (n) for each permutation σ of {1, . . . , n}.
                             µ 1 2 −1 ∂
4.   For the matrix A = 2 4 6 , find
                            0 0 −8
     (a) a basis for the row space,
     (b) a basis for the column space, and
     (c) the rank of the matrix.
5.   Let A be an n-by-n matrix of rank one. Prove that there exists an n-dimensional
     column vector c and an n-dimensional row vector r such that A = cr.
6.   Let A be a k-by-n matrix, and let R be a reduced row-echelon form of A.
     (a) Prove for each r that the rows of R whose first r entries are 0 form a basis
         for the vector subspace of all members of the row space of A whose first r
         entries are 0.
     (b) Prove that the reduced row-echelon form of A is unique in the sense that any
         two sequences of steps of row reduction lead to the same reduced form.
7.   Let E be an finite set of N points, let V be the N -dimensional vector space of
     all real-valued functions on E, and let n be an integer with 0 < n ≤ N . Suppose
     that U is an n-dimensional subspace of V . Prove that there exists a subset D of
     n points in E such that the vector space of restrictions to D of the members of
     U has dimension n.
                          2      2
8.   ≥ linear ¥map L : R → R is given in the standard ordered
     A                                                           ≥ by
                                                           n≥ ¥ basis ¥o the matrix
      −6 −12                                                  3    −4
        6 11
                . Find the matrix of L in the ordered basis −2 , 3 .
9.   Let V be the real vector space of all polynomials in x of degree ≤ 2, and let
     L : V → V be the linear map I − D 2 , where I is the identity and D is the
     differentiation operator d/dx. Prove that L is invertible.
10. Let A be in Mkm (C) and B be in Mmn (C). Prove that

                            rank(AB) ≤ max(rank A, rank B).

11. Let A be in Mkn (C) with k > n. Prove that there exists no B in Mnk (C) with
    AB = I .
12. Let A be in Mkn (C) and B be in Mnk (C). Give an example with k = n to show
    that rank(AB) need not equal rank(B A).
84                          II. Vector Spaces over Q, R, and C

13. With the differential equation y 00 (t) = y(t) in Example 2 of Section 3, two
    examples of linear functionals on the vector space of solutions are given by
    `1 (y) = y(0) and `2 (y) = y 0 (0). Find a basis of the space of solutions such that
    {`1 , `2 } is the dual basis.
14. Suppose that a vector space V has a countably infinite basis. Prove that the dual
    V 0 has an uncountable linearly independent set.
15. (a) Give an example of a vector space and three vector subspaces L, M, and N
        such that L ∩ (M + N ) 6= (L ∩ M) + (L ∩ N ).
    (b) Show that inclusion always holds in one direction in (a).
    (c) Show that equality always holds in (a) if L ⊇ M.
16. Construct three vector subspaces M, N1 , and N2 of a vector space V such that
    M ⊕ N1 = M ⊕ N2 = V but N1 6= N2 . What is the geometric picture
    corresponding to this situation?
17. Suppose that x, y, u, and v are vectors in R4 ; let M and N be the vector subspaces
    of R4 spanned by {x, y} and {u, v}, respectively. In which of the following cases
    is it true that R4 = M ⊕ N ?
    (a) x = (1, 1, 0, 0), y = (1, 0, 1, 0), u = (0, 1, 0, 1), v = (0, 0, 1, 1);
    (b) x = (−1, 1, 1, 0), y = (0, 1, −1, 1), u = (1, 0, 0, 0), v = (0, 0, 0, 1);
    (c) x = (1, 0, 0, 1), y = (0, 1, 1, 0), u = (1, 0, 1, 0), v = (0, 1, 0, 1).
18. Section 6 gave definitions and properties of projections and injections associated
    with the direct sum of two vector spaces. Write down corresponding definitions
    and properties for projections and injections in the case of the direct sum of n
    vector spaces, n being an integer > 2.
19. Let T : Rn → Rn be a linear map with ker T ∩ image T = 0.
    (a) Prove that Rn = ker T ⊕ image T .
    (b) Prove that the condition ker T ∩ image T = 0 is satisfied if T 2 = T .
20. If V1 and V2 are two vector spaces over F, prove that (V1 ⊕ V2 )0 is canonically
    isomorphic to V10 ⊕ V20 .
21. Suppose that M is a vector subspace of a vector space V and that q : V → V /M
    is the quotient map. Corresponding to each linear functional y on V /M is a
    linear functional z on V given by z = yq. Why is the correspondence y 7→ z an
    isomorphism between (V /M)0 and Ann M?
22. Let M be a vector subspace of the vector space V , and let q : V → V /M be the
    quotient map. Suppose that N is a vector subspace of V . Prove that V = M ⊕ N
    if and only if the restriction of q to N is an isomorphism of N onto V /M.
23. For a square matrix A of integers, prove that the inverse has integer entries if and
    only if det A = ±1.
                                       10. Problems                                            85

24. Let A be in Mkn (C), and let r = rank A. Prove that r is the largest integer
    such that there exist r row indices i 1 , . . . , ir and r column indices j1 , . . . , jr
    for which the r-by-r matrix formed from these rows and columns of A has
    nonzero determinant. (Educational note: This problem characterizes the subset
    of matrices of rank ≤ r − 1 as the set in which all determinants of r-by-r
    submatrices are zero.)

25. Suppose that a linear combination of functions t 7→ ect with c real vanishes for
    every integer t ∏ 0. Prove that it vanishes for every real t.
                                                    ≥     ¥
                                                       01
26. Find all eigenvalues and eigenvectors of A = −6 5 .

27. Let A and C be n-by-n matrices with C invertible. By making a direct calculation
    with the entries, prove that Tr(C −1 AC) = Tr A.
                                                         0 1 0 0         0    0 
                                                                   0 0 1 0             0   0
                                                                  0 0 0 1             0   0
                                                                                               
                                                                                              
                                                                  0 0 0 0             0   0   
28. Find the characteristic polynomial of the n-by-n matrix                                   .
                                                                             ..               
                                                                                  .           
                                                                   0 0 0 0 ··· 0          1
                                                                   a0 a1 a2 a3 ··· an−2 an−1

29. Let A and B be in Mnn (C).
    (a) Prove under the assumption that A is invertible that det(∏I − AB) =
        det(∏I − B A).
    (b) By working with A + ≤ I and letting ≤ tend to 0, show that the assumption
        in (a) that A is invertible can be dropped.

30. In proving Theorem 2.42a, it is tempting to argue by considering all spanning
    subsets of the given set, ordering them by inclusion downward, and seeking a
    minimal element by Zorn’s Lemma. Give an example of a chain in this ordering
    that has no lower bound, thereby showing that this line of argument cannot work.

Problems 31–34 concern annihilators. Let V be a vector space, let M and N be vector
subspaces, and let ∂ : V → V 00 be the canonical map.
31. If V has an infinite basis, how can we conclude that ∂ does not carry V onto V 00 ?

32. Prove that Ann(M + N ) = Ann M ∩ Ann N .

33. Prove that Ann(M ∩ N ) = Ann M + Ann N .

34. (a) Prove that ∂(M) ⊆ Ann(Ann M).
    (b) Prove that equality holds in (a) if V is finite-dimensional.
    (c) Give an infinite-dimensional example in which equality fails in (a).
86                           II. Vector Spaces over Q, R, and C

Problems 35–39 concern operations by blocks within matrices.

35. Let A be a k-by-m matrix of the form A = ( A1 A2 ), where A1 has size
    k-by-m 1 , A2 has size                                               0
                        µ k-by-m
                            ∂    2 , and m 1 + m 2 = m. Let B by an m -by-n matrix
                         B1
    of the form B =          , where B1 has size m 01 -by-n, B2 has size m 02 -by-n, and
                         B2
    m 01 + m 02 = m 0 .
    (a) If m 1 = m 01 and m 2 = m 02 , prove
                                       µ     that AB =∂A1 B1 + A2 B2 .
                                         B1 A1 B1 A2
    (b) If k = n, prove that B A =                      .
                                         B2 A2 B2 A2
    (c) Deduce a general rule for block multiplication of matrices that are in 2-by-2
         block form.

36. Let µA be in∂Mkk (C), B be in Mkn (C), and D be in Mnn (C). Prove that
          A B
    det          = det A det D.
          0 D

                              ∂ (C). Suppose that A is invertible and that AC =
37. Let A, B, C, and Dµbe in Mnn
                        A B
    C A. Prove that det          = det(AD − C B).
                        C D

38. Let A be in Mkn (C) and B be in Mnk (C) with k ≤ n. Let Ik be the k-by-
    k identity, and let In be the n-by-n identity. Using Problem 29, prove that
    det(∏In − B A) = ∏n−k det(∏Ik − AB).

39. Prove the following block-form generalization of the expansion-in-cofactors
    formula. For each subset S of {1, . . . , n}, let S c be the complementary subset
    within {1, . . . , n}, and let sgn(S, S c ) be the sign of the permutation that carries
    (1, . . . , n) to the members of S in order, followed by the members of S c in order.
    Fix k with 1 ≤ k ≤ n − 1, and let the subset S have |S| = k. For an n-by-n
    matrix A, define A(S) to be the square matrix of size k obtained by using the
    rows of A indexed by 1, . . . , k and the columns indexed by the members of S.
          b
    Let A(S)      be the square matrix of size k − 1 obtained by using the rows of A
    indexed by k + 1, . . . , n and the columns indexed by the members of S c . Prove
    that

                                  X
                     det A =                  sgn(S, S c ) det A(S) det A(S).
                                                                        b
                               S⊆{1,...,n},
                                 |S|=k




Problems 40–44 compute the determinants of certain matrices known as Cartan
matrices. These have geometric significance in the theory of Lie groups.
                                    10. Problems                                      87
                                     2 −1 0 0 ···            0   0
                                     −1 2 −1 0 ···            0   0
                                    0 −1 2 −1 ···            0   0
                                                                    
                                   
                                    0 0 −1 2 ···             0   0
40. Let An be the n-by-n matrix                                   . Using expansion in
                                                    ..            
                                                         .        
                                       0    0   0   0 ··· 2 −1
                                       0    0   0   0 ··· −1 2
    cofactors about the last row, prove that det An = 2 det An−1 − det An−2 for
    n ∏ 3.

41. Computing det A1 and det A2 directly and using the recursion in Problem 40,
    prove that det An = n + 1 for n ∏ 1.
42. Let Cn for n ∏ 2 be the matrix An except that the (1, 2)th entry is changed from
    −1 to −2.
    (a) Expanding in cofactors about the last row, prove that the argument of Prob-
        lem 40 is still applicable when n ∏ 4 and a recursion formula for det Cn
        results with the same coefficients.
    (b) Computing det C2 and det C3 directly and using the recursion equation in
        (a), prove that det Cn = 2 for n ∏ 2.
43. Let Dn for n ∏ 3 be the matrix An except that the upper left 3-by-3 piece is
                   µ 2 −1 0 ∂ µ 2 0 −1 ∂
    changed from −1 2 −1 to            0 2 −1 .
                      0 −1   2       −1 −1      2
    (a) Expanding in cofactors about the last row, prove that the argument of Prob-
        lem 40 is still applicable when n ∏ 5 and a recursion formula for det Dn
        results with the same coefficients.
    (b) Show that D3 can be transformed into A3 by suitable interchanges of rows
        and interchanges of columns, and conclude that det D3 = det A3 = 4.
    (c) Computing det D4 directly and using (b) and the recursion equation in (a),
        prove that det Dn = 4 for n ∏ 3.
44. Let E n for n ∏ 4 be the matrix An except that the upper left 4-by-4 piece is
                                                 
                      2 −1 0 0              2 −1 0 0
                     −1 2 −1 0            −1 2 0 −1 
    changed from     0 −1 2 −1
                                    to     0 0 2 −1
                                                      .
                      0 0 −1 2              0 −1 −1 2
    (a) Expanding in cofactors about the last row, prove that the argument of Prob-
        lem 40 is still applicable when n ∏ 6 and a recursion formula for det E n
        results with the same coefficients.
    (b) Show that E 4 can be transformed into A4 by suitable interchanges of rows
        and interchanges of columns, and conclude that det E 4 = det A4 = 5.
    (c) Show that E 5 can be transformed into D5 by suitable interchanges of rows
        and interchanges of columns, and conclude that det E 5 = det D5 = 4.
    (d) Using (b) and (c) and the recursion equation in (a), prove that det E n = 9−n
        for n ∏ 4.
88                           II. Vector Spaces over Q, R, and C

Problems 45–48 relate determinants to areas and volumes. They begin by showing
how a computation of an area in R2 leads to a determinant, they then show how
knowledge of the answer and of the method of row reduction illuminate the result,
and finally they indicate how the result extends to R3 . If u and v are vectors in R2 , let
us say that the parallelogram determined by u and v is the parallelogram with vertices
0, u, v, and u + v. If u, v, and w are in R3 , the parallelepiped determined by u, v, and
w is the parallelepiped with vertices 0, u, v, w, u + v, u + w, v + w, and u + v + w.
45. The area of a trapezoid is the product of the average of the two parallel sides by
                                            ≥ ¥Compute the area of the parallelogram
     the distance between≥the¥ parallel sides.
     determined by u = ac and v = db in the diagram below as the area of a
     large rectangle minus the area ≥of two ¥ trapezoids minus the area of two triangles,
                                        a b
     recognizing the answer as det c d except for a minus sign. To what extent is
     the answer dependent on the picture?



                   c



                   d


                                   a                              b

              FIGURE 2.6. Area of a parallelogram as a difference of areas.
                                                                             ≥ ¥
46. What is the geometric effect on the parallelogram of replacing the matrix ac db
                  ≥ ¥≥ ¥                                    ≥ ¥       ≥ ¥
    by the matrix ac db    1s
                           01
                               , i.e., of right-multiplying  a b
                                                             c d
                                                                  by    1s
                                                                        01
                                                                            ? What
    does this change do to the area? What algebraic operation does this change
    correspond to?

            ≥ same
47. Answer the        ¥ ≥ ¥as in Problem 46 for right multiplication
                ¥ ≥ questions                           ≥ ¥          by the
             10    01     q 0                            10
    matrices t 1 , 1 0 , 0 1 for a nonzero number q, and 0 r for a nonzero
     number r.
48. Explain on the basis of Problems 45–47 why if three column vectors u, v, and w
    in R3 are assembled into a 3-by-3 matrix A and A is invertible, then the volume
    of the parallelepiped determined by u, v, and w has to be | det A|.
                                       CHAPTER III

                                Inner-Product Spaces



Abstract. This chapter investigates the effects of adding the additional structure of an inner product
to a finite-dimensional real or complex vector space.
    Section 1 concerns the effect on the vector space itself, defining inner products and their cor-
responding norms and giving a number of examples and formulas for the computation of norms.
Vector-space bases that are orthonormal play a special role.
    Section 2 concerns the effect on linear maps. The inner product makes itself felt partly through
the notion of the adjoint of a linear map. The section pays special attention to linear maps that are
self-adjoint, i.e., are equal to their own adjoints, and to those that are unitary, i.e., preserve norms of
vectors.
    Section 3 proves the Spectral Theorem for self-adjoint linear maps on finite-dimensional inner-
product spaces. The theorem says in part that any self-adjoint linear map has an orthonormal basis
of eigenvectors. The Spectral Theorem has several important consequences, one of which is the
existence of a unique positive semidefinite square root for any positive semidefinite linear map. The
section concludes with the polar decomposition, showing that any linear map factors as the product
of a unitary linear map and a positive semidefinite one.



                       1. Inner Products and Orthonormal Sets

In this chapter we examine the effect of adding further geometric structure to
the structure of a real or complex vector space as defined in Chapter II. To be
a little more specific in the cases of R2 and R3 , the development of Chapter II
amounted to working with points, lines, planes, coordinates, and parallelism, but
nothing further. In the present chapter, by comparison, we shall take advantage
of additional structure that captures the notions of distances and angles.
   We take F to be R or C, continuing to call its members the scalars. We
do not allow F to be Q in this chapter; the main results will make essential
use of additional facts about R and C beyond those of addition, subtraction,
multiplication, and division. The relevant additional facts are summarized in
Sections A3 and A4 of the appendix.1
   1 The theory of Chapter II will be observed in Chapter IV to extend to any “field” F in place of Q

or R or C, but the theory of the present chapter is limited to R and C, as well as some other special
fields that we shall not try to isolate.

                                                    89
90                                   III. Inner-Product Spaces

   Many of the results that we obtain will be limited to the finite-dimensional case.
The theory of inner-product spaces that we develop has an infinite-dimensional
generalization, but useful results for the generalization make use of a hypothesis
of “completeness” for an inner-product space that we are not in a position to
verify in examples.2
   Let V be a vector space over F. An inner product on V is a function from
V × V into F, which we here denote by ( · , · ), with the following properties:
     (i) the function u 7→ (u, v) of V into F is linear,
    (ii) the function v 7→ (u, v) of V into F is conjugate linear in the sense
         that it satisfies (u, v1 + v2 ) = (u, v1 ) + (u, v2 ) for v1 and v2 in V and
         (u, cv) = c̄(u, v) for v in V and c in F,
   (iii) (u, v) = (v, u) for u and v in V ,
   (iv) (v, v) ∏ 0 for all v in V ,
    (v) (v, v) = 0 only if v = 0 in V .
The overbars in (ii) and (iii) indicate complex conjugation. Property (ii) reduces
when F = R to the fact that v 7→ (u, v) is linear. Properties (i) and (ii) together
are summarized by saying that ( · , · ) is bilinear if F = R or sesquilinear if
F = C. Property (iii) is summarized when F = R by saying that ( · , · ) is
symmetric, or when F = C by saying that ( · , · ) is Hermitian symmetric.
   An inner-product space, for purposes of this book, is a vector space over R
or C with an inner product in the above sense.3,4

   EXAMPLES.
   (1) V = Rn with ( · ,√· ) !  as the dot √
                                           product, i.e., with (x, y) = y t x =
                             x1              y1 !
                              .              .
x1 y1 + · · · + xn yn if x = .. and y = .. . The traditional notation for the
                                   xn                  yn
dot product is x · y.
  (2)√ V != Cn with√ ( · !
                         , · ) defined by (x, y) = ȳ t x = x1 ȳ1 + · · · + xn ȳn if
          x1                y1
          ..                ..
x=         .   and y =       .   . Here ȳ denotes the entry-by-entry complex conjugate
          xn                yn
of y. The sesquilinear expression ( · , · ) is different from the complex bilinear
dot product x · y = x1 y1 + · · · + xn yn .
     2A careful study in the infinite-dimensional case is normally made only after the development
of a considerable number of topics in real analysis.
    3 When the scalars are complex, many books emphasize the presence of complex scalars by

referring to the inner product as a “Hermitian inner product.” This book does not need to distinguish
the complex case very often and therefore will not use the modifier “Hermitian” with the term “inner
product.”
    4 Some authors, particularly in connection with mathematical physics, reverse the roles of the

two variables, defining inner products to be conjugate linear in the first variable and linear in the
second variable.
                          1. Inner Products and Orthonormal Sets                     91

     (3) V equal to the vector space of all complex-valued polynomials with ( f, g) =
R1
 0    f (x)g(x) dx.
                                                                    p
   Let V be an inner-product space. If v is in V , define kvk = (v, v), calling
k · k the norm associated with the inner product. The norm of v is understood to
be the nonnegative square root of the nonnegative real number (v, v) and is well
defined
q        as a consequence of (iv). In the case of Rn , kxk is the Euclidean distance
  x12 + · · · + xn2 from the origin to the column vector x = (x1 , . . . , xn ). In this
interpretation the dot product of two nonzero vectors in Rn is shown in analytic
geometry to be given by x · y = kxkkyk cos θ, where θ is the angle between the
vectors x and y.
   Direct expansion of norms squared of sums of vectors using bilinearity or
sesquilinearity leads to certain formulas of particular interest. The formula that
we shall use most frequently is

                       ku + vk2 = kuk2 + 2 Re(u, v) + kvk2 ,

which generalizes from R2 a version of the law of cosines in trigonometry relating
the lengths of the three sides of a triangle when one of the angles is known. With
the additional hypothesis that (u, v) = 0, this formula generalizes from R2 the
Pythagorean Theorem

                              ku + vk2 = kuk2 + kvk2 .

Another such formula is the parallelogram law

         ku + vk2 + ku − vk2 = 2kuk2 + 2kvk2               for all u and v in V,

which is proved by computing ku + vk2 and ku − vk2 by the law of cosines and
adding the results. The name “parallelogram law” is explained by the geometric
interpretation in the case of the dot product for R2 and is illustrated in Figure 3.1.
That figure uses the familiar interpretation of vectors in R2 as arrows, two arrows
being identified if they are translates of one another; thus the arrow from v to u
represents the vector u − v.
   The parallelogram law is closely related to a formula for recovering the inner
product from the norm, namely
                                       1X k
                            (u, v) =       i ku + i k vk2 ,
                                       4 k

where the sum extends for k ∈ {0, 2} if the scalars are real and extends for
k ∈ {0, 1, 2, 3} if the scalars are complex. This formula goes under the name
92                                III. Inner-Product Spaces

polarization. To° prove it, we¢expand ku + i k vk2 = kuk2 + 2 Re(u, i k v) + kvk2
= kuk2 + 2 Re (−i)k (u, v) + kvk2 . Multiplying°       by i ¢k and summing on k
             P k                    P
shows that k i ku + i k vk2 = 2 k i k Re (−i)k (u, v) . If k is even, then
i k Re((−i)k P                           while if k is odd, then i k Re((−i)k z) =
             z) = Re z for any complex z,P
                 k          k
i Im z. So 2 k i Re((−i) z) = 4z, and k i k ku +i k vk2 = 4(u, v), as asserted.

                                                                  u+v
                              v

                                                     u−v

                                                              u
                          0

      FIGURE 3.1. Geometric interpretation of the parallelogram law: the sum
            of the squared lengths of the four sides of a parallelogram
             equals the sum of the squared lengths of the diagonals.

   Proposition 3.1 (Schwarz inequality).              In any inner-product space V ,
|(u, v)| ≤ kukkvk for all u and v in V .
   REMARK. The proof is written so as to use properties (i) through (iv) in the
definition of inner product but not (v), a situation often encountered with integrals.
   PROOF. Possibly replacing u by eiθ u for some real θ, we may assume that
(u, v) is real. In the case that kvk 6= 0, the law of cosines gives
      Ø                  Ø
      Øu − kvk−2 (u, v)v Ø2 = kuk2 − 2kvk−2 |(u, v)|2 + kvk−4 |(u, v)|2 kvk2 .

The left side is ∏ 0, and the right side simplifies to kuk2 − kvk−2 |(u, v)|2 . Thus
the inequality follows in this case.
   In the case that kvk = 0, it is enough to prove that (u, v) = 0 for all u. If c is
a scalar, then we have
                                °        ¢                            °         ¢
    ku + cvk2 = kuk2 + 2 Re c(u, v) + |c|2 kvk2 = kuk2 + 2 Re c(u, v) .

The left side is ∏ 0 as c varies, but the right side is < 0 for a suitable choice of c
unless (u, v) = 0. This completes the proof.                                        §

     Proposition 3.2. In any inner-product space V , the norm satisfies
      (a) kvk ∏ 0 for all v in V , with equality if and only if v = 0,
      (b) kcvk = |c|kvk for all v in V and all scalars c,
      (c) ku + vk ≤ kuk + kvk for all u and v in V .
                             1. Inner Products and Orthonormal Sets                              93

   PROOF. Conclusion (a) is immediate from properties (iv) and (v) of an inner
product, and (b) follows since kcvk2 = (cv, cv) = cc̄(v, v) = |c|2 kvk2 . Finally
we use the law of cosines and the Schwarz inequality (Proposition 3.1) to write
ku +vk2 = kuk2 +2 Re(u, v)+kvk2 ≤ kuk2 +2kukkvk+kvk2 = (kuk+kvk)2 .
Taking the square root of both sides yields (c).                               §

   Two vectors u and v in V are said to be orthogonal if (u, v) = 0, and one
sometimes writes u ⊥ v in this case. The notation is a reminder of the interpre-
tation in the case of dot product—that dot product 0 means that the cosine of the
angle between the two vectors is 0 and the vectors are therefore perpendicular.
An orthogonal set in V is a set of vectors such that each pair is orthogonal.
   The nonzero members of an orthogonal set are linearly independent. In fact, if
{v1 , . . . , vk } is an orthogonal set of nonzero vectors and some linear combination
has c1 v1 + · · · + ck vk = 0, then the inner product of this relation with v j gives
0 = (c1 v1 + · · · + ck vk , v j ) = c j kv j k2 , and we see that c j = 0 for each j.
   A unit vector in V is a vector u with kuk = 1. If v is any nonzero vector,
then v/kvk is a unit vector. An orthonormal set in V is an orthogonal set of
unit vectors. Under the assumption that V is finite-dimensional, an orthonormal
basis of V is an orthonormal set that is a vector-space basis.5

   EXAMPLES.
   (1) In Rn or Cn , the standard basis {e1 , . . . , en } is an orthonormal set.
   (2) Let V be the complex inner-product space of all complex finite linear
combinations, for n from −N to +N , of the functions Rx 7→ einx on the closed
                                                               1  π
interval [−π, π], the inner product being ( f, g) = 2π            −π f (x)g(x) dx. With
respect to this inner product, the functions einx form an orthonormal set.

   A simple but important exercise in an inner-product space is to resolve a vector
into the sum of a multiple of a given unit vector and a vector orthogonal to the
given unit vector. This exercise is solved as follows: If v is given and u is a unit
vector, then v decomposes as
                                          °            ¢
                          v = (v, u)u + v − (v, u)u .

Here
°     (v, u)u is ¢a multiple of u, and the two components are orthogonal since
 u, v − (v, u)u = (u, v) − (v, u)(u, u) = (u, v) − (u, v) = 0. This decom-
position is unique since if v = v1 + v2 with v1 = cu and (v2 , u) = 0, then the
inner product of v = v1 + v2 with u yields (v, u) = (cu, u) + (v2 , u) = c. Hence
   5 In  the infinite-dimensional theory the term “orthonormal basis” is used for an orthonormal set
that spans V when limits of finite sums are allowed, in addition to finite sums themselves; when V
is infinite-dimensional, an orthonormal basis is never large enough to be a vector-space basis.
94                                 III. Inner-Product Spaces

c must be (v, u), v1 must be (v, u)u, and v2 must be v − (v, u)u. Figure 3.2
illustrates the decomposition, and Proposition 3.3 generalizes it by replacing the
multiples of a single unit vector by the span of a finite orthonormal set.
                                                       v

                    v − (v, u)u

                                                                  (v, u)u
                                                    u
                                    0
          FIGURE 3.2. Resolution of v into a component (v, u)u parallel
              to a unit vector u and a component orthogonal to u.

   Proposition 3.3. Let V be an inner-product space. If {u 1 , . . . , u k } is an or-
thonormal set in V and if v is given in V , then there exists a unique decomposition
                              v = c1 u 1 + · · · + ck u k + v ⊥
with v ⊥ orthogonal to u j for 1 ≤ j ≤ k. In this decomposition c j = (v, u j ).
   REMARK. The proof illustrates a technique that arises often in mathematics.
We seek to prove an existence–uniqueness theorem, and we begin by making
calculations toward uniqueness that narrow down the possibilities. We are led to
some formulas or conditions, and we use these to define the object in question and
thereby prove existence. Although it may not be so clear except in retrospect, this
was the technique that lay behind proving the equivalence of various conditions
for the invertibility of a square matrix in Section I.6. The technique occurred
again in defining and working with determinants in Section II.7.
    PROOF OF UNIQUENESS. Taking the inner product of both sides with u j , we
obtain (v, u j ) = (c1 u 1 + · · · + ck u k + v ⊥ , u j ) = c j for each j. Then c j = (v, u j )
is forced, and v ⊥ must be given by v − (v, u 1 )u 1 − · · · − (v, u k )u k .               §
   PROOF OF EXISTENCE. Putting c j = (v, u j ), we need check only that the
difference v −(v, u 1 )u 1 −· · ·−(v, u k )u k is orthogonal to each u j with 1 ≤ j ≤ k.
Direct calculation gives
°     P                  ¢                P
 v − i (v, u i )u i , u j = (v, u j ) − i ((v, u i )u i , u j ) = (v, u j ) − (v, u j ) = 0,
and the proof is complete.                                                                   §

    Corollary 3.4 (Bessel’s inequality). Let V be an inner-product space. If
                                                                             P
{u 1 , . . . , u k } is an orthonormal set in V and if v is given in V , then kj=1 |(v, u j )|2
≤ kvk2 with equality if and only if v is in span{u 1 , . . . , u k }.
                             1. Inner Products and Orthonormal Sets                                 95
                                                               Pk
   PROOF. Using Proposition 3.3, write v =                          j=1 (v, u j )u j    + v ⊥ with v ⊥
orthogonal to u 1 , . . . , u k . Then
                       ° Pk                              Pk                              ¢
             kvk2 =           i=1 (v, u i )u i   + v⊥,      j=1 (v, u j )u j     + v⊥
                       P                                       °P                             ¢
                   =        i, j (v, u i )(v, u j )(u i , u j ) +    i   (v, u i )u i , v ⊥
                            ° ⊥ P                    ¢
                       +     v , j (v, u j )u j + kv ⊥ k2
                       P
                   =    i, j (v, u i )(v, u j )δi j + 0    + 0 + kv ⊥ k2
                       Pk               2          ⊥ 2
                   =    j=1 |(v, u j )| + kv k .


From Proposition 3.3 we know that v is in span{u 1 , . . . , u k } if and only if v ⊥ = 0,
and the corollary follows.                                                              §

   We shall now impose the condition of finite dimensionality in order to obtain
suitable kinds of orthonormal sets. The argument will enable us to give a basis-
free interpretation of Proposition 3.3 and Corollary 3.4, and we shall obtain
equivalent conditions for the vector v ⊥ in Proposition 3.3 and Corollary 3.4 to
be 0 for every v.
   If an ordered set of k linearly independent vectors in the inner-product space
V is given, the above proposition suggests a way of adjusting the set so that it
becomes orthonormal. Let us write the formulas here and carry out the verifi-
cation via Proposition 3.3 in the proof of Proposition 3.5 below. The method
of adjusting the set so as to make it orthonormal is called the Gram–Schmidt
orthogonalization process. The given linearly independent set is denoted by
{v1 , . . . , vk }, and we define
                           v1
                   u1 =         ,
                          kv1 k
                   u 02 = v2 − (v2 , u 1 )u 1 ,
                           u 02
                   u2 =           ,
                          ku 02 k
                   u 03 = v3 − (v3 , u 1 )u 1 − (v3 , u 2 )u 2 ,
                              u 03
                   u3 =              ,
                             ku 03 k
                       ..
                        .
                   u 0k = vk − (vk , u 1 )u 1 − · · · − (vk , u k−1 )u k−1 ,
                              u 0k
                   uk =              .
                             ku 0k k
96                                 III. Inner-Product Spaces

   Proposition 3.5. If {v1 , . . . , vk } is a linearly independent set in an inner-
product space V , then the Gram–Schmidt orthogonalization process replaces
{v1 , . . . , vk } by an orthonormal set {u 1 , . . . , u k } such that span{v1 , . . . , v j } =
span{u 1 , . . . , u j } for all j.
   PROOF. We argue by induction on j. The base case is j = 1, and the result
is evident in this case. Assume inductively that u 1 , . . . , u j−1 are well defined
and orthonormal and that span{v1 , . . . , v j−1 } = span{u 1 , . . . , u j−1 }. Proposition
3.3 shows that u 0j is orthogonal to u 1 , . . . , u j−1 . If u 0j = 0, then v j has to be
in span{u 1 , . . . , u j−1 } = span{v1 , . . . , v j−1 }, and we have a contradiction to the
assumed linear independence of {v1 , . . . , vk }. Thus u 0j 6= 0, and {u 1 , . . . , u j } is a
well-defined orthonormal set. This set must be linearly independent, and hence its
linear span is a j-dimensional vector subspace of the linear span of {v1 , . . . , v j }.
By Corollary 2.4, the two linear spans coincide. This completes the induction
and the proof.                                                                                §

   Corollary 3.6. If V is a finite-dimensional inner-product space, then any
orthonormal set in a vector subspace S of V can be extended to an orthonormal
basis of S.
   PROOF. Extend the given orthonormal set to a basis of S by Corollary 2.3b.
Then apply the Gram–Schmidt orthogonalization process. The given vectors do
not get changed by the process, as we see from the formulas for the vectors u 0j
and u j , and hence the result is an extension of the given orthonormal set to an
orthonormal basis.                                                             §

   Corollary 3.7. If S is a vector subspace of a finite-dimensional inner-product
space V , then S has an orthonormal basis.
   PROOF. This is the special case of Corollary 3.6 in which the given orthonormal
set is empty.                                                                   §

   The set of all vectors orthogonal to a subset M of the inner-product space V
is denoted by M ⊥ . In symbols,

                      M ⊥ = {u ∈ V | (u, v) = 0 for all v ∈ M}.

We see by inspection that M ⊥ is a vector subspace. Moreover, M ∩ M ⊥ = 0
since any u in M ∩ M ⊥ must have (u, u) = 0. The interest in the vector subspace
M ⊥ comes from the following proposition.

  Theorem 3.8 (Projection Theorem). If S is a vector subspace of the finite-
dimensional inner-product space V , then every v in V decomposes uniquely as
v = v1 + v2 with v1 in S and v2 in S ⊥ . In other words, V = S ⊕ S ⊥ .
                          1. Inner Products and Orthonormal Sets                       97

  REMARKS. Because of this proposition, S ⊥ is often called the orthogonal
complement of the vector subspace S.
   PROOF. Uniqueness follows from the fact that S ∩ S ⊥ = 0. For existence,
use of Corollaries 3.7 and 3.6 produces an orthonormal basis {u 1 , . . . , u r } of S
and extends it to an orthonormal basis {u 1 , . . . , u n } of V . The vectors u j for
 j > r are orthogonal toP   each u i with i ≤ r and hence are inPS ⊥ . If v is given
in S, we can write v = nj=1 u j as v = v1 + v2 with v1 = ri=1 (v, u i )u i and
      P
v2 = nj=r+1 (v, u j )u j , and this decomposition for all v shows that V = S + S ⊥ .
                                                                                    §

   Corollary 3.9. If S is a vector subspace of the finite-dimensional inner-product
space V , then
    (a) dim V = dim S + dim S ⊥ ,
   (b) S ⊥⊥ = S.
   PROOF. Conclusion (a) is immediate from the direct-sum decomposition V =
S ⊕ S ⊥ of Theorem 3.8. For (b), the definition of orthogonal complement gives
S ⊆ S ⊥⊥ . On the other hand, application of (a) twice shows that S and S ⊥⊥ have
the same finite dimension. By Corollary 2.4, S ⊥⊥ = S.                         §

   Section II.6 introduced “projection” mappings in the setting of any direct sum
of two vector spaces, and we shall use those mappings in connection with the
decomposition V = S ⊕ S ⊥ of Theorem 3.8. We make one adjustment in working
with the projections, changing their ranges from the image, namely S or S ⊥ , to
the larger space V . In effect, a linear map p1 or p2 as in Section II.6 will be
replaced by i 1 p1 or i 2 p2 .
   Specifically let E : V → V be the linear map that is the identity on S and is 0
on S ⊥ . Then E is called the orthogonal projection of V on S. The linear map
I − E is the identity on S ⊥ and is 0 on S. Since S = S ⊥⊥ , I − E is the orthogonal
projection of V on S ⊥ . It is the linear map that picks out the S ⊥ component
relative to the direct-sum decomposition V = S ⊥ ⊕ S ⊥⊥ . Proposition 3.3 and
Corollary 3.4 can be restated in terms of orthogonal projections.

   Corollary 3.10. Let V be a finite-dimensional inner-product space, let S be a
vector subspace of V , let {u 1 , . . . , u k } be an orthonormal basis of S, and let E be
the orthogonal projection of V on S. If v is in V , then
                                           k
                                           X
                                  E(v) =          (v, u j )u j
                                            j=1
                                           k
                                           X
and                           kE(v)k2 =           |(v, u j )|2 .
                                            j=1
98                               III. Inner-Product Spaces
                                     P
The vector v ⊥ in the expansion v = kj=1 (v, u j )u j + v ⊥ of Proposition 3.3 is
equal to (I − E)v, and the equality of norms
                                      k
                                      X
                           kvk2 =           |(v, u j )|2 + kv ⊥ k2
                                      j=1

has the interpretations that
                          kvk2 = kE(v)k2 + k(I − E)vk2
and that equality holds in Bessel’s inequality if and only if E(v) = v.
                        P
    PROOF. Write v = kj=1 (v, u j )u j + v ⊥ as in Proposition 3.3. Then each u j
is in S, and the vector v ⊥ , being orthogonal to each member of a basis of S, is in
S ⊥ . This proves the formula for E(v), and the formula for kE(v)k2 follows by
applying Corollary 3.4 to v − v ⊥ .
    Reassembling v, we now have v = E(v) + v ⊥ , and hence v ⊥ = v − E(v) =
(I − E)v. Finally the decomposition v = E(v) + (I − E)(v) is into orthogonal
terms, and the Pythagorean Theorem shows that kvk2 = kE(v)k2 + k(I − E)vk2 .
                                                                                  §

   Theorem 3.11 (Parseval’s equality). If V is a finite-dimensional inner-product
space, then the following conditions on an orthonormal set {u 1 , . . . , u m } are
equivalent:
    (a) {u 1 , . . . , u m } is a vector-space basis of V , hence an orthonormal basis,
               P vector orthogonal to all of u 1 , . . . , u m is 0,
    (b) the only
    (c) v = mj=1 (v, u j )u j for all v in V ,
                     P
    (d) kvk2 = mj=1 |(v, u j )|2 for all v in V ,
                       P
    (e) (v, w) = mj=1 (v, u j )(w, u j ) for all v and w in V .
   PROOF. Let S = span{u 1 , . . . , u m }, and let E be the orthogonal projection of
V on S. If (a) holds, then S = V and S ⊥ = 0. Thus (b) holds.
   If (b) holds, then S ⊥ = 0 and E is the identity. Thus (c) holds by Corollary
3.10.
   If (c) holds, then Corollary 3.4 shows that (d) holds.
   If (d) holds, we use polarization to prove (e). Let k be in {0, 2} if F = R, or in
{0, 1, 2, 3} if F = C. Conclusion (d) gives us
                 Xm                               Xm       °                     ¢
kv + i k wk2 =      |(v + i k w, u j )|2 = kvk2 +     2 Re (v, u j )i k (w, u j ) + kwk2 .
                j=1                                  j=1

Multiplying by i k and summing over k, we obtain
                           Xm X         °                        ¢
               4(v, w) = 2        i k Re (−i)k (v, u j )(w, u j ) .
                                j=1   k
                                       2. Adjoints                                99
                                                      P
In the proof of polarization, we saw that 2 k i k Re((−i)k z) = 4z. Hence
             P
4(v, w) = 4 mj=1 (v, u j )(w, u j ). This proves (e).
   If (e) holds, we take w = v in (e) and apply Corollary 3.10 to see that
kE(v)k2 = kvk2 for all v. Then k(I − E)vk2 = 0 for all v, and E(v) = v
for all v. Hence S = V , and {u 1 , . . . , u m } is a basis. This proves (a). §

   Theorem 3.12 (Riesz Representation Theorem). If ` is a linear functional on
the finite-dimensional inner-product space V , then there exists a unique v in V
with `(u) = (u, v) for all u in V .
   PROOF. Uniqueness is immediate by subtracting two such expressions, since if
(u, v) = 0 for all u, then the special case u = v gives (v, v) = 0 and v = 0. Let
us prove existence. If ` = 0, take v = 0. Otherwise let S = ker `. Corollary 2.15
shows that dim S = dim V − 1, and Corollary 3.9a then shows that dim S ⊥ = 1.
Let w be a nonzero vector in S ⊥ . This vector w must have `(w) 6= 0 since
S ∩ S ⊥ = 0, and we let v be the member of S ⊥ given by

                                      `(w)
                                     v=    w.
                                     kwk2
                          °    `(u)
                                     ¢                        `(u)
For any u in V , we have ` u − `(w) w = 0, and hence u −      `(w)   w is in S. Since
        ⊥         `(u)
v is in S , u −   `(w)   w is orthogonal to v. Thus

             ≥ `(u)    ¥ ≥ `(u)    `(w) ¥        `(w) kwk2
  (u, v) =         w, v =       w,      w = `(u)           = `(u).
              `(w)         `(w)    kwk2          `(w) kwk2
This proves existence.                                                             §


                                      2. Adjoints

Throughout this section, V will denote a finite-dimensional inner-product space
with inner product ( · , · ) and with scalars from F, with F equal to R or C. We
shall study aspects of linear maps L : V → V related to the inner product on V .
The starting point is to associate to any such L another linear map L ∗ : V → V
known as the “adjoint” of V , and then to investigate some of its properties.
A tool in this investigation will be the scalar-valued function on V × V given
by (u, v) 7→ (L(u), v), which captures the information in any matrix of L
without requiring the choice of an ordered basis. This function determines L
uniquely because an equality (L(u), v) = (L 0 (u), v) for all u and v implies
(L(u) − L 0 (u), v) = 0 for all u and v, in particular for v = L(u) − L 0 (u); thus
kL(u) − L 0 (u)k2 = 0 and L(u) = L 0 (u) for all u.
100                                 III. Inner-Product Spaces

   Proposition 3.13. Let L : V → V be a linear map on the finite-dimensional
inner-product space V . For each u in V , there exists a unique vector L ∗ (u) in V
such that
                  (L(v), u) = (v, L ∗ (u))       for all v in V .
As u varies, this formula defines L ∗ as a linear map from V to V .
   REMARK. The linear map L ∗ : V → V is called the adjoint of L.
   PROOF. The function v 7→ (L(v), u) is a linear functional on V , and Theorem
3.12 shows that it is given by the inner product with a unique vector of V . Thus
we define L ∗ (u) to be the unique vector of V with (L(v), u) = (v, L ∗ (u)) for all
v in V .
   If c is a scalar, then the uniqueness and the computation (v, L ∗ (cu)) =
(L(v), cu) = c̄(L(v), u) = c̄(v, L ∗ (u)) = (v, cL ∗ (u)) yield L ∗ (cu) = cL ∗ (u).
Similarly the uniqueness and the computation
      (v, L ∗ (u 1 + u 2 )) = (L(v), u 1 + u 2 ) = (L(v), u 1 ) + (L(v), u 2 )
                           = (v, L ∗ (u 1 )) + (v, L ∗ (u 2 )) = (v, L ∗ (u 1 ) + L ∗ (u 2 ))
yield L ∗ (u 1 + u 2 ) = L ∗ (u 1 ) + L ∗ (u 2 ). Therefore L ∗ is linear.                      §

    The passage L 7→ L ∗ to the adjoint is a function from HomF (V, V ) to itself that
is conjugate linear, and it reverses the order of multiplication: (L 1 L 2 )∗ = L ∗2 L ∗1 .
Since the formula (L(v), u) = (v, L ∗ (u)) in the proposition is equivalent to the
formula (u, L(v)) = (L ∗ (u), v), we see that L ∗∗ = L.
    All of the results in Section II.3 concerning the association of matrices to linear
maps are applicable here, but our interest now will be in what happens when the
bases we use are orthonormal. Recall from Section II.3 that if 0 = (u 1 , .µ. . , u n∂)
                                                                                  L
and 1 = (v1 , . . . , vn ) are any ordered bases of V , then the matrix A =
                                                         µ         ∂             10
                                                           L(u j )
associated to the linear map L : V → V has Ai j =                     .
                                                             1      i

   Lemma 3.14. If L : V → V is a linear map on the finite-dimensional inner-
product space V and if 0 = (u 1 , . . . , u n ) and 1    ∂ 1 , . . . , vn ) are ordered
                                                    µ = (v
                                                       L
orthonormal bases of V , then the the matrix A =           has Ai j = (L(u j ), vi ).
                                                      10
   PROOF. Applying Theorem 3.11c, we have
             µ           ∂      µP                            ∂
                 L(u j )             i 0 (L(u j ), vi 0 )vi 0
      Ai j =                =
                  1       i
                                            1                  i
             X                     µ ∂          X
                                    vi 0
           =      (L(u j ), vi 0 )          =         (L(u j ), vi 0 )δii 0 = (L(u j ), vi ).   §
               0
                                    1 i            0
                 i                                 i
                                            2. Adjoints                                         101

   Proposition 3.15. If L : V → V is a linear map on the finite-dimensional
inner-product space V and if 0 = (u 1 , . . . , u n ) and
                                                       µ 1=                   ) are ordered
                                                            ∂ (v1 , . . . , vnµ      ∂
                                                          L             ∗        L∗
orthonormal bases of V , then the matrices A =                and A =                  of L
                                                         10                     01
and its adjoint are related by Ai∗j = A ji .
    PROOF. Lemma 3.14 and the definition of L ∗ give Ai∗j = (L ∗ (v j ), u i ) =
(v j , L(u i )) = (L(u i ), v j ) = A ji .                                     §

   Accordingly, we define A∗ = A t for any square matrix A, sometimes calling
A∗ the adjoint6 of A.
   A linear map L : V → V is called self-adjoint if L ∗ = L. Correspondingly a
square matrix A is self-adjoint if A∗ = A. It is more common, however, to say
that a matrix with A∗ = A is symmetric if F = R or Hermitian7 if F = C. A
real Hermitian matrix is symmetric, and the term “Hermitian” is thus applicable
also when F = R.
   Any Hermitian matrix A arises from a self-adjoint linear map L. Namely,
we take V to be Fn with the usual inner product, and we let 0 and 1 each be
the standard ordered basis 6 = (e1 , . . . , en ). This basis is orthonormal, and we
define
µ      L by the matrix product L(v) = Av for any column vector v. We know that
       ∂
    L
          = A. Since A∗ = A, we conclude from Proposition 3.15 that L ∗ = L.
  66
Thus we are free to deduce properties of Hermitian matrices from properties of
self-adjoint linear maps.
   Self-adjoint linear maps will be of special interest to us. Nontrivial examples
of self-adjoint linear maps, constructed without simply writing down Hermitian
matrices, may be produced by the following proposition.

   Proposition 3.16. If V is a finite-dimensional inner-product space and S is a
vector subspace of V , then the orthogonal projection E : V → V of V on S is
self-adjoint.
   PROOF. Let v = v1 +v2 and u = u 1 +u 2 be the decompositions of two members
of V according to V = S ⊕ S ⊥ . Then we have (v, E ∗ (u)) = (E(v), u) =
(v1 , u 1 + u 2 ) = (v1 , u 1 ) = (v, u 1 ) = (v, E(u)), and the proposition follows by
the uniqueness in Proposition 3.13.                                                  §
    6 The name “adjoint” happens to coincide with the name for a different notion that arose in

connection with Cramer’s rule in Section II.7. The two notions never seem to arise at the same time,
and thus no confusion need occur.
    7 The term “Hermitian” is used also for a class of linear maps in the infinite-dimensional case,

but care is needed because the terms “Hermitian” and “self-adjoint” mean different things in the
infinite-dimensional case.
102                               III. Inner-Product Spaces

   To understand Proposition 3.16 in terms of matrices, take an ordered or-
thonormal basis (u 1 , . . . , u r ) of S, and extend it to an ordered orthonormal basis
0 = (u 1 , . . . , u n ) of V . Then
                                            Ω
                                               uj      for j ≤ r,
                                 E(u j ) =
                                               0       for j > r,
            µ             ∂
                E(u j )
and hence                   equals the j th standard basis vector e j if j ≤ r and equals 0 if
                   0                     µ      ∂
                                             E
 j > r. Consequently the matrix                   is diagonal with 1’s in the first r diagonal
                                           00
entries and 0’s elsewhere. This matrix is equal to its conjugate transpose, as it
must be according to Propositions 3.15 and 3.16.

    Proposition 3.17. If V is a finite-dimensional inner-product space and
L : V → V is a self-adjoint linear map, then (L(v), v) is in R for every v
in V , and consequently every eigenvalue of L is in R. Conversely if F = C and
if L : V → V is a linear map such that (L(v), v) is in R for every v in V , then L
is self-adjoint.
   REMARK. The hypothesis F = C is essential in the ≥converse.
                                                            ¥ In fact, the 90
                                                                              ◦
                                                        0 1
rotation L of R2 whose matrix in the standard basis is −1 0 is not self-adjoint
but does have L(v) · v = 0 for every v in R2 .
   PROOF. If L = L ∗ , then (L(v), v) = (v, L ∗ (v)) = (v, L(v)) = (L(v), v),
and hence (L(v), v) is real-valued. If v is an eigenvector with eigenvalue ∏, then
substitution of L(v) = ∏v into (L(v), v) = (L(v), v) gives ∏kvk2 = ∏¯ kvk2 .
Since v 6= 0, ∏ must be real.
   For the converse we begin with the special case that (L(w), w) = 0 for all w.
For 0 ≤ k ≤ 3, we then have

(−i)k (L(u), v)+i k (L(v), u) = (L(u+i k v), u+i k v)−(L(u), u)−(L(v), v) = 0.

Taking k = 0 gives (L(u), v) + (L(v), u) = 0, while taking k = 1 gives
(L(u), v) − (L(v), u) = 0. Hence (L(u), v) = 0 for all u and v. Since the
function (u, v) 7→ L(u, v) determines L, we obtain L = 0.
   In the general case, (L(v), v) real-valued implies that (L(v), v) = (L ∗ (v), v)
for all v. Therefore ((L − L ∗ )(v), v) = 0 for all v, and the special case shows
that L − L ∗ = 0. This completes the proof.                                      §

   We conclude this section by examining one further class of linear maps having
a special relationship with their adjoints.
                                        2. Adjoints                                    103

   Proposition 3.18. If V is a finite-dimensional inner-product space, then the
following conditions on a linear map L : V → V are equivalent:
    (a) L ∗ L = I ,
    (b) L carries some orthonormal basis of V to an orthonormal basis,
    (c) L carries each orthonormal basis of V to an orthonormal basis,
    (d) (L(u), L(v)) = (u, v) for all u and v in V ,
    (e) kL(v)k = kvk for all v in V .

   REMARK. A linear map satisfying these equivalent conditions is said to be
orthogonal if F = R and unitary if F = C.
    PROOF. We prove that (a), (d), and (e) are equivalent and that (b), (c), and (d)
are equivalent.
    If (a) holds and u and v are given in V , then (L(u), L(v)) = (L ∗ L(u), v) =
(I (u), v) = (u, v), and (d) holds. If (d) holds, then setting u = v shows that (e)
holds. If (e) holds, we use polarization twice to write
                          P  1 k          k     2
                                                          P    1 k
      (L(u), L(v)) =       k 4 i kL(u) + i L(v)k =           k 4 i kL(u   + i k v)k2
                          P 1 k        k  2
                      =    k 4 i ku + i vk = (u, v).

Then ((L ∗ L − I )(u), v) = 0 for all u and v, and we conclude that (a) holds.
   Since (b) is a special case of (c) and (c) is a special case of (d), proving that (b)
implies (d) will prove that (b), (c), and (d) are equivalent. Thus let {u 1 , . . . , u n }
be an orthonormal basis of V such that {L(u 1 ), . . . , L(u n )} is an orthonormal
basis, and let u and v be given. Then
                        ° °P                  ¢ °P                   ¢¢
       (L(u), L(v)) = L         i (u, u i )u i , L    j (v, u j )u j
                        P
                     = i, j (u, u i )(v, u j )(L(u i ), L(u j ))
                        P                            P
                     = i, j (u, u i )(v, u j )δi j = i (u, u i )(v, u i ) = (u, v),

the last equality following from Parseval’s equality (Theorem 3.11).                    §

   As with self-adjointness, we use the geometrically meaningful definition for
linear maps to obtain a definition for matrices: a square matrix A with A∗ A = I
is said to be orthogonal if F = R and unitary if F = C. The condition is that
                      Pinverse
A is invertible and its                              Pterms of individual entries,
                                equals its adjoint. In
                           ∗
the condition is that k Aik  Ak j = δi j , hence that k Aki Ak j = δi j . This is the
condition that the columns of A form an orthonormal basis relative to the usual
inner product on Rn or Cn . A real unitary matrix is orthogonal.
   If A is an orthogonal or unitary matrix, we can construct a corresponding
orthogonal or unitary linear map on Rn or Cn relative to the standard ordered
104                              III. Inner-Product Spaces

basis 6. Namely, we define L(v) = Av, and Proposition 3.15 shows that L is
orthogonal or unitary: L ∗ L(v) = A∗ Av = I v = v. Proposition 3.19 below
gives a converse.
   Let us notice that an orthogonal or unitary matrix A necessarily has | det A| = 1.
In fact, the formula A∗ = (A)t implies that det A∗ = det A. Then

         1 = det I = det A∗ A = det A∗ det A = det A det A = | det A|2 .

An orthogonal matrix thus has determinant ±1, while we conclude for a unitary
matrix only that the determinant is a complex number of absolute value 1.

   EXAMPLES.
   (1)≥The 2-by-2¥orthogonal matrices of determinant +1 are all matrices of the
         cos θ sin θ
form − sin θ cos θ . The 2-by-2 orthogonal matrices of determinant −1 are the
           ≥         ¥
              1 0
product of 0 −1 and the 2-by-2 orthogonal matrices of determinant +1.

        ¥ 2-by-2 unitary matrices of determinant +1 are all matrices of the form
≥ (2) The
  α β
 −β̄ ᾱ
         with |α|2 +|β|2 = 1; these may be regarded as parametrizing the points of
the unit sphere S 3 of R4 . The    ¥ unitary matrices of arbitrary determinant are
                             ≥ 2-by-2
                               1 0
the products of all matrices 0 eiθ and the 2-by-2 unitary matrices of determinant
+1.

    Proposition 3.19. If V is a finite-dimensional inner-product space, if 0 =
(u 1 , . . . , u n ) and 1 = (v1 , . . . , vn ) are ordered orthonormal bases of V , and if
L : V → V is a linear      µ map  ∂ that is orthogonal if F = R and unitary if F = C,
                               L
then the matrix A =                   is orthogonal or unitary.
                             10
                                                                       µ ∗ ∂µ         ∂
                                                                          L        L
    PROOF. Proposition 3.15 and Theorem 2.16 give A∗ A =                                =
µ         ∂                                                              01      10
     I
             , and the right side is the identity matrix, as required.                  §
  11
                                                                ∂µ
                                                             I
   One consequence of Proposition 3.19 is that any matrix          relative to two
                                                            10
ordered orthonormal bases is orthogonal or unitary, since the identity function
I : V → V is certainly orthogonal or unitary. Thus a change from writing the
matrix of a linear map L in one ordered orthonormal basis 0 to writing the matrix
µ L in
of    ∂ anotherµordered∂ orthonormal basis 1 is implemented by the µformula     ∂
    L         −1    L                                                        I
        =C               C, where C is the orthogonal or unitary matrix          .
   00              11                                                      10
                                   3. Spectral Theorem                                 105
                                                                     ∂     µ
                                                                   L
   Another consequence of Proposition 3.19 is that the matrix          of an
                                                                  00
orthogonal or unitary linear map L in an ordered orthonormal basis 0 is an
orthogonal
µ     ∂     or unitary matrix. We have defined det L to be the determinant of
   L
        relative to any 0, and we conclude that | det L| = 1.
  00


                                3. Spectral Theorem

In this section we deal with the geometric structure of certain kinds of linear maps
from finite-dimensional inner-product spaces into themselves. We shall see that
linear maps that are self-adjoint or unitary, among other possible conditions, have
bases of eigenvectors in the sense of Section II.8. Moreover, such a basis may
be taken to be orthonormal. When an ordered basis of eigenvectors is used for
expressing the linear map as a matrix, the result is that the matrix is diagonal.
Thus these linear maps have an especially uncomplicated structure. In terms of
matrices, the result is that a Hermitian or unitary matrix A is similar to a diagonal
matrix D, and the matrix C with D = C −1 AC may be taken to be unitary. We
begin with a lemma.

   Lemma 3.20. If L : V → V is a self-adjoint linear map on an inner-
product space V , then v 7→ (L(v), v) is real-valued, every eigenvalue of L is
real, eigenvectors under L for distinct eigenvalues are orthogonal, and every
vector subspace S of V with L(S) ⊆ S has L(S ⊥ ) ⊆ S ⊥ .
   PROOF. The first two conclusions are contained in Proposition 3.17. If v1 and
v2 are eigenvectors of L with distinct real eigenvalues ∏1 and ∏2 , then

 (∏1 − ∏2 )(v1 , v2 ) = (∏1 v1 , v2 ) − (v1 , ∏2 v2 ) = (L(v1 ), v2 ) − (v1 , L(v2 )) = 0.

Since ∏1 6= ∏2 , we must have (v1 , v2 ) = 0. If S is a vector subspace with
L(S) ⊆ S, then also L(S ⊥ ) ⊆ S ⊥ because s ∈ S and s ⊥ ∈ S ⊥ together imply

                            0 = (L(s), s ⊥ ) = (s, L(s ⊥ )).                             §

   Theorem 3.21 (Spectral Theorem). Let L : V → V be a self-adjoint linear
map on an inner-product space V . Then V has an orthonormal basis of eigenvec-
tors of L. In addition, for each scalar ∏, let

                             V∏ = {v ∈ V | L(v) = ∏v},

so that V∏ when nonzero is the eigenspace of L for the eigenvalue ∏. Then the
eigenvalues of L are all real, the vector subspaces V∏ are mutually orthogonal,
106                                    III. Inner-Product Spaces

and any orthonormal basis of V of eigenvectors of L is the union of orthonormal
bases of the V∏ ’s. Correspondingly if A is any Hermitian n-by-n matrix, then
there exists a unitary matrix C such that C −1 AC is diagonal with real entries. If
the matrix A has real entries, then C may be taken to be an orthogonal matrix.
   PROOF. Lemma 3.20 shows that the eigenvalues of L are all real and that the
vector subspaces V∏ are mutually orthogonal.
   To proceed further, we first assume that F = C. Applying the Fundamental
Theorem of Algebra (Theorem 1.18) to the characteristic polynomial of L, we see
that L has at least one eigenvalue, say ∏1 . Then L(V∏1 ) ⊆ V∏1 , and Lemma 3.20
shows that L((V∏1 )⊥ ) ⊆ (V∏1 )⊥Ø . The vector subspace (V∏1 )⊥ is an inner-product
space, and the claim is that L Ø(V∏ )⊥ is self-adjoint. In fact, if v1 and v2 are in
                                            1
(V∏1 )⊥ , then
              ° Ø                         ¢ °       Ø            ¢
               (L Ø(V∏   )⊥
                              )∗ (v1 ), v2 = v1 , L Ø(V∏    (v2 ) = (v1 , L(v2 ))
                                                               )⊥
                     1                                     1
                                                              ° Ø                ¢
                                            = (L(v1 ), v2 ) = L Ø(V∏ )⊥ (v1 ), v2 ,
                                                                       1


and the claim is proved. Since ∏1 is an eigenvalue of L, dim(V∏1 )⊥ < dim V .
Therefore we can now set up an induction that ultimately exhibits V as an orthog-
onal direct sum V = V∏1 ⊕ · · · ⊕ V∏k . If v is an eigenvector of L with eigenvalue
∏0 , then either ∏0 = ∏ j for some j in this decomposition, in which case v is in
V∏j , or ∏0 is not equal to any ∏ j , in which case v, by the lemma, is orthogonal
to all vectors in V∏1 ⊕ · · · ⊕ V∏k , hence to all vectors in V ; being orthogonal to
all vectors in V , v must be 0. Choosing an orthonormal basis for each V∏j and
taking their union provides an orthonormal basis of eigenvectors and completes
the proof for L when F = C.
    Next assume that A is a Hermitian n-by-n matrix. We define a linear map
L : Cn → Cn by L(v) = Av, and we know from Proposition 3.15 that L is self-
adjoint. The case just proved shows that L has an ordered orthonormal basis 0
                         eigenvalues
of eigenvectors, all the µ     ∂        being real. If 6 denotes the standard ordered
                             L
basis of Cn , then D =           is diagonal with real entries and is equal to
                           00
                              µ        ∂µ        ∂µ        ∂
                                   I         L         I
                                                                = C −1 AC,
                                  06        66        60
             µ  ∂
             L
where C =        . The matrix C is unitary by Proposition 3.19, and the formula
            60
D = C −1 AC shows that A is as asserted.
  Now let us return to L and suppose that F = R. The idea is to use the
same argument as above in the case that F = C, but we need a substitute for
                                    3. Spectral Theorem                                 107

the use of the Fundamental Theorem of Algebra. Fixing any orthonormal basis
of V , let A be the matrix of L. Then A is Hermitian with real entries. The
previous paragraph shows that any Hermitian matrix,   Qmwhether or mnot  real, has
a characteristic polynomial that splits as a product j=1 (∏ − r j ) with all r j
                                                                      j

real. Consequently L has this property as well. Thus any self-adjoint L when
F = R has an eigenvalue. Returning to the argument for L above when F = C,
we readily see that it now applies when F = R.
   Finally if A is a Hermitian matrix with real entries, then we can define a self-
adjoint linear map L : Rn → Rn by L(v) = Av, obtain an orthonormal basis
of eigenvectors for L, and argue as above to obtain D = C −1 AC, where D is
diagonal and C is unitary. The matrix C has columns that are eigenvectors in Rn
of the associated L, and these have real entries. Thus C is orthogonal.          §

     An important application of the Spectral Theorem is to the formation of a
square root for any “positive semidefinite” linear map. We say that a linear map
L : V → V on a finite-dimensional inner-product space is positive semidefinite
if L ∗ = L and (L(v), v) ∏ 0 for all v in V . If F = C, then the condition L ∗ = L
is redundant, according to Proposition 3.17, but that fact will not be important
for us. Similarly an n-by-n matrix A is positive semidefinite if A∗ = A and
x̄ t Ax ∏ 0 for all column vectors x. An example of a positive semidefinite n-by-n
matrix is any matrix A = B ∗ B, where B is an arbitrary k-by-n matrix. In fact, if
x is in Fn , then x̄ t B ∗ Bx = (Bx)t (Bx), and the right side is ∏ 0, being a sum of
absolute values squared.

   Corollary 3.22. Let L : V → V be a positive semidefinite linear map on a
finite-dimensional inner-product space, and let A be an n-by-n Hermitian matrix.
Then
    (a) L or A is positive semidefinite if and only if all of its eigenvalues are ∏ 0.
    (b) whenever L or A is positive semidefinite, L or A is invertible if and only
        if (L(v), v) > 0 for all v 6= 0 or x̄ t Ax > 0 for all x 6= 0.
    (c) whenever L or A is positive semidefinite, L or A has a unique positive
        semidefinite square root.

   REMARKS. A positive semidefinite linear map or matrix satisfying the condi-
tion in (b) is said to be positive definite, and the content of (b) is that a positive
semidefinite linear map or matrix is positive definite if and only if it is invertible.

    PROOF. We apply the Spectral Theorem (Theorem 3.21). For each conclusion
the result for a matrix A is a special case of the result for the linear map L, and
it is enough to treat only L. In (a), let (u 1 , . . . , u n ) be an ordered basis of eigen-
108                                       III. Inner-Product Spaces

vectors with respective eigenvalues ∏1 , . . . , ∏n , not necessarily distinct. Then
(L(u j ), u j ) = ∏ j shows the necessity of having ∏ j ∏ 0, while the computation
                                 ° °P                   ¢ P                 ¢
                     (L(v), v) = L      i (v, u i )u i ,     j (v, u j )u j
                                 °P                      P                ¢
                               =     i ∏i (v, u i )u i ,   j (v, u j )u j
                                 P
                               = i ∏i |(v, u i )|2
shows the sufficiency.
   In (b), if L fails to be invertible, then 0 is an eigenvalue for some eigenvector
v 6= 0, and v has (L(v), v) = 0. Conversely if L is invertible, then all the
eigenvalues ∏i are > 0 by (a), and the computation in (a) yields
                   X                     °        ¢X                °       ¢
    (L(v), v) =         ∏i |(v, u i )|2 ∏ min ∏ j     |(v, u i )|2 = min ∏ j kvk2 ,
                                                          j                                   j
                            i                                         i

the last step following from Parseval’s equality (Theorem 3.11).
   For existence in (c), the Spectral Theorem says that there exists an ordered
orthonormal basis 0 = (u 1 , . . . , u n ) of eigenvectors of L, say with respective
eigenvalues ∏1 , . . . , ∏n . The eigenvalues are all ∏ 0 by (a). The linear extension
                                       1/2
of the function P with P(u j ) = ∏ j u j is given by
                                                       n
                                                       X       1/2
                                       P(v) =                 ∏ j (v, u j )u j ,
                                                       j=1

and it has
                  P                            P                               °P                   ¢
      P 2 (v) =       j   ∏ j (v, u j )u j =       j   (v, u j )L(u j ) = L         j   (v, u j )u j = L(v).

Thus P 2 = L. Relative to 0, we have
 µ    ∂
    P       °                                              ¢                       1/2
          = (P(u j ), u 1 )u 1 + · · · + (P(u j ), u n )u n i = (P(u j ), u i ) = ∏ j δi j ,
   00 i j
and this is a Hermitian matrix; Proposition 3.15 therefore shows that P ∗ = P.
Finally
                  ° P 1/2                  P               ¢   1/2          2
      (P(v), v) =      i ∏i (v, u i )u i ,   j (v, u j )u j = ∏i |(v, u i )| ∏ 0,

and thus P is positive semidefinite. This proves existence.
   For uniqueness in (c), let P satisfy P ∗ = P and P 2 = L, and suppose P is
positive semidefinite. Choose an orthonormal basis of eigenvectors u 1 , . . . , u n
of P, say with eigenvalues c1 , . . . , cn , all ∏ 0. Then L(u j ) = P 2 (u j ) = c2j u j ,
and we see that u 1 , . . . , u n form an orthonormal basis of eigenvectors of L with
eigenvalues c2j . On the space where L acts as the scalar ∏i , P must therefore act
                  1/2
as the scalar ∏i . We conclude that P is unique.                                                               §
                                     3. Spectral Theorem                                  109

   The technique of proof of (c) allows one, more generally, to define f (L) for
any function f : R → C whenever L is self-adjoint. Actually, the function f
needs to be defined only on the set of eigenvalues of L for the definition to make
sense.
   At the end of this section, we shall use the existence of the square root in (c) to
obtain the so-called “polar decomposition” of square matrices. But before doing
that, let us mine three additional easy consequences of the Spectral Theorem.
The first deals with several self-adjoint linear maps rather than one, and the other
two apply that conclusion to deal with single linear maps that are not necessarily
self-adjoint.

    Corollary 3.23. Let V be a finite-dimensional inner-product space, and let
L 1 , . . . , L m be self-adjoint linear maps from V to V that commute in the sense that
L i L j = L j L i for all i and j. Then V has an orthonormal basis of simultaneous
eigenvectors of L 1 , . . . , L m . In addition, for each m-tuple of scalars ∏1 , . . . , ∏m ,
let
                     V∏1 ,...,∏m = {v ∈ V | L j (v) = ∏ j v for 1 ≤ j ≤ m}
consist of 0 and the simultaneous eigenvectors of L 1 , . . . , L m corresponding to
∏1 , . . . , ∏m . Then all the eigenvalues ∏ j are real, the vector subspaces V∏1 ,...,∏m
are mutually orthogonal, and any orthonormal basis of V of simultaneous eigen-
vectors of L 1 , . . . , L m is the union of orthonormal bases of the V∏1 ,...,∏m ’s. Corre-
spondingly if A1 , . . . , Am are commuting Hermitian n-by-n matrices, then there
exists a unitary matrix C such that C −1 A j C is diagonal with real entries for all j.
If all the matrices A j have real entries, then C may be taken to be an orthogonal
matrix.
   PROOF. This follows by iterating the Spectral Theorem (Theorem 3.21). In
fact, let {V∏1 } be the system of vector subspaces produced by the theorem for L 1 .
For each j, the commutativity of the linear maps L i forces

         L 1 (L i (v)) = L i (L 1 (v)) = L i (∏1 v) = ∏1 L i (v)      for v ∈ V∏1 ,

and thus L i (V∏1 ) ⊆ V∏1 . The restrictions of L 1 , . . . , L m to V∏1 are self-adjoint
and commute. Let {V∏1 ,∏Ø2 } be the system of vector subspaces produced by the
Spectral Theorem for L 2 ØV∏ . Each of these, by the commutativity, is carried
                                    1
into itself by L 3 , . . . , L m , and the restrictions of L 3 , . . . , L m to V∏1 ,∏2 form a
commuting family of self-adjoint linear maps. Continuing in this way, we arrive
at the decomposition asserted by the corollary for L 1 , . . . , L m . The assertion of
the corollary about commuting Hermitian matrices is a special case, in the same
way that the assertions in Theorem 3.21 about matrices were special cases of the
assertions about linear maps.                                                               §
110                            III. Inner-Product Spaces

  A linear map L : V → V , not necessarily self-adjoint, is said to be normal if
L commutes with its adjoint: L L ∗ = L ∗ L.

   Corollary 3.24. Suppose that F = C, and let L : V → V be a normal linear
map on the finite-dimensional inner-product space V . Then V has an orthonormal
basis of eigenvectors of L. In addition, for each complex scalar ∏, let
                            V∏ = {v ∈ V | L(v) = ∏v},
so that V∏ when nonzero is the eigenspace of L for the eigenvalue ∏. Then the
vector subspaces V∏ are mutually orthogonal, and any orthonormal basis of V of
eigenvectors of L is the union of orthonormal bases of the V∏ ’s. Correspondingly
if A is any n-by-n complex matrix such that A A∗ = A∗ A, then there exists a
unitary matrix C such that C −1 AC is diagonal.
                              if F = R: for the linear map L : R2 → R2
  REMARK. The corollary≥fails ¥
                          01
with L(v) = Av and A = −1 0 , L ∗ = L −1 commutes with L, but L has no
eigenvectors in R2 since the characteristic polynomial ∏2 + 1 has no first-degree
factors with real coefficients.
                                    °         ¢ °               ¢
   PROOF. The point is that L = 12 (L + L ∗ ) +i 2i1 (L − L ∗ ) and that 12 (L + L ∗ )
and 2i1 (L − L ∗ ) are self-adjoint. If L commutes with L ∗ , then T1 = 12 (L + L ∗ )
and T2 = 2i1 (L − L ∗ ) commute with each other. We apply Corollary 3.23 to
the commuting self-adjoint linear maps T1 and T2 . The vector subspace Vα,β
produced by Corollary 3.23 coincides with the vector subspace Vα+iβ defined in
the present corollary, and the result for L follows. The result for matrices is a
special case.                                                                      §

   Corollary 3.25. Suppose that F = C, and let L : V → V be a unitary linear
map on the finite-dimensional inner-product space V . Then V has an orthonormal
basis of eigenvectors of L. In addition, for each complex scalar ∏, let
                            V∏ = {v ∈ V | L(v) = ∏v},
so that V∏ when nonzero is the eigenspace of L for the eigenvalue ∏. Then the
eigenvalues of L all have absolute value 1, the vector subspaces V∏ are mutually
orthogonal, and any orthonormal basis of V of eigenvectors of L is the union
of orthonormal bases of the V∏ ’s. Correspondingly if A is any n-by-n unitary
matrix, then there exists a unitary matrix C such that C −1 AC is diagonal; the
diagonal entries of C −1 AC all have absolute value 1.
   PROOF. This is a special case of Corollary 3.24 since a unitary linear map L
has L L ∗ = I = L ∗ L. The eigenvalues all have absolute value 1 as a consequence
of Proposition 3.18e.                                                          §
                                  3. Spectral Theorem                               111

   Now we come to the polar decomposition of linear maps and of matrices.
When F = C, this is a generalization of the polar decomposition z = eiθ r of
complex numbers. When F = R, it generalizes the decomposition x = (sgn x)|x|
of real numbers.

   Theorem 3.26 (polar decomposition). If L : V → V is a linear map on a
finite-dimensional inner-product space, then L decomposes as L = U P, where
P is positive semidefinite and U is orthogonal if F = R and unitary if F = C.
The linear map P is unique, and U is unique if L is invertible. Correspondingly
any n-by-n matrix A decomposes as A = U P, where P is a positive semidefinite
matrix and U is an orthogonal matrix if F = R and a unitary matrix if F = C.
The matrix P is unique, and U is unique if A is invertible.
   REMARKS. As we have already seen in other situations, the motivation for the
proof comes from the uniqueness.
    PROOF OF UNIQUENESS. Let L = U P = U 0 P 0 . Then L ∗ L = P 2 = P 02 . The
linear map L ∗ L is positive semidefinite since its adjoint is (L ∗ L)∗ = L ∗ L ∗∗ =
L ∗ L and since (L ∗ L(v), v) = (L(v), L(v)) ∏ 0. Therefore Corollary 3.22c
shows that L ∗ L has a unique positive semidefinite square root. Hence P = P 0 .
If L is invertible, then P is invertible and L = U P implies that U = L P −1 . The
same argument applies in the case of matrices.                                     §
   PROOF OF EXISTENCE. If L is given, then we have just seen that L ∗ L is
positive semidefinite. Let P be its unique positive semidefinite square root. The
proof is clearer when L is invertible, and we consider that case first. Then we
can set U = L P −1 . Since U ∗ = (P −1 )∗ L ∗ = P −1 L ∗ , we find that U ∗ U =
P −1 L ∗ L P −1 = P −1 P 2 P −1 = I , and we conclude that U is unitary.
   When L is not necessarily invertible, we argue a little differently with the
positive semidefinite square root P of L ∗ L. The kernel K of P is the 0 eigenspace
of P, and the Spectral Theorem (Theorem 3.21) shows that the image of P is the
sum of all the other eigenspaces and is just K ⊥ . Since K ∩ K ⊥ = 0, P is one-one
from K ⊥ onto itself. Thus P(v) 7→ L(v) is a one-one linear map from K ⊥ into
V . Call this function U , so that U (P(v)) = L(v). For any v1 and v2 in V , we
have

      (L(v1 ), L(v2 )) = (L ∗ L(v1 ), v2 ) = (P 2 (v1 ), v2 ) = (P(v1 ), P(v2 )),   (∗)

and hence U : K ⊥ → V preserves inner products. Let {u 1 , . . . , u k } be an
orthonormal basis of K ⊥ , and let {u k+1 , . . . , u n } be an orthonormal basis of
K . Since U preserves inner products and is linear, {U (u 1 ), . . . , U (u k )} is an
orthonormal basis of U (K ⊥ ). Extend {U (u 1 ), . . . , U (u k )} to an orthonormal
basis of V by adjoining vectors vk+1 , . . . , vn , define U (u j ) = v j for k + 1 ≤
112                                III. Inner-Product Spaces

j ≤ n, and write U also for the linear extension to all of V . Since U carries one
orthonormal basis {u 1 , . . . , u n } of V to another, U is unitary. We have U P = L
on K ⊥ , and equation (∗) with v1 = v2 shows that ker L = ker P = K . Therefore
U P = L everywhere.                                                                 §


                                        4. Problems

1.    Let V = Mnn (C), and define an inner product on V by hA, Bi = Tr(B ∗ A). The
      norm k · kHS obtained from this inner product is called the Hilbert–Schmidt
      norm of the matrix in question.
                                    P
      (a) Prove that kAk2HS = i, j |Ai j |2 for A in V .
      (b) Let E i j be the matrix that is 1 in the (i, j)th entry and is 0 elsewhere. Prove
          that the set of all E i j is an orthonormal basis of V .
      (c) Interpret (a) in the light of (b).
      (d) Prove that the Hilbert–Schmidt norm is given on any matrix A in V by
                                         P                    P
                              kAk2HS =       j   kAu j k2 =    i, j   |vi∗ Au j |2 ,

          where {u 1 , . . . , u n } and {v1 , . . . , vn } are any orthonormal bases of Cn and
          v ∗ refers to the conjugate transpose of any member v of Cn .
      (e) Let W be the vector subspace of all diagonal matrices in V . Describe
          explicitly the orthogonal complement W ⊥ , and find its dimension.
2.    Let Vn be the inner-product space over R of all polynomials on [0, 1] of degree
      ≤ n with real coefficients. (The 0 polynomial is to be included.) The Riesz
      Representation Theorem says that there is a unique polynomial pn such that
       ° ¢ R1
      f 12 = 0 f (x) pn (x) dx for all f in Vn . Set up a system of linear equations
      whose solution tells what pn is.
3.    Let V be a finite-dimensional inner-product space, and suppose that L and M
      are self-adjoint linear maps from V to V . Show that L M is self-adjoint if and
      only if L M = M L.
4.    Let V be a finite-dimensional inner-product space. If L : V → V is a linear map
      with adjoint L ∗ , prove that ker L = (image L ∗ )⊥ .
5.    Find all 2-by-2 Hermitian matrices A with characteristic polynomial ∏2 + 4∏ + 6.
6.    Let V1 and V2 be finite-dimensional inner-product spaces over the same F, the
      inner products being ( · , · )1 and ( · , · )2 .
      (a) Using the case when V1 = V2 as a model, define the adjoint of a linear
          map L : V1 → V2 , proving its existence. The adjoint is to be a linear map
          L ∗ : V2 → V1 .
                                       4. Problems                                    113

     (b) If 0 is an orthonormal basis of V1 and 1 is an orthonormal basis of V2 , prove
         that the matrices of L and L ∗ in these bases are conjugate transposes of one
         another.
7.   Suppose that a finite-dimensional inner-product space V is a direct sum V =
     S ⊕ T of vector subspaces. Let E : V → V be the linear map that is the identity
     on S and is 0 on T .
     (a) Prove that V = S ⊥ ⊕ T ⊥ .
     (b) Prove that E ∗ : V → V is the linear map that is the identity on T ⊥ and is 0
         on S ⊥ .
8.   (Iwasawa decomposition) Let g be an invertible n-by-n complex matrix. Apply
     the Gram–Schmidt orthogonalization process to the basis {ge1 , . . . , gen }, where
     {e1 , . . . , en } is the standard basis, and let the resulting orthonormal basis be
     {v1 , . . . , vn }. Define an invertible n-by-n matrix k such that k −1 v j = e j for
     1 ≤ j ≤ n. Prove that k −1 g is upper triangular with positive diagonal entries,
     and conclude that g = k(k −1 g) exhibits g as the product of a unitary matrix and
     an upper triangular matrix whose diagonal entries are positive.
9.   Let A be an n-by-n positive definite matrix.
     (a) Prove that det A > 0.
     (b) Prove for any subset of integers 1 ≤ i 1 < i 2 < · · · < i k ≤ n that the
         submatrix of A built from rows and columns indexed by (i 1 , . . . , i k ) is
         positive definite.
10. Prove that if A is a positive definite n-by-n matrix, then there exists an n-by-n
    upper-triangular matrix B with positive diagonal entries such that A = B ∗ B.
                                                                    ≥ ¥
11. The most general 2-by-2 Hermitian matrix is of the form A = ab̄ db with a and
    d real and with b complex. Find a diagonal matrix D and a unitary matrix U
    such that D = U −1 AU .
12. In the previous problem,
    (a) what conditions on A make A positive definite?
    (b) when A is positive definite, how can its positive definite square root be
         computed explicitly?
13. Prove that if an n-by-n real symmetric matrix A has v t Av = 0 for all v in Rn ,
    then A = 0.
14. Let L : Cn → Cn be a self-adjoint linear map. Show for each x ∈ Cn that there
    is some y ∈ Cn such that (I − L)2 (y) = (I − L)(x).
15. In the polar decomposition L = U P, prove that if P and U commute, then L is
    normal.
16. Let V be an n-dimensional inner-product space over R. What is the largest pos-
    sible dimension of a commuting family of self-adjoint linear maps L : V → V ?
114                                 III. Inner-Product Spaces

17. Let v1 , . . . , vn be an ordered list of vectors in an inner-product space. The
    associated Gram matrix is the Hermitian matrix of inner products given
    by G(v1 , . . . , vn ) = [(vi , v j )], and det G(v1 , . . . , vn ) is called its Gram
    determinant.                               √ c1 !
                                                  ..                     t
    (a) If c1 , . . . , cn are in C, let c =       . . Prove that c G(v1 , . . . , vn )c̄ =
                                                    cn
          kc1 v1 + · · · + cn vn k2 , and conclude that G(v1 , . . . , vn ) is positive semi-
          definite.
      (b) Prove that det G(v1 , . . . , vn ) ∏ 0 with equality if and only if v1 , . . . , vn are
          linearly dependent. (This generalizes the Schwarz inequality.)
      (c) Under what circumstances does equality hold in the Schwarz inequality?
Problems 18–23 introduce the Legendre polynomials and establish some of their
elementary properties, including their orthogonality under the inner product hP, Qi =
R1
 −1 P(x)Q(x) dx. They form the simplest family of classical orthogonal polynomi-
als. They are uniquely determined by the conditions that the n th one Pn , for n ∏ 0,
is of degree n, they are orthogonal under h · , · i, and they are normalized so that
Pn (1) = 1. But these conditions are a little hard to work with initially, and instead
we adopt the recursive definition P0 (x) = 1, P1 (x) = x, and
            (n + 1)Pn+1 (x) = (2n + 1)x Pn (x) − n Pn−1 (x)              for n ∏ 1.
18. (a) Prove that Pn (x) has degree n, that Pn (−x) =          (−1)n Pn (x), and that
                                                                               Pn (1) =
        1. In particular, Pn is an even function if n is even and is an odd function if
        n is odd.
    (b) Let c(n) be the constant term of Pn if n is even and the coefficient of x if n
        is odd, so that c(0) = c(1) = 1. Prove that c(n) = − n−1n c
                                                                    (n−2) for n ∏ 2.

19. This part establishes a useful concrete formula for Pn (x). Let D = d/dx and
    X = x 2 −1, writing X 0 = 2x, X 00 = 2, and X 000 = 0 for the
                                                               P derivatives.
                                                                       ° ¢     Two parts
    of this problem make use of the Leibniz rule D n ( f g) = nk=0 nk (D n−k f )(D k g)
    for higher-order derivatives of a product.
    (a) Verify that D 2 (X n+1 ) = (2n + 1)D(X n X 0 ) − n(2n + 1)X 00 X n − 4n 2 X n−1 .
    (b) By applying D n−1 to the result of (a) and rearranging terms, show that
         D n+1 (X n+1 ) = (2n + 1)X 0 D n (X n ) − 4n 2 D n−1 (X n−1 ).
    (c) Put Rn (x) = (2n n!)−1 D n (X n ) for n ∏ 0. Show that R0 (x) = 1, R1 (x) = x,
         and (n + 1)Rn+1 (x) = (2n + 1)x Rn (x) − n Rn−1 (x) for n ∏ 1.
                                                                   ° d ¢n 2
    (d) (Rodrigues’s formula) Conclude that 2n n!Pn (x) = dx              [(x − 1)n ].
20. Using Rodrigues’s formula and iterated integration by parts, prove that
                       R1
                        −1 Pm (x)Pn (x) dx = 0      for m < n.
      Conclude that {P0 , P1 , . . . , Pn } is an orthogonal basis of the inner-product space
      of polynomials on [−1, 1] of degree ≤ n with inner product h · , · i.
                                       4. Problems                                       115
                                                                      R1         2 )n
21. Arguing as in the previous problem and taking for granted that     −1 (1−x          dx =
    2(2n n!)2                        °      ¢
                                          1 −1
    (2n+1)! , prove that hPn , Pn i = n + 2    .
22. This problem shows that Pn (x) satisfies a certain second-order differential equa-
    tion. Let D = d/dx. The first two parts of this problem use the Leibniz rule
    quoted in Problem 19. Let X = x 2 − 1 and K n = 2n n!, so that Rodrigues’s
    formula says that K n Pn = D n (X n ).
    (a) Expand D n+1 [(D(X n ))X] by the Leibniz rule.
    (b) Observe that (D(X n ))X = n X n X 0 , and expand D n+1 [(n X n )X 0 ] by the
         Leibniz rule.
    (c) Equating the results of the previous two parts, conclude that y = Pn (x)
         satisfies the differential equation (1 − x 2 )y 00 − 2x y 0 + n(n + 1)y = 0.
                    P
23. Let Pn (x) = nk=0 ck x k . Using the differential equation, show that the coeffi-
    cients ck satisfy k(k − 1)ck = [(k − 2)(k − 1) − n(n + 1)]ck−2 for k ∏ 2 and
    that ck = 0 unless n − k is even.

Problems 24–28 concern the complex conjugate of an inner-product space over C.
For any finite-dimensional inner-product space V , the Riesz Representation Theorem
identifies the dual V 0 with V , saying that each member of V 0 is given by taking the
inner product with some member of V . When the scalars are real, this identification
is linear; thus the Riesz theorem uses the inner product to construct a canonical
isomorphism of V onto V 0 . When the scalars are complex, the identification is
conjugate linear, and we do not get an isomorphism of V with V 0 . The complex
conjugate of V provides a substitute result.
24. Let V be a finite-dimensional vector space over C. Define a new complex vector
    space V as follows: The elements of V are the elements of V , and the definition
    of addition is unchanged. However, there is a change in the definition of scalar
    multiplication, in that if v is in V , then the product cv in V is to equal the product
    c̄v in V . Verify that V is indeed a complex vector space.
25. If V is a complex vector space and L : V → V is a linear map, define L : V → V
    to be the same function as L. Prove that L is linear.
26. Suppose that the complex vector space V is actually a finite-dimensional inner-
    product space, with inner product ( · , · )V . Define (u, v)V = (v, u)V . Verify
    that V is an inner-product space.
27. With V as in the previous problem, show that the Riesz Representation Theorem
    uses the inner product to set up a canonical isomorphism of V 0 with V .
28. With V and V as in the two previous problems, let L : V → V be linear, so
    that (L)∗ : V → V is linear. Under the identification of the previous problem
    of V with V 0 , show that (L)∗ corresponds to the contragredient L t as defined in
    Section II.4.
116                                      III. Inner-Product Spaces

Problems 29–32 use inner-product spaces to obtain a decomposition of polynomials
in several variables. A real-valued polynomial function p in x1 , . . . , xn is said to be
homogeneous of degree N if every monomial in p has total degree N . Let VN be
the space of real-valued polynomials in x1 , . . . , xn homogeneous of degree N . For
any homogeneous polynomial p, we define a differential operator @( p) with constant
coefficients by requiring that @( · ) be linear in ( · ) and that

                                                             @ k1 +···+kn
                                   @(x1k1 · · · xnkn ) =                         .
                                                           @ x1k1 · · · @ xnkn

                                                                                           @2                @2
For example, if |x|2 stands for x12 + · · · + xn2 , then @(|x|2 ) = 1 =                   @ x12
                                                                                                  + ··· +   @ xn2
                                                                                                                  .
If p and q are in the same VN , then @(q) p is a constant polynomial, and we define
h p, qi to be that constant. Then h · , · i is bilinear.
29. (a) Prove that h · , · i satisfies h p, qi = hq, pi.
      (b) Prove that hx1k1 · · · xnkn , x1l1 · · · xnln i is positive if (k1 , . . . , kn ) = (l1 , . . . , ln )
          and is 0 otherwise.
      (c) Deduce that h · , · i is an inner product on VN .
30. Call p ∈ VN harmonic if @(|x|2 ) p = 0, and let HN be the vector subspace of
    harmonic polynomials. Prove that the orthogonal complement of |x|2 VN −2 in
    VN relative to h · , · i is HN .
31. Deduce from Problem 30 that each p ∈ VN decomposes uniquely as

                                 p = h N + |x|2 h N −2 + |x|4 h N −4 + · · ·

      with h N , h N −2 , h N −4 , . . . homogeneous harmonic of the indicated degrees.
32. For n = 2, describe a computational procedure for decomposing the element
    x14 + x24 of V4 as in Problem 31.
Problems 33–34 concern products of n-by-n positive semidefinite matrices. They
make use of Problem 26 in Chapter II, which says that det(∏I −C D) = det(∏I − DC).
33. Let A and B be positive semidefinite. Using the positive definite square root of
    B, prove that every eigenvalue of AB is ∏ 0.
34. Let A, B, and C be positive semidefinite, and suppose that ABC is Hermit-
    ian. Under the assumption that C is invertible, introduce the positive definite
    square root P of C. By considering P −1 ABC P −1 , prove that ABC is positive
    semidefinite.
                                      CHAPTER IV

                          Groups and Group Actions



Abstract. This chapter develops the basics of group theory, with particular attention to the role of
group actions of various kinds. The emphasis is on groups in Sections 1–3 and on group actions
starting in Section 6. In between is a two-section digression that introduces rings, fields, vector
spaces over general fields, and polynomial rings over commutative rings with identity.
     Section 1 introduces groups and a number of examples, and it establishes some easy results.
Most of the examples arise either from number-theoretic settings or from geometric situations in
which some auxiliary space plays a role. The direct product of two groups is discussed briefly so
that it can be used in a table of some groups of low order.
     Section 2 defines coset spaces, normal subgroups, homomorphisms, quotient groups, and quotient
mappings. Lagrange’s Theorem is a simple but key result. Another simple but key result is the
construction of a homomorphism with domain a quotient group G/H when a given homomorphism
is trivial on H . The section concludes with two standard isomorphism theorems.
     Section 3 introduces general direct products of groups and direct sums of abelian groups, together
with their concrete “external” versions and their universal mapping properties.
     Sections 4–5 are a digression to define rings, fields, and ring homomorphisms, and to extend the
theories concerning polynomials and vector spaces as presented in Chapters I–II. The immediate
purpose of the digression is to make prime fields and the notion of characteristic available for the
remainder of the chapter. The definitions of polynomials are extended to allow coefficients from any
commutative ring with identity and to allow more than one indeterminate, and universal mapping
properties for polynomial rings are proved.
     Sections 6–7 introduce group actions. Section 6 gives some geometric examples beyond those
in Section 1, it establishes a counting formula concerning orbits and isotropy subgroups, and it
develops some structure theory of groups by examining specific group actions on the group and its
coset spaces. Section 7 uses a group action by automorphisms to define the semidirect product of
two groups. This construction, in combination with results from Sections 5–6, allows one to form
several new finite groups of interest.
     Section 8 defines simple groups, proves that alternating groups on five or more letters are simple,
and then establishes the Jordan–Hölder Theorem concerning the consecutive quotients that arise
from composition series.
     Section 9 deals with finitely generated abelian groups. It is proved that “rank” is well defined
for any finitely generated free abelian group, that a subgroup of a free abelian group of finite rank is
always free abelian, and that any finitely generated abelian group is the direct sum of cyclic groups.
     Section 10 returns to structure theory for finite groups. It begins with the Sylow Theorems,
which produce subgroups of prime-power order, and it gives two sample applications. One of these
classifies the groups of order pq, where p and q are distinct primes, and the other provides the
information necessary to classify the groups of order 12.
     Section 11 introduces the language of “categories” and “functors.” The notion of category is a
precise version of what is sometimes called a “context” at points in the book before this section,

                                                 117
118                              IV. Groups and Group Actions

and some of the “constructions” in the book are examples of “functors.” The section treats in this
language the notions of “product” and “coproduct,” which are abstractions of “direct product” and
“direct sum.”



                               1. Groups and Subgroups

Linear algebra and group theory are two foundational subjects for all of algebra,
indeed for much of mathematics. Chapters II and III have introduced the basics
of linear algebra, and the present chapter introduces the basics of group theory. In
this section we give the definition and notation for groups and provide examples
that fit with the historical development of the notion of group. Many readers will
already be familiar with some group theory, and therefore we can be brief at the
start.
    A group is a nonempty set G with an operation G × G → G satisfying the
three properties (i), (ii), and (iii) below. In the absence of any other information
the operation is usually called multiplication and is written (a, b) 7→ ab with no
symbol to indicate the multiplication. The defining properties of a group are
      (i) (ab)c = a(bc) for all a, b, c in G (associative law),
     (ii) there exists an element 1 in G such that a1 = 1a = a for all a in G
          (existence of identity),
    (iii) for each a in G, there exists an element a −1 in G with aa −1 = a −1 a = 1
          (existence of inverses).
It is immediate from these properties that
       • 1 is unique (since 10 = 10 1 = 1),
       • a −1 is unique (since (a −1 )0 = (a −1 )0 1 = (a −1 )0 (a(a −1 )) = ((a −1 )0 a)(a −1 )
          = 1(a −1 ) = (a −1 )),
       • the existence of a left inverse for each element implies the existence of a
          right inverse for each element (since ba = 1 and cb = 1 together imply
          c = c(ba) = (cb)a = a and hence also ab = cb = 1),
       • 1 is its own inverse (since 11 = 1),
       • ax = ay implies x = y, and xa = ya implies x = y (cancellation laws)
          (since x = 1x = (a −1 a)x = a −1 (ax) = a −1 (ay) = (a −1 a)y = 1y = y
          and since a similar argument proves the second implication).
Problem 2 at the end of Chapter II shows that the associative law extends to
products of any finite number of elements of G as follows: parentheses can
be inserted in any fashion in such a product, and the value of the product is
unchanged; hence any expression a1 a2 · · · an in G is well defined without the use
of parentheses.
    The group whose only element is the identity 1 will be denoted by {1}. It is
called the trivial group.
                                1. Groups and Subgroups                             119

   We come to other examples in a moment. First we make three more definitions
and offer some comments. A subgroup H of a group G is a subset containing
the identity that is closed under multiplication and inverses. Then H itself is a
group because the associativity in G implies associativity in H . The intersection
of any nonempty collection of subgroups of G is again a subgroup.
   An isomorphism of a group G 1 with a group G 2 is a function ϕ : G 1 → G 2
that is one-one onto and satisfies ϕ(ab) = ϕ(a)ϕ(b) for all a and b in G 1 . It is
immediate that
       • ϕ(1) = 1 (by taking a = b = 1),
       • ϕ(a −1 ) = ϕ(a)−1 (by taking b = a −1 ),
       • ϕ −1 : G 2 → G 1 satisfies ϕ −1 (cd) = ϕ −1 (c)ϕ −1 (d) (by °taking c = ϕ(a)¢
         and d = ϕ(b) on the right side and then observing that ϕ ϕ −1 (c)ϕ −1 (d)
         = ϕ(ab) = ϕ(a)ϕ(b) = cd = ϕ(ϕ −1 (cd))).
The first and second of these properties show that an isomorphism respects all the
structure of a group, not just products. The third property shows that the inverse
of an isomorphism is an isomorphism, hence that the relation “is isomorphic to” is
symmetric. Since the identity isomorphism exhibits this relation as reflexive and
since the use of compositions shows that it is transitive, we see that “is isomorphic
to” is an equivalence relation. Common notation for an isomorphism between
G 1 and G 2 is G 1 ∼= G 2 ; because of the symmetry, one can say that G 1 and G 2
are isomorphic.
   An abelian group is a group G with the additional property
    (iv) ab = ba for all a and b in G (commutative law).
In an abelian group the operation is sometimes, but by no means always, called
addition instead of “multiplication.” Addition is typically written (a, b) 7→ a+b,
and then the identity is usually denoted by 0 and the inverse of a is denoted by −a,
the negative of a. Depending on circumstances, the trivial abelian group may
be denoted by {0} or 0. Problem 3 at the end of Chapter II shows for an abelian
group G with its operation written additively that n-fold sums of elements of G
can be written in any order: a1 + a2 + · · · + an = aσ (1) + aσ (2) + · · · + aσ (n) for
each permutation σ of {1, . . . , n}.

   Historically the original examples of groups arose from two distinct sources,
and it took a while for the above definition of group to be distilled out as the
essence of the matter.
   One of the two sources involved number systems and vectors. Here are
examples.

   EXAMPLES.
   (1) Additive groups of familiar number systems. The systems in question are
the integers Z, the rational numbers Q, the real numbers R, and the complex
120                               IV. Groups and Group Actions

numbers C. In each case the set with its usual operation of addition forms an
abelian group. The group properties of Z under addition are taken as known in
advance in this book, as mentioned in Section A3 of the appendix, and the group
properties of Q, R, and C under addition are sketched in Sections A3 and A4 of
the appendix as part of the development of these number systems.
   (2) Multiplicative groups connected with familiar number systems. In the
cases of Q, R, and C, the nonzero elements form a group under multiplication.
These groups are denoted by Q× , R× , and C× . Again the properties of a group
for each of them are properties that are sketched during the development of each
of these number systems in Sections A3 and A4 of the appendix. With Z, the
nonzero integers do not form a group under multiplication, because only the two
units, i.e., the divisors +1 and −1 of 1, have inverses. The units do form a group,
however, under multiplication, and the group of units is denoted by Z× .
   (3) Vector spaces under addition. Spaces such as Qn and Rn and Cn provide
us with further examples of abelian groups. In fact, the defining properties of
addition in a vector space are exactly the defining properties of an abelian group.
Thus every vector space provides us with an example of an abelian group if we
simply ignore the scalar multiplication.
   (4) Integers modulo m, under addition. Another example related to number
systems is the additive group of integers modulo a positive integer m. Let us say
that an integer n 1 is congruent modulo m to an integer n 2 if m divides n 1 − n 2 .
One writes n 1 ≡ n 2 or n 1 ≡ n 2 mod m or n 1 = n 2 mod m for this relation.1 It
is an equivalence relation, and we can write [n] for the equivalence class of n
when it is helpful to do so. The division algorithm (Proposition 1.1) tells us that
each equivalence class has one and only one member between 0 and m − 1. Thus
there are exactly m equivalence classes, and we know a representative of each.
The set of classes will be denoted by2 Z/mZ. The point is that Z/mZ inherits
an abelian-group structure from the abelian-group structure of Z. Namely, we
attempt to define
                               [a] + [b] = [a + b].
To see that this formula actually defines an operation on Z/mZ, we need to
check that the result is meaningful if the representatives of the classes [a] and
[b] are changed. Thus let [a] = [a 0 ] and [b] = [b0 ]. Then m divides a − a 0 and
b − b0 , and m must divide the sum (a − a 0 ) + (b − b0 ) = (a + b) − (a 0 + b0 );
consequently [a + b] = [a 0 + b0 ], and addition is well defined. The same kind of
   1 This notation was anticipated in a remark explaining the classical form of the Chinese Remainder

Theorem (Corollary 1.9).
    2 The notation Z/(m) is an allowable alternative. Some authors, particularly in topology, write

Zm for this set, but the notation Zm can cause confusion since Z p is the standard notation for the
“ p-adic integers” when p is prime. These are defined in Chapter VI of Advanced Algebra.
                               1. Groups and Subgroups                           121

argument shows that the associativity and commutativity of addition in Z imply
associativity and commutativity in Z/mZ. The identity element is [0], and group
inverses (negatives) are given by −[a] = [−a]. Therefore Z/mZ is an abelian
group under addition, and it has m elements. If x and y are members of Z/mZ,
their sum is often denoted by x + y mod m.

   The other source of early examples of groups historically has the members of
the group operating as transformations of some auxiliary space. Before abstract-
ing matters, let us consider some concrete examples, ignoring some of the details
of verifying the defining properties of a group.

   EXAMPLES, CONTINUED.
   (5) Permutations. A permutation of a nonempty finite set E of n elements is a
one-one function from E onto itself. Permutations were introduced in Section I.4.
The product of two permutations is just the composition, defined by (σ τ )(x) =
σ (τ (x)) for x in E, with the symbol ◦ for composition dropped. The resulting
operation makes the set of permutations of E into a group: we already observed
in Section I.4 that composition is associative, and it is plain that the identity
permutation may be taken as the group identity and that the inverse function to
a permutation is the group inverse. The group is called the symmetric group
on the n letters of E. It has n! members for n ∏ 1. The notation Sn is often
used for this group, especially when E = {1, . . . , n}. Signs ±1 were defined
for permutations in Section I.4, and we say that a permutation is even or odd
according as its sign is +1 or −1. The sign of a product is the product of the
signs, according to Proposition 1.24, and it follows that the even permutations
form a subgroup of Sn . This subgroup is called the alternating group on n
letters and is denoted by An . It has 12 (n!) members if n ∏ 2.
    (6) Symmetries of a regular polygon. Imagine a regular polygon in R2 centered
at the origin. The plane-geometry rotations and reflections about the origin that
carry the polygon to itself form a group. If the number of sides of the polygon
is n, then the group always contains the rotations through all multiples of the
angle 2π/n. The rotations themselves form an n-element subgroup of the group
of all symmetries. To consider what reflections give symmetries, we distinguish
the cases n odd and n even. When n is odd, the reflection in the line that passes
through any vertex and bisects the opposite side carries the polygon to itself, and
no other reflections have this property. Thus the group of symmetries contains n
reflections. When n is even, the reflection in the line passing through any vertex
and the opposite vertex carries the polygon to itself, and so does the reflection in
the line that bisects a side and also the opposite side. There are n/2 reflections of
each kind, and hence the group of symmetries again contains n reflections. The
group of symmetries thus has 2n elements in all cases. It is called the dihedral
122                           IV. Groups and Group Actions

group Dn . The group Dn is isomorphic to a certain subgroup of the permutation
group Sn . Namely, we number the vertices of the polygon, and we associate to
each member of Dn the permutation that moves the vertices the way the member
of Dn does.
    (7) General linear group. With F equal to Q or R or C, consider any n-
dimensional vector space V over F. One possibility is V = Fn , but we do not
insist on this choice. Among all one-one functions carrying V onto itself, let
G consist of the linear ones. The composition of two linear maps is linear, and
the inverse of an invertible function is linear if the given function is linear. The
result is a group known as the general linear group GL(V ). When V = Fn ,
we know from Chapter II that we can identify linear maps from Fn to itself with
matrices in Mnn (F) and that composition corresponds to matrix multiplication.
It follows that the set of all invertible matrices in Mnn (F) is a group, which is
denoted by GL(n, F), and that this group is isomorphic to GL(Fn ). The set SL(V )
or SL(n, F) of all members of GL(V ) or GL(n, F) of determinant 1 is a group
since the determinant of a product is the product of the determinants; it is called
the special linear group. The dihedral group Dn is isomorphic to a subgroup of
GL(2, R) since each rotation and reflection of R2 that fixes the origin is given by
the operation of a 2-by-2 matrix.
    (8) Orthogonal and unitary groups. If V is a finite-dimensional inner-product
space over R or C, Chapter III referred to the linear maps carrying the space
to itself and preserving lengths of vectors as orthogonal in the real case and
unitary in the complex case. Such linear maps are invertible. The condition of
preserving lengths of vectors is maintained under composition and inverses, and
it follows that the orthogonal or unitary linear maps form a subgroup O(V ) or
U(V ) of the general linear group GL(V ). One writes O(n) for O(Rn ) and U(n)
for U(Cn ). The subgroup of members of O(V ) or O(n) of determinant 1 is called
the rotation group SO(V ) or SO(n). The subgroup of members of U(V ) or U(n)
of determinant 1 is called the special unitary group SU(V ) or SU(n).

   Before coming to Example 9, let us establish a closure property under the
arithmetic operations for certain subsets of C. We are going to use the theories of
polynomials as in Chapter I and of vector spaces as in Chapter II with the rationals
Q as the scalars. Fix a complex number θ, and form the result of evaluating at θ
every polynomial in one indeterminate with coefficients in Q. The resulting set
of complex numbers comes by substituting θ for X in the members of Q[X], and
we denote this subset of C by Q[θ].
   Suppose that θ has the property that the set {1, θ, θ 2 , . . . , θ n } is linearly de-
pendent over Q for some integer n ∏ 1, i.e., has the property that F0 (θ)p= 0 for
some nonzero
           p member
                p       F0 of Q[X] of degree ≤ n. For example,
                                                         p               if θ = 2, then
the set {1, 2, ( 2)2 } is linearly dependent since 2 − ( 2)2 = 0; if θ = e2πi/5 ,
                                    1. Groups and Subgroups                                    123

then {1, θ, θ 2 , θ 3 , θ 4 , θ 5 } is linearly dependent since 1 − θ 5 = 0, or alternatively
since 1 + θ + θ 2 + θ 3 + θ 4 = 0.
   Returning to the general θ, we lose no generality if we assume that the polyno-
mial F0 has degree exactly n. If we divide the equation F0 (θ) = 0 by the leading
coefficient, we obtain an equality θ n = G 0 (θ), where G 0 is the zero polynomial
or is a nonzero polynomial of degree at most n − 1. Then θ n+m = θ m G 0 (θ), and
we see inductively that every power θ r with r ∏ n is a linear combination of the
members of the set {1, θ, θ 2 , . . . , θ n−1 }. This set is therefore a spanning set for
the vector space Q[θ], and we find that Q[θ] is finite-dimensional, with dimension
at most n. Since every positive integer power of θ lies in Q[θ] and since these
powers are closed under multiplication, the vector space Q[θ] is closed under
multiplication. More striking is that Q[θ] is closed under division, as is asserted
in the following proposition.

   Proposition 4.1. Let θ be in C, and suppose for some integer n ∏ 1 that the set
{1, θ, θ 2 , . . . , θ n } is linearly dependent over Q. Then the finite-dimensional ra-
tional vector space Q[θ] is closed under taking reciprocals (of nonzero elements),
as well as multiplication, and hence is closed under division.
   REMARKS. Under the hypotheses of Proposition 4.1, Q[θ] is called an
algebraic number field,3 or simply a number field, and θ is called an algebraic
number. The relevant properties of C that are used in proving the proposition
are that C is closed under the usual arithmetic operations, that these satisfy the
usual properties, and that Q is a subset of C. The deeper closure properties of C
that are developed in Sections A3 and A4 of the appendix play no role.
   PROOF. We have seen that Q[θ] is closed under multiplication. If x is a nonzero
member of Q[θ], then all positive powers of x must be in Q[θ], and the fact that
dim Q[θ] ≤ n forces {1, x, x 2 , . . . , x n } to be linearly dependent. Therefore there
are integers j and k with 0 ≤ j < k ≤ n such that c j x j +c j+1 x j+1 +· · ·+ck x k = 0
for some rational numbers c j , . . . , ck with ck 6= 0. Since x is assumed nonzero,
we can discard unnecessary terms and arrange that c j 6= 0. Then
                                                             k− j−1
                    1 = x(−c−1          −1            −1
                            j c j+1 − c j c j+2 x − c j ck x        ),

and the reciprocal of x has been exhibited as in Q[θ].                                          §

  EXAMPLES, CONTINUED.
  (9) Galois’s notion of automorphisms of number fields. Let θ be a complex
number as in Proposition 4.1. The subject of Galois theory, whose details will
   3 The  definition of “algebraic number field” that is given later in the book is ostensibly more
general, but the Theorem of the Primitive Element in Chapter IX will show that it amounts to the
same thing as this.
124                          IV. Groups and Group Actions

be discussed in Chapter IX and whose full utility will be glimpsed only later,
works in an important special case with the “automorphisms” of Q[θ] that fix Q.
The automorphisms are the one-one functions from Q[θ] onto itself that respect
addition and multiplication and carry every element of Q to itself. The identity
is such a function, the composition of two such functions is again one, and the
inverse of such a function is again one. Therefore the automorphisms of Q[θ]
form a group under composition. We call this group Gal(Q[θ]/Q). Let us see
that it is finite. In fact, if σ is in Gal(Q[θ]/Q), then σ is determined by its effect
on θ, since we must have σ (F(θ)) = F(σ (θ)) for every F in Q[X]. We know
that there is some nonzero polynomial F0 (X) such that F0 (θ) = 0. Applying σ
to this equality, we see that F0 (σ (θ)) = 0. Therefore σ (θ) has to be a root of
F0 . Viewing F0 as in C[X], we can apply Corollary 1.14 and see that F0 has only
finitely many complex roots. Therefore there are only finitely many possibilities
for σ , and the group Gal(Q[θ]/Q) has to be finite. Galois theory shows that
this group gives considerable insight into the structure of Q[θ]. For example it
allows one to derive the Fundamental Theorem of Algebra (Theorem 1.18) just
from algebra and the Intermediate Value Theorem (Section A3 of the appendix);
it allows one to show the impossibility of certain constructions in plane geometry
by straightedge and compass; and it allows one to show that a quintic polynomial
with rational coefficients need not have a root that is expressible in terms of
rational numbers, arithmetic operations, and the extraction of square roots, cube
roots, and so on. We return to these matters in Chapter IX.

    Examples 5–9, which all involve auxiliary spaces, fit the pattern that the
members of the group are invertible transformations of the auxiliary space and the
group operation is composition. This notion will be abstracted in Section 6 and
will lead to the notion of a “group action.” For now, let us see why we obtained
groups in each case. If X is any nonempty set, then the set of invertible functions
 f : X → X forms a group under composition, composition being defined by
( f g)(x) = f (g(x)) with the usual symbol ◦ dropped. The associative law is just
a matter of unwinding this definition:
      (( f g)h)(x) = ( f g)(h(x)) = f (g(h(x))) = f ((gh)(x)) = ( f (gh))(x).
The identity function is the identity of the group, and inverse functions provide
the inverse elements in the group.
   For our examples, the set X was E in Example 5, R2 in Example 6, V or Fn
in Example 7, V or Qn or Rn or Cn in Example 8, and Q[θ] in Example 9. All
that was needed in each case was to know that our set G of invertible functions
from X to itself formed a subgroup of the set of all invertible functions from X
to itself. In other words, we had only to check that G contained the identity and
was closed under composition and inversion. Associativity was automatic for G
because it was valid for the group of all invertible functions from X to itself.
                               1. Groups and Subgroups                           125

   Actually, any group can be realized in the fashion of Examples 5–9. This is
the content of the next proposition.

   Proposition 4.2 (Cayley’s Theorem). Any group G is isomorphic to a sub-
group of invertible functions on a set X. The set X can be taken to be G itself.
In particular any finite group with n elements is isomorphic to a subgroup of the
symmetric group Sn .
   PROOF. Define X = G, put f a (x) = ax for a in G, and let G 0 = { f a | a ∈ G}.
To see that G 0 is a group, we need G 0 to contain the identity and to be closed
under composition and inverses. Since f 1 is the identity, the identity is indeed
in G 0 . Since f ab (x) = (ab)x = a(bx) = f a (bx) = f a ( f b (x)) = ( f a f b )(x),
G 0 is closed under composition. The formula f a f a −1 = f 1 = f a −1 f a then shows
that f a −1 = ( f a )−1 and that G 0 is closed under inverses. Thus G 0 is a group.
   Define ϕ : G → G 0 by ϕ(a) = f a . Certainly ϕ is onto G 0 , and it is one-
one because ϕ(a) = ϕ(b) implies f a = f b , f a (1) = f b (1), and a = b. Also,
ϕ(ab) = f ab = f a f b = ϕ(a)ϕ(b), and hence ϕ is an isomorphism.
   In the case that G is finite with n elements, G is exhibited as isomorphic
to a subgroup of the group of permutations of the members of G. Hence it is
isomorphic to a subgroup of Sn .                                                    §

   It took the better part of a century for mathematicians to sort out that two
distinct notions are involved here—that of a group, as defined above, and that
of a group action, as will be defined in Section 6. In sorting out these matters,
mathematicians realized that it is wise to study the abstract group first and then
to study the group in the context of its possible group actions. This does not at all
mean ignoring group actions until after the study of groups is complete; indeed,
we shall see in Sections 6, 7, and 10 that group actions provide useful tools for
the study of abstract groups.

   We turn to a discussion of two general group-theoretic notions—cyclic group
and the direct product of two or more groups. The second of these notions will
be discussed only briefly now; more detail will come in Section 3.
   If a is an element of a group, we define a n for integers n > 0 inductively
by a 1 = a and a n = a n−1 a. Then we can put a 0 = 1 and a −n = (a −1 )n
for n > 0. A little checking, which we omit, shows that the ordinary rules of
exponents apply: a m+n = a m a n and a mn = (a m )n for all integers m and n. If the
underlying group is abelian and additive notation is being used, these formulas
read (m + n)a = ma + na and (mn)a = n(ma).
   A cyclic group is a group with an element a such that every element is a power
of a. The element a is called a generator of the group, and the group is said to
be generated by a.
126                          IV. Groups and Group Actions

   Proposition 4.3. Each cyclic group G is isomorphic either to the additive
group Z of integers or to the additive group Z/mZ of integers modulo m for some
positive integer m.
   PROOF. If all a n are distinct, then the rule a m+n = a m a n implies that the
function n 7→ a n is an isomorphism of Z with G. On the other hand, if a k = a l
with k > l, then a k−l = 1 and there exists a positive integer n such that a n = 1.
Let m be the least positive integer with a m = 1. For any integers q and r, we
have a qm+r = (a m )q a r = a r . Thus the function ϕ : Z/mZ → G given by
ϕ([n]) = a n is well defined, is onto G, and carries sums in Z/mZ to products in
G. If 0 ≤ l < k < m, then a k 6= a l since otherwise a k−l would be 1. Hence ϕ is
one-one, and we conclude that ϕ : Z/mZ → G is an isomorphism.                    §

   Let us denote abstract cyclic groups by C∞ and Cm , the subscript indicating
the number of elements. Finite cyclic groups arise in guises other than as Z/mZ.
For example the set of all elements e2πik/m in C, with multiplication as opera-
tion,
≥      forms a group isomorphic
                        ¥          to Cm . So does the set of all rotation matrices
  cos 2πk/m − sin 2πk/m
  sin 2πk/m cos 2πk/m
                          with matrix multiplication as operation.

   Proposition 4.4. Any subgroup of a cyclic group is cyclic.
   REMARK. The proof of Proposition 4.4 exhibits a one-one correspondence
between the subgroups of Z/mZ and the positive integers k dividing m.
    PROOF. Let G be a cyclic group with generator a, and let H be a subgroup.
We may assume that H 6= {1}. Then there exists a positive integer n such that
a n is in H , and we let k be the smallest such positive integer. If n is any integer
such that a n is in H , then Proposition 1.2 produces integers x and y such that
xk + yn = d, where d = GCD(k, n). The equation a d = (a k )x (a n ) y exhibits
a d as in H , and the minimality of k forces d ∏ k. Since GCD(k, n) ≤ k, we
conclude that d = k. Hence k divides n. Consequently H consists of the powers
of a k and is cyclic.                                                              §

   A notion of the direct product of two groups is definable in the same way as
was done with vector spaces in Section II.6, except that a little care is needed in
saying how this construction interacts with mappings. As with the corresponding
construction for vector spaces, one can define an explicit “external” direct product,
and one can recognize a given group as an “internal” direct product, i.e., as
isomorphic to an external direct product. We postpone a fuller discussion of direct
product, as well as all comments about direct sums and mappings associated with
direct sums and direct products, to Section 3.
   The external direct product G 1 × G 2 of two groups G 1 and G 2 is a group
whose underlying set is the set-theoretic product of G 1 and G 2 and whose group
                                   1. Groups and Subgroups                                   127

law is (g1 , g2 )(g10 , g20 ) = (g1 g10 , g2 g20 ). The identity is (1, 1), and the formula for
inverses is (g1 , g2 )−1 = (g1−1 , g2−1 ). The two subgroups G 1 × {1} and {1} × G 2
of G 1 × G 2 commute with each other.
    A group G is the internal direct product of two subgroups G 1 and G 2 if the
function from the external direct product G 1 × G 2 to G given by (g1 , g2 ) 7→ g1 g2
is an isomorphism of groups. The literal analog of Proposition 2.30, which gave
three equivalent definitions of internal direct product4 of vector spaces, fails here.
It is not sufficient that G 1 and G 2 be two subgroups such that G 1 ∩ G 2 = {1} and
every element in G decomposes as a product g1 g2 with g1 ∈ G 1 and g2 ∈ G 2 .
For example, with G = S3 , the two subgroups

             G 1 = {1, (1 2)}          and       G 2 = {1, (1 2 3), (1 3 2)}

have these properties, but G is not isomorphic to G 1 × G 2 because the elements
of G 1 do not commute with the elements of G 2 .

   Proposition 4.5. If G is a group and G 1 and G 2 are subgroups, then the
following conditions are equivalent:
    (a) G is the internal direct product of G 1 and G 2 ,
    (b) every element in G decomposes uniquely as a product g1 g2 with g1 ∈ G 1
        and g2 ∈ G 2 , and every member of G 1 commutes with every member of
        G 2,
    (c) G 1 ∩ G 2 = {1}, every element in G decomposes as a product g1 g2 with
        g1 ∈ G 1 and g2 ∈ G 2 , and every member of G 1 commutes with every
        member of G 2 .

    PROOF. We have seen that (a) implies (b). If (b) holds and g is in G 1 ∩ G 2 ,
then the formula 1 = gg −1 and the uniqueness of the decomposition of 1 as a
product together imply that g = 1. Hence (c) holds.
    If (c) holds, define ϕ : G 1 × G 2 → G by ϕ(g1 , g2 ) = g1 g2 . This map is
certainly onto G. To see that it is one-one, suppose that ϕ(g1 , g2 ) = ϕ(g10 , g20 ).
Then g1 g2 = g10 g20 and hence g10 −1 g1 = g20 g2−1 . Since G 1 ∩ G 2 = {1}, g10 −1 g1 =
g20 g2−1 = 1. Thus (g1 , g2 ) = (g10 , g20 ), and ϕ is one-one. Finally the fact that
elements of G 1 commute with elements of G 2 implies that ϕ((g1 , g2 )(g10 , g20 )) =
ϕ(g1 g10 , g2 g20 ) = g1 g10 g2 g20 = g1 g2 g10 g20 = ϕ(g1 , g2 )ϕ(g10 , g20 ). Therefore ϕ is an
isomorphism, and (a) holds.                                                                    §

  Here are two examples of internal direct products of groups. In each let
R+ be the multiplicative group of positive real numbers. The first example is
   4 The direct sum and direct product of two vector spaces were defined to be the same thing in

Chapter II.
128                          IV. Groups and Group Actions

R× ∼= C2 ×R+ with C2 providing the sign. The second example is C× ∼  = S 1 ×R+ ,
       1
where S is the multiplicative group of complex numbers of absolute value 1; the
isomorphism here is given by the polar-coordinate mapping (eiθ , r) 7→ eiθ r.

   We conclude this section by giving an example of a group that falls outside
the pattern of the examples above and by summarizing what groups we have
identified with ≤ 15 elements.

   EXAMPLES, CONTINUED.
   (10) Groups associated with the quaternions. The set H of quaternions is an
object like R or C in that it has both an addition/subtraction and a multiplica-
tion/division, but H is unlike R and C in that multiplication is not commutative.
We give two constructions. In one we start from R4 with the standard basis
vectors written as 1, i, j, k. The multiplication table for these basis vectors is

                   11 = 1, 1i = i,   1j = j,  1k = k,
                    i1 = i, ii = −1, ij = k,  ik = −j,
                   j1 = j, ji = −k, jj = −1, jk = i,
                   k1 = k, ki = j,   kj = −i, kk = −1,
and the multiplication is extended to general elements by the usual distributive
laws. The multiplicative identity is 1, and multiplicative inverses of nonzero
elements are given by
            (a1 + bi + cj + k)−1 = s −1 a1 − s −1 bi − s −1 cj − s −1 dk
          p
with s = a 2 + b2 + c2 + d 2 . Since ij = k while ji = −k, multiplication is not
commutative. What takes work to see is that multiplication is associative. To see
this, we give another construction, using M22 (C). Within M22 (C), take
           ≥ ¥              ≥     ¥           ≥      ¥             ≥        ¥
                              i 0               0 −1                   0 −i
      1 = 10 01 ,      i = 0 −i ,         j= 1 0 ,            k = −i 0 ,

and define H to be the linear span, with real coefficients, of these matrices. The
operations are the usual matrix addition and multiplication. Then multiplication
is associative, and we readily verify the multiplication table for 1, i, j, k. A little
computation verifies also the formula for multiplicative inverses. The set H×
of nonzero elements forms a group under multiplication, and it is isomorphic to
R+ × SU(2), where
                                n≥        ¥Ø                o
                                     α β Ø      2      2
                     SU(2) =       −β̄ ᾱ  Ø |α|  + |β|  = 1
is the 2-by-2 special unitary group defined in Example 8. Of interest for our
current purposes is the 8-element subgroup ±1, ±i, ±j, ±k, which is called the
quaternion group and will be denoted by H8 .
                        2. Quotient Spaces and Homomorphisms                     129

  The order of a finite group is the number of elements in the group. Let us list
some of the groups we have discussed that have order at most 15:

 1   C1                                             9    C9 , C3 × C3
 2   C2                                             10   C10 , D5
 3   C3                                             11   C11
 4   C4 , C2 × C2                                   12   C12 , C6 × C2 , D6 , A4
 5   C5                                             13   C13
 6   C6 , D3                                        14   C14 , D7
 7   C7                                             15   C15
 8   C8 , C4 × C2 , C2 × C2 × C2 , D4 , H8

No two groups in the above table are isomorphic, as one readily checks by counting
elements of each “order” in the sense of the next section. We shall see in Section 10
and in the problems at the end of the chapter that the above table is complete
through order 15 except for one group of order 12. Some groups that we have
discussed have been omitted from the above table because of isomorphisms with
the groups above. For example, S2 ∼    = C2 , A3 ∼= C3 , C3 × C2 ∼= C6 , S3 ∼  = D3 ,
C5 × C2 ∼ = C10 , C4 × C3 ∼    = C12 , D3 × C2 ∼    = D6 , C7 × C2 ∼     = C14 , and
C5 × C3 ∼= C15 .


                  2. Quotient Spaces and Homomorphisms

Let G be a group, and let H be a subgroup. For purposes of this paragraph, say
that g1 in G is equivalent to g2 in G if g1 = g2 h for some h in H . The relation
“equivalent” is an equivalence relation: it is reflexive because 1 is in H , it is
symmetric since H is closed under inverses, and it is transitive since H is closed
under products. The equivalence classes are called left cosets of H in G. The
left coset containing an element g of G is the set g H = {gh | h ∈ H }.

   EXAMPLES.
   (1) When G = Z and H = mZ, the left cosets are the sets r + mZ, i.e., the
sets {x ∈ Z | x ≡ r mod m} for the various values of r.
   (2) When G = S3 and H = {(1), (1 3)}, there are three left cosets: H ,
(1 2)H = {(1 2), (1 3 2)}, and (2 3)H = {(2 3), (1 2 3)}.

   Similarly one can define the right cosets Hg of H in G. When G is nonabelian,
these need not coincide with the left cosets; in Example 2 above with G = S3
and H = {(1), (1 3)}, the right coset H (1 2) = {(1 2), (1 2 3)} is not a left
coset.
130                         IV. Groups and Group Actions

   Lemma 4.6. If H is a subgroup of the group G, then any two left cosets of H
in G have the same cardinality, namely card H .
  REMARKS. We shall be especially interested in the case that card H is finite,
and then we write |H | = card H for the number of elements in H .
  PROOF. If g1 H and g2 H are given, then the map g 7→ g2 g1−1 g is one-one on
G and carries g1 H onto g2 H . Hence g1 H and g2 H have the same cardinality.
Taking g1 = 1, we see that this common cardinality is card H .              §
  We write G/H for the set {g H } of all left cosets of H in G, calling it the
quotient space or left-coset space of G by H . The set {Hg} of right cosets is
denoted by H \G.
   Theorem 4.7 (Lagrange’s Theorem). If G is a finite group, then |G| =
|G/H | |H |. Consequently the order of any subgroup of G divides the order
of G.
   REMARK. Using the formula in Theorem 4.7 three times yields the conclusion
that if H and K are subgroups of a finite group G with K ⊆ H , then |G/K | =
|G/H | |H/K |.
   PROOF. Lemma 4.6 shows that each left coset has |H | elements. The left
cosets are disjoint and exhaust G, and there are |G/H | left cosets. Thus G has
|G/H | |H | elements.                                                        §
    If a is an element of a group G, then we have seen that the powers a n of a form
a cyclic subgroup of G that is isomorphic either to Z or to some group Z/mZ
for a positive integer m. We say that a has finite order m when the cyclic group
is isomorphic to Z/mZ. Otherwise a has infinite order. In the finite-order case
the order of a is thus the least positive integer n such that a n = 1.
  Corollary 4.8. If G is a finite group, then each element a of G has finite order,
and the order of a divides the order of G.
   PROOF. The order of a equals |H | if H = {a n | n ∈ Z}, and Corollary 4.8 is
thus a special case of Theorem 4.7.                                          §
   Corollary 4.9. If p is a prime, then the only group of order p, up to isomor-
phism, is the cyclic group C p , and it has no subgroups other than {1} and C p
itself.
   PROOF. Suppose that G is a finite group of order p and that H 6= {1} is a
subgroup of G. Let a 6= 1 be in H , and let P = {a n | n ∈ Z}. Since a 6= 1,
Corollary 4.8 shows that the order of a is an integer > 1 that divides p. Since p
is prime, the order of a must equal p. Then |P| = p. Since P ⊆ H ⊆ G and
|G| = p, we must have P = G.                                                   §
                          2. Quotient Spaces and Homomorphisms                         131

   Let G 1 and G 2 be groups. We say that ϕ : G 1 → G 2 is a homomorphism
if ϕ(ab) = ϕ(a)ϕ(b) for all a and b in G. In other words, ϕ is to respect
products, but it is not assumed that ϕ is one-one or onto. Any homomorphism ϕ
automatically respects the identity and inverses, in the sense that
     • ϕ(1) = 1 (since ϕ(1) = ϕ(11) = ϕ(1)ϕ(1)),
     • ϕ(a −1 ) = ϕ(a)−1 (since 1 = ϕ(1) = ϕ(aa −1 ) = ϕ(a)ϕ(a −1 ) and
        similarly 1 = ϕ(a −1 )ϕ(a)).

   EXAMPLES. The following functions are homomorphisms: any isomorphism,
the function ϕ : Z → Z/mZ given by ϕ(k) = k mod m, the function ϕ : Sn →
{±1} given by ϕ(σ ) = sgn σ , the function ϕ : Z → G given for fixed a in G by
ϕ(n) = a n , and the function ϕ : G L(n, F) → F× given by ϕ(A) = det A.

    The image of a homomorphism ϕ : G 1 → G 2 is just the image of ϕ considered
as a function. It is denoted by image ϕ = ϕ(G 1 ) and is necessarily a subgroup of
G 2 since if ϕ(g1 ) = g2 and ϕ(g10 ) = g20 , then ϕ(g1 g10 ) = g2 g20 and ϕ(g1−1 ) = g2−1 .
    The kernel of a homomorphism ϕ : G 1 → G 2 is the set ker ϕ = ϕ −1 ({1}) =
{x ∈ G 1 | ϕ(x) = 1}. This is a subgroup since if ϕ(x) = 1 and ϕ(y) = 1, then
ϕ(x y) = ϕ(x)ϕ(y) = 1 and ϕ(x −1 ) = ϕ(x)−1 = 1.
    The homomorphism ϕ : G 1 → G 2 is one-one if and only if ker ϕ is the trivial
group {1}. The necessity follows since 1 is already in ker ϕ, and the sufficiency
follows since ϕ(x) = ϕ(y) implies that ϕ(x y −1 ) = 1 and therefore that x y −1 is
in ker ϕ.
    The kernel H of a homomorphism ϕ : G 1 → G 2 has the additional property
of being a normal subgroup of G 1 in the sense that ghg −1 is in H whenever g
is in G 1 and h is in H , i.e., g Hg −1 = H . In fact, if h is in ker ϕ and g is in G 1 ,
then ϕ(ghg −1 ) = ϕ(g)ϕ(h)ϕ(g)−1 = ϕ(g)ϕ(g)−1 = 1 shows that ghg −1 is in
ker ϕ.

   EXAMPLES.
    (1) Any subgroup H of an abelian group G is normal since ghg −1 = gg −1 h =
h. The alternating subgroup An of the symmetric group Sn is normal since An
is the kernel of the homomorphism σ 7→ sgn σ .
   (2) The subgroup H = {1, (1 3)} of S3 is not normal since (1 2)H (1 2)−1 =
{1, (2 3)}.
   (3) If a subgroup H of a group G has just two left cosets, then H is normal
even if G is an infinite group. In fact, suppose G = H ∪ g0 H whenever g0 is not
in H . Taking inverses of all elements of G, we see that G = H ∪ Hg1 whenever
g1 is not in H . If g in G is given, then either g is in H and g Hg −1 = H , or g is
not in H and g H = Hg, so that g Hg −1 = H in this case as well.
132                                IV. Groups and Group Actions

  Let H be a subgroup of G. Let us look for the circumstances under which
G/H inherits a multiplication from G. The natural definition is

                                                     ?
                                   (g1 H )(g2 H ) = g1 g2 H,

but we have to check that this definition makes sense. The question is whether
we get the same left coset as product if we change the representatives of g1 H and
g2 H from g1 and g2 to g1 h 1 and g2 h 2 . Since our prospective definition makes
(g1 h 1 H )(g2 h 2 H ) = g1 h 1 g2 h 2 H , the question is whether g1 h 1 g2 h 2 H equals
g1 g2 H . That is, we ask whether g1 h 1 g2 h 2 = g1 g2 h for some h in H . If this
equality holds, then h 1 g2 h 2 = g2 h, and hence g2−1 h 2 g2 equals hh −1  2 , which is
an element of H . Conversely if every expression g2−1 h 2 g2 is in H , then we can
go backwards and see that g1 h 1 g2 h 2 = g1 g2 h for some h in H , hence see that
G/H indeed inherits a multiplication from G. Thus a necessary and sufficient
condition for G/H to inherit a multiplication from G is that the subgroup H is
normal. According to the next proposition, the multiplication inherited by G/H
when this condition is satisfied makes G/H into a group.

   Proposition 4.10. If H is a normal subgroup of a group G, then G/H becomes
a group under the inherited multiplication (g1 H )(g2 H ) = (g1 g2 )H , and the
function q : G → G/H given by q(g) = g H is a homomorphism of G onto
G/H with kernel H . Consequently every normal subgroup of G is the kernel of
some homomorphism.
   REMARKS. When H is normal, the group G/H is called a quotient group of G,
and the homomorphism q : G → G/H is called the quotient homomorphism.5
In the special case that G = Z and H = mZ, the construction reduces to the
construction of the additive group of integers modulo m and accounts for using
the notation Z/mZ for that group.
   PROOF. The coset 1H is the identity, and (g H )−1 = g −1 H . Also, the com-
putation (g1 Hg2 H )g3 H = g1 g2 g3 H = g1 H (g2 Hg3 H ) proves associativity.
Certainly q is onto G/H . It is a homomorphism since q(g1 g2 ) = g1 g2 H =
g1 Hg2 H = q(g1 )q(g2 ).                                                    §

   In analogy with what was shown for vector spaces in Proposition 2.25, quo-
tients in the context of groups allow for the factorization of certain homomor-
phisms of groups. The appropriate result is stated as Proposition 4.11 and is
pictured in Figure 4.1. We can continue from there along the lines of Section II.5.

   5 Some   authors call G/H a “factor group.” A “factor set,” however, is something different.
                        2. Quotient Spaces and Homomorphisms                    133

   Proposition 4.11. Let ϕ : G 1 → G 2 be a homomorphism between groups, let
H0 = ker ϕ, let H be a normal subgroup of G 1 contained in H0 , and define
q : G 1 → G 1 /H to be the quotient homomorphism. Then there exists a
homomorphism ϕ : G 1 /H → G 2 such that ϕ = ϕ ◦ q, i.e, ϕ(g1 H ) = ϕ(g1 ). It
has the same image as ϕ, and ker ϕ = {h 0 H | h 0 ∈ H0 }.
                                           ϕ
                                 G1     −−−→ G 2
                                  
                                  
                                 qy            ϕ


                               G 1 /H
   FIGURE 4.1. Factorization of homomorphisms of groups via the quotient
                      of a group by a normal subgroup.

   REMARK. One says that ϕ factors through G 1 /H or descends to G 1 /H . See
Figure 4.1.
   PROOF. We will have ϕ ◦ q = ϕ if and only if ϕ satisfies ϕ(g1 H ) = ϕ(g1 ).
What needs proof is that ϕ is well defined. Thus suppose that g1 and g10 are in the
same left coset, so that g10 = g1 h with h in H . Then ϕ(g10 ) = ϕ(g1 )ϕ(h) = ϕ(g1 )
since H ⊆ ker ϕ, and ϕ is therefore well defined.
   The computation ϕ(g1 Hg2 H ) = ϕ(g1 g2 H ) = ϕ(g1 g2 ) = ϕ(g1 )ϕ(g2 ) =
ϕ(g1 H )ϕ(g2 H ) shows that ϕ is a homomorphism. Since image ϕ = image ϕ, ϕ
is onto image ϕ. Finally ker ϕ consists of all g1 H such that ϕ(g1 H ) = 1. Since
ϕ(g1 H ) = ϕ(g1 ), the condition that g1 is to satisfy is that g1 be in ker ϕ = H0 .
Hence ker ϕ = {h 0 H | h 0 ∈ H0 }, as asserted.                                  §

   Corollary 4.12. Let ϕ : G 1 → G 2 be a homomorphism between groups, and
suppose that ϕ is onto G 2 and has kernel H . Then ϕ exhibits the group G 1 /H as
canonically isomorphic to G 2 .
   PROOF. Take H = H0 in Proposition 4.11, and form ϕ : G 1 /H → G 2 with
ϕ = ϕ ◦ q. The proposition shows that ϕ is onto G 2 and has trivial kernel, i.e.,
the identity element of G 1 /H . Having trivial kernel, ϕ is one-one.         §

   Theorem 4.13 (First Isomorphism Theorem). Let ϕ : G 1 → G 2 be a
homomorphism between groups, and suppose that ϕ is onto G 2 and has kernel
K . Then the map H1 7→ ϕ(H1 ) gives a one-one correspondence between
     (a) the subgroups H1 of G 1 containing K and
     (b) the subgroups of G 2 .
Under this correspondence normal subgroups correspond to normal subgroups.
If H1 is normal in G 1 , then g H1 7→ ϕ(g)ϕ(H1 ) is an isomorphism of G 1 /H1 onto
G 2 /ϕ(H1 ).
134                            IV. Groups and Group Actions

   REMARK. In the special case of the last statement that ϕ : G 1 → G 2 is a
quotient map q : G → G/K and H is a normal subgroup of G containing K , the
last statement of the theorem asserts the isomorphism
                                             ±
                            G/H ∼ = (G/K ) (H/K ).
    PROOF. The passage from (a) to (b) is by direct image under ϕ, and the passage
from (b) to (a) will be by inverse image under ϕ −1 . Certainly the direct image of
a subgroup as in (a) is a subgroup as in (b). To prove the one-one correspondence,
we are to show that the inverse image of a subgroup as in (b) is a subgroup as in
(a) and that these two constructions invert one another.
    For any subgroup H2 of G 2 , ϕ −1 (H2 ) is a subgroup of G 1 . In fact, if g1 and
  0
g1 are in ϕ −1 (H2 ), we can write ϕ(g1 ) = h 2 and ϕ(g10 ) = h 02 with h 2 and h 02 in
H2 . Then the equations ϕ(g1 g10 ) = h 2 h 02 and ϕ(g1−1 ) = ϕ(g1 )−1 = h −1    2 show
         0       −1            −1
that g1 g1 and g1 are in ϕ (H2 ).
    Moreover, the subgroup ϕ −1 (H2 ) contains ϕ −1 ({1}) = K . Therefore the
inverse image under ϕ of a subgroup as in (b) is a subgroup as in (a). Since ϕ is
a function, we have ϕ(ϕ −1 (H2 )) = H2 . Thus passing from (b) to (a) and back
recovers the subgroup of G 2 .
    If H1 is a subgroup of G 1 containing K , we still need to see that H1 =
ϕ −1 (ϕ(H1 )). Certainly H1 ⊆ ϕ −1 (ϕ(H1 )). For the reverse inclusion let g1 be
in ϕ −1 (ϕ(H1 )). Then ϕ(g1 ) is in ϕ(H1 ), i.e., ϕ(g1 ) = ϕ(h 1 ) for some h 1 in H1 .
Since ϕ is a homomorphism, ϕ(g1 h −1                         −1
                                         1 ) = 1. Thus g1 h 1 is in ker ϕ = K , which
is contained in H1 by assumption. Then h 1 and g1 h −1   1 are in H1 , and hence their
product (g1 h −1
               1 )h  1 = g 1 is in H1 . We conclude  that ϕ −1 (ϕ(H1 )) ⊆ H1 , and thus
passing from (a) to (b) and then back recovers the subgroup of G 1 containing K .
    Next let us show that normal subgroups correspond to normal subgroups. If H2
is normal in G 2 , let H1 be the subgroup ϕ −1 (H2 ) of G 1 . For h 1 in H1 and g1 in G 1 ,
we can write ϕ(h 1 ) = h 2 with h 2 in H2 , and then ϕ(g1 h 1 g1−1 ) = ϕ(g1 )h 2 ϕ(g1 )−1
is in ϕ(g1 )H2 ϕ(g1 )−1 = H2 . Hence g1 h 1 g1−1 is in ϕ −1 (H2 ) = H1 . In the reverse
direction let H1 be normal in G 1 , and let g2 be in G 2 . Since ϕ is onto G 2 , we can
write g2 = ϕ(g1 ) for some g1 in G 1 . Then g2 ϕ(H1 )g2−1 = ϕ(g1 )ϕ(H1 )ϕ(g1 )−1 =
ϕ(g1 H1 g1−1 ) = ϕ(H1 ). Thus ϕ(H1 ) is normal.
    For the final statement let H2 = ϕ(H1 ). We have just proved that this image
is normal, and hence G 2 /H2 is a group. The mapping 8 : G 1 → G 2 /H2 given
by 8(g1 ) = ϕ(g1 )H2 is the composition of two homomorphisms and hence is a
homomorphism. Its kernel is

      {g1 ∈ G 1 | ϕ(g1 ) ∈ H2 } = {g1 ∈ G 1 | ϕ(g1 ) ∈ ϕ(H1 )} = ϕ −1 (ϕ(H1 )),

and this equals H1 by the first conclusion of the theorem. Applying Corollary
4.12 to 8, we obtain the required isomorphism 8 : G 1 /H1 → G 2 /ϕ(H1 ).   §
                                3. Direct Products and Direct Sums                                135

   Theorem 4.14 (Second Isomorphism Theorem). Let H1 and H2 be subgroups
of a group G with H2 normal in G. Then H1 ∩ H2 is a normal subgroup of H1 , the
set H1 H2 of products is a subgroup of G with H2 as a normal subgroup, and the
map h 1 (H1 ∩ H2 ) 7→ h 1 H2 is a well-defined canonical isomorphism of groups

                               H1 /(H1 ∩ H2 ) ∼
                                              = (H1 H2 )/H2 .

    PROOF. The set H1 ∩ H2 is a subgroup, being the intersection of two subgroups.
For h 1 in H1 , we have h 1 (H1 ∩ H2 )h −1             −1
                                         1 ⊆ h 1 H1 h 1 ⊆ H1 since H1 is a subgroup
and h 1 (H1 ∩ H2 )h −1                −1
                      1 ⊆ h 1 H2 h 1 ⊆ H2 since H2 is normal in G. Therefore
                −1
h 1 (H1 ∩ H2 )h 1 ⊆ H1 ∩ H2 , and H1 ∩ H2 is normal in H1 .
    The set H1 H2 of products is a subgroup since h 1 h 2 h 01 h 02 = h 1 h 01 (h 01 −1 h 2 h 01 )h 02
and since (h 1 h 2 )−1 = h −1        −1 −1
                            1 (h 1 h 2 h 1 ), and H2 is normal in H1 H2 since H2 is
normal in G.
    The function ϕ(h 1 (H1 ∩ H2 )) = h 1 H2 is well defined since H1 ∩ H2 ⊆ H2 ,
and ϕ respects products. The domain of ϕ is {h 1 (H1 ∩ H2 ) | h 1 ∈ H1 }, and the
kernel is the subset of this such that h 1 lies in H2 as well as H1 . For this to happen,
h 1 must be in H1 ∩ H2 , and thus the kernel is the identity coset of H1 /(H1 ∩ H2 ).
Hence ϕ is one-one.
    To see that ϕ is onto (H1 H2 )/H2 , let h 1 h 2 H2 be given. Then h 1 (H1 ∩ H2 )
maps to h 1 H2 , which equals h 1 h 2 H2 . Hence ϕ is onto.                                       §


                          3. Direct Products and Direct Sums

We return to the matter of direct products and direct sums of groups, direct
products having been discussed briefly in Section 1. In a footnote in Section II.4
we mentioned a general principle in algebra that “whenever a new systematic con-
struction appears for the objects under study, it is well to look for a corresponding
construction with the functions relating these new objects.” This principle will
be made more precise in Section 11 of the present chapter with the aid of the
language of “categories” and “functors.”
   Another principle that will be relevant for us is that constructions in one context
in algebra often recur, sometimes in slightly different guise, in other contexts. One
example of the operation of this principle occurs with quotients. The construction
and properties of the quotient of a vector space by a vector subspace, as in Section
II.5, is analogous in this sense to the construction and properties of the quotient of
a group by a normal subgroup, as in Section 2 in the present chapter. The need for
the subgroup to be normal is an example of what is meant by “slightly different
guise.” Anyway, this principle too will be made more precise in Section 11 of
the present chapter using the language of categories and functors.
136                          IV. Groups and Group Actions

   Let us proceed with an awareness of both these principles in connection with
direct products and direct sums of groups, looking for analogies with what hap-
pened for vector spaces and expecting our work to involve constructions with
homomorphisms as well as with groups.
   The external direct product G 1 × G 2 was defined as a group in Section 1 to
be the set-theoretic product with coordinate-by-coordinate multiplication. There
are four homomorphisms of interest connected with G 1 × G 2 , namely
                i1 : G 1 → G 1 × G 2      given by i 1 (g1 ) = (g1 , 1),
                i2 : G 2 → G 1 × G 2      given by i 2 (g2 ) = (1, g2 ),
               p1 : G 1 × G 2 → G 1       given by p1 (g1 , g2 ) = g1 ,
               p2 : G 1 × G 2 → G 2       given by p2 (g1 , g2 ) = g2 .
Recall from the discussion before Proposition 4.5 that Proposition 2.30 for the
direct product of two vector spaces does not translate directly into an analog for
the direct product of groups; instead that proposition is replaced by Proposition
4.5, which involves some condition of commutativity.
    Warned by this anomaly, let us work with mappings rather than with groups
and subgroups, and let us use mappings in formulating a definition of the direct
product of groups. As with the direct product of two vector spaces, the mappings
to use are p1 and p2 but not i 1 and i 2 . The way in which p1 and p2 enter is
through the effect of the direct product on homomorphisms. If ϕ1 : H → G 1
and ϕ2 : H → G 2 are two homomorphisms, then h 7→ (ϕ1 (h), ϕ2 (h)) is the
corresponding homomorphism of H into G 1 × G 2 . In order to state matters fully,
let us give the definition with an arbitrary number of factors.
    Let S be an arbitrary nonempty set of groups, and let G s be the group cor-
responding to the member Q     s of S. The external direct product of the G s ’s
consists of a group s∈S G s and a system of group homomorphisms. The
group
S       as a set is ×s∈S G s , whose elements are arbitrary functions from S to
° s∈S G s¢°such     ¢ the value of the function at s is in G s , and the group law is
                 that
 {gs }s∈S {gs0 }s∈S = {gs gs0 }s∈S . The group
                                            ° homomorphisms
                                                    ¢              are the coordinate
                  Q
mappings ps0 : s∈S G s → G s0 with ps0 {gs }s∈S = gs0 . The individual groups
G s are called the factors, and a direct Q product of n Q groups may be written as
G 1 ×· · ·×G n instead of with the symbol . The group s∈S G s has the universal
mapping property described in Proposition 4.15 and pictured in Figure 4.2.
   Proposition 4.15 (universal mapping property of  Qexternal direct product). Let
{G s | s ∈ S} be a nonempty set of groups, and let s∈S G s be the external direct
      Q the associated group homomorphisms being the coordinate mappings
product,
ps0 : s∈S G s → G s0 . If H is any group and {ϕs | s ∈ S} is a system of group
homomorphisms
           Q      ϕs : H → G s , then there exists a unique group homomorphism
ϕ : H → s∈S G s such that ps0 ◦ ϕ = ϕs0 for all s0 ∈ S.
                           3. Direct Products and Direct Sums                      137

                                                ϕs0
                                   G s0   √−−− H
                                     x
                                 ps0 
                                             ϕ

                                Q
                                  s∈S G s

FIGURE 4.2. Universal mapping property of an external direct product of groups.

       ° . Existence
   PROOF             ¢ of ϕ is proved by taking ϕ(h) = {ϕs (h)}s∈S . Then pQ  s0 (ϕ(h))
= ps0 {ϕs (h)}s∈S = ϕs0 (h) as required. For uniqueness let ϕ 0 : H → s∈S G s
be a homomorphism with ps0 ◦ ϕ 0 = ϕs0 for all s0 ∈ S. For each h in H , we can
write ϕ 0 (h) = {ϕ 0 (h)s }s∈S . For s0 in S, we then have ϕs0 (h) = ( ps0 ◦ ϕ 0 )(h) =
ps0 (ϕ 0 (h)) = ϕ 0 (h)s0 , and we conclude that ϕ 0 = ϕ.                             §

    Now we give an abstract definition of direct product that allows for the possi-
bility that the direct product is “internal” in the sense that the various factors are
identified as subgroups of a given group. The definition is by means of the above
universal mapping property and will be seen to characterize the direct product up
to canonical isomorphism. Let S be an arbitrary nonempty set of groups, and let
G s be the group corresponding to the member s of S. A direct product of the
G s ’s consists of a group G and a system of group homomorphisms ps : G → G s
for s ∈ S with the following universal mapping property: whenever H is a
group and {ϕs | s ∈ S} is a system of group homomorphisms ϕs : H → G s , then
there exists a unique group homomorphism ϕ : H → G such that ps ◦ ϕ = ϕs
for all s ∈ S. Proposition 4.15 proves existence of a direct product, and the next
proposition addresses uniqueness. A Ødirect product is internal if each G s is a
subgroup of G and each restriction ps ØG s is the identity map.
                                           ϕs
                                   G s √−−− H
                                     x
                                     
                                  ps      ϕ


                                   G
     FIGURE 4.3. Universal mapping property of a direct product of groups.

    Proposition 4.16. Let S be a nonempty set of groups, and let G s be the group
corresponding to the member s of S. If (G, { ps }) and (G 0 , { ps0 }) are two direct
products, then the homomorphisms ps : G → G s and ps0 : G 0 → G s are onto
G s , there exists a unique homomorphism 8 : G 0 → G such that ps0 = ps ◦ 8 for
all s ∈ S, and 8 is an isomorphism.
  PROOF. In Figure 4.3 let H = G 0 and ϕs = ps0 . If 8 : G 0 → G is the
homomorphism produced by the fact that G is a direct product, then we have
138                         IV. Groups and Group Actions

ps ◦8 = ps0 for all s. Reversing the roles of G and G 0 , we obtain a homomorphism
80 : G → G 0 with ps0 ◦80 = ps for all s. Therefore ps ◦(8◦80 ) = ps0 ◦80 = ps .
    In Figure 4.3 we next let H = G and ϕs = ps for all s. Then the identity 1G
on G has the same property ps ◦ 1G = ps relative to all ps that 8 ◦ 80 has, and the
uniqueness says that 8 ◦ 80 = 1G . Reversing the roles of G and G 0 , we obtain
80 ◦ 8 = 1G 0 . Therefore 8 is an isomorphism.
    For uniqueness suppose that 9 : G 0 → G is another homomorphism with
  0
ps = ps ◦ 9 for all s ∈ S. Then the argument of the previous paragraph shows
that 80 ◦9 = 1G 0 . Applying 8 on the left gives 9 = (8◦80 )◦9 = 8◦(80 ◦9) =
8 ◦ 1G 0 = 8. Thus 9 = 8.
    Finally we have to show that the s th mapping of a direct product is onto
G s . It isQenough to show that ps0 is onto G s . Taking G as the external direct
product s∈S G s with ps equal to the coordinate mapping, form the isomorphism
80 : G → G 0 that has just been proved to exist. This satisfies ps = ps0 ◦ 80 for
all s ∈ S. Since ps is onto G s , ps0 must be onto G s .                         §

   Let us turn to direct sums. Part of what we seek is a definition that allows
for an abstract characterization of direct sums in the spirit of Proposition 4.16.
In particular, the interaction with homomorphisms is to be central to the dis-
cussion. In the case of two factors, we use i 1 and i 2 rather than p1 and p2 . If
ϕ1 : G 1 → H and ϕ2 : G 2 → H are two homomorphisms, then the correspond-
ing homomorphism ϕ of G 1 ⊕ G 2 to H is to satisfy ϕ1 = ϕ ◦ i 1 and ϕ2 = ϕ ◦ i 2 .
With G 1 ⊕ G 2 defined, as expected, to be the same group as G 1 × G 2 , we are led
to the formula
                 ϕ(g1 , g2 ) = ϕ(g1 , 1)ϕ(1, g2 ) = ϕ1 (g1 )ϕ2 (g2 ).
The images of commuting elements under a homomorphism have to commute,
and hence H had better be abelian. Then in order to have an analog of Proposition
4.16, we will want to specialize H at some point to G 1 ⊕ G 2 , and therefore G 1
and G 2 had better be abelian. With these observations in place, we are ready for
the general definition.
    Let S be an arbitrary nonempty set of abelian groups, and let G s be the group
corresponding to the member s of S. We shall use additive notation for the group
        L in each G s . The external direct sum of the G s ’s consists of an abelian
operation
group s∈S G s Q   and a system of group homomorphisms i s for s ∈ S. The group is
                                                                            L many
the subgroup of s∈S G s of all elements that are equal to 0 in all but finitely
coordinates. The group homomorphisms are the mappings i s0 : G s0 → s∈S G s
carrying a member gs0 of G s0 to the element that is gs0 in coordinate s0 and is 0
at all other coordinates. The individual groups are called the summands, and
aLdirect sum of n abelian groups may be written as G 1 ⊕ · · · ⊕ G n . The group
    s∈S G s has the universal mapping property described in Proposition 4.17 and
 pictured in Figure 4.4.
                          3. Direct Products and Direct Sums                     139


                                                                 L sum). Let
   Proposition 4.17 (universal mapping property of external direct
{G s | s ∈ S} be a nonempty set of abelian groups, and let s∈S G s be the
external direct sum, theLassociated group homomorphisms being the embedding
mappings i s0 : G s0 → s∈S G s . If H is any abelian group and {ϕs | s ∈ S} is a
system of group homomorphisms
                     L            ϕs : G s → H , then there exists a unique group
homomorphism ϕ : s∈S G s → H such that ϕ ◦ i s0 = ϕs0 for all s0 ∈ S.
                                                ϕs
                                   G s0        −−−→ H
                                     
                                     
                                i s0 y               ϕ

                               L
                                  s∈S     Gs
       FIGURE 4.4. Universal mapping property of an external direct sum
                              of abelian groups.
                                              °         ¢ P
  PROOF. Existence of ϕ is proved by taking ϕ {gs }s∈S = s ϕs (gs ). The sum
on the right side is meaningful since the element {gs }s∈S of the direct sum has
only finitely many nonzero coordinates. Since H is abelian, the computation
       °        ¢   °         ¢ P               P
      ϕ {gs }s∈S + ϕ {gs0 }s∈S = s ϕs (gs ) + s ϕs (gs0 )
                                 P                         P
                               = s (ϕs (gs ) + ϕs (gs0 )) = s ϕs (gs + gs0 )
                                  °              ¢      °                  ¢
                               = ϕ {gs + gs0 }s∈S = ϕ {gs }s∈S + {gs0 }s∈S

shows that ϕ is a homomorphism. If gs0 is given and {gs }s∈S denotes the el-
                              th
ement
  °               s0 in the s0 coordinate and is 0 elsewhere, then ϕ(i s0 (gs0 )) =
        that¢ is gP
ϕ {gs }s∈S = s ϕs (gs ), and the right side equals ϕs0 (gs0 ) since gs = 0 for all
other s’s. Thus ϕ ◦ i s0 = ϕL s0 .
    For uniqueness let ϕ 0 : s∈S G s → H be a homomorphism withL     ϕ 0 ◦ i s0 = ϕs0
                                    0
for all s0 ∈ S. Then the value of ϕ is determined at all elements of Ls∈S G s that
are 0 in all but one coordinate. Since the most generalL  member of s∈S G s is a
finite sum of such elements, ϕ 0 is determined on all of s∈S G s .                 §

     Now we give an abstract definition of direct sum that allows for the possibility
that the direct sum is “internal” in the sense that the various constituents are
identified as subgroups of a given group. Again the definition is by means of a
universal mapping property and will be seen to characterize the direct sum up to
canonical isomorphism. Let S be an arbitrary nonempty set of abelian groups,
and let G s be the group corresponding to the member s of S. A direct sum of
the G s ’s consists of an abelian group G and a system of group homomorphisms
i s : G s → G for s ∈ S with the following universal mapping property: when-
ever H is an abelian group and {ϕs | s ∈ S} is a system of group homomorphisms
140                           IV. Groups and Group Actions

ϕs : G s → H , then there exists a unique group homomorphism ϕ : G → H
such that ϕ ◦ i s = ϕs for all s ∈ S. Proposition 4.17 proves existence of a direct
sum, and the next proposition addresses uniqueness. A direct sum is internal if
each G s is a subgroup of G and each mapping i s is the inclusion mapping.
                                           ϕs
                                    G s −−−→ H
                                      
                                      
                                   is y     ϕ


                                    G
  FIGURE 4.5. Universal mapping property of a direct sum of abelian groups.

   Proposition 4.18. Let S be a nonempty set of abelian groups, and let G s be
the group corresponding to the member s of S. If (G, {i s }) and (G 0 , {i s0 }) are
two direct sums, then the homomorphisms i s : G s → G and i s0 : G s → G 0 are
one-one, there exists a unique homomorphism 8 : G → G 0 such that i s0 = 8 ◦ i s
for all s ∈ S, and 8 is an isomorphism.

    PROOF. In Figure 4.5 let H = G 0 and ϕs = i s0 . If 8 : G → G 0 is the
homomorphism produced by the fact that G is a direct sum, then we have 8 ◦ i s
= i s0 for all s. Reversing the roles of G and G 0 , we obtain a homomorphism
80 : G 0 → G with 80 ◦ i s0 = i s for all s. Therefore (80 ◦ 8) ◦ i s = 80 ◦ i s0 = i s .
    In Figure 4.5 we next let H = G and ϕs = i s for all s. Then the identity 1G
on G has the same property 1G ◦ i s = i s relative to all i s that 80 ◦ 8 has, and the
uniqueness says that 80 ◦ 8 = 1G . Reversing the roles of G and G 0 , we obtain
8 ◦ 80 = 1G 0 . Therefore 8 is an isomorphism.
    For uniqueness suppose that 9 : G → G 0 is another homomorphism with
  0
i s = 9 ◦ i s for all s ∈ S. Then the argument of the previous paragraph shows that
80 ◦ 9 = 1G . Applying 8 on the left gives 9 = (8 ◦ 80 ) ◦ 9 = 8 ◦ (80 ◦ 9) =
8 ◦ 1G = 8. Thus 9 = 8.
    Finally we have to show that the s th mapping of a direct sum is one-one on
G s . ItLis enough to show that i s0 is one-one on G s . Taking G as the external direct
sum s∈S G s with i s equal to the embedding mapping, form the isomorphism
80 : G 0 → G that has just been proved to exist. This satisfies i s = 80 ◦ i s0 for all
s ∈ S. Since i s is one-one, i s0 must be one-one.                                    §

   EXAMPLE. The group Q× is the direct sum of copies of Z, one for each prime,
plus one copy of Z/2Z. If p is a prime, the mapping i p : Z → Q× is given
by i p (n) = pn . The remaining coordinate gives the sign. The isomorphism
results from unique factorization, only finitely many primes being involved for
any particular nonzero rational number.
                                  4. Rings and Fields                             141

                               4. Rings and Fields

In this section we begin a two-section digression in order to develop some more
number theory beyond what is in Chapter I and to make some definitions as new
notions arise. In later sections of the present chapter, some of this material will
yield further examples of concrete groups and tools for working with them.
   We begin with the additive group Z/mZ of integers modulo a positive integer
m. We continue to write [a] for the equivalence class of the integer a when it is
helpful to do so. Our interest will be in the multiplication structure that Z/mZ
inherits from multiplication in Z. Namely, we attempt to define
                                   [a][b] = [ab].
To see that this formula is meaningful in Z/mZ, we need to check that the same
equivalence class results on the right side if the representatives of [a] and [b]
are changed. Thus let [a] = [a 0 ] and [b] = [b0 ]. Then m divides a − a 0 and
b − b0 and must divide the sum of products (a − a 0 )b + a 0 (b − b0 ) = ab − a 0 b0 .
Consequently [ab] = [a 0 b0 ], and multiplication is well defined. If x and y are in
Z/mZ, their product is often denoted by x y mod m.
   The same kind of argument as just given shows that the associativity of multi-
plication in Z and the distributive laws imply corresponding facts about Z/mZ.
The result is that Z/mZ is a “commutative ring with identity” in the sense of the
following definitions.
   A ring is a set R with two operations R × R → R, usually called addition
and multiplication and often denoted by (a, b) 7→ a + b and (a, b) 7→ ab, such
that
     (i) R is an abelian group under addition,
    (ii) multiplication is associative in the sense that a(bc) = (ab)c for all a, b, c
         in R,
   (iii) the two distributive laws
         a(b + c) = (ab) + (ac)          and       (b + c)a = (ba) + (ca)
        hold for all a, b, c in R.
The additive identity is denoted by 0, and the additive inverse of a is denoted by
−a. A sum a + (−b) is often abbreviated a − b. By convention when parentheses
are absent, multiplications are to be carried out before additions and subtractions.
Thus the distributive laws may be rewritten as
             a(b + c) = ab + ac         and        (b + c)a = ba + ca.
A ring R is called a commutative ring if multiplication satisfies the commutative
law
   (iv) ab = ba for all a and b in R.
142                                IV. Groups and Group Actions

A ring R is called a ring with identity6 if there exists an element 1 such that
1a = a1 = a for all a in R. It is immediate from the definitions that
      • 0a = 0 and a0 = 0 in any ring (since, in the case of the first formula,
         0 = 0a − 0a = (0 + 0)a − 0a = 0a + 0a − 0a = 0a),
      • the multiplicative identity is unique in a ring with identity (since 10 =
         10 1 = 1),
      • (−1)a = −a = a(−1) in any ring with identity (partly since 0 = 0a =
         (1 + (−1))a = 1a + (−1)a = a + (−1)a).
   In a ring with identity, it will be convenient not to insist that the identity be
different from the zero element 0. If 1 and 0 do happen to coincide in R, then it
readily follows that 0 is the only element of R, and R is said to be the zero ring.
   The set Z of integers is a basic example of a commutative ring with identity.
Returning to Z/mZ, suppose now that m is a prime p. If [a] is in Z/ pZ with a
in {1, 2, . . . , p − 1}, then GCD(a, p) = 1 and Proposition 1.2 produces integers
r and s with ar + ps = 1. Modulo p, this equation reads [a][r] = [1]. In other
words, [r] is a multiplicative inverse of [a]. The result is that Z/ pZ, when p is a
prime, is a “field” in the sense of the following definition.
   A field F is a commutative ring with identity such that F 6= 0 and such that
    (v) to each a 6= 0 in F corresponds an element a −1 in F such that aa −1 = 1.
In other words, F× = F − {0} is an abelian group under multiplication. Inverses
are necessarily unique as a consequence of one of the properties of groups.
   When p is prime, we shall write F p for the field Z/ pZ. Its multiplicative
group F× p has order p − 1, and Lagrange’s Theorem (Corollary 4.8) immediately
implies that a p−1 ≡ 1 mod p whenever a and p are relatively prime. This result
is known as Fermat’s Little Theorem.7
   For general m, certain members of Z/mZ have multiplicative inverses. The
product of two such elements is again one, and the inverse of one is again one.
Thus, even though Z/mZ need not be a field, the subset (Z/mZ)× of members
of Z/mZ with multiplicative inverses is a group. The same argument as when m
is prime shows that the class of a has an inverse if and only if GCD(a, m) = 1.
The number of such classes was defined in Chapter I in terms of the Euler ϕ
function as ϕ(m), and a formula for ϕ(m) was obtained in Corollary 1.10. The
   6 Some   authors, particularly when discussing only algebra, find it convenient to incorporate the
existence of an identity into the definition of a ring. However, in real analysis some important natural
rings do not have an identity, and the theory is made more complicated by forcing an identity into
the picture. For example the space of integrable functions on R forms a very natural ring, with
convolution as multiplication, and there is no identity; forcing an identity into the picture in such
a way that the space remains stable under translations makes the space large and unwieldy. The
distinction between working with rings and working with rings with identity will be discussed further
in Section 11.
    7 As opposed to Fermat’s Last Theorem, which lies deeper.
                                    4. Rings and Fields                                143

conclusion is that (Z/mZ)× is an abelian group of order ϕ(m). Application of
Lagrange’s Theorem yields Euler’s generalization of Fermat’s Little Theorem,
namely that a ϕ(m) ≡ 1 mod m for every positive integer m and every integer a
relatively prime to m.
    More generally, in any ring R with identity, a unit is defined to be any element
a such that there exists an element a −1 with aa −1 = a −1 a = 1. The element a −1
is unique if it exists8 and is called the multiplicative inverse of a. The units of R
form a group denoted by R × . For example the group Z× consists of +1 and −1,
and the zero ring R has R × = {0}. If R is a nonzero ring, then 0 is not in R × .
    Here are some further examples of fields.

   EXAMPLES OF FIELDS.
   (1) Q, R, and C. These are all fields.
   (2) Q[θ]. This was introduced between Examples 8 and 9 of Section 1. It
is assumed that θ is a complex number and that there exists an integer n > 0
such that the complex numbers 1, θ, θ 2 , . . . , θ n are linearly dependent over Q.
The set Q[θ] is defined to be the linear span over Q of all powers 1, θ, θ 2 , . . . of
θ, which is the same as the linear span of the finite set 1, θ, θ 2 , . . . , θ n−1 . The
set Q[θ] was shown in Proposition 4.1 to be a subset of C that is closed under
the arithmetic operations, including the passage to reciprocals in the case of the
nonzero elements. It is therefore a field.
   (3) A field of 4 elements. Let F4 = {0, 1, θ, θ +1}, where θ is some symbol not
standing for 0 or 1. Define addition in F4 and multiplication in F×    4 by requiring
that a + 0 = 0 + a = a for all a, that

        1 + 1 = 0,             1 + θ = (θ + 1),       1 + (θ + 1) = θ,
        θ + 1 = (θ + 1),       θ + θ = 0,             θ + (θ + 1) = 1,
  (θ + 1) + 1 = θ,       (θ + 1) + θ = 1,       (θ + 1) + (θ + 1) = 0,
and that

          11 = 1,             1θ = θ,             1(θ + 1) = (θ + 1),
          θ1 = θ,             θθ = (θ + 1),       θ(θ + 1) = 1,
    (θ + 1)1 = (θ + 1), (θ + 1)θ = 1,       (θ + 1)(θ + 1) = θ.
The result is a field. With this direct approach a certain amount of checking is
necessary to verify all the properties of a field. We shall return to this matter in
Chapter IX when we consider finite fields more generally, and we shall then have
a way of constructing F4 that avoids tedious checking.
   8 In fact, if b and c exist with ab = ca = 1, then a is a unit with a −1 = b = c because

b = 1b = (ca)b = c(ab) = c1 = c.
144                           IV. Groups and Group Actions

   In analogy with the theory of groups, we define a subring of a ring to be a
nonempty subset that is closed under addition, negation, and multiplication. The
set 2Z of even integers is a subring of the ring Z of integers. A subfield of a field is
a subset containing 0 and 1 that is closed under addition, negation, multiplication,
and multiplicative inverses for its nonzero elements. The set Q of rationals is a
subfield of the field R of reals.
   Intermediate between rings and fields are two kinds of objects—integral do-
mains and division rings—that arise frequently enough to merit their own names.
   The setting for the first is a commutative ring R. A nonzero element a of
R is called a zero divisor if there is some nonzero b in R with ab = 0. For
example the element 2 in the ring Z/6Z is a zero divisor because 2 · 3 = 0.
An integral domain is a nonzero commutative ring with identity having no zero
divisors. Fields have no zero divisors since if a and b are nonzero, then ab = 0
would force b = 1b = (a −1 a)b = a −1 (ab) = a −1 0 = 0 and would give a
contradiction; therefore every field is an integral domain. The ring of integers
Z is another example of an integral domain, and the polynomial rings Q[X] and
R[X] and C[X] introduced in Section I.3 are further examples. A cancellation
law for multiplication holds in any integral domain:
                   ab = ac with a 6= 0          implies      b = c.
In fact, ab = ac implies a(b − c) = 0; since a 6= 0, b − c must be 0.
   The other object with its own name is a division ring, which is a nonzero ring
with identity such that every nonzero element is a unit. The commutative division
rings are the fields, and we have encountered only one noncommutative division
ring so far. That is the set H of quaternions, which was introduced in Section 1.
Division rings that are not fields will play only a minor role in this book but are
of great interest in Chapters II and III of Advanced Algebra.
   Let us turn to mappings. A function ϕ : R → R 0 between two rings is an
isomorphism of rings if ϕ is one-one onto and satisfies ϕ(a + b) = ϕ(a) + ϕ(b)
and ϕ(ab) = ϕ(a)ϕ(b) for all a and b in R. In other words, ϕ is to be an
isomorphism of the additive groups and to satisfy ϕ(ab) = ϕ(a)ϕ(b). Such a
mapping carries the identity, if any, in R to the identity of R 0 . The relation “is
isomorphic to” is an equivalence relation. Common notation for an isomorphism
of rings is R ∼ = R 0 ; because of the symmetry, one can say that R and R 0 are
isomorphic.
   A function ϕ : R → R 0 between two rings is a homomorphism of rings if ϕ
satisfies ϕ(a + b) = ϕ(a) + ϕ(b) and ϕ(ab) = ϕ(a)ϕ(b) for all a and b in R.
In other words, ϕ is to be a homomorphism of the additive groups and to satisfy
ϕ(ab) = ϕ(a)ϕ(b).
   EXAMPLES OF HOMOMORPHISMS OF RINGS.
   (1) The mapping ϕ : Z → Z/mZ given by ϕ(k) = k mod m.
                                         4. Rings and Fields                                       145

   (2) The evaluation mapping ϕ : R[X] → R given by P(X) 7→ P(r) for some
fixed r in R.
   (3) Mappings with the direct product Z×Z. The additive group Z×Z becomes
a commutative ring with identity under coordinate-by-coordinate multiplication,
namely (a, a 0 ) + (b, b0 ) = (a + b, a 0 + b0 ). The identity is (1, 1). Projection
(a, a 0 ) 7→ a to the first coordinate is a homomorphism of rings Z × Z → Z that
carries identity to identity. Inclusion a 7→ (a, 0) of Z into the first coordinate is
a homomorphism of rings Z → Z × Z that does not carry identity to identity.9

  Proposition 4.19. If R is a ring with identity 1 R , then there exists a unique
homomorphism of rings ϕ1 : Z → R such that ϕ(1) = 1 R .
    PROOF. The formulas for manipulating exponents of an element in a group,
when translated into the additive notation for addition in R, say that n 7→ nr
satisfies (m + n)r = mr + nr and (mn)r = m(nr) for all r in R and all
integers m and n. The first of these formulas implies, for any r in R, that
ϕr (n) = nr is a homomorphism between the additive groups of Z and R, and
it is certainly uniquely determined by its value for n = 1. The distributive
laws imply that √r (r 0 ) = r 0r is another homomorphism of additive groups.
Hence √r ◦ ϕr 0 and ϕr 0 r are homomorphisms between the additive groups of
Z and R. Since (√r ◦ ϕr 0 )(1) = √r (r 0 ) = r 0r = ϕr 0 r (1), we must have
(√r ◦ ϕr 0 )(m) = ϕr 0 r (m) for all integers m. Thus (mr 0 )r = m(r 0r) for all
m. Putting r = n1 R and r 0 = 1 R proves the fourth equality of the computation
                ϕ1 (mn) = (mn)1 R = m(n1 R )
                        = m(1 R (n1 R )) = (m1 R )(n1 R ) = ϕ1 (m)ϕ1 (n),
and shows that ϕ1 is in fact a homomorphism of rings.                                               §

    The image of a homomorphism ϕ : R → R 0 of rings is a subring of R 0 , as is
easily checked. The kernel turns out to be more than just of subring of R. If a
is in the kernel and b is any element of R, then ϕ(ab) = ϕ(a)ϕ(b) = 0ϕ(b) = 0
and similarly ϕ(ba) = 0. Thus the kernel of a ring homomorphism is closed
under products of members of the kernel with arbitrary members of R. Adapting
a definition to this circumstance, one says that an ideal I of R (or two-sided
ideal in case of ambiguity) is an additive subgroup such that ab and ba are in I
whenever a is in I and b is in R. Briefly then, the kernel of a homomorphism of
rings is an ideal.
    Conversely suppose that I is an ideal in a ring R. Since I is certainly an
additive subgroup of an abelian group, we can form the additive quotient group
   9 Sometimes   authors who build the existence of an identity into the definition of “ring” insist as
a matter of definition that homomorphisms of rings carry identity to identity. Such authors would
then exclude this particular mapping from consideration as a homomorphism.
146                               IV. Groups and Group Actions

R/I . It is customary to write the individual cosets in additive notation, thus as
r + I . In analogy with Proposition 4.10, we have the following result for the
present context.

   Proposition 4.20. If I is an ideal in a ring R, then a well-defined operation
of multiplication is obtained within the additive group R/I by the definition
(r1 + I )(r2 + I ) = r1r2 + I , and R/I becomes a ring. If R has an identity 1, then
1 + I is an identity in R/I . With these definitions the function q : R → R/I
given by q(r) = r + I is a ring homomorphism of R onto R/I with kernel I .
Consequently every ideal of R is the kernel of some homomorphism of rings.
   REMARKS. When I is an ideal, the ring R/I is called a quotient ring10 of R,
and the homomorphism q : R → R/I is called the quotient homomorphism.
In the special case that R = Z and I = mZ, the construction of R/I reduces to
the construction of Z/mZ as a ring at the beginning of this section.
    PROOF. If we change the representatives of the cosets from r1 and r2 to r1 + i 1
and r2 + i 2 with i 1 and i 2 in I , then (r1 + i 1 )(r2 + i 2 ) = r1r2 + (i 1r2 + r1 i 2 + i 1 i 2 )
is in r1r2 + I by the closure properties of I . Hence multiplication is well defined.
    The associativity of this multiplication follows from the associativity of mul-
tiplication in R because
°                    ¢
  (r1 + I )(r2 + I ) (r3 + I ) = (r1r2 + I )(r3 + I ) = (r1r2 )r3 + I = r1 (r2r3 ) + I
                                                                         °                       ¢
                                 = (r1 + I )(r2r3 + I ) = (r1 + I ) (r2 + I )(r3 + I ) .

Similarly the computation
              °                     ¢
     (r1 + I ) (r2 + I ) + (r3 + I ) = r1 (r2 + r3 ) + I = (r1r2 + r1r3 ) + I
                                      = (r1 + I )(r2 + I ) + (r1 + I )(r3 + I )

yields one distributive law, and the other distributive law is proved in the same
way. If R has an identity 1, then (1 + I )(r + I ) = 1r + I = r + I and
(r + I )(1 + I ) = r1 + I = r + I show that 1 + I is an identity in R/I .
   Finally we know that the quotient map q : R → R/I is a homomorphism of
additive groups, and the computation q(r1r2 ) = r1r2 + I = (r1 + I )(r2 + I ) =
q(r1 )q(r2 ) shows that q is a homomorphism of rings.                           §

   EXAMPLES OF IDEALS.
   (1) The ideals in the ring Z coincide with the additive subgroups and are the
sets mZ; the reason each mZ is an ideal is that if a and b are integers and m
divides a, then m divides ab.
    10 Quotient rings are known also as “factor rings.” A “ring of quotients,” however, is something

different.
                                   4. Rings and Fields                              147

   (2) The ideals in a field F are 0 and F itself, no others; in fact, if a 6= 0 is in
an ideal and b is in F, then the equality b = (ba −1 )a shows that b is in the ideal
and that the ideal therefore contains all elements of F.
    (3) If R is Q[X] or R[X] or C[X], then every ideal I is of the form I = R f (X)
for some polynomial f (X). In fact, we can take f (X) = 0 if I = 0. If I 6= 0,
let f (X) be a nonzero member of I of lowest possible degree. If A(X) is in I ,
then Proposition 1.12 shows that A(X) = f (X)B(X) + C(X) with C(X) = 0 or
deg C < deg f . The equality C(X) = A(X) − f (X)B(X) shows that C(X) is in
I , and the minimality of deg f implies that C(X) = 0. Thus A(X) = f (X)B(X).
   (4) In a ring R with identity 1, an ideal I is a proper subset of R if and only if 1
is not in I . In fact, I is certainly a proper subset if 1 is not in I . In the converse
direction if 1 is in I , then every element r = r1, for r in R, lies in I . Hence
I = R, and I is not a proper subset.

   In analogy with what was shown for vector spaces in Proposition 2.25 and
for groups in Proposition 4.11, quotients in the context of rings allow for the
factorization of certain homomorphisms of rings. The appropriate result is stated
as Proposition 4.21 and is pictured in Figure 4.6.

   Proposition 4.21. Let ϕ : R1 → R2 be a homomorphism of rings, let I0 =
ker ϕ, let I be an ideal of R1 contained in I0 , and let q : R1 → R1 /I be the quotient
homomorphism. Then there exists a homomorphism of rings ϕ : R1 /I → R2
such that ϕ = ϕ ◦ q, i.e., ϕ(r1 + I ) = ϕ(r1 ). It has the same image as ϕ, and
ker ϕ = {r + I | r ∈ I0 }.
                                             ϕ
                                    R1    −−−→ R2
                                    
                                    
                                   qy            ϕ


                                  R1 /I
     FIGURE 4.6. Factorization of homomorphisms of rings via the quotient
                             of a ring by an ideal.

   REMARK. One says that ϕ factors through R1 /I or descends to R1 /I .

   PROOF. Proposition 4.11 shows that ϕ descends to a homomorphism ϕ of
the additive group of R1 /I into the additive group of R2 and that all the other
conclusions hold except possibly for the fact that ϕ respects multiplication. To
see that ϕ respects multiplication, we just compute that ϕ((r + I )(r 0 + I )) =
ϕ(rr 0 + I ) = ϕ(rr 0 ) = ϕ(r)ϕ(r 0 ) = ϕ(r + I )ϕ(r 0 + I ).                  §
148                          IV. Groups and Group Actions

   An example of special interest occurs when ϕ is a homomorphism of rings
ϕ : Z → R and the ideal mZ of Z is contained in the kernel of ϕ. Then the
proposition says that ϕ descends to a homomorphism of rings ϕ : Z/mZ → R.
We shall make use of this result shortly. But first let us state a different special
case as a corollary.

   Corollary 4.22. Let ϕ : R1 → R2 be a homomorphism of rings, and suppose
that ϕ is onto R2 and has kernel I . Then ϕ exhibits the ring R1 /I as canonically
isomorphic to R2 .
   PROOF. Take I = I0 in Proposition 4.21, and form ϕ : R1 /I → R2 with
ϕ = ϕ ◦ q. The proposition shows that ϕ is onto R2 and has trivial kernel, i.e.,
the identity element of R1 /I . Having trivial kernel, ϕ is one-one.         §

  Proposition 4.23. Any field F contains a subfield isomorphic to the rationals
Q or to some field F p with p prime.
   REMARKS. The subfield in the proposition is called the prime field of F. The
characteristic of F is defined to be 0 if the prime field is isomorphic to Q and to
be p if the prime field is isomorphic to F p .
    PROOF. Proposition 4.19 produces a homomorphism of rings ϕ1 : Z → F
with ϕ1 (1) = 1. The kernel of ϕ1 is an ideal, necessarily of the form mZ with
m an integer ∏ 0, and the image of ϕ1 is a commutative subring with identity in
F. Let ϕ 1 : Z/mZ → F be the descended homomorphism given by Proposition
4.21. The integer m cannot factor nontrivially, say as m = rs, because otherwise
ϕ 1 (r) and ϕ 1 (s) would be nonzero members of F with ϕ 1 (r)ϕ 1 (s) = ϕ 1 (rs) =
ϕ 1 (0) = 0, in contradiction to the fact that a field has no zero divisors.
    Thus m is prime or m is 0. If m is a prime p, then Z/ pZ is a field, and the
image of ϕ 1 is the required subfield of F. Thus suppose that m = 0. Then ϕ1
is one-one, and F contains a subring with identity isomorphic to Z. Define a
function 81 : Q → F by saying that if k and l are integers with l 6= 0, then
81 (kl −1 ) = ϕ1 (k)ϕ1 (l)−1 . This is well defined because ϕ1 (l) 6= 0 and because
k1l1−1 = k2l2−1 implies k1l2 = k2l1 and hence ϕ1 (k1 )ϕ1 (l2 ) = ϕ1 (k2 )ϕ1 (l1 ) and
ϕ1 (k1 )ϕ1 (l1 )−1 = ϕ1 (k2 )ϕ1 (l2 )−1 . We readily check that 81 is a homomorphism
with kernel 0. Then F contains the subfield 81 (Q) isomorphic to Q.                §


                      5. Polynomials and Vector Spaces

In this section we complete the digression begun in Section 4. We shall be using
the elementary notions of rings and fields established in Section 4 in order to
                                 5. Polynomials and Vector Spaces                                  149

work with (i) polynomials over any commutative ring with identity and (ii) vector
spaces over arbitrary fields.
   It is an important observation that a good deal of what has been proved so
far in this book concerning polynomials when F is Q or R or C remains valid
when F is any field. Specifically all the results in Section I.3 through Theorem
1.17 on the topic of polynomials in one indeterminate remain valid as long as the
coefficients are from a field. The theory breaks down somewhat when one tries to
extend it by allowing coefficients that are not in a field or by allowing more than
one indeterminate. Because of this circumstance and because we have not yet
announced a universal mapping property for polynomial rings and because we
have not yet addressed the several-variable case, we shall briefly review matters
now while extending the reach of the theory that we have.
   Let R be a nonzero commutative ring with identity, so that 1 6= 0. A polynomial
in one indeterminate is to be an expression P(X) = an X n +· · ·+a2 X 2 +a1 X +a0
in which X is a symbol, not a variable. Nevertheless, the usual kinds of ma-
nipulations with polynomials are to be valid. This description lacks precision
because X has not really been defined adequately. To make a precise definition,
we remove X from the formalism and simply define the polynomial to be the
tuple (a0 , a1 , . . . , an , 0, 0, . . . ) of its coefficients. Thus a polynomial in one
indeterminate with coefficients in R is an infinite sequence of members of R
such that all terms of the sequence are 0 from some point on. The indexing of the
sequence is to begin with 0, and X is to refer to the polynomial (0, 1, 0, 0, . . . ).
We may refer to a polynomial P as P(X) if we want to emphasize that the
indeterminate is called X. Addition and negation of polynomials are defined in
coordinate-by-coordinate fashion by

 (a0 , a1 , . . . , an , 0, 0, . . . ) + (b0 ,b1 , . . . , bn , 0, 0, . . . )
                                               = (a0 + b0 , a1 + b1 , . . . , an + bn , 0, 0, . . . ),
        −(a0 , a1 , . . . , an , 0, 0, . . . ) = (−a0 , −a1 , . . . , −an , 0, 0, . . . ),

and the set R[X] of polynomials is then an abelian group isomorphic to the direct
sum of infinitely many copies of the additive group of R. As in Section I.3, X n
is to be the polynomial whose coefficients are 1 in the n th position, with n ∏ 0,
and 0 in all other positions. Polynomial multiplication is then defined so as to
match multiplication of expressions an X n + · · · + a1 X + a0 if the product is
expanded out, powers of X are added, and the terms containing like powers of X
are collected. Thus the precise definition is that

    (a0 , a1 , . . . , 0, 0, . . . )(b0 , b1 , . . . , 0, 0, . . . ) = (c0 , c1 , . . . , 0, 0, . . . ),
              PN
where c N = k=0         ak b N −k . It is a simple matter to check that this multiplication
makes R[X] into a commutative ring.
150                          IV. Groups and Group Actions

   The polynomial with all entries 0 is denoted by 0 and is called the zero
polynomial. For all polynomials P = (a0 , . . . , an , 0, . . . ) other than 0, the
degree of P, denoted by deg P, is defined to be the largest index n such that
an 6= 0. In this case, an is called the leading coefficient, and an X n is called the
leading term; if an = 1, the polynomial is called monic. The usual convention
with the 0 polynomial is either to leave its degree undefined or to say that the
degree is −∞; let us follow the latter approach in this section in order not to have
to separate certain formulas into cases.
   There is a natural one-one homomorphism of rings ∂ : R → R[X] given by
∂(c) = (c, 0, 0, . . . ) for c in R. This sends the identity of R to the identity of
R[X]. Thus we can identify R with the constant polynomials, i.e., those of
degree ≤ 0.
   If P and Q are nonzero polynomials, then

                       deg(P + Q) ≤ max(deg P, deg Q).

In this formula equality holds if deg P 6= deg Q. In the case of multiplication, let
P and Q have respective leading terms am X m and bn X n . All the coefficients of
P Q are 0 beyond the (m + n)th , and the (m + n)th is am bn . This in principle could
be 0 but is nonzero if R is an integral domain. Thus P and Q nonzero implies
                   Ω
                       ≤ deg P + deg Q        for general R,
          deg(P Q)
                       = deg P + deg Q        if R is an integral domain.

It follows in particular that R[X] is an integral domain if R is.
    Normally we shall write out specific polynomials using the informal notation
with powers of X, using the more precise notation with tuples only when some
ambiguity might otherwise result.
    In the special case that R is a field, Section I.3 introduced the notion of
evaluation of a polynomial P(X) at a point r in the field, thus providing a mapping
P(X) 7→ P(r) from R[X] to R for each r in R. We listed a number of properties
of this mapping, and they can be summarized in our present language by the
statement that the mapping is a homomorphism of rings. Evaluation is a special
case of a more sweeping property of polynomials given in the next proposition
as a universal mapping property of R[X].

    Proposition 4.24. Let R be a nonzero commutative ring with identity, and
let ∂ : R → R[X] be the identification of R with constant polynomials. If T is
any commutative ring with identity, if ϕ : R → T is a homomorphism of rings
sending 1 into 1, and if t is in T , then there exists a unique homomorphism of
rings 8 : R[X] → T carrying identity to identity such that 8(∂(r)) = ϕ(r) for
all r ∈ R and 8(X) = t.
                              5. Polynomials and Vector Spaces                            151

   REMARKS. The mapping 8 is called the substitution homomorphism ex-
tending ϕ and substituting t for X, and the mapping is written P(X) 7→ P ϕ (t).
The notation means that ϕ is to be applied to the coefficients of P and then X is
to be replaced by t. A diagram of this homomorphism as a universal mapping
property appears in Figure 4.7. In the special case that T = R and ϕ is the
identity, 8 reduces to evaluation at t, and the mapping is written P(X) 7→ P(t),
just as in Section I.3.
                                                ϕ
                                       R    −−−→ T
                                       
                                       
                                      ∂y            8


                             R[X]
FIGURE 4.7. Substitution homomorphism for polynomials in one indeterminate.

    PROOF. Define 8(a0 , a1 , . . . , an , 0, . . . ) = ϕ(a0 ) + ϕ(a1 )t + · · · + ϕ(an )t n .
It is immediate that 8 is a homomorphism of rings sending the identity ∂(1) =
(1, 0, 0, . . . ) of R[X] to the identity ϕ(1) of T . If r is in R, then 8(∂(r)) =
8(r, 0, 0, . . . ) = ϕ(r). Also, 8(X) = 8(0, 1, 0, 0, . . . ) = t. This proves
existence. Uniqueness follows since ∂(R) and X generate R[X] and since a
homomorphism defined on R[X] is therefore determined by its values on ∂(R)
and X.                                                                                    §

   The formulation of the proposition with the general ϕ : R → T , rather than just
the identity mapping on R, allows several kinds of applications besides the routine
evaluation mapping. An example of one kind occurs when R = C, T = C[X],
and ϕ : C → C[X] is the composition of complex conjugation on C followed
by the identification of complex numbers with constant polynomials in C[X]; the
proposition then says that complex conjugation of the coefficients of a member
of C[X] is a ring homomorphism. This observation simplifies the solution of
Problem 7 in Chapter I. Similarly one can set up matters so that the proposition
shows the passage from Z[X] to (Z/mZ)[X] by reduction of coefficients modulo
m to be a ring homomorphism.
   Still a third kind of application is to take T in the proposition to be a ring with the
same kind of universal mapping property that R[X] has, and the consequence is an
abstract characterization of R[X]. We carry out the details below as Proposition
4.25. This result will be applied later in this section to the several-indeterminate
case to show that introducing several indeterminates at once yields the same ring,
up to canonical isomorphism, as introducing them one at a time.

   Proposition 4.25. Let R and S be nonzero commutative rings with identity,
let X 0 be an element of S, and suppose that ∂0 : R → S is a one-one ring
152                         IV. Groups and Group Actions

homomorphism of R into S carrying 1 to 1. Suppose further that (S, ∂0 , X 0 )
has the following property: whenever T is a commutative ring with identity,
ϕ : R → T is a homomorphism of rings sending 1 into 1, and t is in T , then there
exists a unique homomorphism 80 : S → T carrying identity to identity such
that 80 (∂0 (r)) = ϕ(r) for all r ∈ R and 80 (X 0 ) = t. Then there exists a unique
homomorphism of rings 9 : R[X] → S such that 9 ◦ ∂ = ∂0 and 9(X) = X 0 ,
and 9 is an isomorphism.
   REMARK. A somewhat weaker conclusion than in the proposition is that any
triple (S, ∂0 , X 0 ) having the same universal mapping property as (R[X], ∂, X) is
isomorphic to (S, ∂0 , X 0 ), the isomorphism being unique.
   PROOF. In the universal mapping property for S, take T = R[X], ϕ = ∂, and
t = X. The hypothesis gives us a ring homomorphism 80 : S → R[X] with
80 (1) = 1, 80 ◦ ∂0 = ∂, and 80 (X 0 ) = X. Next apply Proposition 4.24 with
T = S, ϕ = ∂0 , and t = X 0 . We obtain a ring homomorphism 8 : R[X] → S
with 8(1) = 1, 8◦ ∂ = ∂0 , and 8(X) = X 0 . Then 80 ◦ 8 is a ring homomorphism
                                                                  Ø
from R[X] to itself carrying 1 to 1, fixing X, and having 80 ◦ 8Ø∂(R) = ∂. From
the uniqueness in Proposition 4.24 when T = R[X], ϕ = ∂, and t = X, we see
that 80 ◦ 8 is the identity on R[X]. Reversing the roles of 8 and 80 and applying
the uniqueness in the universal mapping property for S, we see that 8 ◦ 80 is the
identity on S. Therefore 8 may be taken as the isomorphism 9 in the statement
of the proposition. This proves existence for 9, and uniqueness follows since
∂(R) and X together generate R[X] and since 9 is a homomorphism.               §

   If P is a polynomial over R in one indeterminate and r is in R, then r is a
root of P if P(r) = 0. We know as a consequence of Corollary 1.14 that for
any prime p, any polynomial in F p [X] of degree n ∏ 1 has at most n roots. This
result does not extend to Z/mZ for all positive integers m: when m = 8, the
polynomial X 2 − 1 has 4 roots, namely 1, 3, 5, 7. This result about F p [X] has
the following consequence.

   Proposition 4.26. If F is a field, then any finite subgroup of the multiplicative
group F× is cyclic.
   PROOF. Let C be a subgroup of F× of finite order n. Lagrange’s Theorem
(Corollary 4.8) shows that the order of each element of C divides n. With h
defined as the maximum order of an element of C, it is enough to show that
h = n. Let a be an element of order h. The polynomial X h − 1 has at most h
roots by Corollary 1.14, and a is one of them, by definition of “order.” If h < n,
then it follows that some member b of C is not a root of X h − 1. The order h 0
of b is then a divisor of n but cannot be a divisor of h since otherwise we would
                0   0        0
have bh = (bh )h/ h = 1h/ h = 1. Consequently there exists a prime p such that
                          5. Polynomials and Vector Spaces                    153

some power pr of p divides h 0 but not h. Let s < r be the exact power of p
                                                                        s
dividing h, and write h = mps , so that GCD(m, pr ) = 1 and a 0 = a p has order
                0  r            0     q            r
m. Put q = h / p , so that b = b has order p . The proof will be completed
by showing that c = a 0 b0 has order mpr = hpr−s > h, in contradiction to the
maximality of h.
                                                           r           r      r
    Let t be the order of c. On the one hand, from cmp = (a 0 )mp (b0 )mp =
    r+s    r        r+s      0       r+s   0
a hp bmp q = a hp bmh = (a h ) p (bh )m = 1, we see that t divides mpr . On
the other hand, 1 = ct says that (a 0 )t = (b0 )−t . Raising both sides to the pr
                            r            r
power gives 1 = ((b0 ) p )−t = (a 0 )t p , and hence m divides t pr ; by Corollary
1.3, m divides t. Raising both sides of (a 0 )t = (b0 )−t to the m th power gives
1 = ((a 0 )m )t = (b0 )−tm , and hence pr divides tm; by Corollary 1.3, pr divides
t. Applying Corollary 1.4, we conclude that mpr divides t. Therefore t = mpr ,
and the proof is complete.                                                      §

  Corollary 4.27. The multiplicative group of a finite field is cyclic.
  PROOF. This is a special case of Proposition 4.26.                           §

   A finite field F can have a nonzero polynomial that is 0 at every element of F.
Indeed, every element of F p is a root of X p − X, as a consequence of Fermat’s
Little Theorem. It is for this reason that it is unwise to confuse a polynomial in
an indeterminate with a “polynomial function.”
   Let us make the notion of a polynomial function of one variable rigorous. If
P(X) is a polynomial with coefficients in the commutative ring R with identity,
then Proposition 4.24 gives us an evaluation homomorphism P 7→ P(r) for each
r in R. The function r 7→ P(r) from R into R is the polynomial function
associated to the polynomial P. This function is a member of the     ° commutative
                                                                                 ¢
ring of all R-valued functions on R, and the mapping P 7→ r 7→ P(r) is
a homomorphism of rings. What we know from Corollary 1.14 is that this
homomorphism is one-one if R is an infinite field.   Q A negative result is that
if R is a finite commutative ring with identity, then r∈R (X − r) is a polynomial
that maps to the 0 function, and hence the homomorphism is not one-one. A more
general positive result than the one above for infinite fields is the following.

  Proposition 4.28.
    (a) If R is a nonzero commutative ring with identity and P(X) is a member of
R[X] with a root r, then P(X) = (X − r)Q(X) for some Q(X) in R[X].
    (b) If R is an integral domain, then a nonzero member of R[X] of degree n
has at most n roots.
    (c) If R is an infinite integral domain, then the ring homomorphism of R[X]
to the ring of polynomial functions from R to R, given by evaluation, is one-one.
154                             IV. Groups and Group Actions

   PROOF. For (a), we proceed by induction on the degree of P, the base case of
the induction being degree ≤ 0. If the conclusion has been proved for degree < n
with n ∏ 1, let the leading term of P be an X n . Then P(X) = an (X −r)n + A(X)
with deg A < n. Evaluation at r gives, by virtue of Proposition 4.24, 0 = 0+ A(r).
By the inductive hypothesis, A(X) = (X −r)B(X). Then P(X) = (X −r)Q(X)
with Q(X) = an (X − r)n−1 + B(X), and the induction is complete.
   For (b), let P(X) have degree n with at least n + 1 distinct roots r1 , . . . , rn+1 .
Part (a) shows that P(X) = (X − r1 )P1 (X) with deg P1 = n − 1. Also, 0 =
P(r2 ) = (r2 − r1 )P1 (r2 ). Since r2 − r1 6= 0 and since R has no zero divisors,
P1 (r2 ) = 0. Part (a) then shows that P1 (X) = (X − r2 )P2 (X), and substitution
gives P(X) = (X − r1 )(X − r2 )P2 (X). Continuing in this way, we obtain
P(X) = (X − r1 ) · · · (X − rn )Pn (X) with deg Pn = 0. Since P 6= 0, Pn 6= 0.
So Pn is a nonzero constant polynomial Pn (X) = c 6= 0. Evaluating at rn+1 , we
obtain 0 = (rn+1 − r1 ) · · · (rn+1 − rn )c with each factor nonzero, in contradiction
to the fact that R is an integral domain.
   For (c), a polynomial in the kernel of the ring homomorphism has every member
of R as a root. If R is infinite, (b) shows that such a polynomial is necessarily
the zero polynomial. Thus the kernel is 0, and the ring homomorphism has to be
one-one.                                                                              §

  Let us turn our attention to polynomials in several indeterminates. Fix the
nonzero commutative ring R with identity, and let n be a positive integer. Infor-
mally a polynomial over R in n indeterminates is to be a finite sum
                            X                      j
                                   r j1 ,..., jn X 11 · · · X njn
                              j1 ∏0,..., jn ∏0

with each r j1 ,..., jn in R. To make matters precise, we work just with the system of
coefficients, just as in the case of one indeterminate.
    Let J be the set of integers ∏ 0, and let J n be the set of n-tuples of elements of
J . A member of J n may be written as j = ( j1 , . . . , jn ). Addition of members of
J n is defined coordinate by coordinate. Thus j + j 0 = ( j1 + j10 , . . . , jn + jn0 ) if
 j = ( j1 , . . . , jn ) and j 0 = ( j10 , . . . , jn0 ). A polynomial in n indeterminates with
coefficients in R is a function f : J n → R such that f ( j) 6= 0 for only finitely
many j ∈ J n . Temporarily let us write S for the set of all such polynomials for a
particular n. If f and g are two such polynomials, their sum h and product k are
the polynomials defined by
                                h( j) = f ( j) + g( j),
                                          P
                                k(i) =          f ( j)g( j 0 ).
                                             j+ j 0 =i

Under these definitions, S is a commutative ring.
                            5. Polynomials and Vector Spaces                      155

   Define a mapping ∂ : R → S by
                               Ω
                                 r              if j = (0, . . . , 0),
                    ∂(r)( j) =
                                 0              otherwise.

Then ∂ is a one-one homomorphism of rings, ∂(0) is the zero element of S and is
called simply 0, and ∂(1) is a multiplicative identity for S. The polynomials in
the image of ∂ are called the constant polynomials.
    For 1 ≤ k ≤ n, let ek be the member of J n that is 1 in the k th place and is 0
elsewhere. Define X k to be the polynomial that assigns 1 to ek and assigns 0 to all
other members of J n . We say that X k is an indeterminate. If j = ( j1 , . . . , jn )
is in J n , define X j to be the product
                                            j
                                 X j = X 11 · · · X njn .

If r is in R, we allow ourselves to abbreviate ∂(r)X j as r X j , and any such polyno-
mial is called a monomial. The monomial r X j is the polynomial that assigns r to
j and assigns 0 to all other members of J n . Then it follows immediately from the
definitions that each polynomial has a unique expansion as a finite   Psum of nonzero
monomials. Thus the most general member of S is of the form j∈J n r j X j with
only finitely many nonzero terms. This is called the monomial expansion of the
given polynomial.                                                    P
    We may now write R[X 1 , . . . , X n ] for S. A polynomial j∈J n r j X j may
be conveniently abbreviated as P or as P(X) or as P(X 1 , . . . , X n ) when its
monomial expansion is either understood or irrelevant.
    The degree of the 0 polynomial is defined for this section to be −∞, and the
degree of any monomial r X j with r 6= 0 is defined to be the integer

                   | j| = j1 + · · · + jn         if j = ( j1 , . . . , jn ).

Finally the degree of any nonzero polynomial P, denoted by deg P, is defined to
be the maximum of the degrees of the terms in its monomial expansion. If all the
nonzero monomials in the monomial expansion of a polynomial P have the same
degree d, then P is said to be homogeneous of degree d. Under these definitions
the 0 polynomial has degree −∞ but is homogeneous of every degree. If P and
Q are homogeneous polynomials of degrees d and d 0 , then P Q is homogeneous
of degree dd 0 (and possibly equal to the 0 polynomial).
   In any event, by grouping terms in the monomial expansion of a polynomial
according to their degree, we see that every polynomial is uniquely the sum
of nonzero homogeneous polynomials of distinct degrees. Let us call this the
homogeneous-polynomial expansion of the given polynomial. Let us expand
two such nonzero polynomials P and Q in this fashion, writing P = Pd1 +· · ·+Pdk
156                               IV. Groups and Group Actions

and Q = Q d10 + · · · + Q dl0 with d1 < · · · < dk and d10 < · · · < dl0 . Then we see
directly that
                       deg(P + Q) ≤ max(deg P, deg Q),
                               deg(P Q) ≤ deg P + deg Q.
In the formula for deg(P + Q), the term that is potentially of largest degree is
Pdk + Q dl0 , and it is of degree max(deg P, deg Q) if deg P 6= deg Q. In the
formula for deg(P Q), the term that is potentially of largest degree is Pdk Q dl0 . It
is homogeneous of degree dk + dl0 , but it could be 0. Some proof is required that
it is not 0 if R is an integral domain, as follows.

  Proposition 4.29. If R is an integral domain, then R[X 1 , . . . , X n ] is an integral
domain.
     PROOF. Let P and Q be nonzero homogeneous polynomials with deg P = d
and deg Q = d 0 . We are to prove that P Q 6= 0. We introduce an ordering on the
set of all members j of J n , saying j = ( j1 , . . . , jn ) > j 0 = ( j10 , . . . , jn0 ) if there
                              0
is some k such that P ji = ji for      i < k and jk > jk0 . In the monomial expansion
                                  j
of P as P(X) = | j|=d a j X , let i be the largest n-tuple j in the ordering such
                                            P               0
that a j 6= 0. Similarly with Q(X) = | j 0 |=d 0 b j 0 X j , let i 0 be the largest n-tuple
 j 0 in the ordering such that b j 0 6= 0. Then
                                              0
                                                    X                         0
                   P(X)Q(X) = ai bi 0 X i+i +                 a j b j 0 X j+ j ,
                                                        j, j 0 with
                                                     ( j, j 0 )6=(i,i 0 )
                              P
and all terms in the sum j, j 0 on the right side have j + j 0 < i + i 0 . Thus
             0
ai bi 0 X i+i is the only term in the monomial expansion of P(X)Q(X) involving
                       0
the monomial X i+i . Since R is an integral domain and ai and bi 0 are nonzero,
ai bi 0 is nonzero. Thus P(X)Q(X) is nonzero.                                §

    Proposition 4.30. Let R be a nonzero commutative ring with identity, let
R[X 1 , . . . , X n ] be the ring of polynomials in n indeterminates, and define
∂ : R → R[X 1 , . . . , X n ] to be the identification of R with constant polynomials.
If T is any commutative ring with identity, if ϕ : R → T is a homomorphism
of rings sending 1 into 1, and if t1 , . . . , tn are in T , then there exists a unique
homomorphism 8 : R[X 1 , . . . , X n ] → T carrying identity to identity such that
8(∂(r)) = ϕ(r) for all r ∈ R and 8(X j ) = t j for 1 ≤ j ≤ n.
   REMARKS. The mapping 8 is called the substitution homomorphism ex-
tending ϕ and substituting t j for X j for 1 ≤ j ≤ n, and the mapping is written
P(X 1 , . . . , X n ) 7→ P ϕ (t1 , . . . , tn ). The notation means that ϕ is to be applied
to each coefficient of P and then X 1 , . . . , X n are to be replaced by t1 , . . . , tn .
                                5. Polynomials and Vector Spaces                               157

A diagram of this homomorphism as a universal mapping property appears
in Figure 4.8. In the special case that T = R × · · · × R (cf. Example 3 of
homomorphisms in Section 4) and ϕ is the identity, 8 reduces to evaluation at
(t1 , . . . , tn ), and the mapping is written P(X 1 , . . . , X n ) 7→ P(t1 , . . . , tn ).
                                                         ϕ
                                          R           −−−→ T
                                          
                                          
                                         ∂y                  8


                               R[X 1 , . . . , X n ]
 FIGURE 4.8. Substitution homomorphism for polynomials in n indeterminates.
                                    P                                   j        j
   PROOF. If P(X 1 , . . . , X n ) = j1 ∏0,..., jn ∏0 a j1 ,..., jn X 11 · · · X nn is the monomial
expansion of a member P of R[X 1 , . . . , X n ], then 8(P) is defined to be the cor-
                         P                             j          j
responding finite sum j1 ∏0,..., jn ∏0 a j1 ,..., jn t1 1 · · · tn n . Existence readily follows,
and uniqueness follows since ∂(R) and X 1 , . . . , X n generate R[X 1 , . . . , X n ] and
since 8 is a homomorphism.                                                                       §

  Corollary 4.31. If R is a nonzero commutative ring with identity, then
R[X 1 , . . . , X n−1 ][X n ] is isomorphic as a ring to R[X 1 , . . . , X n ].
   REMARK. The proof will show that the isomorphism is the expected one.
    PROOF. In the notation with n-tuples and J n , any (n − 1)-tuple may be iden-
tified with an n-tuple by adjoining 0 as its n th coordinate, and in this way, every
monomial in R[X 1 , . . . , X n−1 ] can be regarded as a monomial in R[X 1 , . . . , X n ].
The extension of this mapping to sums gives us a one-one homomorphism of rings
∂0 : R[X 1 , . . . , X n−1 ] → R[X 1 , . . . , X n ]. We are going to use Proposition 4.25
to prove the isomorphism of rings R[X 1 , . . . , X n−1 ][X n ] ∼    = R[X 1 , . . . , X n ]. In
the notation of that proposition, the role of R is played by R[X 1 , . . . , X n−1 ],
we take S = R[X 1 , . . . , X n ], and we have constructed ∂0 . We are to show that
(S, ∂0 , X n ) satisfies a certain universal mapping property. Thus suppose that T is a
commutative ring with identity, that t is in T , and that ϕ 0 : R[X 1 , . . . , X n−1 ] → T
is a homomorphism of rings carrying identity to identity.
    We shall apply Proposition 4.30 in order to obtain the desired homomorphism
80 : S → T . Let ∂n−1 : R → R[X 1 , . . . , X n−1 ] be the identification of R
with constant polynomials in R[X 1 , . . . , X n−1 ], and let ∂n = ∂0 ◦ ∂n−1 be the
identification of R with constant polynomials in S. Define ϕ : R → T by
ϕ = ϕ 0 ◦∂n−1 , and take tn = t and t j = ϕ 0 (X j ) for 1 ≤ j ≤ n−1. Then Proposition
4.30 produces a homomorphism of rings 80 : S → T with 80 (∂n (r)) = ϕ(r) for
r ∈ R, 80 (∂0 (X j )) = ϕ 0 (X j ) for 1 ≤ j ≤ n − 1, and 80 (X n ) = tn . The equations
                  80 (∂0 (∂n−1 (r))) = 80 (∂n (r)) = ϕ(r) = ϕ 0 (∂n−1 (r))
and                     80 (∂0 (X j )) = ϕ 0 (X j )
158                             IV. Groups and Group Actions

show that 80 ◦ ∂0 = ϕ 0 on R[X 1 , . . . , X n ]. Also, 80 (X n ) = tn = t. Thus the
mapping 80 sought by Proposition 4.25 exists. It is unique since R[X 1 , . . . , X n−1 ]
and X n together generate S. The conclusion from Proposition 4.25 is that S is
isomorphic to R[X 1 , . . . , X n−1 ][X n ] via the expected isomorphism of rings. §

   We conclude the discussion of polynomials in several variables by making the
notion of a polynomial function of several variables rigorous. If P(X 1 , . . . , X n )
is a polynomial in n indeterminates with coefficients in the commutative ring
R with identity, then Proposition 4.30 gives us an evaluation homomorphism
P 7→ P(r1 , . . . , rn ) for each n-tuple (r1 , . . . , rn ) of members of R. The function
(r1 , . . . , rn ) 7→ P(r1 , . . . , rn ) from R × · · · × R into R is the polynomial
function associated to the polynomial P. This function is a member of the
commutative
         °          ring of all R-valued functions
                                              ¢        on R × · · · × R, and the mapping
P 7→ (r1 , . . . , rn ) 7→ P(r1 , . . . , rn ) is a homomorphism of rings.

   Corollary 4.32. If R is an infinite integral domain, then the ring homomor-
phism of R[X 1 , . . . , X n ] to polynomial functions from R × · · · × R to R, given
by evaluation, is one-one.
   REMARK. This result extends Proposition 4.28 to several indeterminates.
  PROOF. We proceed by induction on n, the case n = 1 being handled by
Proposition 4.28. Assume the result for n − 1 indeterminates. If P 6= 0 is in
R[X 1 , . . . , X n ], Corollary 4.31 allows us to write

                                               k
                                               X
                     P(X 1 , . . . , X n ) =         Pi (X 1 , . . . , X n−1 )X ni
                                               i=1

for some k, with each Pi in R[X 1 , . . . , X n−1 ] and with Pk (X 1 , . . . , X n−1 ) 6= 0.
By the inductive hypothesis, Pk (r1 , . . . , rn−1 ) is nonzero for some elements
                                          Pk
r1 , . . . , rn−1 of R. So the polynomial i=0    Pi (r1 , . . . , rn−1 )X ni in R[X n ] is not
the 0 polynomial, and Proposition 4.28 shows that it is not 0 when evaluated at
some rn . Then P(r1 , . . . , rn ) 6= 0.                                                    §

   It is possible also to introduce polynomial rings in infinitely many variables.
These will play roles only as counterexamples in this book, and thus we shall not
stop to treat them in detail.

   We complete this section with some remarks about vector spaces. The defini-
tion of a vector space over a general field F remains the same as in Section II.1,
where F is assumed to be Q or R or C. We shall make great use of the fact that all
the results in Chapter II concerning vector spaces remain valid when Q or R or
                                    6. Group Actions and Examples                                    159

C is replaced by a general field F. The proofs need no adjustments, and it is not
necessary to write out the details. For the moment we make only the following
application of vector spaces over general fields, but the extended theory of vector
spaces will play an important role in most of the remaining chapters of this book.

  Proposition 4.33. If F is a finite field, then the number of elements in F is a
power of a prime.
   REMARK. We return to this matter in Chapter IX, showing at that time that for
each prime power pn > 1, there is one and only one field with pn elements, up
to isomorphism.
   PROOF. The characteristic of F cannot be 0 since F is finite, and hence it is some
prime p. Denote the prime field of F by F p . By restricting the multiplication
so that it is defined only on F p × F, we make F into a vector space over F p ,
necessarily finite-dimensional. Proposition 2.18 shows that F is isomorphic as a
vector space to the space (F p )n of n-dimensional column vectors for some n, and
hence F must have pn elements.                                                      §


                              6. Group Actions and Examples

Let X be a nonempty set, let F(X) be the group of invertible functions from X
onto itself, the group operation being composition, and let G be a group. A group
action of G on X is a homomorphism of G into F(X). When X = {1, . . . , n},
the group F(X) is just the symmetric group Sn . Thus Examples 5–9 of groups
in Section 1 are all in fact subgroups of various groups F(X) and are therefore
examples of group actions. Thus every group of permutations of {1, . . . , n}, every
dihedral group acting on R2 , and every general linear group or subgroup acting
on a finite-dimensional vector space over Q or R or C or an arbitrary field F
provides an example. So do the orthogonal and unitary groups acting on Rn and
Cn , as well as the automorphism group of any number field.
   We saw an indication in Section 1 that many early examples of groups arose in
this way. One source of examples that is of some importance and was not listed in
Section 1 occurs in the geometry of R2 . The translations in R2 , together with the
rotations about arbitrary points of R2 and the reflections about arbitrary lines in
R2 , form a group G of rigid motions of the plane.11 This group G is a subgroup
of F(R2 ), and thus G acts on R2 . More generally, whenever a nonempty set X
has a notion of distance, the set of isometries of X, i.e., the distance-preserving
members of F(X), forms a subgroup of F(X), and thus the group of isometries
of X acts on X.
   11 One   can show that G is the full group of rigid motions of R2 , but this fact will not concern us.
160                                   IV. Groups and Group Actions

   At any rate a group action τ of G on X, being a homomorphism of G into
F(X), is of the form g 7→ τg , where τg is in F(X) and τg1 g2 = τg1 τg2 . There is
an equivalent way of formulating matters that does not so obviously involve the
notion of a homomorphism. Namely, we write τg (x) = gx. In this notation the
group action becomes a function G × X → X with (g, x) 7→ gx such that
       (i) (g1 g2 )x = g1 (g2 x) for all g1 and g2 in G and for all x in X (from the fact
           that τg1 g2 = τg1 τg2 ),
      (ii) 1x = x for all x in X (from the fact that τ1 = 1).
Conversely if G × X → X satisfies (i) and (ii), then the formulas x = 1x =
(gg −1 )x = g(g −1 x) and x = 1x = (g −1 g)x = g −1 (gx) show that the function
x 7→ gx from X to itself is invertible with inverse x 7→ g −1 x. Consequently
the definition τg (x) = gx makes g 7→ τg a function from G into F(X), and (i)
shows that τ is a homomorphism. Thus (i) and (ii) indeed give us an equivalent
formulation of the notion of a group action. Both formulations are useful.
   Quite often the homomorphism G → F(X) of a group action is one-one, and
then G can be regarded as a subgroup of F(X). Here is an important geometric
example in which the homomorphism is not one-one.

   EXAMPLE. Linear fractional transformations. Let X = C ∪ {∞}, a set that
becomes the Riemann sphere in complex analysis. The group G = GL(2, C)
acts on X by the linear fractional transformations
                                      µ               ∂
                                          a       b               az + b
                                                          (z) =          ,
                                          c       d               cz + d

the understanding being that the image of ∞ is ac−1 and the image of −dc−1
is ∞, just as if we were to pass to a limit in each case. Property (ii) of a group
action is clear. To verify (i), we simply calculate that

             µ             ∂ µµ               ∂      ∂    °     ¢
                 a0   b0          a    b               a 0 az+b
                                                            cz+d + b
                                                                     0
                                                  (z) = ° az+b ¢
                 c0   d0          c    d               c0 cz+d + d 0
                                                            (a 0 a + b0 c)z + (a 0 b + b0 d)
                                                          =
                                                            (c0 a + d 0 c)z + (c0 b + d 0 d)
                                                            µµ 0         ∂µ          ∂∂
                                                                 a b0        a b
                                                          =                             (z),
                                                                  c0 d 0     c d

and indeed we have a group action. Let SL(2, R) be the subgroup of real matrices
in GL(2, C) of determinant 1, and let Y be the subset of X where Im z > 0, not
                                  6. Group Actions and Examples                               161

including ∞. The members of SL(2, R) carry the subset Y into itself, as we see
from the computation

                     az + b      (az + b)(cz̄ + d)      adz + bcz̄
                Im          = Im                   = Im
                     cz + d          |cz + d|2           |cz + d|2
                                   (ad − bc) Im z      Im z
                              =                   =           .
                                     |cz + d|2      |cz + d|2

Since the effect of a matrix g −1 is to invert the effect of g, and since both g and
g −1 carry Y to itself, we conclude that SL(2, R) acts on Y = {z ∈ C | Im z > 0}
by linear fractional transformations. In similar fashion one can verify that the
subgroup12 of GL(2, C)
                 Ωµ         ∂Ø                                     æ
                      α β ØØ                         2       2
                                α ∈ C, β ∈ C, |α| − |β| = 1
                      β̄ ᾱ Ø

acts on {z ∈ C | |z| < 1} by linear fractional transformations.

   One group action can yield many others. For example, from an action of G on
X, we can construct an action on the space of all complex-valued functions on
X. The definition is (g f )(x) = f (g −1 x), the use of the inverse being necessary
in order to verify property (i) of a group action:

         ((g1 g2 ) f )(x) = f ((g1 g2 )−1 x) = f ((g2−1 g1−1 )x)
                          = f (g2−1 (g1−1 x)) = (g2 f )(g1−1 x) = (g1 (g2 f ))(x).

There is nothing special about the complex numbers as range for the functions
here. We can allow any set as range, and we can even allow G to act on the range,
as well as on the domain.13 If G acts on X and Y , then the set of functions from
X to Y inherits a group action under the definition

                                   (g f )(x) = g( f (g −1 x)),

as is easily checked. In other words, we are to use g −1 where the domain enters
the formula and we are to use g where the range enters the formula.
   If V is a vector space over a field F, a representation of G on V is a group
action of G on V by linear functions. Specifically for each g ∈ G, τg is to be a
    12 This subgroup is commonly called SU(1, 1) for reasons that are not relevant to the current

discussion.
    13 When C was used as range in the previous display, the group action of G on C was understood

to be trivial in the sense that gz = z for every g in G and z in C.
162                                       IV. Groups and Group Actions

member of the group of linear maps from V into itself. Usually one writes τ (g)
instead of τg in representation theory, and thus the condition is that τ (g) is to be
linear for each g ∈ G and we are to have τ (1) = 1 and τ (g1 g2 ) = τ (g1 )τ (g2 ) for
all g1 and g2 . There are interesting examples both when V is finite-dimensional
and when V is infinite-dimensional.14

   EXAMPLES OF REPRESENTATIONS.
   (1) If m ∏ 1, then the additive group Z/mZ acts linearly on R2 by
                         µ                                     ∂
                             cos 2πk
                                  m          − sin 2πk
                                                    m
          τ (k) =                                                  ,       k ∈ {0, 1, 2, . . . , m − 1}.
                             sin 2πk
                                  m           cos 2πk
                                                    m

Each τ (k) is a rotation matrix about the origin through an angle that is a multiple of
2π/m. These transformations of R2 form a subgroup of the group of symmetries
of a regular k-gon centered at the origin in R2 .
   (2) The dihedral group D3 acts linearly on R2 with
          ≥        ¥                 ≥          ¥                      µ          p    ∂                        µ            p    ∂
              10                         1 0                               − 12   2
                                                                                   3
                                                                                                                    − 12 −   2
                                                                                                                              3
τ (1) =       01
                       , τ (2 3) =       0 −1
                                                    , τ (1 2) =             p
                                                                             3    1
                                                                                           , τ (1 3) =                  p
                                                                                                                         3   1
                                                                                                                                      ,
                                                                            2     2                                 −   2    2

                                         µ            p    ∂                           µ              p     ∂
                                             − 12 −   2
                                                       3
                                                                                            − 12      2
                                                                                                       3
                       τ (1 2 3) =            p
                                               3
                                                               , τ (1 3 2) =                    p               .
                                              2     − 12                                    −   2
                                                                                                 3
                                                                                                     − 12

Each of these matrices carries into itself the equilateral triangle with center at the
origin and one vertex at (1, 0). To obtain these matrices, we number the vertices
#1, #2, #3 counterclockwise with the vertex at (1, 0) as #1.
   (3) The symmetric group Sn acts linearly on Rn by permuting the indices
of standard basis vectors. For example, with n = 3, we have (1 3)e1 = e3 ,
(1 3)e2 = e2 , etc. The matrices may be computed by the techniques of Section
II.3. With n = 3, we obtain, for example,
                                  µ0 0 1∂                                                       µ0 0 1∂
                       (1 3) 7→      010                   and         (1 2 3) 7→                    100        .
                                     100                                                             010


   (4) If G acts on a set X, then the corresponding action (g f )(x) = f (g −1 x) on
complex-valued functions is a representation on the vector space of all complex-
valued functions on X. This vector space is infinite-dimensional if X is an infinite
set. The linearity of the action on functions follows from the definitions of addition
    14 In some settings a continuity assumption may be added to the definition of a representation, or

the field F may be restricted in some way. We impose no such assumption here at this time.
                                  6. Group Actions and Examples                                    163

and scalar multiplication of functions. In fact, let functions f 1 and f 2 be given,
and let c be a scalar. Then
            (g( f 1 + f 2 ))(x) = ( f 1 + f 2 )(g −1 x) = f 1 (g −1 x) + f 2 (g −1 x)
                                = (g f 1 )(x) + (g f 2 )(x) = (g f 1 + g f 2 )(x)
and

    (g(c f 1 ))(x) = (c f 1 )(g −1 x) = c( f 1 (g −1 x)) = c((g f 1 )(x)) = (c(g f 1 ))(x).

    One more important class of group actions consists of those that are closely
related to the structure of the group itself. Two simple ones are the action of G
on itself by left translations (g1 , g2 ) 7→ g1 g2 and the action of G on itself by
right translations (g1 , g2 ) 7→ g2 g1−1 . More useful is the action of G on a quotient
space G/H , where H is a subgroup. This action is given by (g1 , g2 H ) 7→ g1 g2 H .
There are still others, and some of them are particularly handy in analyzing finite
groups. We give some applications in the present section and the next, and we
postpone others to Section 10. Before describing some of these actions in detail,
let us make some general definitions and establish two easy results.
    Let G × X → X be a group action. If p is in X, then G p = {g ∈ G | gp = p}
is a subgroup of G called the isotropy subgroup at p or stabilizer  T       of G at p.
This is not always a normal subgroup; however, the subgroup p∈G G p that fixes
all points of X is the kernel of the homomorphism G → F(X) defining the group
action, and such a kernel has to be normal.
    Let p and q be in X. We say that p is equivalent to q for the purposes of
this paragraph if p = gq for some g ∈ G. The result is an equivalence relation:
it is reflexive since p = 1 p, it is symmetric since p = gq implies g −1 p = q,
and it is transitive since p = gq and q = g 0r together imply p = (gg 0 )r. The
equivalence classes are called orbits of the group action. The orbit of a point p
in X is Gp = {gp | g ∈ G}. If Y = Gp is an orbit,15 or more generally if Y is
any subset of X carried to itself by every element of G, then G × Y → Y is a
group action. In fact, each function y 7→ gy is invertible on Y with y 7→ g −1 y
as the inverse function, and properties (i) and (ii) of a group action follow from
the same properties for X.
    A group action G × X → X is said to be transitive if there is just one orbit,
hence if X = Gp for each p in X. It is simply transitive if it is transitive and if
for each p and q in X, there is just one element g of G with gp = q.
    15 Although the notation G for the isotropy subgroup and Gp for the orbit are quite distinct in
                                 p
print, it is easy to confuse the two in handwritten mathematics. Some readers may therefore prefer
a different notation for one of them. The notation Z G ( p) for the isotropy subgroup is one that is in
common use; its use is consistent with the notation for the “centralizer” of an element in a group,
which will be defined shortly. Another possibility, used by many mathematicians, is to write G · p
for the orbit.
164                         IV. Groups and Group Actions

   Proposition 4.34. Let G × X → X be a group action, let p be in X, and let
H be the isotropy subgroup at p. Then the map G → Gp given by g 7→ gp
descends to a well-defined map G/H → Gp that is one-one from G/H onto the
orbit Gp and respects the group actions.
   REMARK. In other words, a group action of G on a single orbit is always
isomorphic as a group action to the action of G on some quotient space G/H .
   PROOF. Let ϕ : G → Gp be defined by ϕ(g) = gp. For h in H = G p ,
ϕ(gh) = (gh) p = g(hp) = gp = ϕ(g) shows that ϕ descends to a well-defined
function ϕ : G/H → Gp, and ϕ is certainly onto Gp. If ϕ(g1 H ) = ϕ(g2 H ),
then g1 p = ϕ(g1 p) = ϕ(g2 p) = g2 p, and hence g2−1 g1 p = p, g2−1 g1 is in H ,
g1 is in g2 H , and g1 H = g2 H . Thus ϕ is one-one.
   Respecting the group action means that ϕ(gg 0 H ) = gϕ(g 0 H ), and this identity
holds since gϕ(g 0 H ) = gϕ(g 0 ) = g(g 0 p) = (gg 0 ) p = ϕ(gg 0 ) = ϕ(gg 0 H ). §

   A simple consequence is the following important counting formula in the
case of a group action by a finite group.

   Corollary 4.35. Let G be a finite group, let G × X → X be a group action,
let p be in X, and G p be the isotropy group at p, and let Gp be the orbit of p.
Then |G| = |Gp| |G p |.
   PROOF. Proposition 4.34 shows that the action of G on some G/G p is the most
general group action on a single orbit, G p being the isotropy subgroup. Thus the
corollary follows from Lagrange’s Theorem (Theorem 4.7) with H = G p and
G/H = Gp.                                                                      §

   We turn to applications of group actions to the structure of groups. If H is a
subgroup of a group G, the index of H in G is the number of elements in G/H ,
finite or infinite. The first application notes a situation in which a subgroup of a
finite group is automatically normal.

   Proposition 4.36. Let G be a finite group, and let p be the smallest prime
dividing the order of G. If H is a subgroup of G of index p, then H is normal.
   REMARKS. The most important case is p = 2: any subgroup of index 2 is
automatically normal, and this conclusion is valid even if G is infinite, as was
already pointed out in Example 3 of Section 2. If G is finite and if 2 divides the
order of G, there need not, however, be any subgroup of index 2; for example,
the alternating group A4 has order 12, and Problem 11 at the end of the chapter
shows that A4 has no subgroup of order 6.
                             6. Group Actions and Examples                         165

   PROOF. Let X = G/H , and restrict the group action G × X → X to an action
H × X → X. The subset {1H } is a single orbit under H , and the remaining p − 1
members of G/H form a union of orbits. Corollary 4.35 shows that the number
of elements in an orbit has to be a divisor of |H |, and the smallest divisor of |H |
other than 1 is ∏ p since the smallest divisor of |G| other than 1 equals p and
since |H | divides |G|. Hence any orbit of H containing more than one element
has at least p elements. Since only p − 1 elements are left under consideration,
each orbit under H contains only one element. Therefore hg H = g H for all h
in H and g in G. Then g −1 hg is in H , and we conclude that H is normal.         §

   If G is a group, the center Z G of G is the set of all elements x such that gx = xg
for all g in G. The center of G is a subgroup (since gx = xg and gy = yg together
imply g(x y) = xgy = (x y)g and xg −1 = g −1 (gx)g −1 = g −1 (xg)g −1 = g −1 x),
and every subgroup of the center is normal since x ∈ Z G and g ∈ G together
imply gxg −1 = x. Here are examples: the center of a group G is G itself if and
only if G is abelian, the center of the quaternion group H8 is {±1}, and the center
of any symmetric group Sn with n ∏ 3 is {1}.
   If x is in G, the centralizer of x in G, denoted by Z G (x), is the set of all g
such that gx = xg. This is a subgroup of G, and it equals G itself if and only if
x is in the center of G. For example the centralizer of i in H8 is the 4-element
subgroup {±1, ±i}.
   Having made these definitions, we introduce a new group action of G on G,
namely (g, x) 7→ gxg −1 . The orbits are called the conjugacy classes of G. If x
and y are two elements of G, we say that x is conjugate to y if x and y are in
the same conjugacy class. In other words, x is conjugate to y if there is some g
in G with gxg −1 = y. The result is an equivalence relation. Let us write C`(x)
for the conjugacy class of x. We can easily compute the isotropy subgroup G x
at x under this action; it consists of all g ∈ G such that gxg −1 = x and hence is
exactly the centralizer Z G (x) of x in G. In particular, C`(x) = {x} if and only
if x is in the center Z G . Applying Corollary 4.35, we immediately obtain the
following result.

   Proposition 4.37. If G is a finite group, then |G| = | C`(x)| |Z G (x)| for all x
in G.

   Thus | C`(x)| is always a divisor of |G|, and it equals 1 if and only if x is in the
center Z G . Let us apply these considerations to groups whose order is a power
of a prime.

   Corollary 4.38. If G is a finite group whose order is a positive power of a
prime, then the center Z G is not {1}.
166                          IV. Groups and Group Actions

    PROOF. Let |G| = pn with p prime and with n > 0. The conjugacy classes of
G exhaust G, and thus the sum of all | C`(x)|’s equals |G|. Since | C`(x)| = 1
if and only if x is in Z G , the sum of |Z G | and all the | C`(x)|’s that are not 1 is
equal to |G|. All the terms | C`(x)| that are not 1 are positive powers of p, by
Proposition 4.37, and so is |G|. Therefore p divides |Z G |.                         §

  Corollary 4.39. If G is a finite group of order p2 with p prime, then G is
abelian.
   PROOF. From Corollary 4.38 we see that either |Z G | = p2 , in which case G is
abelian, or |Z G | = p. We show that the latter is impossible. If fact, if x is not in
Z G , then Z G (x) is a subgroup of G that contains Z G and the element x. It must
then have order p2 and be all of G. Hence every element of G commutes with x,
and x is in Z G , contradiction.                                                    §

   Corollary 4.40. If G is a finite group whose order is a positive power pn of
a prime p, then there exist normal subgroups G k of G for 0 ≤ k ≤ n such that
|G k | = pk for all k ≤ n and such that G k $ G k+1 for all k < n.
    PROOF. We proceed by induction on n. The base case of the induction is
n = 1 and is handled by Corollary 4.9. Assume inductively that the result
holds for n, and let G have order pn+1 . Corollary 4.38 shows that Z G 6= {1}.
Any element 6= 1 in Z G must have order a power of p, and some power of
it must therefore have order p. Thus let a be an element of Z G of order p,
and let H be the subgroup consisting of the powers of a. Then H is normal
and has order p. Let G 0 = G/H be the quotient group, and let ϕ : G → G 0
be the quotient homomorphism. The group G 0 has order pn , and the inductive
hypothesis shows that G 0 has normal subgroups G 0k for 0 ≤ k ≤ n such that
|G 0k | = pk for k ≤ n and G 0k ⊆ G 0k+1 for k ≤ n − 1. For 1 ≤ k ≤ n + 1, define
               0
G k = ϕ −1 (G k−1  ), and let G 0 = {1}. The First Isomorphism Theorem (Theorem
4.13) shows that each G k forØ k ∏ 1 is a normal subgroup of G containing H and
that ϕ(G k ) = G 0k−1 . Then ϕ ØG k is a homomorphism of G k onto G 0k−1 with kernel
H , and hence |G k | = |G 0k−1 | |H | = pk−1 p = pk . Therefore the G k ’s will serve
as the required subgroups of G.                                                    §

   It is not always so easy to determine the conjugacy classes in a particular group.
For example, in GL(n, C) the question of conjugacy is the question whether
two matrices are similar in the sense of Section II.3; this will be one of the
main problems addressed in Chapter V. By contrast, the problem of conjugacy in
symmetric groups has a simple answer. Recall that every permutation is uniquely
the product of disjoint cycles. The cycle structure of a permutation consists of
the number of cycles of each length in this decomposition.
                                  7. Semidirect Products                               167

   Lemma 4.41. Let σ and τ be members of the symmetric group Sn . If σ
is expressed as the product of disjoint cycles, then τ σ τ −1 has the same cycle
structure as σ , and the expression for τ σ τ −1 as the product of disjoint cycles is
obtained from that for σ by substituting τ (k) for k throughout.

° REMARK¢°  . For example, if¢σ = (a b)(c d e), then τ σ τ −1 decomposes as
 τ (a) τ (b) τ (c) τ (d) τ (e) .
   PROOF. Because the conjugate of a product equals the product of the conju-
gates, it is enough to handle a cycle ∞ = (a1 a2 · · · ar ) appearing in σ . The
corresponding cycle ∞ 0 = τ ∞ τ −1 is asserted to be ∞ 0 = (τ (a1 ) τ (a2 ) · · · τ (ar )).
Application of τ −1 to τ (a j ) yields a j , application of σ to this yields a j+1 if j < r
and a1 if j = r, and application of τ to the result yields τ (a j+1 ) or τ (a1 ). For
each of the symbols b not in the list {a1 , . . . , ar }, τ ∞ τ −1 (τ (b)) = τ (b) since
∞ (b) = b. Thus τ ∞ τ −1 = ∞ 0 , as asserted.                                            §

   Proposition 4.42. Let H be a subgroup of a symmetric group Sn . If C`(x)
denotes a conjugacy class in H , then all members of C`(x) have the same cycle
structure. Conversely if H = Sn , then the conjugacy class of a permutation σ
consists of all members of Sn having the same cycle structure as σ .
   PROOF. The first conclusion is immediate from Lemma 4.41. For the second
conclusion, let σ and σ 0 have the same cycle structure, and let τ be the permutation
that moves, for each k, the k th symbol appearing in the disjoint-cycle expansion
of σ into the k th symbol in the corresponding expansion of σ 0 . Define τ on
the remaining symbols in any fashion at all. Application of the lemma shows
that τ σ τ −1 = σ 0 . Thus any two permutations with the same cycle structure are
conjugate.                                                                         §


                               7. Semidirect Products

One more application of group actions to the structure theory of groups will
be to the construction of “semidirect products” of groups. If H is a group,
then an isomorphism of H with itself is called an automorphism. The set of
automorphisms of H is a group under composition, and we denote it by Aut H .
We are going to be interested in “group actions by automorphisms,” i.e., group
actions of a group G on a space X when X is itself a group and the action by each
member of G is an automorphism of the group structure of X; the group action
is therefore a homomorphism of the form τ : G → Aut X.

  EXAMPLE 1. In R2 , we can identify the additive group of the underlying
vector space with the group of translations `v (w) = v + w; the identification
168                             IV. Groups and Group Actions

associates a translation ` with the member `(0) of R2 . Let H be the group of
translations.
           ≥ The rotations ¥  about the origin in R2 , namely the linear maps with
               cos θ sin θ
matrices − sin θ cos θ , form a group G = SO(2) that acts on R2 , hence acts on
the set H of translations. The linearity of the rotations says that the action of
G = SO(2) on the translations is by automorphisms of H , i.e., that each rotation,
in its effect on G, is in Aut H . Out of these data—the two groups G and H and a
homomorphism of G into Aut H —we will construct below what amounts to the
group of all rotations (about any point) and translations of R2 . The construction
is that of a “semidirect product.”

   EXAMPLE 2. Take any group G, and let G act on X = G by conjugation. Each
conjugation x 7→ gxg −1 is an automorphism of G, and thus the action of G on
itself by conjugation is an action by automorphisms.

    Let G and H be groups. Suppose that a group action τ : G → F(H ) is given
with G acting on H by automorphisms. That is, suppose that each map h 7→ τg (h)
is an automorphism of H . We define a group G ×τ H whose underlying set will be
the Cartesian product G × H . The motivation for the definition of multiplication
comes from Example 2, in which τg (h) = ghg −1 . We want to write a product
g1 h 1 g2 h 2 in the form g 0 h 0 , and we can do so using the formula
                                                       °     ¢°               ¢
              g1 h 1 g2 h 2 = g1 g2 (g2−1 h 1 g2 )h 2 = g1 g2 (τg−1 (h 1 ))h 2 .
                                                                      2


Similarly the formula for inverses is motivated by the formula

                (gh)−1 = h −1 g −1 = g −1 (gh −1 g −1 ) = g −1 τg (h −1 ).

  Proposition 4.43. Let G and H be groups, and let τ be a group action of G on
H by automorphisms. Then the set-theoretic product G × H becomes a group
G ×τ H under the definitions
                       (g1 , h 1 )(g2 , h 2 ) = (g1 g2 , (τg−1 (h 1 ))h 2 )
                                                             2

and                             (g, h)−1 = (g −1 , τg (h −1 )).

The mappings i 1 : G → G ×τ H and i 2 : H → G ×τ H given by i 1 (g) = (g, 1)
and i 2 (h) = (1, h) are one-one homomorphisms, and p1 : G ×τ H → G given
by p1 (g, h) = g is a homomorphism onto G. The images G 0 = i 1 (G) and H 0 =
i 2 (H ) are subgroups of G ×τ H with H 0 normal such that G 0 ∩ H 0 = {1}, such that
every element of G ×τ H is the product of an element of G 0 and an element of H 0 ,
and such that conjugation of G 0 on H 0 is given by i 1 (g)i 2 (h)i 1 (g)−1 = i 2 (τg (h)).
                                      7. Semidirect Products                                   169

  REMARK. The group G ×τ H is called the external semidirect product16 of
G and H with respect to τ .
  PROOF. For associativity we compute directly that
       °                      ¢
        (g1 , h 1 )(g2 , h 2 ) (g3 , h 3 ) = (g1 g2 g3 , τg−1 (τg−1 (h 1 )h 2 )h 3 )
                                                           3     2
                  °                      ¢
and    (g1 , h 1 ) (g2 , h 2 )(g3 , h 3 ) = (g1 g2 g3 , τg−1 g−1 (h 1 )τg−1 (h 2 )h 3 ).
                                                             3   2           3

Since
        τg−1 (τg−1 (h 1 )h 2 ) = (τg−1 τg−1 (h 1 ))τg−1 (h 2 ) = τg−1 g−1 (h 1 )τg−1 (h 2 ),
           3     2                   3    2          3               3   2        3

we have a match. It is immediate that (1, 1) is a two-sided identity. Since
(g, h)(g −1 , τg (h −1 )) = (1, τg (h)τg (h −1 )) = (1, τg (hh −1 )) = (1, τg (1)) =
(1, 1) and (g −1 , τg (h −1 ))(g, h) = (1, τg−1 (τg (h −1 ))h) = (1, τ1 (h −1 )h) = (1, 1),
(g −1 , τg (h −1 )) is indeed a two-sided inverse of (g, h). It is immediate from the
definition of multiplication that i 1 , i 2 , and p1 are homomorphisms, that i 1 and i 2
are one-one, that p1 is onto, that G 0 ∩ H 0 = {1}, and that G ×τ H = G 0 H 0 . Since i 1
and i 2 are homomorphisms, G 0 and H 0 are subgroups. Since H 0 is the kernel of p1 ,
H 0 is normal. Finally the definition of multiplication gives i 1 (g)i 2 (h)i 1 (g)−1 =
(g, h)(g, 1)−1 = (g, h)(g −1 , 1) = (1, (τg (h))1) = i 2 (τg (h)), and the proof is
complete.                                                                                §

   Proposition 4.44. Let S be a group, let G and H be subgroups with H normal,
and suppose that G ∩ H = {1} and that every element of S is the product of an
element of G and an element of H . For each g ∈ G, define an automorphism τg
of H by τg (h) = ghg −1 . Then τ is a group action of G on H by automorphisms,
and the mapping G ×τ H → S given by (g, h) 7→ gh is an isomorphism of
groups.
   REMARKS. In this case we call S an internal semidirect product of G and
H with respect to τ . We shall not attempt to write down a universal mapping
property that characterizes internal semidirect products.
   PROOF. Since τg1 g2 (h) = g1 g2 hg2−1 g1−1 = g1 τg2 (h)g1−1 = τg1 τg2 (h) and since
each τg is an automorphism of H , τ is an action by automorphisms. Proposition
4.43 therefore shows that G ×τ H is a well-defined group. The function ϕ from
G ×τ H to S given by ϕ(g, h) = gh is a homomorphism by the same computation
that motivated the definition of multiplication in a semidirect product, and ϕ is
onto S since every element of S lies in the set G H of products. If gh = 1, then
g = h −1 exhibits g as in G ∩ H = {1}. Hence g = 1 and h = 1. Therefore ϕ is
one-one and must be an isomorphism.                                                 §
    16 The notation n is used by some authors in place of × . The normal subgroup goes on the open
                                                           τ
side of the n and on the side of the subscript τ in ×τ .
170                         IV. Groups and Group Actions

   EXAMPLE 1. Dihedral groups Dn . We show that Dn is the internal semidirect
product of a 2-element group and the rotation subgroup. Let H be the group
of rotations about the origin through multiples of the angle 2π/n. This group
is cyclic of order n, and it is normal in Dn because it is of index 2. If s is any
of the reflections in Dn , then G = {1, s} is a subgroup of Dn of order 2 with
G ∩ H = {1}. Counting the elements, we see that every element of Dn is of the
form r k or sr k , in other words that the set of products G H is all of Dn . Thus
Proposition 4.44 shows that Dn is an (internal) semidirect product of G and H
with respect to some τ : G → Aut H . To understand the homomorphism τ , let us
write the members of H as the powers of r, where r is rotation counterclockwise
about the origin through the angle 2π/n. For the reflection s (or indeed for any
reflection in Dn ), a look at the geometry shows that sr k s −1 = r −k for all k. In
other words, the automorphism τ (1) leaves each element of H fixed while τ (s)
sends each k mod n to −k mod n. The map that sends each element of a cyclic
group to its group inverse is indeed an automorphism of the cyclic group, and
thus τ is indeed a homomorphism of G into Aut H .

   EXAMPLE 2. Construction of a nonabelian group of order 21. Let H = C7 ,
written multiplicatively with generator a, and let G = C3 , written multiplicatively
with generator b. To arrange for G to act on H by automorphisms, we make use
of a nontrivial automorphism of H of order 3. Such a mapping is a k 7→ a 2k . In
fact, there is no doubt that this mapping is an automorphism, and we have to see
that it has order 3. The effect of applying it twice is a k 7→ a 4k , and the effect
of applying it three times is a k 7→ a 8k . But a 8k = a k since a 7 = 1, and thus
the mapping a k 7→ a 2k indeed has order 3. We send bn into the n th power of
this automorphism, and the result is a homomorphism τ : G → Aut H . The
semidirect product G ×τ H is certainly a group of order 3 × 7 = 21. To see
that it is nonabelian, we observe from the group law in Proposition 4.43 that
ab = bτb−1 (a) = ba 4 . Thus ab 6= ba, and G ×τ H is nonabelian.

   It is instructive to generalize the construction in Example 2 a little bit. To do
so, we need a lemma.

   Lemma 4.45. If p is a prime, then the automorphisms of the additive group
of the field F p are the multiplications by the members of the multiplicative group
F×
 p , and consequently Aut C p is isomorphic to a cyclic group C p−1 .

   PROOF. Let us write Aut F p for the automorphism group of the additive group
of F p . Each function ϕa : F p → F p given by ϕa (n) = na, taken modulo
p, is in Aut F p as a consequence of the distributive law. We define a function
8 : Aut F p → F×  p by 8(ϕ) = ϕ(1) for ϕ ∈ Aut F p . Again by the distributive
law ϕ(n) = nϕ(1) for every integer n. Thus if ϕ1 and ϕ2 are in Aut F p , then
                        8. Simple Groups and Composition Series                   171

8(ϕ1 ◦ ϕ2 ) = (ϕ1 ◦ ϕ2 )(1) = ϕ1 (ϕ2 (1)) = ϕ2 (1)ϕ1 (1), and consequently 8 is a
homomorphism. If a member ϕ of Aut F p has 8(ϕ) = 1 in F×       p , then ϕ(1) = 1
and therefore ϕ(n) = nϕ(1) = n for all n. Therefore ϕ is the identity in Aut F p .
We conclude that 8 is one-one. If a is given in F×  p , then 8(ϕa ) = ϕa (1) = a,
and hence 8 is onto F×p . Therefore  8  is an isomorphism   of Aut F p and F×
                                                                            p . By
Corollary 4.27, 8 exhibits Aut F p as isomorphic to the cyclic group C p−1 .     §

   Proposition 4.46. If p and q are primes with p < q such that p divides q − 1,
then there exists a nonabelian group of order pq.
   REMARKS. For p = 2, the divisibility condition is automatic, and the proof
will yield the dihedral group Dq . For p = 3 and q = 7, the condition is that 3
divides 7 − 1, and the constructed group will be the group in Example 2 above.
    PROOF. Let G = C p with generator a, and let H = Cq . Lemma 4.45 shows that
Aut Cq ∼ = Cq−1 . Let b be a generator of Aut Cq . Since p divides q − 1, b(q−1)/ p
has order p. Then the map a k 7→ bk(q−1)/ p is a well-defined homomorphism
τ of G into Aut H , and it determines a semidirect product S = G ×τ H , by
Proposition 4.43. The order of S is pq, and the multiplication is nonabelian since
for h ∈ H , we have (a, 1)(1, h) = (a, h) and (1, h)(a, 1) = (a, τa −1 (h)) =
(a, b−(q−1)/ p (h)), but b−(q−1)/ p is not the identity automorphism of H because
it has order p.                                                                 §


                  8. Simple Groups and Composition Series

A group G 6= {1} is said to be simple if its only normal subgroups are {1} and G.
    Among abelian groups the simple ones are the cyclic groups of prime order.
Indeed, a cyclic group C p of prime order has no nontrivial subgroups at all, by
Corollary 4.9. Conversely if G is abelian and simple, let a 6= 1 be in G. Then
{a n } is a cyclic subgroup and is normal since G is abelian. Thus {a n } is all of G,
and G is cyclic. The group Z is not simple, having the nontrivial subgroup 2Z,
and the group Z/(rs)Z with r > 1 and s > 1 is not simple, having the multiples
of r as a nontrivial subgroup. Thus G has to be cyclic of prime order.
    The interest is in nonabelian simple groups. We shall establish that the alter-
nating groups An are simple for n ∏ 5, and some other simple groups will be
considered in Problems 55–62 at the end of the chapter.

   Theorem 4.47. The alternating group An is simple if n ∏ 5.
   PROOF. Let K 6= {1} be a normal subgroup of An . Choose σ in K with σ 6= 1
such that σ (i) = i for the maximum possible number of integers i with 1 ≤ i ≤ n.
172                           IV. Groups and Group Actions

The main step is to show that σ is a 3-cycle. Arguing by contradiction, suppose
that σ is not a 3-cycle. Then there are two cases.
    The first case is that the decomposition of σ as the product of disjoint cycles
contains a k-cycle for some k ∏ 3. Without loss of generality, we may take the
cycle in question to be ∞ = (1 2 3 · · · ), and then σ = ∞ρ = (1 2 3 · · · )ρ
with ρ equal to a product of disjoint cycles not containing the symbols appearing
in ∞ . Being even and not being a 3-cycle, σ moves at least two other symbols
besides the three listed ones, say 4 and 5. Put τ = (3 4 5). Lemma 4.41 shows
that σ 0 = τ σ τ −1 = ∞ 0 ρ 0 = (1 2 4 · · · )ρ 0 with ρ 0 not containing any of the
symbols appearing in ∞ 0 . Thus σ 0 σ −1 moves 3 into 4 and cannot be the identity.
But σ 0 σ −1 is in K and fixes all symbols other than 1, 2, 3, 4, 5 that are fixed by
σ . In addition, σ 0 σ −1 fixes 2, and none of 1, 2, 3, 4, 5 is fixed by σ . Thus σ 0 σ −1
is a member of K other than the identity that fixes fewer symbols than σ , and we
have arrived at a contradiction.
    The second case is that σ is a product σ = (1 2)(3 4) · · · of disjoint
transpositions. There must be at least two factors since σ is even. Put τ =
(1 2)(4 5), the symbol 5 existing since the group An in question has n ∏ 5. Then
σ 0 = (1 2)(3 5) · · · . Since σ 0 σ −1 carries 4 into 5, σ 0 σ −1 is a member of K other
than the identity. It fixes all symbols other than 1, 2, 3, 4, 5 that are fixed by σ ,
and in addition it fixes 1 and 2. Thus σ 0 σ −1 fixes more symbols than σ does, and
again we have arrived at a contradiction.
    We conclude that K contains a 3-cycle, say (1 2 3). If i, j, k, l, m are five
arbitrary symbols, then we can construct a permutation τ with τ (1) = i, τ (2) = j,
τ (3) = k, τ (4) = l, and τ (5) = m. If τ is odd, we replace τ by τ (l m), and the
result is even. Thus we may assume that τ is in An and has τ (1) = i, τ (2) = j,
and τ (3) = k. Lemma 4.41 shows that τ σ τ −1 = (i j k). Since K is normal,
we conclude that K contains all 3-cycles.
    To complete the proof, we show for n ∏ 3 that every element of An is a product
of 3-cycles. If σ is in An , we use Corollary 1.22 to decompose σ as a product of
transpositions. Since σ is even, we can group these in pairs. If the members of a
pair of transpositions are not disjoint, then their product is a 3-cycle. If they are
disjoint, then the identity (1 2)(3 4) = (1 2 3)(2 3 4) shows that their product
is a product of 3-cycles. This completes the proof.                                     §

   Let G be a group. A descending sequence

                           G n ⊇ G n−1 ⊇ · · · ⊇ G 1 ⊇ G 0

of subgroups of G with G n = G, G 0 = {1}, and each G k−1 normal in G k is
called a normal series for G. The normal series is called a composition series if
each inclusion G k ⊇ G k−1 is proper and if each consecutive quotient G k /G k−1
is simple.
                       8. Simple Groups and Composition Series                 173

   EXAMPLES.
   (1) Let G be a cyclic group of order N . A normal series for G consists of
certain subgroups of G, all necessarily cyclic by Proposition 4.4. Their respective
orders Nn , Nn−1 , . . . , N1 , N0 have Nn = N , N0 = 1, and Nk−1 | Nk for all k.
The series is a composition series if and only if each quotient Nk /Nk−1 is prime.
In this case the primes that occur are exactly the prime divisors of N , and a
prime p occurs r times if pr is the exact power of p that divides N . Thus the
consecutive quotients from a composition series of this G, up to isomorphisms,
are independent of the particular composition series—though they may arise in a
different order.
   (2) For G = Z, a normal series is of the form

                 Z ⊇ m 1 Z ⊇ m 1 m 2 Z ⊇ m 1 m 2 m 3 Z ⊇ · · · ⊇ 0.

The group G = Z has no composition series.
  (3) For the symmetric group G = S4 , let C2 × C2 refer to the 4-element
subgroup {1, (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)}. The series

                 S4 ⊇ A4 ⊇ C2 × C2 ⊇ {1, (1 2)(3 4)} ⊇ {1}

is a composition series, the consecutive quotients being C2 , C3 , C2 , C2 . Each
term in the composition series except for {1, (1 2)(3 4)} is actually normal in
the whole group G, but there is no way to choose the 2-element subgroup to make
it normal in G. The other two possible choices of 2-element subgroup, which
lead to different composition series but with isomorphic consecutive quotients,
are obtained by replacing {1, (1 2)(3 4)} by {1, (1 3)(2 4)} and again by
{1, (1 4)(2 3)}.
    (4) For the symmetric group G = S5 , the series

                                 S5 ⊇ A5 ⊇ {1}

is a composition series, the consecutive quotients being C2 and A5 .
    (5) Let G be a finite group of order pn with p prime. Corollary 4.40 produces
a composition series, and this time all the subgroups are normal in G. The
successive normal subgroups have orders pk for k = n, n − 1, . . . , 0, and each
consecutive quotient is isomorphic to C p .

   Historically the Jordan–Hölder Theorem addressed composition series for
groups, showing that the consecutive quotients, up to isomorphisms, are indepen-
dent of the particular composition series. They can then consistently be called the
composition factors of the group. Finding the composition factors of a particular
174                               IV. Groups and Group Actions

group may be regarded as a step toward understanding the structure of the group.
A generalization of the Jordan–Hölder Theorem due to Zassenhaus and Schreier
applies to normal series in situations in which composition series might not exist,
such as Example 2 above. We prove the Zassenhaus–Schreier Theorem, and the
Jordan–Hölder Theorem is then a special case.

   Two normal series

                             G m ⊇ G m−1 ⊇ · · · ⊇ G 1 ⊇ G 0
and                          Hn ⊇ Hn−1 ⊇ · · · ⊇ H1 ⊇ H0

for the same group G are said to be equivalent normal series if m = n and the
order of the consecutive quotients G m /G m−1 , G m−1 /G m−2 , . . . , G 1 /G 0 may be
rearranged so that they are respectively isomorphic to Hm /Hm−1 , Hm−1 /Hm−2 ,
. . . , H1 /H0 . One normal series is said to be a refinement of another if the
subgroups appearing in the second normal series all appear as subgroups in the
first normal series.

  Lemma 4.48 (Zassenhaus). Let G 1 , G 2 , G 01 , and G 02 be subgroups of a group
G with G 01 ⊆ G 1 and G 02 ⊆ G 2 , G 01 normal in G 1 , and G 02 normal in G 2 . Then
(G 1 ∩ G 02 )G 01 is normal in (G 1 ∩ G 2 )G 01 , (G 01 ∩ G 2 )G 02 is normal in (G 1 ∩ G 2 )G 02 ,
and

      ((G 1 ∩ G 2 )G 01 )/((G 1 ∩ G 02 )G 01 ) ∼
                                               = ((G 1 ∩ G 2 )G 02 )/((G 01 ∩ G 2 )G 02 ).

    PROOF. Let us check that (G 1 ∩ G 02 )G 01 is normal in (G 1 ∩ G 2 )G 01 . Handling
conjugation by members of G 1 ∩ G 2 is straightforward: If g is in G 1 ∩ G 2 ,
then g(G 1 ∩ G 02 )g −1 = G 1 ∩ G 02 since g is in G 1 and gG 02 g −1 = G 02 . Also,
gG 01 g −1 = G 01 since g is in G 1 . Hence g(G 1 ∩ G 02 )G 01 g −1 = (G 1 ∩ G 02 )G 01 .
    Handling conjugation by members of G 01 requires a little trick: Let g be in G 01
and let hg 0 be in (G 1 ∩ G 02 )G 01 . Then g(hg 0 )g −1 = h(h −1 gh)g 0 g −1 . The left
factor h is in G 1 ∩ G 02 . The remaining factors are in G 01 ; for g 0 and g −1 , this is
a matter of definition, and for h −1 gh, it follows because h is in G 1 and g is in
G 01 . Thus g(G 1 ∩ G 02 )G 01 g −1 = (G 1 ∩ G 02 )G 01 , and (G 1 ∩ G 02 )G 01 is normal in
(G 1 ∩ G 2 )G 01 . The other assertion about normal subgroups holds by symmetry
in the indexes 1 and 2.
    By the Second Isomorphism Theorem (Theorem 4.14),

              (G 1 ∩ G 2 )/(((G 1 ∩ G 02 )G 01 ) ∩ (G 1 ∩ G 2 ))
                           ∼
                           = ((G 1 ∩ G 2 )(G 1 ∩ G 0 )G 0 )/((G 1 ∩ G 0 )G 0 )                 (∗)
                                                        2        1               2   1
                            = ((G 1 ∩    G 2 )G 01 )/((G 1   ∩   G 02 )G 01 ).
                          8. Simple Groups and Composition Series                         175

Since we have

((G 1 ∩ G 02 )G 01 ) ∩ (G 1 ∩ G 2 ) = ((G 1 ∩ G 02 )G 01 ) ∩ G 2 = (G 1 ∩ G 02 )(G 01 ∩ G 2 ),

we can rewrite the conclusion of (∗) as

  (G 1 ∩ G 2 )/((G 1 ∩ G 02 )(G 01 ∩ G 2 )) ∼
                                            = ((G 1 ∩ G 2 )G 01 )/((G 1 ∩ G 02 )G 01 ). (∗∗)

The left side of (∗∗) is symmetric under interchange of the indices 1 and 2. Hence
so is the right side, and the lemma follows.                                    §

   Theorem 4.49 (Schreier). Any two normal series of a group G have equivalent
refinements.
   PROOF. Let the two normal series be

                            G m ⊇ G m−1 ⊇ · · · ⊇ G 1 ⊇ G 0 ,
                                                                                          (∗)
                            Hn ⊇ Hn−1 ⊇ · · · ⊇ H1 ⊇ H0 ,

and define
                      G i j = (G i ∩ Hj )G i+1        for 0 ≤ j ≤ n,
                                                                                         (∗∗)
                      Hji = (G i ∩ Hj )Hj+1           for 0 ≤ i ≤ m.
Then we obtain respective refinements of the two normal series (∗) given by

                G = G 00 ⊇ G 01 ⊇ · · · ⊇ G 0n
                  ⊇ G 10 ⊇ G 11 ⊇ · · · ⊇ G 1n · · · ⊇ G m−1,n = {1},
                                                                                          (†)
                G = H00 ⊇ H01 ⊇ · · · ⊇ H0m
                  ⊇ H10 ⊇ H11 ⊇ · · · ⊇ H1m · · · ⊇ Hn−1,m = {1}.

The containments G in ⊇ G i+1,0 and Hjm ⊇ Hj+1,0 are equalities in (†), and
the only nonzero consecutive quotients are therefore of the form G i j /G i, j+1 and
Hji /Hj,i+1 . For these we have

   G i j /G i, j+1 = ((G i ∩ Hj )G i+1 )/((G i ∩ Hj+1 )G i+1 )         by (∗∗)
                   ∼
                   = ((G i ∩ Hj )Hj+1 )/((G i+1 ∩ Hj )Hj+1 )           by Lemma 4.48
                   = Hji /Hj,i+1                                       by (∗∗),

and thus the refinements (†) are equivalent.                                                §
176                          IV. Groups and Group Actions

   Corollary 4.50 (Jordan–Hölder Theorem). Any two composition series of a
group G are equivalent as normal series.
    PROOF. Let two composition series be given. Theorem 4.49 says that we
can insert terms in each so that the refined series have the same length and are
equivalent. Since the given series are composition series, the only way to insert
a new term is by repeating some term, and the repetition results in a consecutive
quotient of {1}. Because of Theorem 4.49 we know that the quotients {1} from
the two refined series must match. Thus the number of terms added to each series
is the same. Also, the quotients that are not {1} must match in pairs. Thus the
given composition series are equivalent.                                       §



             9. Structure of Finitely Generated Abelian Groups

A set of generators for a group G is a set such that each element of G is a finite
product of generators and their inverses. (A generator and its inverse are allowed
to occur multiple times in a product.)
    In this section we shall study abelian groups having a finite set of generators.
Such groups are said to be finitely generated abelian groups, and our goal is
to classify them up to isomorphism. We use additive notation for all our abelian
groups in this section. We begin by introducing an analog Zn for the integers Z
of the vector space Rn for the reals R, and along with it a generalization.
    A free abelian group is any abelian group isomorphic to a direct sum, finite or
infinite, of copies of the additive group Z of integers. The external direct sum of n
copies of Z will be denoted by Zn . Let us use Proposition 4.17 to see that we can
recognize groups isomorphic to free abelian groups by means of the following
condition: an abelian group G is isomorphic to a free abelian group if and only if
it has a Z basis, i.e., a subset that generates G and is such that no nontrivial linear
combination, with integer coefficients, of the members of the subset is equal to
the 0 element of the group. It will be helpful to use terminology adapted from the
theory of vector spaces for this latter condition—that the subset is to be linearly
independent over Z.
    Let us give the proof that the condition is necessary and sufficient for G to be
free abelian. In one direction if G is an external direct sum of copies of Z, then
the members of G that are 1 in one coordinate and are 0 elsewhere form a Z basis.
Conversely if {gs }s∈S is a Z basis, let G s0 be the subgroup of multiples of gs0 , and
let ϕs0 be the inclusion homomorphism    Lof G s0 into G. Proposition 4.17 produces
a unique group homomorphism ϕ : s∈S G s → G such that ϕ ◦ i s0 = ϕs0 for
all s0 ∈ S. The spanning condition for the Z basis says that ϕ is onto G, and the
linear independence condition for the Z basis says that ϕ has 0 kernel.
                       9. Structure of Finitely Generated Abelian Groups                   177

   The similarity between vector-space bases and Z bases suggests further com-
parison of vector spaces and abelian groups. With vector spaces over a field, every
vector space has a basis over the field. However, it is exceptional for an abelian
group to have a Z basis. Two examples that hint at the difficulty are the additive
group Z/mZ with m > 1 and the additive group Q. The group Z/mZ has no
nonempty linearly independent set, while the group Q has a linearly independent
set of one element, no spanning set of one element, and no linearly independent
set of more than one element. Here are two positive examples.

   EXAMPLES.
   (1) The additive group of all points in Rn whose coordinates are integers. The
standard basis of Rn is a Z basis.
   (2) The additive©group of
                           ° 1all1 points
                                   ¢™     (x, y) in R2 with x and y both in Z or both
        1
in Z + 2 . The set (1, 0), 2 , 2 is a Z basis.

   Next we take a small step that eliminates technical complications from the
discussion, proving that any subgroup of a finitely generated abelian group is
finitely generated.

   Lemma 4.51. Let ϕ : G → H be a homomorphism of abelian groups. If
ker ϕ and image ϕ are finitely generated, then G is finitely generated.
   PROOF. Let {x1 , . . . , xm } and {y1 , . . . , yn } be respective finite sets of generators
for ker ϕ and image ϕ. For 1 ≤ j ≤ n, choose x j0 in G with ϕ(x j0 ) = yj .
We shall prove that {x1 , . . . , xm , x10 , . . . , xn0 } is a set of generators for G. Thus
let x be in G. Since ϕ(x) is in image ϕ, there exist integers a1 , . . . , an with
ϕ(x) = a1 y1 + · · · + an yn . The element x 0 = a1 x10 + · · · + an xn0 of G has
ϕ(x 0 ) = a1 y1 + · · · + an yn = ϕ(x). Therefore ϕ(x − x 0 ) = 0, and there exist
integers b1 , . . . , bm with x − x 0 = b1 x1 + · · · + bm xm . Hence

  x = b1 x1 + · · · + bm xm + x 0 = b1 x1 + · · · + bm xm + a1 x10 + · · · + an xn0 . §

  Proposition 4.52. Any subgroup of a finitely generated abelian group is finitely
generated.
    PROOF. Let G be finitely generated with a set {g1 , . . . , gn } of n generators, and
define G k = Zg1 + · · · + Zgk for 1 ≤ k ≤ n. If H is any subgroup of G, define
Hk = H ∩ G k for 1 ≤ k ≤ n. We shall prove by induction on k that every Hk
is finitely generated, and then the case k = n gives the proposition. For k = 1,
G 1 = Zg1 is a cyclic group, and any subgroup of it is cyclic by Proposition 4.4
and hence is finitely generated.
    Assume inductively that every subgroup of G k is known to be finitely generated.
                                                                                   Ø
Let q : G k+1 → G k+1 /G k be the quotient homomorphism, and let ϕ = q Ø Hk+1 ,
178                             IV. Groups and Group Actions

mapping Hk+1 into G k+1 /G k . Then ker ϕ = Hk+1 ∩ G k is a subgroup of G k and
is finitely generated by the inductive hypothesis. Also, image ϕ is a subgroup of
G k+1 /G k , which is a cyclic group with generator equal to the coset of gk+1 . Since
a subgroup of a cyclic group is cyclic, image ϕ is finitely generated. Applying
Lemma 4.51 to ϕ, we see that Hk+1 is finitely generated. This completes the
induction and the proof.                                                            §

   A free abelian group has finite rank if it has a finite Z basis, hence if it is
isomorphic to Zn for some n. The first theorem is that the integer n is determined
by the group.

   Theorem 4.53. The number of Z summands in a free abelian group of finite
rank is independent of the direct-sum decomposition of the group.
   We define this number to be the rank of the free abelian group. Actually,
“rank” is a well-defined cardinal in the infinite-rank case as well, because the rank
coincides in that case with the cardinality of the group. In any event, Theorem
4.53 follows immediately by two applications of the following lemma.

  Lemma 4.54. If G is a free abelian group with a finite Z basis x1 , . . . , xn , then
any linearly independent subset of G has ≤ n elements.
    PROOF. Let {y1 , . . . , ym } be a linearly independent set in G. Since {x
                                                                             P1 , . . . , xn }
is a Z basis, we can define an m-by-n matrix C of integers by yi = nj=1 Ci j x j .
As a matrix in Mmn (Q), C has rank ≤ n. Consequently if m > n, then the rows
are linearly dependent
               Pm           over Q, and we can find rational numbers q1 , . . . , qm not
all 0 such that i=1                                                   Pm integer to clear
                       qi Ci j = 0 for all j. Multiplying by a suitable
fractions, we obtain integers k1 , . . . , km not all 0 such that i=1 ki Ci j = 0 for
all j. Then we have
          Pm           Pm      Pn             n °P
                                              P     m         ¢      n
                                                                     P
              ki yi =       ki     Ci j x j =         ki C i j x j =   0x j = 0,
           i=1         i=1    j=1           j=1   i=1               j=1

in contradiction to the linear independence of {y1 , . . . , ym } over Z. Therefore
m ≤ n.                                                                           §

   Now we come to the two main results of this section. The first is a special
case of the second by Proposition 4.52 and Lemma 4.54. The two will be proved
together, and it may help to regard the proof of the first as a part of the proof of
the second.

   Theorem 4.55. A subgroup H of a free abelian group G of finite rank n is
free abelian of rank ≤ n.
   REMARK. This result persists in the case of infinite rank, but we do not need
the more general result and will not give a proof.
                       9. Structure of Finitely Generated Abelian Groups                     179

   Theorem 4.56 (Fundamental Theorem of Finitely Generated Abelian Groups).
Every finitely generated abelian group is a finite direct sum of cyclic groups. The
cyclic groups may be taken to be copies of Z and various C pk with p prime, and
in this case the cyclic groups are unique up to order and to isomorphism.
   REMARKS. The main conclusion of the theorem is the decomposition of each
finitely generated abelian group into the direct sum of cyclic groups. An alterna-
tive decomposition of the given group that forces uniqueness is as the direct sum
of copies of Z and finite cyclic groups Cd1 , . . . , Cdr such that d1 | d2 , d2 | d3 , . . . ,
dr−1 | dr . A proof of the additional statement appears in the problems at the end
of Chapter VIII. The integers d1 , . . . , dr are sometimes called the elementary
divisors of the group.

   Let us establish the setting for the proof of Theorem 4.56. Let G be the given
group, and say that it has a set of n generators. Proposition 4.17 produces a
homomorphism ϕ : Zn → G that carries the standard generators x1 , . . . , xn of
Zn to the generators of G, and ϕ is onto G. Let H be the kernel of ϕ. As a
subgroup of Zn , H is finitely generated, by Proposition 4.52. Let y1 , . . . , ym
be generators. Theorem 4.55 predicts that H is in fact free abelian, hence that
{y1 , . . . , ym } could be taken to be linearly independent over Z with m ≤ n, but
we do not assume that knowledge in the proof of Theorem 4.56.
   The motivation for the main part of the proof of Theorem 4.56 comes from
the elementary theory of vector spaces, particularly from the method of using a
basis for a finite-dimensional vector space to find a basis of a vector subspace
when we know a finite spanning set for the vector subspace. Thus let V be a
finite-dimensional vector space over R, with basis {x j }nj=1 , and let U be a vector
                                     m
subspace with spanning set {yi }i=1      . To produce a vector-space basis for U , we
imagine expanding the yi ’s as linear combinations of x1 , . . . , xn . We can think
symbolically of this expansion as expressing each yi as √           product of a row
                                                                the !
                                                                       x1
                                                                           ..
vector of real numbers times the formal “column vector”                     .   . The entries of
                                                                       xn
this column vector are vectors, but there is no problem in working with it√since
                                                                              !             y1
                                                                                            ..
this is all just a matter of notation anyway. Then the formal column vector                  .
                                                                                           ym
of m members of U equals√the!product of an m-by-n matrix of real numbers times
                          x1
                           .
the formal column vector .. . We know from Chapter II that the procedure for
                                xn
finding a basis of U is to row reduce this matrix of real numbers. The nonzero
rows of the result determine a basis of the span of the m vectors we have used, and
this basis is related tidily to the given basis for V . We can compare the two bases
180                               IV. Groups and Group Actions

to understand the relationship between U and V . To prove Theorem 4.56, we
would like to use the same procedure, but we have to work with an integer matrix
and avoid division. This means that only two of the three usual row operations are
fully available for the row reduction; division of a row by an integer is allowable
only when the integer is ±1. A partial substitute for division comes by using the
steps of the Euclidean algorithm via the division algorithm (Proposition
                                                                  µ           ∂ but
                                                                           1.1),
                                                                    2 1 1
even that is not enough. For example, if the m-by-n matrix is                   , no
                                                                    0 0 3
further row reduction is possible with integer operations. However, the equations
tell us that H is the subgroup of Z3 generated by (2, 1, 1) and (0, 0, 3), and it is
not at all clear how to write Z3 /H as a direct sum of cyclic groups.
   The row operations have the effect of changing the set of generators of H
while maintaining the fact that they generate H . What is needed is to allow also
column reduction with integer operations. Steps of this kind have the effect of
changing the Z basis of Zn . When steps of this kind are allowed, we can produce
new generators of H and a new basis of Zn so that the two can be compared.
With the example above, suitable column operations are
       µ            ∂         µ                    ∂        µ           ∂        µ           ∂
           2 1 1                  1 2 1                         1 0 0                1 0 0
                         7→                            7→                   7→                   .
           0 0 3                  0 0 3                         0 0 3                0 3 0

The equations with the new generators say that y10 = x10 and y20 = 3x20 . Thus H is
the subgroup Z ⊕ 3Z ⊕ 0Z, nicely aligned with Z3 = Z ⊕ Z ⊕ Z. The quotient
is (Z/Z) ⊕ (Z/3Z) ⊕ (Z/0Z) ∼    = C3 ⊕ Z.
    The proof of Theorem 4.56 will make use of an algorithm that uses row and
column operations involving only allowable divisions and that converts the matrix
C of coefficients so that its nonzero entries are the diagonal entries Cii for
1 ≤ i ≤ r and no other entries. The algorithm in principle can be very slow, and
it may be helpful to see what it does in an ordinary example.

  EXAMPLE. Suppose that the relationship between generators y1 , y2 , y3 of H
and the standard Z basis {x1 , x2 } of Z2 is
                √        !        µ        ∂                            √            !
                    y1                                                      3 5
                                      x1
                    y2       =C                ,            where C =       7 13         .
                                      x2
                    y3                                                      5 9

In row reduction in vector-space theory, we would start by dividing the first row
of C by 3, but division by 3 is not available in the present context. Our target for
the upper-left entry is GCD(3, 7, 5) = 1, and we use the division algorithm one
step at a time to get there. To begin with, it says that 7 = 2 · 3 + 1 and hence
7 − 2 · 3 = 1. The first step of row reduction is then to replace the second row by
                      9. Structure of Finitely Generated Abelian Groups             181

the difference of it and 2 times the first row. The result can be achieved by left
multiplication by
                    √            !                  √       !
                        1 0 0                         3 5
                      −2 1 0              and is      1 3 .
                        0 0 1                         5 9
We write this step as
                                                           
                                                     100
                        √             !              √
                                          left by  −2 1 0   !
                          3 5                           3 5
                                                     001
                          7 13 |−−−−−−−−−−→ 1 3 .
                          5 9                           5 9
The entry 1 in the first column is our target for this stage since GCD(3, 7, 5) = 1.
The next step interchanges two rows to move the 1 to the upper left entry, and the
subsequent step uses the 1 to eliminate the other entries of the first column:
                                                                        
                                010                                  100
        √       !    left by  1 0 0     √           !    √        !
                                                          left by  −3 1 0 
          3 5                     1 3                        1    3
                         001                      −5 0 1
          1 3 |−−−−−−−−−→ 3 5 |−−−−−−−−−−→ 0 −4 .
          5 9                     5 9                        0 −6
The algorithm next seeks to eliminate the off-diagonal entry 3 in the first row.
This is done by a column operation:
                                     µ     ∂
                    √        !                √          !
                      1    3 right by 1 −3       1     0
                                       0 1
                      0 −4 |−−−−−−−−−→ 0 −4 .
                      0 −6                       0 −6
With two further row operations we are done:
                                                                             
                                10    0                                   100
       √         !    left by  0 1 −1       √            !         √      !
                                                                left by  0 1 0 
          1     0                       1      0                       1 0
                             00 1                            031
          0 −4 |−−−−−−−−−−→ 0                  2 |−−−−−−−−−→ 0 2 .
          0 −6                          0 −6                           0 0
Our steps are summarized by the fact that the matrix A with
       √           !√               !√               !√             !√            !
         1 0 0          1 0       0       1 0 0           0 1 0           1 0 0
 A= 0 1 0               0 1 −1          −3 1 0            1 0 0         −2 1 0
         0 3 1          0 0       1     −5 0 1            0 0 1           0 0 1
                                 µ         ∂ √1 0!
                                   1 −3
has                          AC               = 0 2
                                   0    1
                                                   0 0
and by the fact that the integer matrices to the left and right of C have determinant
                                                              ≥      ¥−1
                                                                1 −3
±1. The determinant condition ensures that A−1 and 0 1                   have integer
entries, according to Cramer’s rule (Proposition 2.38).
182                           IV. Groups and Group Actions

   Lemma 4.57. If C is an m-by-n matrix of integers, then there exist an m-by-m
matrix A of integers with determinant ±1 and an n-by-n matrix B of integers
with determinant ±1 such that for some r ∏ 0, the nonzero entries of D = AC B
are exactly the diagonal entries D11 , D22 , . . . , Drr .
    PROOF. Given C, choose (i, j) with |Ci j | 6= 0 but |Ci j | as small as possible.
(If C = 0, the algorithm terminates.) Possibly by interchanging two rows and/or
then two columns (a left multiplication with determinant −1 and then a right
multiplication with determinant −1), we may assume that (i, j) = (1, 1). By the
division algorithm write, for each i,
                    Ci1 = qi C11 + ri       with 0 ≤ ri < |C11 |,
and replace the i th row by the difference of the i th row and qi times the first row (a
left multiplication). If some ri is not 0, the result will leave a nonzero entry in the
first column that is < |C11 | in absolute value. Permute the least such ri 6= 0 to the
upper left and repeat the process. Since the least absolute value is going down,
this process at some point terminates with all ri equal to 0. The first column then
has a nonzero diagonal entry and is otherwise 0.
    Now consider C1 j and apply the division algorithm and column operations
in similar fashion in order to process the first row. If we get a smaller nonzero
remainder, permute the smallest one to the first column. Repeat this process until
the first row is 0 except for entry C11 . Continue alternately with row and column
operations in this fashion until both C1 j = 0 for j > 1 and Ci1 = 0 for i > 1.
    Repeat the algorithm for the (m − 1)-by-(n − 1) matrix consisting of rows 2
through m and columns 2 through n, and continue inductively. The algorithm
terminates when either the reduced-in-size matrix is empty or is all 0. At this
point the original matrix has been converted into the desired “diagonal form.” §

   Lemma 4.58. Let G 1 , . . . , G n be abelian groups, and for 1 ≤ j ≤ n, let Hj
be a subgroup of G j . Then
       (G 1 ⊕ · · · ⊕ G n )/(H1 ⊕ · · · ⊕ Hn ) ∼
                                               = (G 1 /H1 ) ⊕ · · · ⊕ (G n /Hn ).
   PROOF. Let ϕ : G 1 ⊕ · · · ⊕ G n → (G 1 /H1 ) ⊕ · · · ⊕ (G n /Hn ) be the
homomorphism defined by ϕ(g1 , . . . , gn ) = (g1 H1 , . . . , gn Hn ). The mapping
ϕ is onto (G 1 /H1 ) ⊕ · · · ⊕ (G n /Hn ), and the kernel is H1 ⊕ · · · ⊕ Hn . Then
Corollary 4.12 shows that ϕ descends to the required isomorphism.                §

   PROOF OF THEOREM 4.55 AND MAIN CONCLUSION OF THEOREM 4.56. Given G
with n generators, we set up matters as indicated immediately after the statement
of Theorem 4.56, writing      √ !          √ !
                                    y1           x1
                                    ..           ..
                                     .   =C       .   ,
                                   ym            xn
                        9. Structure of Finitely Generated Abelian Groups                        183

where x1 , . . . , xn are the standard generators of Zn , y1 , . . . , ym are the generators
of the kernel of the homomorphism from Zn onto G, and C is a matrix of integers.
Applying Lemma 4.57, let A and B be square integer matrices of determinant ±1
such that D = AC B is diagonal as in the statement of the lemma. Define
                √ z1 !        √ y1 !               √ u1 !              √ x1 !
                    ..           ..                   ..           −1     ..
                     . =A .              and           . =B                . .
                   zm             ym                           un                   xn

Substitution gives
            √ z1 !   √ y1 !          √ x1 !      √ u1 !
               ..      ..         −1    ..          .
                . = A . = (AC B)B        . = AC B .. .
                 zm              ym                            xn                     un

   If (c1 · · · cn ) and (d1 · · · dn ) = (c1 · · · cn )B −1 are row vectors, then the
formula
                                                                        
                                                    u1                    x1
                                                     .                      .
        c1 u 1 + · · · + cn u n = (c1 · · · cn )  ..  = (d1 · · · dn )  .. 
                                                    un                    xn
                                = d1 x1 + · · · + dn xn                            (∗)
shows that {u 1 , . . . , u n } generates the same subset of Zn as {x1 , . . . , xn }. Since
(c1 · · · cn ) is nonzero if and only if (d1 · · · dn ) is nonzero, the formula (∗) shows
also that the linear independence of {x1 , . . . , xn } implies that of {u 1 , . . . , u n }.
Hence {u 1 , . . . , u n } is a Z basis of Zn . Similarly {y1 , . . . , ym } and {z 1 , . . . , z m }
generate the same subgroup H of Zn . Therefore we can compare H and Zn
using {z 1 , . . . , z m } and {u 1 , . . . , u n }. Since D is diagonal, the equations relating
{z 1 , . . . , z m } and {u 1 , . . . , u n } are z j = D j j u j for j ≤ min(m, n) and z j = 0 for
min(m, n) < j ≤ m. If q = min(m, n), then we see that
                        m
                        P              q
                                       P                   m
                                                           P               q
                                                                           P
                H=            Zz i =         Dii Zu i +           Zz i =         Dii Zu i .
                        i=1            i=1                i=q+1            i=1

Since the set {u 1 , . . . , u q } is linearly independent over Z, this sum exhibits H as
given by
                                    H = D11 Z ⊕ · · · ⊕ Dqq Z
with D11 u 1 , . . . , Dqq u q as a Z basis. Consequently H has been exhibited as free
abelian of rank ≤ q ≤ n. This proves Theorem 4.55. Applying Lemma 4.58 to
the quotient Zn /H and letting D11 , . . . , Drr be the nonzero diagonal entries of
D, we see that H has rank r, and we obtain an expansion of G in terms of cyclic
groups as
                              G = C D11 ⊕ · · · ⊕ C Drr ⊕ Zn−r .
This proves the main conclusion of Theorem 4.56.                                                  §
184                               IV. Groups and Group Actions

    PROOF OF THE DECOMPOSITION Q   WITH CYCLIC GROUPS OF PRIME-POWER ORDER.
                                     N     k
It is enough to prove that if m = j=1    p j j with the p j equal to distinct primes,
                     k                    k
then Z/mZ ∼  = (Z/ p11 Z) ⊕ · · · ⊕ (Z/ p NN Z). This is a variant of the Chinese
Remainder Theorem (Corollary 1.9). For the proof let
                     ϕ : Z → (Z/ p1k1 Z) ⊕ · · · ⊕ (Z/ pkNN Z)
                                           °                       ¢
be the homomorphism given by ϕ(s) = s mod p1k1 , . . . , s mod pkNN for s ∈ Z.
Since ϕ(m) = (0, . . . , 0), ϕ descends to a homomorphism
                      ϕ : Z/mZ → (Z/ p1k1 Z) ⊕ · · · ⊕ (Z/ pkNN Z).
                                                                   k
The map ϕ is one-one because if ϕ(s) = 0, then p j j divides s for all j. Since
      k
the p j j are relatively prime in pairs, their product m divides s. Since m divides s,
s ≡ 0 mod m. The map ϕ is onto since it is one-one and since the finite sets
Z/mZ and (Z/ p1k1 Z) ⊕ · · · ⊕ (Z/ pkNN Z) both have m elements.                    §
   PROOF OF UNIQUENESS OF THE DECOMPOSITION. Write G = Zs ⊕ T , where
                            T = (Z/ p1l1 Z) ⊕ · · · ⊕ (Z/ plMM Z)
and the p j ’s are not necessarily distinct. The subgroup T is the subgroup of
elements of finite order in G, and it is well defined independently of the decom-
position of G as the direct sum of cyclic groups. The quotient G/T ∼      = Zs is
free abelian of finite rank, and its rank s is well defined by Theorem 4.53. Thus
the number s of factors of Z in the decomposition of G is uniquely determined,
and we need only consider uniqueness of the decomposition of the finite abelian
group T .
   For p prime the elements of T of order pa for some a are those in the sum of
                 l
the groups Z/ p jj Z for which p j = p, and we are reduced to considering a group
                              H = Z/ pl1 Z ⊕ · · · ⊕ Z/ pl M 0 Z
with p fixed and l1 ≤ · · · ≤ l M 0 . The set of p j powers of elements of H
is a subgroup of H and is given by Z/ plt − j Z ⊕ · · · ⊕ Z/ pl M 0 − j Z if lt is the
first index ∏ j, while the set of p j+1 powers of elements of H is given by
Z/ plt 0 − j−1 Z ⊕ · · · ⊕ Z/ pl M 0 − j Z if lt 0 is the first index ∏ j + 1. Therefore Lemma
4.58 gives
p j H/ p j+1H ∼  = (Z/ plt 0 − j−1 Z)/(Z/ plt 0 − j Z)⊕· · ·⊕(Z/ pl M 0 − j−1 Z)/(Z/ pl M 0 − j Z).
Each term of p j H/ p j+1 H has order p, and thus
                                | p j H/ p j+1 H | = p|{i | li > j}| .
Hence H determines the integers l1 , . . . , l M 0 , and uniqueness is proved.                  §
                                 10. Sylow Theorems                             185

                              10. Sylow Theorems

This section continues the use of group actions to obtain results concerning
structure theory for abstract groups. We shall prove the three Sylow Theorems,
which are a starting point for investigations of the structure of finite groups that
are deeper than those in Sections 6 and 7. We state the three theorems as the parts
of Theorem 4.59.

   Theorem 4.59 (Sylow Theorems). Let G be a finite group of order pm r, where
p is prime and p does not divide r. Then
    (a) G contains a subgroup of order pm , and any subgroup of G of order pl
        with 0 ≤ l < m is contained in a subgroup of order pm ,
    (b) any two subgroups of order pm in G are conjugate in G, i.e., any two
        such subgroups P1 and P2 have P2 = a P1 a −1 for some a ∈ G,
    (c) the number of subgroups of order pm is of the form pk + 1 and divides r.
   REMARK. A subgroup of order pm as in the theorem is called a Sylow
p-subgroup of G. A consequence of (a) when m ∏ 1 is that G has a subgroup
of order p; this special case is sometimes called Cauchy’s Theorem in group
theory.

   Before coming to the proof, let us carefully give two simple applications
to structure theory. The applications combine Theorem 4.59, some results of
Sections 6 and 7, and Problems 35–38 and 45–48 at the end of the chapter.

   Proposition 4.60. If p and q are primes with p < q, then there exists a
nonabelian group of order pq if and only if p divides q − 1, and in this case the
nonabelian group is unique up to isomorphism. It may be taken to be a semidirect
product of the cyclic groups C p and Cq with Cq normal.
   REMARK. It follows from Theorem 4.56 that the only abelian group of order
pq, up to isomorphism, is C p × Cq ∼  = C pq . If p = 2 in the proposition, then q
is odd and p divides q − 1; the proposition yields the dihedral group Dq . For
p > 2, the divisibility condition may or may not hold: For pq = 15, the condition
does not hold, and hence every group of order 15 is cyclic. For pq = 21, the
condition does hold, and there exists a nonabelian group of order 21; this group
was constructed explicitly in Example 2 in Section 7.
  PROOF. Existence of a nonabelian group of order pq, together with the
semidirect-product structure, is established by Proposition 4.46 if p divides q −1.
Let us see uniqueness and the necessity of the condition that p divide q − 1.
  If G has order pq, Theorem 4.59a shows that G has a Sylow p-subgroup Hp
and a Sylow q-subgroup Hq . Corollary 4.9 shows that these two groups are cyclic.
186                          IV. Groups and Group Actions

The conjugates of Hq are Sylow q-subgroups, and Theorem 4.59c shows that the
number of such conjugates is of the form kq + 1 and divides p. Since p < q,
k = 0. Therefore Hq is normal. (Alternatively, one can apply Proposition 4.36
to see that Hq is normal.)
   Each element of G is uniquely a product ab with a in Hp and b in Hq . For the
uniqueness, if a1 b1 = a2 b2 , then a2−1 a1 = b2 b1−1 is an element of Hp ∩ Hq . Its
order must divide both p and q and hence must be 1. Thus the pq products ab
with a in Hp and b in Hq are all different. Since the number of them equals the
order of G, every member of G is such a product. By Proposition 4.44, G is a
semidirect product of Hp and Hq .
   If the action of Hp on Hq is nontrivial, then Problem 37 at the end of the chapter
shows that p divides q − 1, and Problem 38 shows that the group is unique up
to isomorphism. On the other hand, if the action is trivial, then G is certainly
abelian.                                                                           §

  Proposition 4.61. If G is a group of order 12, then G contains a subgroup
H of order 3 and a subgroup K of order 4, and at least one of them is normal.
Consequently there are exactly five groups of order 12, up to isomorphism—two
abelian and three nonabelian.
   REMARK. The second statement follows from the first, as a consequence of
Problems 45–48 at the end of the chapter. Those problems show how to construct
the groups.
    PROOF. Theorem 4.59a shows that H may be taken to be a Sylow 3-subgroup
and K may be taken to be a Sylow 2-subgroup. We have to prove that either H
or K is normal.
    Suppose that H is not normal. Theorem 4.59c shows that the number of
Sylow 3-subgroups is of the form 3k + 1 and divides 4. The subgroup H , not
being normal, fails to equal one of its conjugates, which will be another Sylow
3-subgroup; hence k > 0. Therefore k = 1, and there are four Sylow 3-subgroups.
The intersection of any two such subgroups is a subgroup of both and must be
trivial since 3 is prime. Thus the set-theoretic union of the Sylow 3-subgroups
accounts for 4 · 2 + 1 elements. None of these elements apart from the identity
lies in K , and thus K contributes 3 further elements, for a total of 12. Thus
every element of G lies in K or a conjugate of H . Consequently K equals every
conjugate of K , and K is normal.                                             §

   Let us see where we are with classifying finite groups of certain orders, up to
isomorphism. A group of order p is cyclic by Corollary 4.9, and a group of order
p2 is abelian by Corollary 4.39. Groups of order pq are settled by Proposition
4.60. Thus for p and q prime, we know the structure of all groups of order p,
                                  10. Sylow Theorems                               187

p2 , and pq. Problems 39–44 at the end of the chapter tell us the structure of the
groups of order 8, and Proposition 4.61 and Problems 45–48 tell us the structure
of the groups of order 12. In particular, the table at the end of Section 1, which
gives examples of nonisomorphic groups of order at most 15, is complete except
for the one group of order 12 that is discussed in Problem 48.
   Problems 30–34 and 49–54 at the end of the chapter go in the direction of
classifying finite groups of certain other orders.
   Now we return to Theorem 4.59. The proof of the theorem makes use of the
theory of group actions as in Section 6. In fact, the proof of existence of Sylow
p-subgroups is just an elaboration of the argument used to prove Corollary 4.38,
saying that a group of prime-power order has a nontrivial center. The relevant
action for the existence part of the proof is the one (g, x) 7→ gxg −1 given by
conjugation of the elements of the group, the orbit of x being the conjugacy class
C`(x). Proposition 4.37 shows that |G| = | C`(x)||Z G (x)|, where Z G (x) is the
centralizer of x. Since the disjoint union of the conjugacy classes is all of |G|,
we have                                 X
                  |G| = |Z G | +                   |G|/|Z G (x j )|,
                                    representatives x j
                                 of each conjugacy class
                                     with | C`(x)|6=1

a formula sometimes called the class equation of G.

   PROOF OF EXISTENCE OF SYLOW p-SUBGROUPS IN THEOREM 4.59a. We induct
on |G|, the base case being |G| = 1. Suppose that existence holds for groups of
order < |G|. Without loss of generality suppose that m > 0, so that p divides
|G|.
   First suppose that p does not divide |Z G |. Referring to the class equation
of G, we see that p must fail to divide some integer |G|/|Z G (x j )| for which
|Z G (x j )| < |G|. Since pm is the exact power of p dividing |G|, we conclude that
pm divides this |Z G (x j )| and pm+1 does not. Since |Z G (x j )| < |G|, the inductive
hypothesis shows that Z G (x j ) has a subgroup of order pm , and this is a Sylow
p-subgroup of G.
   Now suppose that p divides |Z G |. The group Z G is finitely generated abelian,
hence is a direct sum of cyclic groups by Theorem 4.56. Thus Z G contains an
element c of order p. The cyclic group C generated by c then has order p. Being
a subgroup of Z G , C is normal in G. The group G/C has order pm−1r, and
the inductive hypothesis implies that G/C has a subgroup H of order pm−1 . If
ϕ : G → G/C denotes the quotient map, then ϕ −1 (H ) is a subgroup of G of
order |H || ker ϕ| = pm−1 p = pm .                                                   §

   For the remaining parts of Theorem 4.59, we make use of a different group
action. If 0 denotes the set of all subgroups of G, then G acts on 0 by conjugation:
188                         IV. Groups and Group Actions

(g, H ) 7→ g Hg −1 . The orbit of a subgroup of H consists of all subgroups
conjugate to H in G, and the isotropy subgroup at the point H in 0 is

                            {g ∈ G | g Hg −1 = H }.

This is a subgroup N (H ) of G known as the normalizer of H in G. It has the
properties that N (H ) ⊇ H and that H is a normal subgroup of N (H ). The
counting formula of Corollary 4.35 gives
                       Ø                 Ø
                       Ø{g Hg −1 | g ∈ G}Ø = |G/N (H )|.

Meanwhile, application of Lagrange’s Theorem (Theorem 4.7) to the three quo-
tients G/H , G/N (H ), and N (H )/H shows that

                        |G/H | = |G/N (H )||N (H )/H |,

with all three factors being integers.
  Now assume as in the statement of Theorem 4.59 that |G| = pm r with p prime
and p not dividing r. In this setting we have the following lemma.

  Lemma 4.62. If P is a Sylow p-subgroup of G and if H is a subgroup of the
normalizer N (P) whose order is a power of p, then H ⊆ P.
   PROOF. Since H ⊆ N (P) and P is normal in N (P), the set H P of products is
a group, by the same argument as used for Hp Hq in the proof of Proposition 4.60.
Then H P/P ∼   = H/(H ∩ P) by the Second Isomorphism Theorem (Theorem
4.14), and hence |H P/P| is some power pk of p. By Lagrange’s Theorem
(Theorem 4.7), |H P| = pm+k with k ∏ 0. Since no subgroup of G can have
order pl with l > m, we must have k = 0. Thus H P = P and H ⊆ P.               §

    PROOF OF THE REMAINDER OF THEOREM 4.59. Within the set 0 of all subgroups
of G, let 5 be the set of all subgroups of G of order pm . We have seen that 5 is
not empty. Since the conjugate of a subgroup has the same order as the subgroup,
5 is the union of orbits in 0 under conjugation by G. Thus we can restrict the
group action by conjugation from G × 0 → 0 to G × 5 → 5.
    Let P and P 0 be members of 5, and let 6 and 6 0 be the G orbits of P and
  0
P under conjugation. Suppose that 6 and 6 0 are distinct orbits of G. Let us
restrict the group action by conjugation from G × 5 → 5 to P × 5 → 5. The
G orbits 6 and 6 0 then break into P orbits, and the counting formula Corollary
4.35 says for each orbit that
                                             Ø                           Ø
      pm = |P| = #{subgroups in a P orbit} × Øisotropy subgroup within P Ø.
                             11. Categories and Functors                        189

Hence the number of subgroups in a P orbit is of the form pl for some l ∏ 0.
   Suppose that l = 0. Then the P orbit is some singleton set {P 00 }, and the
corresponding isotropy subgroup within P is all of P:

                    P = { p ∈ P | p P 00 p−1 = P 00 } ⊆ N (P 00 ).

Lemma 4.62 shows that P ⊆ P 00 , and therefore P = P 00 . Thus l = 0 only for the
P orbit {P}. In other words, the number of elements in any P orbit other than
{P} is divisible by p. Consequently |6| ≡ 1 mod p while |6 0 | ≡ 0 mod p, the
latter because 6 and 6 0 are assumed distinct. But this conclusion is asymmetric
in the G orbits 6 and 6 0 , and we conclude that 6 and 6 0 must coincide. Hence
there is only one G orbit in 5, and it has kp + 1 members for some k. This proves
parts (b) and (c) except for the fact that kp + 1 divides r.
    For this divisibility let us apply the counting formula Corollary 4.35 to the
orbit 6 of G. The formula gives |G| = |6| |isotropy subgroup|, and hence |6|
divides |G| = pm r. Since |6| = kp + 1, we have GCD(|6|, p) = 1 and also
GCD(|6|, pm ) = 1. By Corollary 1.3, kp + 1 divides r.
    Finally we prove that any subgroup H of G of order pl lies in some Sylow
p-subgroup. Let 6 = 5 again be the G orbit in 0 of subgroups of order pm ,
and restrict the action by conjugation from G × 6 → 6 to H × 6 → 6. Each
H orbit in 6 must have pa elements for some a, by one more application of the
counting formula Corollary 4.35. Since |6| ≡ 1 mod p, some H orbit has one
element, say the H orbit of P. Then the isotropy subgroup of H at the point P
is all of H , and H ⊆ N (P). By Lemma 4.62, H ⊆ P. This completes the proof
of Theorem 4.59.                                                               §


                         11. Categories and Functors

The mathematics thus far in the book has taken place in several different contexts,
and we have seen that the same notions sometimes recur in more than one context,
possibly with variations. For example we have worked with vector spaces, inner-
product spaces, groups, rings, and fields, and we have seen that each of these areas
has its own definition of isomorphism. In addition, the notion of direct product
or direct sum has arisen in more than one of these contexts, and there are other
similarities. In this section we introduce some terminology to make the notion
of “context” precise and to provide a setting for discussing similarities between
different contexts.
   A category C consists of three things:
     • a class of objects, denoted by Obj(C ),
     • for any two objects A and B in the category, a set Morph(A, B) of
        morphisms,
190                          IV. Groups and Group Actions

      • for any three objects A, B, and C in the category, a law of composition
        for morphisms, i.e., a function carrying Morph(A, B)×Morph(B, C) into
        Morph(A, C), with the image of ( f, g) under composition written as g f ,
and these are to satisfy certain properties that we list in a moment. When more
than one category is under discussion, we may use notation like MorphC (A, B)
to distinguish between the categories.
    We are to think initially of the objects as the sets we are studying with a par-
ticular kind of structure on them; the morphisms are then the functions from one
object to another that respect this additional structure, and the law of composition
is just composition of functions. Indeed, the defining conditions that are imposed
on general categories are arranged to be obvious for this special kind of category,
and this setting accounts for the order in which we write the composition of two
morphisms. But the definition of a general category is not so restrictive, and it is
important not to restrict the definition in this way.
    The properties that are to be satisfied to have a category are as follows:
     (i) the sets Morph(A1 , B1 ) and Morph(A2 , B2 ) are disjoint unless A1 =
         A2 and B1 = B2 (because two functions are declared to be different
         unless their domains match and their ranges match, as is underscored in
         Section A1 of the appendix),
    (ii) the law of composition satisfies the associativity property h(g f ) = (hg) f
         for f ∈ Morph(A, B), g ∈ Morph(B, C), and h ∈ Morph(C, D),
   (iii) for each object A, there is an identity morphism 1 A in Morph(A, A) such
         that f 1 A = f and 1 A g = g for f ∈ Morph(A, B) and g ∈ Morph(C, A).
A subcategory S of a category C by definition is a category with Obj(S ) ⊆ Obj(C )
and MorphS (A, B) ⊆ MorphC (A, B) whenever A and B are in Obj(S ), and it
is assumed that the laws of composition in S and C are consistent when both are
defined.
   Here are several examples in which the morphisms are functions and the law
of composition is ordinary composition of functions. They are usually identified
in practice just by naming their objects, since the morphisms are understood to
be all functions from one object to another respecting the additional structure on
the objects.

  EXAMPLES OF CATEGORIES.
 (1) The category of all sets. An object A is a set, and a morphism in the set
Morph(A, B) is a function from A into B.
  (2) The category of all vector spaces over a field F. The morphisms are linear
maps.
  (3) The category of all groups. The morphisms are group homomorphisms.
                                     11. Categories and Functors                                      191

    (4) The category of all abelian groups. The morphisms again are group
homomorphisms. This is a subcategory of the previous example.
    (5) The category of all rings. The morphisms are all ring homomorphisms.
The kernel and the image of a morphism are necessarily objects of the category.
    (6) The category of all rings with identity. The morphisms are all ring homo-
morphisms carrying identity to identity. This is a subcategory of the previous
example. The image of a morphism is necessarily an object of the category, but
the kernel of a morphism is usually not in the category.
    (7) The category of all fields. The morphisms are as in Example 6, and the
result is a subcategory of Example 6. In this case any morphism is necessarily
one-one and carries inverses to inverses.
    (8) The category of all group actions by a particular group G. If G acts on X
and on Y , then a morphism from the one space to the other is a G equivariant
mapping from X to Y , i.e., a function ϕ : X → Y such that ϕ(gx) = gϕ(x) for
all x in X.
    (9) The category of all representations by a particular group G on a vector space
over a particular field F. The morphisms are the linear G equivariant functions.
This is a subcategory of the previous example.

    Readers who are familiar with point-set topology will recognize that one can
impose topologies on everything in the above examples, insisting that the func-
tions be continuous, and again we obtain examples of categories. For example the
category of all topological spaces consists of objects that are topological spaces
and morphisms that are continuous functions. The category of all continuous
group actions by a particular topological group has objects that are group actions
G × X → X that are continuous functions, and the morphisms are the equivariant
functions that are continuous.
    Readers who are familiar with manifolds will recognize that another example
is the category of all smooth manifolds, which consists of objects that are smooth
manifolds and morphisms that are smooth functions.
    The morphisms in a category need not be functions in the usual sense. An
important example is the “opposite category” C opp to a category C, which is a
handy technical device and is discussed in Problems 78–80 at the end of the
chapter.
    In all of the above examples of categories, the class of objects fails to be a set.
This behavior is typical. However, it does not cause problems in practice because
in any particular argument involving categories, we can restrict to a subcategory
for which the objects do form a set.17
    17 For the interested reader, a book that pays closer attention to the inherent set-theoretic difficul-

ties in the theory is Mac Lane’s Categories for the Working Mathematician.
192                                IV. Groups and Group Actions

    If C is a category, a morphism ϕ ∈ Morph(A, B) is said to be an isomorphism if
there exists a morphism √ ∈ Morph(B, A) such that √ϕ = 1 A and ϕ√ = 1 B . In
this case we say that A is isomorphic to B in the category C. Let us check that the
morphism √ is unique if it exists. In fact, if √ 0 is a member of Morph(B, A) with
√ 0 ϕ = 1 A and ϕ√ 0 = 1 B , then √ = 1 A √ = (√ 0 ϕ)√ = √ 0 (ϕ√) = √ 0 1 B = √ 0 .
We can therefore call √ the inverse to ϕ.
    The relation “is isomorphic to” is an equivalence relation.18 In fact, the relation
is symmetric by definition, and it is reflexive because 1 A ∈ Morph(A, A) has 1 A
as inverse. For transitivity let ϕ1 ∈ Morph(A, B) and ϕ2 ∈ Morph(B, C) be iso-
morphisms, with respective inverses √1 ∈ Morph(B, A) and √2 ∈ Morph(C, B).
Then ϕ2 ϕ1 is in Morph(A, C), and √1 √2 is in Morph(C, A). Calculation gives
(√1 √2 )(ϕ2 ϕ1 ) = √1 (√2 (ϕ2 ϕ1 )) = √1 ((√2 ϕ2 )ϕ1 ) = √1 (1 B ϕ1 ) = √1 ϕ1 = 1 A ,
and similarly (ϕ2 ϕ1 )(√1 √2 ) = 1C . Therefore ϕ2 ϕ1 ∈ Morph(A, C) is an isomor-
phism, and “is isomorphic to” is an equivalence relation. When A is isomorphic
to B, it is permissible to say that A and B are isomorphic.
    The next step is to abstract a frequent kind of construction that we have
used with our categories. If C and D are two categories, a covariant functor
F : C → D associates to each object A in Obj(C ) an object F(A) in Obj(D) and
to each pair of objects A and B and morphism f in MorphC (A, B) a morphism
F( f ) in MorphD (F(A), F(B)) such that
      (i) F(g f ) = F(g)F( f ) for f ∈ MorphC (A, B) and g ∈ MorphC (B, C),
     (ii) F(1 A ) = 1 F(A) for A in Obj(C ).

   EXAMPLES OF COVARIANT FUNCTORS.
   (1) Inclusion of a subcategory into a category is a covariant functor.
   (2) Let C be the category of all sets. If F carries each set X to the set 2 X of
all subsets of X, then F is a covariant functor as soon as its effect on functions
between sets, i.e., its effect on morphisms, is defined in an appropriate way.
Namely, if f : X → Y is a function, then F( f ) is to be a function from
F(X) = 2 X to F(Y ) = 2Y . That is, we need a definition of F( f )(A) as a subset
of Y whenever A is a subset of X. A natural way of making such a definition is
to put F( f )(A) = f (A), and then F is indeed a covariant functor.
   (3) Let C be any of Examples 2 through 6 of categories above, and let D be
the category of all sets, as in Example 1 of categories. If F carries an object A in
C (i.e., a vector space, group, ring, etc.) into its underlying set and carries each
morphism into its underlying function between two sets, then F is a covariant
functor and furnishes an example of what is called a forgetful functor.
    18 Technically one considers relations only when they are defined on sets, and the class of objects

in a category is typically not a set. However, just as with vector spaces, groups, and so on, we can
restrict attention in any particular situation to a subcategory for which the objects do form a set, and
then there is no difficulty.
                                11. Categories and Functors                            193

   (4) Let C be the category of all vector spaces over a field F, let U be a
vector space over F, and let F : C → C be defined on a vector space to
be the vector space of linear maps F(V ) = HomF (U, V ). The set of mor-
phisms MorphC (V1 , V° 2 ) is HomF (V1 , V2 ). If f is in¢ MorphC (V1 , V2 ), then F( f )
is to be in MorphC HomF (U, V1 ), HomF (U, V2 ) , and the definition is that
F( f )(L) = f ◦ L for L ∈ HomF (U, V1 ). Then F is a covariant functor:
to check that F(g f ) = F(g)F( f ) when g is in MorphC (V2 , V3 ), we write
F(g f )(L) = g f ◦ L = g ◦ f L = g ◦ F( f ) = F(g)F( f ).
   (5) Let C be the category of all groups, let D be the category of all sets, let G
be a group, and let F : C → D be the functor defined as follows. For a group
H , F(H ) is the set of all group homomorphisms from G into H . The set of
morphisms MorphC (H1 , H2 ) is the set of group homomorphisms from H1 into
H2 . If f is in MorphC (H1 , H2 ), then F( f ) is to be a function with domain the set
of homomorphisms from G into H1 and with range the set of homomorphisms
from G into H2 . Let F( f )(ϕ) = ϕ ◦ f . Then F is a covariant functor.
   (6) Let C be the category of all sets, and let D be the category of all abelian
groups. To a set S, associate the free abelian group F(S) with S as Z basis.
If f : S → S 0 is a function, then the universal mapping property of external
direct sums of abelian groups (Proposition 4.17) yields a corresponding group
homomorphism from F(S) to F(S 0 ), and we define this group homomorphism
to be F( f ). Then F is a covariant functor.
   (7) Let C be the category of all finite sets, fix a commutative ring R with
identity, and let D be the category of all commutative rings with identity. To
a finite set S, associate the commutative ring F(S) = R[{X s | s ∈ S}]. If
 f : S → S 0 is a function, then the properties of substitution homomorphisms
give us a corresponding homomorphism of rings with identity carrying F(S) to
F(S 0 ), and the result is a covariant functor.

   There is a second kind of functor of interest to us. If C and D are two categories,
a contravariant functor F : C → D associates to each object A in Obj(C ) an
object F(A) in Obj(D) and to each pair of objects A and B and morphism f in
MorphC (A, B) a morphism F( f ) in MorphD (F(B), F(A)) such that
     (i) F(g f ) = F( f )F(g) for f ∈ MorphC (A, B) and g ∈ MorphD (B, C),
    (ii) F(1 A ) = 1 F(A) for A in Obj(C ).

   EXAMPLES OF CONTRAVARIANT FUNCTORS.
   (1) Let C be the category of all vector spaces over a field F, let W be a
vector space over F, and let F : C → C be defined on a vector space to be
the vector space of linear maps F(V ) = HomF (V, W ). The set of morphisms
MorphC (V1 , V2 ) is HomF (V1 , V2 ). If f is in MorphC (V1 , V2 ), then F( f ) is to be in
194                          IV. Groups and Group Actions
           °                                  ¢
MorphC HomF (V2 , W ), HomF (V1 , W ) , and the definition is that F( f )(L) =
L ◦ f for L ∈ HomF (V2 , W ). Then F is a contravariant functor: to check
that F(g f ) = F( f )F(g) when g is in MorphC (V2 , V3 ), we write F(g f )(L) =
L ◦ g f = Lg ◦ f = F( f )(Lg) = F( f )F(g)(L).
   (2) Let C be the category of all vector spaces over a field F, define F of a
vector space V to be the dual vector space V 0 , and define F of a linear mapping
 f between two vector spaces V and W to be the contragredient f t carrying W 0
into V 0 , defined by f t (w0 )(v) = w0 ( f (v)). This is the special case of Example 1
of contravariant functors in which W = F. Hence F is a contravariant functor.
   (3) Let C be the category of all groups, let D be the category of all sets, let G
be a group, and let F : C → D be the functor defined as follows. For a group
H , F(H ) is the set of all group homomorphisms from H into G. The set of
morphisms MorphC (H1 , H2 ) is the set of group homomorphisms from H1 into
H2 . If f is in MorphC (H1 , H2 ), then F( f ) is to be a function with domain the set
of homomorphisms from H2 into G and with range the set of homomorphisms
from H1 into G. The definition is F( f )(ϕ) = ϕ ◦ f . Then F is a contravariant
functor.

   It is an important observation about functors that the composition of two
functors is a functor. This is immediate from the definition. If the two functors
are both covariant or both contravariant, then the composition is covariant. If
one of them is covariant and the other is contravariant, then the composition is
contravariant.
                                          α
                                    A −−−→      B
                                               
                                    
                                   βy
                                                ∞
                                                y

                                    C −−−→ D
                                          δ

       FIGURE 4.9. A square diagram. The square commutes if ∞ α = δβ.

   In the subject of category theory, a great deal of information is conveyed by
“commutative diagrams” of objects and morphisms. By a diagram is meant a
directed graph, usually but not necessarily planar, in which the vertices represent
some relevant objects in a category and the arrows from one vertex to another
represent morphisms of interest between pairs of these objects. Often the vertices
and arrows are labeled, but in fact labels on the vertices can be deduced from the
labels on the arrows since any morphism determines its “domain” and “range”
as a consequence of defining property (i) of categories. A diagram is said to be
commutative if for each pair of vertices A and B and each directed path from
A to B, the compositions of the morphisms along each path are the same. For
                              11. Categories and Functors                       195

example a square as in Figure 4.9 is commutative if ∞ α = δβ. The triangular
diagrams in Figures 4.1 through 4.8 are all commutative.
                   F(α)                                     G(α)
           F(A) −−−→ F(B)                             G(A) √−−− G(B)
                                                      x        x
                      F(∞ )                                   G(∞ )
         F(β)y        y                  and        G(β)        

           F(C) −−−→ F(D)                             G(C) √−−− G(D)
                    F(δ)                                    G(δ)

        FIGURE 4.10. Diagrams obtained by applying a covariant functor F
            and a contravariant functor G to the diagram in Figure 4.9.

   Functors can be applied to diagrams, yielding new diagrams. For example,
suppose that Figure 4.9 is a diagram in the category C, that F : C → D is a
covariant functor, and that G : C → D is a contravariant functor. Then we
can apply F and G to the diagram in Figure 4.9, obtaining the two diagrams in
the category D that are pictured in Figure 4.10. If the diagram in Figure 4.9 is
commutative, then so are the diagrams in Figure 4.10, as a consequence of the
effect of functors on compositions of morphisms.
   The subject of category theory seeks to analyze functors that make sense for
all categories, or at least all categories satisfying some additional properties.
The most important investigation of this kind is concerned with homology and
cohomology, as well as their ramifications, for “abelian categories,” which include
several important examples affecting algebra, topology, and several complex
variables. The topic in question is called “homological algebra” and is discussed
further in Advanced Algebra, particularly in Chapter IV.
   There are a number of other functors that are investigated in category theory,
and we mention four:
    •   products, including direct products,
    •   coproducts, including direct sums,
    •   direct limits, also called inductive limits,
    •   inverse limits, also called projective limits.
We discuss general products and coproducts in the present section, omitting a
general discussion of direct limits and inverse limits. Inverse limits will arise in
Section VII.6 of Advanced Algebra for one category in connection with Galois
groups, but we shall handle that one situation on its own without attempting a
generalization. An attempt in the 1960s to recast as much mathematics as possible
in terms of category theory is now regarded by many mathematicians as having
been overdone, and it seems wiser to cast bodies of mathematics in the framework
of category theory only when doing so can be justified by the amount of time saved
by eliminating redundant arguments.
196                                IV. Groups and Group Actions

     When a category C and a nonempty set S are given, we can define a category
C S . The objects of C S are functions on S with the property that the value of the
function at each s in S is in Obj(C ), two such functions being regarded as the
same if they consist of the same ordered pairs.19 Let us refer to such a function
as an S-tuple of members° of Obj(C ), denoting  ¢     it by an expression like {X s }s∈S .
A morphism in MorphCS {X s }s∈S , {Ys }s∈S is an S-tuple { f s }s∈S of morphisms
of C such that f s lies in MorphC (X s , Ys ) for all s, and the law of composition of
such morphisms takes place coordinate by coordinate.
     Let {X s }s∈S be an object in C S . A product of {X s }s∈S is a pair (X, { ps }s∈S )
such that X is in Obj(C ) and each ps is in MorphC (X, X s ) with the following
universal mapping property: whenever A in Obj(C ) is given and a morphism
ϕs ∈ MorphC (A, X s ) is given for each s, then there exists a unique morphism
ϕ ∈ MorphC (A, X) such that ps ϕ = ϕs for all s. The relevant diagram is pictured
in Figure 4.11.
                                                  ϕs
                                         X s √−−− A
                                           x
                                           
                                        ps      ϕ


                                          X
       FIGURE 4.11. Universal mapping property of a product in a category.

   EXAMPLES OF PRODUCTS.
   (1) Products exist in the category of vector spaces over a field F. If vector
                                                         Qproduct exists in the
spaces Vs indexed by a nonempty set S are given, then their
category, and an example is their external direct product s∈S Vs , according to
Figure 2.4 and the discussion around it.
    (2) Products exist in the category of all groups. If groups G s indexed by a
                                Q product exists in the category, and an example
nonempty set S are given, then their
is their external direct product s∈S G s , according
                                           Q         to Figure 4.2 and Proposition
4.15. If the groups G s are abelian, then s∈S G s is abelian, and it follows that
products exist in the category of all abelian groups.
   (3) Products exist in the category of all sets. If sets X s indexed by a nonempty
set S are given, then their product exists in the category, and an example is their
Cartesian product ×s∈S X s , as one easily checks.
   (4) Products exist in the category of all rings and in the category of all rings with
identity. If objects Rs in the category indexed by a nonempty set S are given, then
   19 In other words, the range of such a function is considered as irrelevant. We might think of the
range as Obj(C) except for the fact that a function is supposed to have a set as range and Obj(C) need
not be a set.
                              11. Categories and Functors                          197

their
Q product may be taken as an abelian group to be the external direct product
  s∈S Rs , with multiplication defined coordinate by coordinate, and the group
homomorphisms ps are easily checked to be morphisms in the category.

   A product of objects in a category need not exist in the category. An artificial
example may be formed as follows: Let C be a category with one object G, namely
a group of order 2, and let Morph(G, G) = {0, 1G }, the law of composition being
the usual composition. Let S be a 2-element set, and let the corresponding objects
be X 1 = G and X 2 = G. The claim is that the product X 1 × X 2 does not exist in C.
In fact, take A = G. There are four S-tuples of morphisms (ϕ1 , ϕ2 ) meeting the
conditions of the definition. Yet the only possibility for the product is X = G, and
then there are only two possible ϕ’s in Morph(A, X). Hence we cannot account
for all possible S-tuples of morphisms, and the product cannot exist.
   The thing that category theory addresses is the uniqueness. A product is
always unique up to canonical isomorphism, according to Proposition 4.63. We
proved uniqueness for products in the special cases of Examples 1 and 2 above
in Propositions 2.32 and 4.16.

   Proposition 4.63. Let C be a category, and let S be a nonempty set. If {X s }s∈S
is an object in C S and if (X, { ps }) and (X 0 , { ps0 }) are two products, then there
exists a unique morphism 8 : X 0 → X such that ps0 = ps ◦ 8 for all s ∈ S, and
8 is an isomorphism.
  REMARK. There is no assertion that ps is onto X s . In fact, “onto” has no
meaning for a general category.
    PROOF. In Figure 4.11 let A = X 0 and ϕs = ps0 . If 8 ∈ Morph(X 0 , X)
is the morphism produced by the fact that X is a direct product, then we have
ps 8 = ps0 for all s. Reversing the roles of X and X 0 , we obtain a morphism
80 ∈ Morph(X, X 0 ) with ps0 80 = ps for all s. Therefore ps (880 ) = ( ps 8)80 =
ps0 80 = ps .
    In Figure 4.11 we next let A = X and ϕs = ps for all s. Then the identity 1 X
in Morph(X, X) has the same property ps 1 X = ps relative to all ps that 880 has,
and the uniqueness in the statement of the universal mapping property implies that
880 = 1 X . Reversing the roles of X and X 0 , we obtain 80 8 = 1 X 0 . Therefore
8 is an isomorphism.
    For uniqueness suppose that 9 ∈ Morph(X 0 , X) is another morphism with
  0
ps = ps 9 for all s ∈ S. Then the argument of the previous paragraph shows that
80 9 = 1 X 0 . Consequently 9 = 1 X 9 = (880 )9 = 8(80 9) = 81 X 0 = 8, and
9 = 8.                                                                          §

  If products always exist in a particular category, they are not unique, only
unique up to canonical isomorphism. Such a product is commonly denoted by
198                                IV. Groups and Group Actions
Q
   s∈S X s , even though it is not uniquely defined. It is customary to treat the
                                                   S
product over S as a covariant functor F     Q : C → C, the effect of the functor on
objects being given by F({X s }s∈S ) = s∈S X s . For a well-defined functor we
have to fix a choice of product for each object under consideration20 in Obj(C S ).
For the effect of F on morphisms, we argue with the universal mapping property.
                                                 S
                                        ° Qin C , let f s be in
Thus let {X s }s∈S and {Ys }s∈S be objects                            ° QC (X s , Ys ) for all
                                                                 ¢ Morph                    ¢
s, and let the products in question
                            °Q      be       s∈S
                                             ¢   X  s , { p }
                                                           s s∈S  and    s∈S Ys , {qs }s∈S .
Then f s0 ps0 is in MorphC      s∈S
                                 °Q X s , Ys0 for each ¢s0 , and the universal mapping
                                               Q
property gives us f in MorphC        s∈S X s ,    s∈S Ys such that qs f = f s ps for all
s. We define this f to be F({ f s }s∈S ), and we readily check that F is a functor.
     We turn to coproducts, which include direct sums. Let {X s }s∈S be an object in
C S . A coproduct of {X s }s∈S is a pair (X, {i s }s∈S ) such that X is in Obj(C ) and
each i s is in MorphC (X s , X) with the following universal mapping property:
whenever A in Obj(C ) is given and a morphism ϕs ∈ MorphC (X s , A) is given
for each s, then there exists a unique morphism ϕ ∈ MorphC (X, A) such that
ϕi s = ϕs for all s. The relevant diagram is pictured in Figure 4.12.
                                                  ϕs
                                          X s −−−→ A
                                            
                                            
                                         is y     ϕ


                                          X
      FIGURE 4.12. Universal mapping property of a coproduct in a category.

    EXAMPLES OF COPRODUCTS.
   (1) Coproducts exist in the category of vector spaces over a field F. If vector
                                                          L coproduct exists in
spaces Vs indexed by a nonempty set S are given, then their
the category, and an example is their external direct sum s∈S Vs , according to
Figure 2.5 and the discussion around it.
   (2) Coproducts exist in the category of all abelian groups. If abelian groups G s
indexed by a nonempty set S are given, thenL  their coproduct exists in the category,
and an example is their external direct sum s∈S G s , according to Figure 4.4 and
Proposition 4.17.
    (3) Coproducts exist in the category of all sets. If sets X s indexed by a nonempty
                then their coproduct exists in the category, and an example is their
set S are given,S
disjoint union s∈S {(xs , s) | xs ∈ X s }. The verification appears as Problem 74
at the end of the chapter.
    20 SinceObj(C S ) need not be a set, it is best to be wary of applying the Axiom of Choice when
the indexing of sets is given by Obj(C S ). Instead, one makes the choice only for all objects in some
set of objects large enough for a particular application.
                               11. Categories and Functors                           199

   (4) Coproducts exist in the category of all groups. Suppose that groups G s
indexed by a nonempty set S are given. It will be shown in Chapter VII that
                                       *
the coproduct is the “free product” s∈S G s that is defined in that chapter. In the
special case that each G s is the group Z of integers, the free product coincides
with the free group on S. Therefore, even if all the groups G s are abelian, their
coproduct need not be a subgroup of the direct product and need not even be
abelian. In particular it need not coincide with the direct sum.

    A coproduct of objects in a category need not exist in the category. Problem 76
at the end of the chapter offers an example that the reader is invited to check.

    Proposition 4.64. Let C be a category, and let S be a nonempty set. If {X s }s∈S
is an object in C S and if (X, {i s }) and (X 0 , {i s0 }) are two coproducts, then there
exists a unique morphism 8 : X → X 0 such that i s0 = 8 ◦ i s for all s ∈ S, and 8
is an isomorphism.
   REMARKS. There is no assertion that i s is one-one. In fact, “one-one” has
no meaning for a general category. This proposition may be derived quickly
from Proposition 4.63 by a certain duality argument that is discussed in Problems
78–80 at the end of the chapter. Here we give a direct argument without taking
advantage of duality.
    PROOF. In Figure 4.12 let A = X 0 and ϕs = i s0 . If 8 ∈ Morph(X, X 0 ) is the
morphism produced by the fact that X is a coproduct, then we have 8 i s = i s0 for
all s. Reversing the roles of X and X 0 , we obtain a morphism 80 ∈ Morph(X 0 , X)
with 80 i s0 = i s for all s. Therefore (80 8)i s = 80 i s0 = i s .
    In Figure 4.12 we next let A = X and ϕs = i s for all s. Then the identity 1 X
in Morph(X, X) has the same property 1 X i s = i s relative to all i s that 80 8 has,
and the uniqueness says that 80 8 = 1 X . Reversing the roles of X and X 0 , we
obtain 880 = 1 X 0 . Therefore 8 is an isomorphism.
    For uniqueness suppose that 9 ∈ Morph(X, X 0 ) is another morphism with
  0
i s = 9 i s for all s ∈ S. Then the argument of the previous paragraph shows that
80 9 = 1 X . Consequently 9 = 1 X 0 9 = (880 )9 = 8(80 9) = 81 X = 8, and
9 = 8.                                                                             §
    If coproducts always exist in a particular category, they are not unique, only
unique
`        up to canonical isomorphism. Such a coproduct is commonly denoted by
   s∈S X s , even though it is not uniquely defined. As with product, it is customary
to treat the coproduct over S as a covariant functor`   F : C S → C, the effect of the
functor on objects being given by F({X s }s∈S ) = s∈S X s . For a well-defined
functor we have to fix a choice of coproduct for each object under consideration
in Obj(C S ). For the effect of F on morphisms, we argue with the universal
mapping property. Thus let {X s }s∈S and {Ys }s∈S be objects in C S , let f s be in
200                             IV. Groups and Group Actions
                                                                        °`                   ¢
    ° `C (X s , Ys ) for all¢s, and let the coproducts in °question
Morph                                                               be ¢ s∈S X s , {i s }s∈S
                                                                 `
and       s∈S Ys , { js }s∈S . Then js0 f s0 is in MorphC X s0 ,° `s∈S Ys for   each s¢0 , and
                                                                              `
the universal mapping property gives us f in MorphC                 s∈S X s ,  s∈S Ys such
that f i s = js f s for all s. We define this f to be F({ f s }s∈S ), and we readily check
that F is a functor.
   Universal mapping properties occur in other contexts than for products and
coproducts. We have already seen them in connection with homomorphisms on
free abelian groups and with substitution homomorphisms on polynomial rings,
and more such properties will occur in the development of tensor products in
Chapter VI. A general framework for discussing universal mapping properties
appears in the problems at the end of Chapter VI.


                                      12. Problems

1.    Let G be a group in which all elements other than the identity have order 2. Prove
      that G is abelian.
2.    The dihedral group D4 of order 8 can be viewed as a subgroup of the symmetric
      group S4 of order 8. Find 8 explicit permutations in S4 forming a subgroup
      isomorphic to D4 .
2A. Let g be an element of finite order ord(g) in a group G. Prove that
    (a) g −1 has the same order as g.
    (b) g k = 1 if and only if ord(g) divides k.
    (c) for each r ∈ Z, the order of gr is ord(g)/GCD(ord(g), r).
3.    Suppose G is a finite group, H is a subgroup, and a ∈ G is an element with a l
      in H for some integer l with GCD(l, |G|) = 1. Prove that a is in H .
4.    Let G be a group, and define a new group G 0 to have the same underlying set as
      G but to have multiplication given by a ◦ b = ba. Prove that G 0 is a group and
      that it is isomorphic to G.
5.    Prove that if G is an abelian group and n is an integer, then a 7→ a n is a
      homomorphism of G. Give an example of a nonabelian group for which a 7→ a 2
      is not a homomorphism.
6.    Suppose that G is a group and that H and K are normal subgroups of G with
      H ∩ K = {1}. Verify that the set H K of products is a subgroup and that this
      subgroup is isomorphic as a group to the external direct product H × K .
7.    Take as known that 8191 is prime, so that F8191 is a field. Without carrying
      through the computations and without advocating trial and error, describe what
      steps you would carry out to solve for x mod 8191 such that 1234x ≡ 1 mod
      8191.
                                     12. Problems                                 201

8.   (Wilson’s Theorem) Let p be an odd prime. Starting from the fact that
     1, . . . , p − 1 are roots of the polynomial X p−1 − 1 ≡ 0 mod p in F p , prove
     that ( p − 1)! ≡ −1 mod p.
9.   Classify, up to isomorphism, all groups of order p2 if p is a prime.
10. This problem concerns conjugacy classes in a group G.
    (a) Prove that all elements of a conjugacy class have the same order.
    (b) Prove that if ab is in a conjugacy class, so is ba.
11. (a) Find explicitly all the conjugacy classes in the alternating group A4 .
    (b) For each conjugacy class in A4 , find the centralizer of one element in the
        class.
    (c) Prove that A4 has no subgroup isomorphic to C6 or S3 .
12. Prove that the alternating group A5 has no subgroup of order 30.
13. Let G be a nonabelian group of order pn , where p is prime. Prove that any
    subgroup of order pn−1 is normal.
14. Let G be a finite group, and let H be a normal subgroup. If |H | = p and p is
    the smallest prime dividing |G|, prove that H is contained in the center of G.
15. Let G be a group. An automorphism of G of the form x 7→ gxg −1 is called an
    inner automorphism. Prove that the set of inner automorphisms is a normal
    subgroup of the group Aut G of all automorphisms and is isomorphic to G/Z G .
16. (a) Prove that Aut Cm is isomorphic to (Z/mZ)× .
    (b) Find a value of m for which Aut Cm is not cyclic.
17. Fix n ∏ 2. In the symmetric group Sn , for each integer k with 1 ≤ k ≤ n/2, let
    Ck be the set of elements in Sn that are products of k disjoint transpositions.
                            µ automorphism
    (a) Prove that if τ is an     ∂           of Sn , then τ (C1 ) = Ck for some k.
                               n (2k)!
    (b) Prove that |Ck | =                .
                              2k 2k k!
    (c) Prove that |Ck | 6= |C1 | unless k = 1 or n = 6. (Educational note: From this,
        it follows that τ (C1 ) = C1 except possibly when n = 6. One can deduce
        as a consequence that every automorphism of Sn is inner except possibly
        when n = 6.)
18. Give an example: G is a group with a normal subgroup N , N has a subgroup M
    that is normal in N , yet M is not normal in G.
19. Show that the cyclic group Crs is isomorphic to Cr × Cs if and only if
    GCD(r, s) = 1.
20. How many abelian groups, up to isomorphism, are there of order 27?
202                             IV. Groups and Group Actions

21. Let G be the free abelian group with Z basis {x1 , x2 , x3 }. Let H be the subgroup
    of G generated by {u 1 , u 2 , u 3 }, where
                                     u 1 = 3x1 + 2x2 + 5x3 ,
                                     u 2 = x2 + 3x3 ,
                                     u 3 = x2 + 5x3 .
      Express G/H as a direct sum of cyclic groups.
22. Let {e1 , e2 , e3 , e4 } be the standard basis of R4 . Let G be the additive subgroup
    of R4 generated by the four elements
                                1                                1
             e1 ,   e1 + e2 ,   2 (e1   + e2 + e3 + e4 ),        2 (e1   + e2 + e3 − e4 ),
      and let H be the subgroup of G generated by the four elements
                         e1 − e2 ,      e2 − e3 ,   e3 − e4 ,      e3 + e4 .
      Identify the abelian group G/H as a direct sum of cyclic groups.
23. Let G be the free abelian group with Z basis {x1 , . . . , xn }, and let H be the
                                                     √ u1 !     √ x1 !
                                                        ..          .
    subgroup generated by {u 1 , . . . , u m }, where .     = C .. for an m-by-n
                                                            um                xn
      matrix C of integers. Prove that the number of summands Z in the decomposition
      of G/H into cyclic groups is equal to the rank of the matrix C when C is
      considered as in Mmn (Q).
24. Prove that every abelian group is the homomorphic image of a free abelian group.
25. Let G be a group, and let H and K be subgroups.
    (a) For x and y in G, prove that x H ∩ y K is empty or is a coset of H ∩ K .
    (b) Deduce from (a) that if H and K have finite index in G, then so does H ∩ K .
26. Let G be a free abelian group of finite rank n, and let H be a free abelian subgroup
    of rank n. Prove that H has finite index in G.
27. Let G = S4 be the symmetric group on four letters.
    (a) Find a Sylow 2-subgroup of G. How many Sylow 2-subgroups are there,
        and why?
    (b) Find a Sylow 3-subgroup of G. How many Sylow 3-subgroups are there,
        and why?
28. Let H be a subgroup of a group G. Prove or disprove that the normalizer N (H )
    of H in G is a normal subgroup of G.
29. How many elements of order 7 are there in a simple group of order 168?
30. Let G be a group of order pq 2 , where p and q are primes with p < q. Let Sp
    and Sq be Sylow subgroups for the primes p and q. Prove that G is a semidirect
    product of Sp and Sq with Sq normal.
                                     12. Problems                                  203

31. Suppose that G is a finite group and that H is a subgroup whose index in G is
    a prime p. By considering the action of G on the set of subgroups conjugate
    to H and considering the possibilities for the normalizer N (H ), determine the
    possibilities for the number of subgroups conjugate to H .
32. Let G be a group of order 24, let H be a subgroup of order 8, and assume that H
    is not normal.
    (a) Using the Sylow Theorems, explain why H has exactly 3 conjugates in G,
         counting H itself as one.
    (b) Show how to use the conjugates in (a) to define a homomorphism of G into
         the symmetric group S3 on three letters.
    (c) Use the homomorphism of (b) to conclude that G is not simple.
33. Let G be a group of order 36. Arguing in the style of the previous problem, show
    that there is a nontrivial homomorphism of G into the symmetric group S4 .
34. Let G be a group of order 2 pq, where p and q are primes with 2 < p < q.
    (a) Prove that if q + 1 6= 2 p, then a Sylow q-subgroup is normal.
    (b) Suppose that q + 1 = 2 p, let H be a Sylow p-subgroup, and let K be a
        Sylow q-subgroup. Prove that at least one of H and K is normal, that the
        set H K of products is a subgroup, and that the subgroup H K is cyclic of
        index 2 in G.
Problems 35–38 concern the detection of isomorphisms among semidirect products.
For the first two of the problems, let H and K be groups, and let ϕ1 : H → Aut K
and ϕ2 : H → Aut K be homomorphisms.
35. Suppose that ϕ2 = ϕ1 ◦ϕ for some automorphism ϕ of H . Define √ : H ×ϕ2 K →
     H ×ϕ1 K by √(h, k) = (ϕ(h), k). Prove that √ is an isomorphism.
36. Suppose that ϕ2 = ϕ ◦ ϕ1 for some inner automorphism ϕ of Aut K in the sense
    of Problem 15, i.e., ϕ : Aut K → Aut K is to be given by ϕ(x) = axa −1 with a
    in Aut K . Define √ : H ×ϕ1 K → H ×ϕ2 K by √(h, k) = (h, a(k)). Prove that
    √ is an isomorphism.
37. Suppose that p and q are primes and that the cyclic group C p acts on Cq by
    automorphisms with a nontrivial action. Prove that p divides q − 1.
38. Suppose that p and q are primes such that p divides q − 1. Let τ1 and τ2
    be nontrivial homomorphisms from C p to Aut Cq . Prove that C p ×τ1 Cq ∼    =
    C p ×τ2 Cq , and conclude that there is only one nonabelian semidirect product
    C p ×τ Cq up to isomorphism.
Problems 39–44 discuss properties of groups of order 8, obtaining a classification of
these groups up to isomorphism.
39. Prove that the five groups C8 , C4 × C2 , C2 × C2 × C2 , D4 , and H8 are mutually
    nonisomorphic and that the first three exhaust the abelian groups of order 8, apart
    from isomorphisms.
204                           IV. Groups and Group Actions

40. (a) Find a composition series for the 8-element dihedral group D4 .
    (b) Find a composition series for the 8-element quaternion group H8 .
41. (a) Prove that every subgroup of the quaternion group H8 is normal.
    (b) Identify the conjugacy classes in H8 .
    (c) Compute the order of Aut H8 .
42. Suppose that G is a nonabelian group of order 8. Prove that G has an element of
    order 4 but no element of order 8.
43. Let G be a nonabelian group of order 8, and let K be the copy of C4 generated
    by some element of order 4. If G has some element of order 2 that is not in K ,
    prove that G ∼
                 = D4 .
44. Let G be a nonabelian group of order 8, and let K be the copy of C4 generated
    by some element of order 4. If G has no element of order 2 that is not in K ,
    prove that G ∼
                 = H8 .
Problems 45–48 classify groups of order 12, making use of Proposition 4.61, Prob-
lem 15, and Problems 35–38. Let G be a group of order 12, let H be a Sylow
3-subgroup, and let K be a Sylow 2-subgroup. Proposition 4.61 says that at least one
of H and K is normal. Consequently there are three cases, and these are addressed
by the first three of the problems.
45. Verify that there are only two possibilities for G up to isomorphism if G is abelian.
46. Suppose that K is normal, so that G ∼   = H ×τ K . Prove that either
          (i) τ is trivial or
         (ii) τ is nontrivial and K ∼
                                    = C2 × C2 ,
    and deduce that G is abelian if (i) holds and that G ∼= A4 if (ii) holds.
47. Suppose that H is normal, so that G = K ×τ H . Prove that one of the conditions
          (i) τ is trivial,
         (ii) K ∼= C2 × C2 and τ is nontrivial,
        (iii) K ∼= C4 and τ is nontrivial
    holds, and deduce that G is abelian if (i) holds, that G ∼  = D6 if (ii) holds, and
    that G is nonabelian and is not isomorphic to A4 or D6 if (iii) holds.
48. In the setting of the previous problem, prove that there is one and only one group,
    up to isomorphism, satisfying condition (iii), and find the order of each of its
    elements.
Problems 49–52 assume that p and q are primes with p < q. The problems go in the
direction of classifying finite groups of order p2 q.
49. If G is a group of order p2 q, prove that either p2 q = 12 or a Sylow q-subgroup
     is normal.
50. If p2 divides q −1, exhibit three nonabelian groups of order p2 q that are mutually
    nonisomorphic.
                                      12. Problems                                   205

51. If p divides q − 1 but p2 does not divide q − 1, exhibit two nonabelian groups
    of order p2 q that are not isomorphic.
52. If p does not divide q − 1, prove that any group of order p2 q is abelian.
Problems 53–54 concern nonabelian groups of order 27.
53. (a) Show that multiplication by the elements 1, 4, 7 mod 9 defines a nontrivial
        action of Z/3Z on Z/9Z by automorphisms.
    (b) Show from (a) that there exists a nonabelian group of order 27.
    (c) Show that the group in (b) is generated by elements a and b that satisfy

                                  a 9 = b3 = b−1 aba −4 = 1.

54. Show that any nonabelian group of order 27 having a subgroup H isomorphic to
    C9 and an element of order 3 not lying in H is isomorphic to the group constructed
    in the previous problem.
Problems 55–62 give a construction of infinitely many simple groups, some of them
finite and some infinite. Let F be a field. For n ∏ 2, let SL(n, F) be the special linear
group for the space Fn of n-dimensional column vectors. The center Z of SL(n, F)
consists of the scalar multiples of the identity, the scalar being an n th root of 1. Let
PSL(n, F) = SL(n, F)/Z . It is known that PSL(n, F) is simple except for PSL(2, F2 )
and PSL(2, F3 ). These problems will show that PSL(2, F) is simple if |F| > 5 and
F is not of characteristic 2. Most of the argument will consider SL(2, F), and the
passage to PSL will occur only at the very end. In Problems 56–61, G denotes a
normal subgroup of SL(2, F) that is not contained in the center Z , and it is to be
proved that G = SL(2, F).
55. Suppose that F is a finite field with q elements.
    (a) By considering the possibilities for the first column of a matrix and then
        considering the possibilities for the second column when the first column is
        fixed, compute |GL(2, F)| as a function of q.
    (b) By using the determinant homomorphism, compute |SL(2, F)| in terms of
        |GL(2, F)|.
    (c) Taking into account that F does not have characteristic 2, prove that
        |PSL(2, F)| = 12 |SL(2, F)|.
    (d) Show for a suitable finite field F with more than 5 elements that PSL(2, F)
        has order 168.
56. Let M be a member of G that is not in Z . Since M is not scalar, there exists a
    column vector u with Mu not a multiple of u. Define v = Mu, so that (u, v) is
    an ordered basis of F2 . By rewriting all matrices with the ordered basis (u, v),
    show≥that there
               ¥    is no loss in generality in assuming that G contains a matrix
          0 −1
    A = 1 c if it is ultimately shown that G = SL(2, F).
206                             IV. Groups and Group Actions

57. Let a be a member≥ of the ¥multiplicative group F× to be chosen shortly, and let
                           ca a −1
      B be the member                of SL(2, F). Prove that
                           −a 0
    (a)   B −1 A−1 B A
                     is upper triangular and is in G,
    (b) B −1 A−1 B A has unequal diagonal entries if a 4 6= 1,
    (c) the condition in (b) can be satisfied for a suitable choice of a under the
        assumption that |F| > 5.
                        °x y ¢
58. Suppose that C = 0 x −1 is a member of G for some x 6= ±1 and some y.
                 ≥ ¥
    Taking D = 10 11 and forming C DC −1 D −1 , show that G contains a matrix
         ≥ ¥
    E = 10 ∏1 with ∏ 6= 0.
                          ≥      ¥                                      ≥ ¥
59. By conjugating E by α0 α0−1 , show that the set of ∏ in F such that 10 ∏1 is in
      G is closed under multiplication by squares and under addition and subtraction.
60. Using the identity x = 14 (x ≥+ 1)¥2 − 14 (x − 1)2 , deduce from Problems 56–59
    that G contains all matrices 10 ∏1 with ∏ ∈ F.
               ≥ ¥                     ≥       ¥
                                         1 0
61. Show that 10 ∏1 is conjugate to −∏ 1 , and show that the set of all matrices
    ≥ ¥        ≥     ¥
      1∏
      01
           and ∏10 01 generates SL(2, F). Conclude that G = SL(2, F).
62. Using the First Isomorphism Theorem, conclude that the only normal subgroup
    of PSL(2, F) other than {1} is PSL(2, F) itself.
Problems 63–73 briefly introduce the theory of error-correcting codes. Let F be the
finite field Z/2Z. The vector space Fn over F will be called Hamming space, and
its members are regarded as “words” (potential messages consisting of 0’s and 1’s).
The weight wt(c) of a word c is the number of nonzero entries in c. The Hamming
distance d(a, b) between words a = (a1 , . . . , an ) and b = (b1 , . . . , bn ) is the weight
of a − b, i.e., the number of indices i with 1 ≤ i ≤ n and ai 6= bi . A code is a
nonempty subset C of Fn , and the minimal distance δ(C) of a code is the smallest
value of d(a, b) for a and b in C with a 6= b. By convention if |C| = 1, take
δ(C) = n + 1. One imagines that members of C, which are called code words, are
allowable messages, i.e., words that can be stored and retrieved, or transmitted and
received. A code with minimal distance δ can then detect up to δ − 1 errors in a
word ostensibly from C that has been retrieved from storage or has been received
in a transmission. The code can correct up to (δ − 1)/2 errors because no word of
Fn can be at distance ≤ (δ − 1)/2 from more than one word in C, by Problem 63
below. The interest is in linear codes, those for which C is a vector subspace. It
is desirable that each message have a high percentage of content and a relatively
low percentage of further information used for error correction; thus a fundamental
theoretical problem for linear codes is to find the maximum dimension of a linear
code if n and a lower bound on the minimal distance for the code are given. As a
practical matter, information is likely to be processed in packets of a standard length,
                                      12. Problems                                    207

such as some power of 2. In many situations packets can be reprocessed if they have
been found to have errors. The initial interest is therefore in codes that can recognize
and possibly correct a small number of errors. The problems in this set are continued
at the ends of Chapters VII and IX.
63. Prove that the Hamming distance satisfies d(a, b) ≤ d(a, c) + d(c, b), and
    conclude that if a word w in Fn is at distance ≤ (D − 1)/2 from two distinct
    members of the linear code C, then δ(C) < D.
64. Explain why the minimal distance δ(C) of a linear code C 6= {0} is given by the
    minimal weight of the nonzero words in C.
65. Fix n ∏ 2. List δ(C) and dim C for the following elementary linear codes:
    (a) C = 0.
    (b) C = Fn .
    (c) (Repetition code) C = {0, (1, 1, . . . , 1)}.
    (d) (Parity-check code) C = {c ∈ Fn | wt(c) is even}. (Educational note: To
        use this code, one sends the message in the first n − 1 bits and adjusts the
        last bit so that the word is in C. If there is at most one error in the word, this
        parity bit will tell when there is an error, but it will not tell where the error
        occurs.)
66. One way to get a sense of what members of a linear code C in Fn have small
    weight starts by making a basis for the code into the row vectors of a matrix and
    row reducing the matrix.
    (a) Taking into account the distinction between corner variables and independent
        variables in the process of row reduction, show that every basis vector of C
        has weight at most the sum of 1 and the number of independent variables.
        Conclude that dim C + δ(C) ≤ n + 1.
    (b) Give an example of a linear code with δ(C) = 2 for which equality holds.
    (c) Examining the argument for (a) more closely, show that 2 ≤ dim C ≤ n − 2
        implies dim C + δ(C) ≤ n.                               µ           ∂
                                                                      100110
67. Let C be a linear code with a basis consisting of the rows of     010101      . Show
                                                                      001011
    that δ(C) = 3. Educational note: Thus for n = 6 and δ(C) = 3, we always have
    dim C ≤ 3, and equality is possible.
68. (Hamming codes) The Hamming code C7 of order 7 is a certain linear code
    having dim C7 = 4 that will be seen to have δ(C7 ) = 3. The code words of a
    basis, with their commas removed, may be taken as
                         1110000, 1001100, 0101010, 1101001.
    The basis may be described as follows. Bits 1, 2, 4 are used as checks. The
    remaining bits are used to form the standard basis of F4 . What is put in bits
    1, 2, 4 is the binary representation of the position of the nonzero entry in
208                            IV. Groups and Group Actions

      positions 3, 5, 6, 7. When all 16 members of C7 are listed in the order dictated
      by the bits in positions 3, 5, 6, 7, the resulting list is
            Decimal value     Code word             Decimal value      Code word
             in 3, 5, 6, 7                           in 3, 5, 6, 7
                   0           0000000                        8         1110000
                   1           1101001                        9         0011001
                   2           0101010                        10        1011010
                   3           1000011                        11        0110011
                   4           1001100                        12        0111100
                   5           0100101                        13        1010101
                   6           1100110                        14        0010110
                   7           0001111                        15        1111111
    For the general members of C7 , not just the basis vectors, the check bits in
    positions 1, 2, 4 may be described as follows: the bit in position 1 is a parity
    bit for the positions among 3, 5, 6, 7 having a 1 in their binary expansions, the
    bit in position 2 is a parity bit for the positions among 3, 5, 6, 7 having a 2 in
    their binary expansions, and the bit in position 4 is a parity bit for the positions
    among 3, 5, 6, 7 having a 4 in their binary expansions. The Hamming code C8
    of order 8 is obtained from C7 by adjoining a parity bit in position 8.
    (a) Prove that δ(C7 ) = 3. (Educational note: Thus for n = 7 and δ(C) = 3, we
         always have dim C ≤ 4, and equality is possible.)
    (b) Prove that δ(C8 ) = 4.
    (c) Describe how to form a generalization that replaces n = 8 by n = 2r with
         r ∏ 3. The Hamming codes that are obtained will be called C2r −1 and C2r .
    (d) Prove that dim C2r −1 = dim C2r = 2r −r−1, δ(C2r −1 ) = 3, and δ(C2r ) =
         4.
                        µ1 0 1 0 1 0 1∂
69. The matrix H = 0 1 1 0 0 1 1 , when multiplied by any column vector c in
                           0001111
      the Hamming code C7 , performs the three parity checks done by bits 1, 2, 4 and
      described in the previous problem. Therefore such a c must have H c = 0.
      (a) Prove that the condition works in the reverse direction as well—that H c = 0
          only if c is in C7 .
      (b) Deduce that if a received word r is not in C7 and if r is assumed to match
          some word of C7 except in the i th position, then Hr matches the i th column
          of H and this fact determines the integer i. (Educational note: Thus there is
          a simple procedure for testing whether a received word is a code word and
          for deciding, in the case that it is not a code word, what unique bit to change
          to convert it into a code word.)
70. Let r ∏ 4. Prove for 2r−1 ≤ n ≤ 2r − 1 that any linear code C in Fn with
    δ(C) ∏ 3 has dim C ≤ n − r. Observe that equality holds for C = C2r −1 .
                                     12. Problems                                   209

71. The weight enumerator polynomial of a linear code C is the polynomial
                                                          Pn             n−k Y k , where
    WC (X, Y ) in Z[X, Y ] given by WC (X, Y ) =             k=0 Nk (C)X
    Nk (C) is the number of words of weight k in C.
    (a) Compute WC (X, Y ) for the following linear codes C: the 0 code, the code
        Fn , the repetition code, the parity code, the code in Problem 67, the Hamming
        code C7 , and the Hamming code C8 .
    (b) Why is the coefficient of X n in WC (X, Y ) necessarily equal to 1?
                                    P
    (c) Show that WC (X, Y ) = c∈C X n−wt(c) Y wt(c) .
72. (Cyclic redundancy codes) Cyclic redundancy codes treat blocks of data as
    coefficients of polynomials in F[X]. With the size n of data blocks fixed, one
    fixes a monic generating polynomial G(X) = 1 + a1 X + · · · + ag−1 X g−1 + X g
    with a nonzero constant term and with degree g suitably less than n. Data to
    be transmitted are provided as members (b0 , b1 , . . . , bn−g−1 ) of Fn−g and are
    converted into polynomials B(X) = b0 + b1 X + · · · + bn−g−1 X n−g−1 . Then
    the n-tuple of coefficients of G(X)B(X) is transmitted. To decode a polynomial
    P(X) that is received, one writes P(X) = G(X)Q(X) + R(X) via the division
    algorithm. If R(X) = 0, it is assumed that P(X) is a code word. Otherwise
    R(X) is definitely not a code word. Thus the code C amounts to the system
    of coefficients of all polynomials G(X)B(X) with B(X) = 0 or deg B(X) ≤
    n − g − 1. A basis of C is obtained by letting B(X) run through the monomials
    1, X, . . . , X n−g−1 , and therefore dim C = n −g. Take G(X) = 1+ X + X 2 + X 4
    and n ∏ 8. Prove that δ(C) = 2.
73. (CRC-8) The cyclic redundancy code C bearing the name CRC-8 has G(X) =
    1 + X + X 2 + X 8 . Prove that if 8 ≤ n ≤ 19, then δ(C) = 4. (Educational
    note: It will follow from the theory of finite fields in Chapter IX, together with
    the problems on coding theory at the end of that chapter, that n = 255 plays a
    special role for this code, and δ(C) = 4 in that case.)

Problems 74–77 concern categories and functors. Problem 75 assumes knowledge of
point-set topology.
74. Let C be the category of all sets, the morphisms being the functions between sets.
    Verify that the disjoint union of sets is a coproduct.
75. Let C be the category of all topological spaces, the morphisms being the contin-
    uous functions. Let S be a nonempty set, and let X s be a topological space for
    each s in S.
    (a) Show that the Cartesian product of the spaces X s , with the product topology,
        is a product of the X s ’s.
    (b) Show that the disjoint union of the spaces X s , topologized so that a set E is
        open if and only if its intersection with each X s is open, is a coproduct of
        the X s ’s.
210                            IV. Groups and Group Actions

76. Taking a cue from the example of a category in which products need not exist,
    exhibit a category in which coproducts need not exist.
77. Let C be a category having just one object, say X, and suppose that every member
    of Morph(X, X) is an isomorphism. Prove that Morph(X, X) is a group under
    the law of composition for the category. Can every group be realized in this way,
    up to isomorphism?
Problems 78–80 introduce a notion of duality in category theory and use it to derive
Proposition 4.64 from Proposition 4.63. If C is a category, then the opposite category
C opp is defined to have Obj(C opp ) = Obj(C) and MorphC opp (A, B) = MorphC (B, A).
If ◦ denotes the law of composition in C, then the law of composition ◦opp in C opp is
defined by g ◦opp f = f ◦ g for f ∈ MorphC opp (A, B) and g ∈ MorphC opp (B, C).
78. Verify that C opp is indeed a category, that (C opp )opp = C, and that to pass from
     a diagram involving objects and morphisms in C to a corresponding diagram
     involving the same objects and morphisms considered as in C opp , one leaves all
     the vertices and labels alone and reverses the directions of all the arrows. Verify
     also that the diagram of C commutes if and only if the diagram in C opp commutes.
79. Let C be the category of all sets, the morphisms in MorphC (A, B) being all
    functions from A to B. Show that the morphisms in MorphC opp (A, B) cannot
    necessarily all be regarded as functions from A to B.
80. Suppose that S is a nonempty set and that {X s }s∈S is an object in C.
    (a) Prove that if (X, { ps }s∈S ) is a product of {X s }s∈S in C, then (X, { ps }s∈S ) is
        a coproduct of {X s }s∈S in C opp , and that if (X, { ps }s∈S ) is a coproduct of
        {X s }s∈S in C, then (X, { ps }s∈S ) is a product of {X s }s∈S in C opp .
    (b) Show that Proposition 4.64 for C follows from the validity of Proposition
        4.63 for C opp .
                                      CHAPTER V

             Theory of a Single Linear Transformation



Abstract. This goal of this chapter is to find finitely many canonical representatives of each
similarity class of square matrices with entries in a field and correspondingly of each isomorphism
class of linear maps from a finite-dimensional vector space to itself.
    Section 1 frames the problem in more detail. Section 2 develops the theory of determinants over
a commutative ring with identity in order to be able to work easily with characteristic polynomials
det(X I − A). The discussion is built around the principle of “permanence of identities,” which
allows for passage from certain identities with integer coefficients to identities with coefficients in
the ring in question.
    Section 3 introduces the minimal polynomial of a square matrix or linear map. The Cayley–
Hamilton Theorem establishes that such a matrix satisfies its characteristic equation, and it follows
that the minimal polynomial divides the characteristic polynomial. It is proved that a matrix is
similar to a diagonal matrix if and only if its minimal polynomial is the product of distinct factors
of degree 1. In combination with the fact that two diagonal matrices are similar if and only if their
diagonal entries are permutations of one another, this result solves the canonical-form problem for
matrices whose minimal polynomial is the product of distinct factors of degree 1.
    Section 4 introduces general projection operators from a vector space to itself and relates them to
vector-space direct-sum decompositions with finitely many summands. The summands of a direct-
sum decomposition are invariant under a linear map if and only if the linear map commutes with
each of the projections associated to the direct-sum decomposition.
    Section 5 concerns the Primary Decomposition Theorem, whose subject is the operation of
a linear map L : V → V with V finite-dimensional. The statement is that if L has minimal
polynomial P1 (X)l1 · · · Pk (X)lk with the Pj (X) distinct monic prime, then V has a unique direct-
sum decomposition in which the respective summands are the kernels of the linear maps Pj (L)l j ,
and moreover the minimal polynomial of the restriction of L to the j th summand is Pj (X)l j .
    Sections 6–7 concern Jordan canonical form. For the case that the prime factors of the minimal
polynomial of a square matrix all have degree 1, the main theorem gives a canonical form under
similarity, saying that a given matrix is similar to one in “Jordan form” and that the Jordan form
is completely determined up to permutation of the constituent blocks. The theorem applies to all
square matrices if the field is algebraically closed, as is the case for C. The theorem is stated and
proved in Section 6, and Section 7 shows how to make computations in two different ways.



                                        1. Introduction

This chapter will work with vector spaces over a common field of “scalars,” which
will be called K. As was observed near the end of Section IV.5, all the results
                                                 211
212                      V. Theory of a Single Linear Transformation

concerning vector spaces in Chapter II remain valid when the scalars are taken
from K rather than just Q or R or C. The ring of polynomials in one indeterminate
X over K will be denoted by K[X].
   For the field C of complex numbers, every nonconstant polynomial in C[X]
has a root, according to the Fundamental Theorem of Algebra (Theorem 1.18).
Because of this fact some results in this chapter will take an especially simple
form when K = C, and this simple form will persist for any field with this
same property. Accordingly, we make a definition. Let us say that a field K is
algebraically closed if every nonconstant polynomial in K[X] has a root. We
shall work hard in Chapter IX to obtain examples of algebraically closed fields
beyond K = C, but let us mention now what a few of them are.

   EXAMPLES.
   (1) The subset of C of all roots of polynomials with rational coefficients is an
algebraically closed field.
   (2) For each prime p, we have seen that any finite field of characteristic p has
pn elements for some n. It turns out that there is one and only one field of pn
elements, up to isomorphism, for each n. If we align them suitably for fixed p
and take their union on n, then the result is an algebraically closed field.
   (3) If K is any field, then there exists an algebraically closed field having K as
a subfield. We shall prove this existence in Chapter IX by means of Zermelo’s
Well-Ordering Theorem (which appears in Section A5 of the appendix).

   The general problem to be addressed in this chapter is to find “canonical forms”
for linear maps from finite-dimensional vector spaces to themselves, special ways
of realizing the linear maps that bring out some of their properties. Let us phrase
a specific problem of this kind completely in terms of linear algebra at first. Then
we can rephrase it in terms of a combination of linear algebra and group theory,
and we shall see how it fits into a more general context.
   In terms of matrices, the specific problem is to find a way of deciding whether
two square matrices represent the same linear map in different bases. We know
from Proposition 2.17 that if L : V → V is linear on the finite-dimensional
vector space V and if A is the matrix of L relative to a particular ordered basis in
domain and range, then the matrix B of L in another ordered basis is of the form
B = C −1 AC for some invertible matrix C, i.e., A and B are similar.1 Thus one
kind of solution to the problem would be to specify one representative of each
                                               ¥ not a convenient
similarity class of square matrices. But ≥this is             ≥ ¥ kind of answer
to look for; in fact, the matrices A = 10 02 and B = 20 01 are similar via

   1 A square matrix A with a two-sided inverse is sometimes said to be nonsingular. A square

matrix with no inverse is then said to be singular.
                                     1. Introduction                                213
      ≥ ¥
C = 01 10 , but there is no particular reason to prefer one of A or B to the other.
Thus a “canonical form” for detecting similarity will allow more than one repre-
sentative of each similarity class (but typically only finitely many such represen-
tatives), and a supplementary statement will tell us when two such are similar.
    So far, the best information that we have about solving this problem concerning
square matrices comes from Section II.8. In that section the discussion of eigen-
values gave us some necessary conditions for similarity, but we did not obtain a
useful necessary and sufficient condition.
    In terms of linear maps, what we seek for a linear L : V → V is to use the
geometry of L to construct an ordered basis of V such that L acts in a particularly
simple way on that ordered basis. Ideally the description of how L acts on the
ordered basis is to be detailed enough so that the matrix of L in that ordered basis
is completely determined by the description, even though the ordered basis may
not be determined by it. For example, if L were to have a basis of eigenvectors,
then the description could be that “L has an ordered basis of eigenvectors with
eigenvalues x1 , . . . , xn .” In any ordered basis with this property, the matrix of L
would then be diagonal with diagonal entries x1 , . . . , xn .
    Suppose then that we have this kind of detailed description of how a linear
map L acts on some ordered basis. To what extent is L completely determined?
The answer is that L is determined up to an isomorphism of the underlying vector
space.∂In fact, suppose
µ                  µ        ∂that L and M are linear maps from V to itself such that
    L                  M
          =A=                  for some ordered bases 0 and 1. Then
   00                 11
                µ      ∂            µ    ∂ µ          ∂µ      ∂µ      ∂
                   L                  M           I       M        I
                          =A=               =
                  00                  11        10       00       01
                              µ     ∂−1 µ     ∂µ       ∂ µ −1          ∂
                                 S         M        S         S MS
                          =                              =               ,
                                00        00      00            00
                                                                  µ      ∂    µ      ∂
                                                                      S           I
where S : V → V is the invertible linear map defined by                     =          .
                                                                     00          01
Hence L = S −1 M S and SL = M S. In other words, if we think of having
two copies of V , one called V1 and the other called V2 , that are isomorphic via
S : V1 → V2 , then the effect of M in V2 corresponds under S to the effect of L
in V1 . In this sense, L is determined up to an isomorphism of V .
    Thus we are looking for a geometric description that determines linear maps
up to isomorphism. Two linear maps L and M that are related in this way have
L = S −1 M S for some invertible linear map S. Passing to matrices with respect to
some basis, we see that the matrices of L and M are to be similar. Consequently
our two problems, one to characterize similarity for matrices and the other to
characterize isomorphism for linear maps, come to the same thing.
214                    V. Theory of a Single Linear Transformation

    These two problems have an interpretation in terms of group theory. In the
case of n-by-n matrices, the group GL(n, K) of invertible matrices acts on the set
of all square matrices of size n by conjugation via (g, x) 7→ gxg −1 ; the similarity
classes are exactly the orbits of this group action, and the canonical form is to
single out finitely many representatives from each orbit. In the case of linear
maps, the group GL(V ) of invertible linear maps on the finite-dimensional vector
space V acts by conjugation on the set of all linear maps from V into itself; the
isomorphism classes of linear maps on V are the orbits, and the canonical form
is to single out finitely many representatives from each orbit.
    The above problem, whether for matrices or for linear maps, does not have a
unique acceptable solution. Nevertheless, the text of this chapter will ultimately
concentrate on one such solution, known as the “Jordan canonical form.”
    Now that we have brought group theory into the statement of the problem, we
can put matters in a more general context: The situation is that some “important”
group G acts in an important way on an “interesting” vector space of matrices. The
canonical-form problem for this situation is to single out finitely many represen-
tatives of each orbit and give a way of deciding, in terms of these representatives,
whether two of the given matrices lie in the same orbit. We shall not pursue the
more general problem in the text at this time. However, Problem 1 at the end of
the chapter addresses one version beyond the one concerning similarity: to find
a canonical form for the action of GL(m, K) × GL(n, K) on m-by-n matrices
by ((g, h), x) = gxh −1 . Some other groups that are important in this sense,
besides products of general linear groups, are introduced in Chapter VI, and a
problem at the end of Chapter VI reinterprets two theorems of that chapter as
further canonical-form theorems under the action of a general linear group.
    Let us return to the canonical-form problems for similarity of matrices and
isomorphism of linear maps. The basic tool in studying these problems is the
characteristic polynomial of a matrix or a linear map, as in Chapter II. However,
we subtly used a special feature of Q and R and C in working with characteristic
polynomials in Chapter II: we passed back and forth between the characteristic
polynomial det(∏I − A) as a polynomial in one indeterminate (defined by its
expression after expanding it out) and as a polynomial function of ∏, defined for
each value of ∏ in Q or R or C, one value at a time. This passage was legitimate
because the homomorphism of the ring of polynomials in one indeterminate over
a field to the ring of polynomial functions is one-one when the field is infinite,
by Proposition 4.28c or Corollary 1.14. Some care is required, however, in
working with general fields, and we begin by supplying the necessary details for
justifying manipulations with determinants in a more general setting than earlier.
The end result will be that the characteristic polynomial is a polynomial in one
indeterminate, and we shall henceforth call that indeterminate X, rather than ∏,
so as to emphasize this point of view.
                    2. Determinants over Commutative Rings with Identity                     215

           2. Determinants over Commutative Rings with Identity
Throughout this section let R be a commutative ring with identity. The main case
of interest for us at this time will be that R = K[X] is the polynomial ring in one
indeterminate X over a field K.
   The set of n-by-n matrices with entries in R is an abelian group under entry-
by-entry addition, and matrix multiplication makes it into a ring with identity.
Following tradition, we shall usually write Mn (R) rather than Mnn (R) for this
ring. In this section we shall define a determinant function det : Mn (R) → R and
establish some of its properties. For the case that R is a field, some of our earlier
proofs concerning determinants used vector-space concepts—bases, dimensions,
and so forth—and these are not available for general R. Yet most of the properties
of determinants remain valid for general R because of a phenomenon known as
permanence of identities. We shall not try to state a general theorem about
this principle but instead will be content to observe a pattern in how the relevant
identities are proved.
   If A is in Mn (R), we define its determinant to be
                                X
                    det A =            (sgn σ )A1σ (1) A2σ (2) · · · Anσ (n) ,
                               σ ∈Sn


in effect converting into a definition the formula obtained in Theorem 2.34d when
R is a field.
   A sample of the kind of identity we have in mind is the formula

                 det(AB) = det A det B               for A and B in Mn (R).

The key is that this formula says that two polynomials in 2n 2 variables, with
integer coefficients, are equal whenever arbitrary members of R are substituted
for the variables. Thus let us introduce 2n 2 indeterminates X 11 , X 12 , . . . , X nn
and Y11 , Y12 , . . . , Ynn to correspond to these variables. Forming the commutative
ring S = Z[X 11 , X 12 , . . . , X nn , Y11£, Y
                                              P              § ], we assemble the matrices
                                               12 , . . . , Ynn
X = [X i j ], Y = [Yi j ], and X Y =            k X ik Yk j in Mn (S). Consider the two
members of S given by

det X det Y
     ° P                                             ¢° P                                       ¢
  =         (sgn σ )X 1σ (1) X 2σ (2) · · · X nσ (n)      (sgn σ )Y1σ (1) Y2σ (2) · · · Ynσ (n)
       σ ∈Sn                                            σ ∈Sn


                           P
and         det(X Y ) =           (sgn σ )(X Y )1σ (1) (X Y )2σ (2) · · · (X Y )nσ (n) ,
                          σ ∈Sn
216                    V. Theory of a Single Linear Transformation
                        P
where (X Y )i j = k X ik Yk j . If we fix arbitrary elements x11 , x12 , . . . , xnn and
y11 , y12 , . . . , ynn of Z, then Proposition 4.30 gives us a unique substitution ho-
momorphism 9 : S → Z such that 9(1) = 1, 9(X i j ) = xi j , and 9(Yi j ) = yi j
for all i and j. Writing x = [xi j ] and y = [yi j ] and using that matrices with
integer entries have det(x y) = det x det y because Z is a subset of the field Q, we
see that 9(det(X Y )) = 9(det X det Y ) for each choice of x and y. Since Z is an
infinite integral domain and since x and y are arbitrary, Corollary 4.32 allows us
to deduce that
                                   det(X Y ) = det X det Y
as an equality in S.
   Now we pass from an identity in S to an identity in R. Let 1 R be the identity in
R. Proposition 4.19 gives us a unique homomorphism of rings ϕ1 : Z → R
such that ϕ1 (1) = 1 R . If we fix arbitrary elements A11 , A12 , . . . , Ann and
B11 , B12 , . . . , Bnn of R, then Proposition 4.30 gives us a unique substitution
homomorphism 8 : S → R such that 8(1) = ϕ1 (1) = 1 R , 8(X i j ) = Ai j
for all i and j, and 8(Yi j ) = Bi j for all i and j. Applying 8 to the equality
det(X Y ) = det X det Y , we obtain the identity we sought, namely

               det(AB) = det A det B           for A and B in Mn (R).

   Proposition 5.1. If R is a commutative ring with identity, then the determinant
function det : Mn (R) → R has the following properties:
    (a) det(AB) = det A det B,
    (b) det I = 1,
    (c) det At = det A,
    (d) det C = det A + det B if A, B, and C match in all rows but the j th and if
        the j th row of C is the sum of the j th rows of A and B,
    (e) det B = r det A if A and B match in all rows but the j th and if the j th row
        of B is equal entry by entry to r times the j th row of A for some r in R,
    (f) det ≥A = 0¥if A has two equal rows,
    (g) det A0 DB = det A det D if A is in Mk (R), D is in Ml (R), and k + l = n.

   REMARKS. Properties (d), (e), and (f) imply that usual steps in manipulating
determinants by row reduction continue to be valid.
   PROOF. Part (a) was proved above, and parts (c) through (f) may be proved
in the same way from the corresponding facts about integer matrices in Section
II.7. Part (b) is immediate from the definition.
   For (g), we first prove the result when the entries are in Q, and then we argue
in the same way as with (a) above. When the entries are in Q, row reduction
of D allows us to reduce to the case either that D has a row of 0’s or that D
                   2. Determinants over Commutative Rings with Identity             217
                                                  ≥    ¥
is the identity. If D has a row of 0’s, then det A0 DB and det A det D are both
0 and hence
         ≥     ¥ equal.≥ If ¥
               are           D is the identity, then further row reduction shows
           A B           A 0
that det 0 I = det 0 I , and the right side equals det A = det A det I , as
required.                                                                     §

   Proposition 5.2 (expansion in cofactors). Let R be a commutative ring with
                                      ci j be the member of Mn−1 (R) obtained by
identity, let A be in Mn (R), and let A
               th              th
deleting the i row and the j column from A. Then
                           Pn
    (a) for any j, det A = i=1    (−1)i+ j Ai j det A
                                                    ci j , i.e., det A may be calculated
                                                   th
         by “expansion in cofactors”
                           P          about the j column,
    (b) for any i, det A = nj=1 (−1)i+ j Ai j det A ci j , i.e., det A may be calculated
         by “expansion in cofactors” about the i th row.
   PROOF. This may be derived in the same way from Proposition 2.36 by using
the principle of permanence of identities.                                §

  Corollary 5.3 (Vandermonde matrix and determinant). If r1 , . . . , rn lie in a
commutative ring R with identity, then
                                             
                       1     1     ···    1
                   r1      r2     ···   rn 
                                                Y
                   2          2            2 
                  
              det  1 r     r 2    · · · r n =    (r j − ri ).
                                              
                   ..       ..     ..
                                       .
                                          ..  j>i
                       .      .            .
                    r1n−1 r2n−1 · · · rnn−1
   PROOF. The derivation of this from Proposition 5.2 is the same as the derivation
of Corollary 2.37 from Proposition 2.35.                                         §

   Proposition 5.4 (Cramer’s rule). Let R be a commutative ring with identity,
let A be in Mn (R), and define Aadj in Mn (R) to be the classical adjoint of A,
                                  adj
namely the matrix with entries Ai j = (−1)i+ j det A           ckl defined as in
                                                   cji , where A
                                          adj    adj
the statement of Proposition 5.2. Then A A = A A = (det A)I .
  PROOF. This may be derived from Proposition 2.38 in the same way as for
Propositions 5.1 and 5.2 using the principle of permanence of identities. §

   Corollary 5.5. Let R be a commutative ring with identity, and let A be
in Mn (R). If det A is a unit in R, then A has a two-sided inverse in Mn (R).
Conversely if A has a one-sided inverse in Mn (R), then det A is a unit in R.
   REMARK. If R is a field, then A and any associated linear map are often called
nonsingular if invertible, singular otherwise. When R is not a field, terminology
varies for what to call a noninvertible matrix whose determinant is not 0.
218                   V. Theory of a Single Linear Transformation

   PROOF. If det A is a unit in R, let r be its multiplicative inverse. Then
Proposition 5.4 shows that r Aadj is a two-sided inverse of A. Conversely if A
has, say, a left inverse B, then B A = I implies (det B)(det A) = det I = 1, and
det B is an inverse for det A. A similar argument applies if A has a right inverse.
                                                                                 §

                 3. Characteristic and Minimal Polynomials

Again let K be a field. If A is in Mn (K), the characteristic polynomial of A is
defined to be the member of the ring K[X] of polynomials in one indeterminate
X given by F(X) = det(X I − A). The material of Section 2 shows that F(X)
is well defined, being the determinant of a member of Mn (K[X]). It is apparent
                                           P 2 that F(X) is a monic polynomial
from the definition of determinant in Section
of degree n with coefficient − Tr A = − nj=1 A j j for X n−1 . Evaluating F(X)
at 0, we see that the constant term is (−1)n det A.
   Since the determinant of a product in Mn (K[X]) is the product of the de-
terminants (Proposition 5.1a) and since C −1 (X I − A)C = X I − C −1 AC, we
have
      det(X I − C −1 AC) = (det C)−1 det(X I − A)(det C) = det(X I − A).
Thus similar matrices have equal characteristic polynomials. If V is an n-
dimensional vector space over K and L : V → V is linear, then the matrices of
L in any two ordered bases of V (the domain basis being assumed equal to the
range basis) are similar, and their characteristic polynomials are the same. Conse-
quently we can define the characteristic polynomial of L to be the characteristic
polynomial of any matrix of L.
   The development of characteristic polynomials has thus be redone in a way
that is valid over any field K without making use of the ring homomorphism from
polynomials in one indeterminate over K to polynomial functions from K into
itself. The discussion in Section II.8 of eigenvectors and eigenvalues for members
A of Mn (K) and for linear maps L : V → V with V finite-dimensional over K
is now meaningful, and there is no need to repeat it.
   In particular, the eigenvalues of A and L are exactly the roots of their charac-
teristic polynomial, no matter what K is. If K is algebraically closed, then the
characteristic polynomial has a root, and consequently A and L each have at least
one eigenvalue.
   If L : V → V is linear and V is finite-dimensional, then a vector subspaceØ
U of V is said to be invariant under L if L(U ) ⊆ U . In this case L ØU is a
well-defined linear map from U to itself. Since L(U ) ⊆ U , Proposition 2.25
shows that L : V → V factors through V /U as a linear map L : V /U → V /U .
We shall use this construction, the existence of eigenvalues in the algebraically
closed case, and an induction to prove the following.
                         3. Characteristic and Minimal Polynomials                      219

   Proposition 5.6. If K is an algebraically closed field, if V is a finite-
dimensional vector space over K, and if L : V → V is linear, then V has
an ordered basis in which the matrix of L is upper triangular. Consequently any
member of Mn (K) is similar to an upper triangular matrix.
                                                                
                                                         c1   ∗
   REMARKS. For an upper triangular matrix A =  . . .  in Mn (K), the
                             Q                           0    cn
characteristic polynomial is nj=1 (X − c j ) because the only nonzero term in the
definition of det(X I − A) is the one corresponding to the identity permutation.
Triangular form is not yet the canonical form we seek for a square matrix because
a particular square matrix may be similar to infinitely many matrices in triangular
form.
    PROOF. We proceed by induction on n = dim V , with the base case n = 1
being clear. Suppose that the result holds for all linear maps from spaces of
dimension < n to themselves. Given L : V → V with dim V = n, let v1 be
an eigenvector of L. This exists by the remarks before the proposition since K
is algebraically closed. Let U be the vector subspace Kv1 . Then L(U ) ⊆ U ,
and Proposition 2.25 shows that L : V → V factors through V /U as a linear
map L : V /U → V /U . Since dim V /U = n − 1, the inductive hypothesis
produces an ordered basis (v̄2 , . . . , v̄n ) of V /U such that the matrix of L is upper
                                                                      Pj
triangular in this basis. This condition means that L(v̄ j ) = i=2 ci j v̄i for j ∏ 2.
Select coset representatives v2 , . . . , vn of v̄2 , . . . , v̄n so that v̄ j = v j + U for
                                  Pj
 j ∏ 2. Then L(v j + U ) =             i=2 ci j (vi + U ) for j ∏ 2, and hence L(v j )
                   Pj
lies in the coset i=2 ci j vi + U for j ∏ 2. For each j ∏ 1, we then have
          Pj
L(v j ) = i=2 ci j vi + c1 j v1 for some scalar c1 j , and we see that (v1 , . . . , vn ) is
the required ordered basis.                                                               §

   Let us return to the situation in which K is any field. For a matrix A in Mn (K)
and a polynomial P in K[X], it is meaningful to form P(A). We can do so by
two equivalent methods, both useful. The concrete way of forming P(A) is as
P(A) = cn An + · · · + c1 A + c0 I if P(X) = cn X n + · · · + c1 X + c0 . The
abstract way is to form the subring T of Mn (K) generated by KI and A. This
subring is commutative. We let ϕ : K → T be given by ϕ(c) = cI . Then the
universal mapping property of K[X] given in Proposition 4.24 produces a unique
ring homomorphism 8 : K[X] → T such that 8(c) = cI for all c ∈ K and
8(X) = A. The value of P(A) is the element 8(P) of T .
   For A in Mn (K), let us study all polynomials P such that P(A) = 0. For any
polynomial P and any invertible matrix C, we have

                              P(C −1 AC) = C −1 P(A)C
220                     V. Theory of a Single Linear Transformation

because if P(X) = cn X n + · · · + c1 X + c0 , then

              P(C −1 AC) = cn (C −1 AC)n + · · · + c1 C −1 AC + c0 I
                            = C −1 (cn An + · · · + c1 A + c0 I )C.

Consequently if P(A) = 0, then P(C −1 AC) = 0, and the set of matrices with
P(A) = 0 is closed under similarity. We shall make use of this observation a
little later in this section.

   Proposition 5.7. If A is in Mn (K), then there exists a nonzero polynomial P
in K[X] such that P(A) = 0.
  PROOF. The K vector space Mn (K) has dimension n 2 . Therefore the n 2 + 1
                              2
matrices I, A, A2 , . . . , An are linearly dependent, and we have
                                                             2
                       c0 + c1 A + c2 A2 + · · · + cn 2 An = 0

for some set of scalars not all 0. Then P(A) = 0 for the polynomial P(X) =
                                     2
c0 + c1 X + c2 X 2 + · · · + cn 2 X n ; this P is not the 0 polynomial since at least one
of the coefficients is not 0.                                                          §
   ALTERNATIVE PROOF IF K IS ALGEBRAICALLY CLOSED. Since the set of poly-
nomials P with P(A) = 0 depends only on the similarity class of A, Proposition
                  is no loss
5.6 shows that there        of generality in assuming that A is upper triangular,
                   ∏1   ∗
say of the form  . . . . Then A − ∏ j I is upper triangular with 0 in the j th
                   0 Q ∏n
diagonal entry, and nj=1 (A − ∏ j I ) is upper triangular with 0 in all diagonal
                   ° Qn               ¢n
entries. Therefore    j=1 (A − ∏ j I )   = 0.                                  §

   With A fixed, we continue to consider the set of all polynomials P(X) such
that P(A) = 0. Let us think of P(A) as being computed by the abstract proce-
dure described above, namely as the image of A under the ring homomorphism
8 : K[X] → T such that 8(c) = cI for all c ∈ K and 8(X) = A, where T is
the commutative subring of Mn (K) generated by KI and A. Then the set of all
polynomials P(X) with P(A) = 0 is the kernel of the ring homomorphism 8.
This set is therefore an ideal, and Proposition 5.7 shows that the ideal is nonzero.
We shall apply the following proposition to this ideal.

   Proposition 5.8. If I is a nonzero ideal in K[X], then there exists a unique
monic polynomial of lowest degree in I , and every member of I is the product
of this particular polynomial by some other polynomial.
                         3. Characteristic and Minimal Polynomials                      221

   PROOF. Let B(X) be a nonzero member of I of lowest possible degree;
adjusting B by a scalar factor, we may assume that B is monic. If A is in I ,
then Proposition 1.12 produces polynomials Q and R such that A = B Q + R
and either R = 0 or deg R < deg B. Since I is an ideal, B Q is in I and hence
R = A − B Q is in I . From minimality of the degree of B, we conclude that
R = 0. Hence A = B Q, and A is exhibited as the product of B and some other
polynomial Q. If B1 is a second monic polynomial of lowest degree in I , then we
can take A = B1 to see that B1 = Q B. Since deg B1 = deg B, we conclude that
deg Q = 0. Thus Q is a constant polynomial. Comparing the leading coefficients
of B and B1 , we see that Q(X) = 1.                                            §

   With A fixed in Mn (K), let us apply Proposition 5.8 to the ideal of all polyno-
mials P in K[X] with P(A) = 0. The unique monic polynomial of lowest degree
in this ideal is called the minimal polynomial of A. Let us try to identify this
minimal polynomial.

   Theorem 5.9 (Cayley–Hamilton Theorem). If A is in Mn (K) and if F(X) =
det(X I − A) is its characteristic polynomial, then F(A) = 0.
   PROOF. Let T be the commutative subring of Mn (K) generated by KI and A,
and define a member B(X) of the ring T [X] by B(X) = X I − A. The (i, j)th
entry of B(X) is Bi j (X) = δi j X − Ai j , and F(X) = det B(X).
   Let C(X) = B(X)adj denote the classical adjoint of B(X) as a member of
T [X]; the form of C(X) is given in the statement of Cramer’s rule (Proposition
5.4), and that proposition says that

                        B(X)C(X) = (det B(X))I = F(X)I.
                                                              P
The equality in the (i, j)th entry is the equality δi j F(X) = j Bik (X)Ck j (X) of
members of K[X]. Application of the substitution homomorphism X 7→ A gives
                          P                        P
           δi j F(A) =         Bik (A)Ck j (A) =       (δik A − Aik I )Ck j (A).
                           k                       k


Multiplying on the right by the i th standard basis vector ei and summing on i, we
obtain the equality of vectors
             PP                                        P           °P                    ¢
 F(A)e j =            (δik Aei − Aik ei )Ck j (A) =        Ck j (A)   (δik Aei − Aik ei )
              i   k                                    k             i

                               P                             P
since Ck j (A) is a scalar. But i (δik Aei − Aik ei ) = Aek − i Aik ei = 0 for all
k, and therefore F(A)e j = 0. Since j is arbitrary, F(A) = 0.                  §
222                     V. Theory of a Single Linear Transformation

   Corollary 5.10. If A is in Mn (K), then the minimal polynomial of A divides
the characteristic polynomial of A.
   PROOF. Theorem 5.9 shows that the characteristic polynomial of A lies in
the ideal of all polynomials vanishing on A. Then the corollary follows from
Proposition 5.8.                                                           §

  For our matrix A in Mn (K), let F(X) be the characteristic polynomial, and let
M(X) be the minimal polynomial. By unique factorization (Theorem 1.17), the
monic polynomial F(X) has a factorization into powers of distinct prime monic
polynomials of the form
                            F(X) = P1 (X)k1 · · · Pr (X)kr ,
and this factorization is unique up to the order of the factors. Since M(X) is a
monic polynomial dividing F(X), we must have
                            M(X) = P1 (X)l1 · · · Pr (X)lr
with l1 ≤ k1 , . . . , lr ≤ kr , by the same argument that deduced Corollary 1.7 from
unique factorization in the ring of integers. We shall see shortly that k j > 0
implies l j > 0 if Pj (X) is of degree 1, i.e., if Pj (X) is of the form X − ∏0 ; in other
words, if ∏0 is an eigenvalue of A, then X − ∏0 divides its minimal polynomial.
We return to this point in a moment. Problem 31 at the end of the chapter will
address the same question when Pj (X) has degree > 1.

  EXAMPLES.              ≥ ¥
  (1) In the 2-by-2 case, 0c 0c has minimal polynomial M(X) = X − c, and
≥ ¥
 c 1
 0 c
       has M(X) = (X − c)2 . Both matrices have characteristic polynomial
F(X) = (X − c)2 .
  (2) The k-by-k matrix
                                    c    1 0 ··· 0 0 
                                        0 c 1 ··· 0 0
                                          ..           
                                               .       
                                                       
                                        0 0 0 ··· c 1
                                        0 0 0 ··· 0 c
with c in every diagonal entry, with 1 in every entry just above the diagonal, and
with 0 elsewhere has minimal polynomial M(X) = (X − c)k and characteristic
polynomial F(X) = (X − c)k .
   (3) If a matrix A is made up exclusively of several blocks of the type in
Example 2 with the same c in each case, the i th block being of size ki , then the
minimal polynomialPis M(X) = (X − c)maxi ki , and the characteristic polynomial
is F(X) = (X − c) i ki .
                       3. Characteristic and Minimal Polynomials               223

   (4) If A is made up exclusively of several blocks as in Example 3 but with c
different for each block, then the minimal and characteristic polynomials for A
are obtained by multiplying the minimal and characteristic polynomials obtained
from Example 3 for the various c’s.

    To proceed further, let us change our point of view, working with linear
maps L : V → V , where V is a finite-dimensional vector space over K. We
have already defined the characteristic polynomial of L to be the characteristic
polynomial of the matrix of L in any ordered basis; this is well defined because
similar matrices have the same characteristic polynomial. In analogous fashion
we can define the minimal polynomial of L to be the minimal polynomial of the
matrix of L in any ordered basis; this is well defined since, as we have seen, the
set of polynomials P in one indeterminate with P(A) = 0 is the same as the set
with P(C −1 AC) = 0 if C is invertible.
    Another way of approaching the matter of the minimal polynomial of L is to
define P(L) for any polynomial P in one indeterminate. As with matrices, we
can define P(L) either concretely by substituting L for X in the expression for
P(X), or we can define P(L) abstractly by appealing to the universal mapping
property in Proposition 4.24. For the latter we work with the subring T 0 of linear
maps from V to itself generated by KI and L. This subring is commutative. We
let ϕ : K → T 0 be given by ϕ(c) = cI , and we use Proposition 4.24 to obtain the
unique ring homomorphism 8 : K[X] → T 0 such that 8(c) = cI for all c ∈ K
and 8(X) = L. Then P(L) is the element 8(P) of T 0 . Once P(L) is defined,
we observe that the set of polynomials P(X) such that P(L) = 0 is a nonzero
ideal in K[X]; Proposition 5.8 yields a unique monic polynomial of lowest degree
in this ideal, and that is the minimal polynomial of L.
    Linear maps enable us to make convenient use of invariant subspaces. Recall
from earlier in the section that a vector subspace U of V is said to be invariant
under the linearØ map L : V → V if L(U ) ⊆ U ; in this case we obtain associated
linear maps L ØU : U → U and L : V /U → V /U . Relationships among
the characteristic polynomials and minimal polynomials of these linear maps are
given in the next two propositions.

    Proposition 5.11. Let V be a finite-dimensional vector space over K, let
L : V → V be linear, let U be a proper nonzero invariant subspace under L, and
let L : V /U → V /UØ be the induced linear map on V /U . Then the characteristic
polynomials of L, L ØU , and L are related by
                                     °        Ø ¢
                 det(X I − L) = det X I − L ØU det(X I − L).

   PROOF. Let 0U = (v1 , . . . , vk ) be an ordered basis of U , and extend 0U to
an ordered basis 0 = (v1 , . . . , vn ) of V . Then 0 = (vk+1 + U, . . . , vn + U )
224                    V. Theory of a Single Linear Transformation

is an ordered basis of V /U . Since≥ U is¥invariant under L, the matrix of L in
                                                                      Ø
the ordered basis 0 is of the form A0 DB , where A is the matrix of L ØU in the
ordered basis 0U and D is the matrix of L in the ordered basis 0. Passing to the
characteristic polynomials and applying Proposition 5.1g, we obtain the desired
conclusion.                                                                   §

   Proposition 5.12. Let V be a finite-dimensional vector space over K, let
L : V → V be linear, let U be a proper nonzero invariant subspace under L, and
let L : V /U → ØV /U be the induced linear map on V /U . Then the minimal
polynomials of L ØU and L divide the minimal polynomial of L.
                                                         Ø
   PROOF. Let N (X) be the minimal polynomial of L ØU . Then N (X) is the
unique monic polynomial of lowest degree in the ideal of all polynomials P(X)
such that P(L)u = 0 for all u in U . The minimal polynomial M(X) of L has
this property because M(X)v = 0 for all v in V . Therefore M(X) is in the ideal
and is the product of N (X) and some other polynomial.
   Among linear maps S from V into V carrying U into itself, the function S 7→ S
sending S to the linear map S induced on V /U is a homomorphism of rings. It
follows that if P(X) is a polynomial with P(L) = 0, then P(L) = 0. Taking
P(X) to be the minimal polynomial of L, we see that the minimal polynomial of
L is in the ideal of polynomials vanishing on L. Therefore it is the product of the
minimal polynomial of L and some other polynomial.                               §

   Let us come back to the unproved assertion before the examples—that k j > 0
implies l j > 0 if Pr (X) has degree 1. We prove the linear-function version of
this statement as a corollary of Proposition 5.12.

   Corollary 5.13. If L : V → V is linear on a finite-dimensional vector
space over K and if a first-degree polynomial X − ∏0 divides the characteristic
polynomial of L, then X − ∏0 divides the minimal polynomial of L.
   PROOF. If X −∏0 divides the characteristic polynomial, then ∏0 is an eigenvalue
of L, say with v as an eigenvector. Then U = Kv is an invariant
                                                        Ø         subspace under
                                                        Ø
L, and the characteristic and minimal polynomials of L U are both X − ∏0 . By
Proposition 5.12, X − ∏0 divides the minimal polynomial of L.                   §

   Theorem 5.14. If L : V → V is linear on a finite-dimensional vector space
over K, then L has a basis of eigenvectors if and only if the minimal polynomial
M(X) of L is the product of distinct factors of degree 1; in this case, M(X) equals
(X − ∏1 ) · · · (X − ∏k ), where ∏1 , . . . , ∏k are the distinct eigenvalues of L. Con-
sequently a matrix A in Mn (K) is similar to a diagonal matrix if and only if its
minimal polynomial is the product of distinct factors of degree 1.
                         3. Characteristic and Minimal Polynomials                      225

    PROOF. The easy direction is that v1 , . . . , vn are the members of a basis
of eigenvectors for L with respective eigenvalues µ1 , . . . , µn . In this case, let
∏1 , . . . , ∏k be the distinct members of the set of eigenvalues, with µi = ∏ j (i) for
some function j : {1, . . . , n} → {1, . . . , k}. Then (L − ∏ j I )(v) = 0 for v equal
to any vi with j (i) = j. Since the linear maps L − ∏ j I commute as j varies,
Qk
   j=1 (L−∏ j I )(v) = 0 for v equal to each of v1 , . . . , vn , hence for all v. Therefore
                                                   Q
the minimal polynomial M(X) of L divides kj=1 (X − ∏ j ). On the other hand,
                                                                          Q
Corollary 5.13 shows that the deg M(X) ∏ k. Hence M(X) = kj=1 (X − ∏ j ).
                                          Q
    Conversely suppose that M(X) = kj=1 (X − ∏ j ) with the ∏ j distinct. If S1
                             Q
is the linear map S1 = kj=2 (L − ∏ j I ), then the formula for M(X) shows that
(L − ∏1 I )S1 (v) = 0 for all v in V , and hence image S1 is a vector subspace of the
eigenspace of L for the eigenvalue ∏1 . If v is in ker S1 ∩ image S1 , we then have
                   Q                     Q
0 = S1 (v) = kj=2 (L − ∏ j I )(v) = kj=2 (∏1 − ∏ j )v. Since ∏1 is distinct from
∏2 , . . . , ∏k , we conclude that v = 0, hence that ker S1 ∩ image S1 = 0. Since
dim ker S1 + dim image S1 = dim V , Corollary 2.29 therefore gives

           dim V = dim ker S1 + dim image S1
                 = dim(ker S1 + image S1 ) + dim(ker S1 ∩ image S1 )
                 = dim(ker S1 + image S1 ).

Hence V = ker S1 + image S1 . Since ker S1 ∩ image S1 = 0, we conclude that
V = ker S1 ⊕ image S1 .
    Actually, the same calculation of S1 (v) as above shows that image S1 is the
full eigenspace of L for the eigenvalue ∏1 . In fact, if L(v) = ∏1 v, then S1 (v) =
Qk                                                                 ° Qk              ¢−1
   j=2 (∏1 − ∏ j )v, and hence v equals the image under S1 of         j=2 (∏1 − ∏ j )    v.
    Next, since L commutes with   Ø   S1 , ker S1 is an invariant subspace under    L, and
∏1 is not an eigenvalue of L Øker S1 . Thus X − ∏1 does not divide the minimal
                    Ø
polynomial of L Øker S1 . On the other hand, S1 vanishes on the eigenspaces of
L for eigenvalues ∏2 , . . . , ∏k , and Corollary
                                           Ø         5.13 shows for j ∏ 2 that X − ∏ j
divides the minimal polynomial of L Øker S1 . Taking Proposition 5.12 into account,
                        Ø                                    Q
we conclude that L Ø    ker S1
                               has minimal polynomial k (X − ∏ j ). We have
                                                               j=2
succeeded in splitting off the eigenspace of L under ∏1 as a direct summand and
reducing the proposition to the case of k − 1 eigenvalues. Thus induction shows
that V is the direct sum of its eigenspaces for the eigenvalues ∏2 , . . . , ∏k , and L
thus has a basis of eigenvectors.                                                     §

   Theorem 5.14 comes close to solving the canonical-form problem for similarity
in the case of one kind of square matrices: if the minimal polynomial of A is the
product of distinct factors of degree 1, then A is similar to a diagonal matrix. To
226                      V. Theory of a Single Linear Transformation

complete the solution for this case, all we have to do is to say when two diagonal
matrices are similar to each other; this step is handled by the following easy
proposition.

   Proposition 5.15. Two diagonal matrices A and A0 in Mn (K) with respective
diagonal entries d1 , . . . , dn and d10 , . . . , dn0 are similar if and only if there is a
permutation σ in Sn such that d j0 = dσ ( j) for all j.
                                                                       Q
   PROOF. The respective characteristic polynomials are nj=1 (X − d j ) and
Qn            0                  0
  j=1 (X − d j ). If A and A are similar, then the characteristic polynomials are
equal, and unique factorization (Theorem 1.17) shows that the factors X − d j0
match the factors X − d j up to order. Conversely if there is a permutation σ in
Sn such that d j0 = dσ ( j) for all j, then the matrix C whose j th column is eσ ( j) has
the property that A0 = C −1 AC.                                                           §

   To proceed further with obtaining canonical forms for matrices under similarity
and for linear maps under isomorphism, we shall use linear maps in ways that
we have not used them before. In particular, it will be convenient to be able to
recognize direct-sum decompositions from properties of linear maps. We take up
this matter in the next section.


                               4. Projection Operators

In this section we shall see how to recognize direct-sum decompositions of a
vector space V from the associated projection operators, and we shall relate these
operators to invariant subspaces under a linear map L : V → V .
    If V = U1 ⊕ U2 , then the function E 1 defined by E 1 (u 1 + u 2 ) = u 1 when u 1
is in U1 and u 2 is in U2 is linear, satisfies E 12 = E 1 , and has image E 1 = U1 and
ker E 1 = U2 . We call E 1 the projection of V on U1 along U2 . A decomposition
of V as the direct sum of two vector spaces, when the first of the two spaces is
singled out, therefore determines a projection operator uniquely. A converse is
as follows.

   Proposition 5.16. If V is a vector space and E 1 : V → V is a linear map such
that E 12 = E 1 , then there exists a direct-sum decomposition V = U1 ⊕ U2 such
that E 1 is the projection of V on U1 along U2 . In this case, (I − E 1 )2 = I − E 1 ,
and I − E 1 is the projection of V on U2 along U1 .
   PROOF. Define U1 = image E 1 and U2 = ker E 1 . If v is in image E 1 ∩ ker E 1 ,
then E 1 (v) = 0 since v is in ker E 1 and v = E 1 (w) for some w in V since
                                 4. Projection Operators                             227

v is in image E 1 . Then 0 = E 1 (v) = E 12 (w) = E 1 (w) = v, and therefore
image E 1 ∩ ker E 1 = 0.
    If v ∈ V is given , write v = E 1 (v) + (I − E 1 )(v). Then E 1 (v) is in image E 1 ,
and the computation E 1 (I − E 1 )(v) = (E 1 − E 12 )(v) = (E 1 − E 1 )(v) = 0 shows
that (I − E 1 )(v) = 0. Consequently V = image E 1 + ker E 1 , and we conclude
that V = image E 1 ⊕ ker E 1 .
    Hence V = U1 ⊕ U2 , where U1 = image E 1 and U2 = ker E 1 . In this
notation, E 1 is 0 on U2 . If v is in U1 , then v = E 1 (w) for some w, and we have
v = E 1 (w) = E 12 (w) = E 1 (E 1 (w)) = E 1 (v). Thus E 1 is the identity on U1 and
is the projection as asserted.
    For (I − E 1 )2 , we have (I − E 1 )2 = I − 2E 1 + E 12 = I − 2E 1 + E 1 = I − E 1 ,
and I − E 1 is a projection. It is 1 on U2 and is 0 on U1 , hence is the projection
of V on U2 along U1 .                                                                §

    Let us generalize these considerations to the situation that V is the direct sum
of r vector subspaces. The following facts about the situation in Proposition 5.16,
with the definition E 2 = I − E 1 , are relevant to formulating the generalization:
      (i) E 1 and E 2 have E 12 = E 1 and E 22 = E 2 ,
     (ii) E 1 E 2 = E 2 E 1 = 0,
    (iii) E 1 + E 2 = I .
Suppose that V = U1 ⊕ · · · ⊕ Ur . Define E j (u 1 + · · · + u r ) = u j . Then E j
is linear from V to itself with E j2 = E j , and Proposition 5.16 shows that E j is
the projection of V on U j along the direct sum of the remaining Ui ’s. The linear
maps E 1 , . . . , Er then satisfy
     (i0 ) E j2 = E j for 1 ≤ j ≤ r,
    (ii0 ) E j E i = 0 if i 6= j,
   (iii0 ) E 1 + · · · + Er = I .
A converse is as follows.

   Proposition 5.17. If V is a vector space and E j : V → V for 1 ≤ j ≤ r are
linear maps such that
    (a) E j E i = 0 if i 6= j, and
    (b) E 1 + · · · + Er = I ,
then E j2 = E j for 1 ≤ j ≤ r and the vector subspaces U j = image E j have the
properties that V = U1 ⊕ · · · ⊕ Ur and that E j is the projection of V on U j along
the direct sum of all Ui but U j .
    PROOF. Multiplying (b) through by E j on the left and applying (a) to each
term on the left side except the j th , we obtain E j2 = E j . Therefore, for each j,
E j is a projection on U j along some vector subspace depending on j.
228                     V. Theory of a Single Linear Transformation

    If v is in V , then (b) gives v = E 1 (v) + · · · + Er (v) and shows that V =
U1 + · · · + Ur . Suppose  P that v is in the intersection of U j with the sum of the
other Ui ’s. Write v = i6= j u i with u i = E i (wi ) in Ui . Applying E j and using
                                                      P
the fact that v is in U j , we obtain v = E j (v) = i6= j E j E i (wi ). Every term of
the right side is 0 by (a), and hence v = 0. Thus V = U1 ⊕ · · · ⊕ Ur .
    Since E j E i = 0 for i 6= j, E j is 0 on each Ui for i 6= j. Therefore the sum of
all Ui except U j is contained in the kernel of E j . Since the image and kernel of
E j intersect in 0, the sum of all Ui except U j is exactly equal to the kernel of E j .
This completes the proof.                                                           §

   Proposition 5.18. Suppose that a vector space V is a direct sum V =
U1 ⊕ · · · ⊕ Ur of vector subspaces, that E 1 , . . . , Er are the corresponding pro-
jections, and that L : V → V is linear. Then all the subspaces U j are invariant
under L if and only if L E j = E j L for all j.
   PROOF. If L(U j ) ⊆ U j for all j, then i 6= j implies E i L(U j ) ⊆ E i (U j ) = 0
and L E i (U j ) = L(0) = 0. Also, v ∈ U j implies E j L(v) = L(v) = L E j (v).
Hence E i L = E i L for all i.
   Conversely if E j L = L E j and if v is in U j , then E j L(v) = L E j (v) = L(v)
shows that L(v) is in U j . Therefore L(U j ) ⊆ U j for all j.                      §


                            5. Primary Decomposition

For the case that the minimal polynomial of a linear map L : V → V is the product
of distinct factors of degree 1, Theorem 5.14 showed that V is a direct sum of its
eigenspaces. The proof used elementary vector-space techniques from Chapter
II but did not take full advantage of the machinery developed in the present
chapter for passing back and forth between polynomials in one indeterminate
and the values of polynomials on L. Let us therefore rework the proof of that
proposition, taking into account the discussion of projections in Section 4.
    We seek an eigenspace decomposition V = V∏1 ⊕ · · · ⊕ V∏k relative to L.
Proposition 5.17 suggests looking for the corresponding decomposition of the
identity operator as a sum of projections: I = E 1 + · · · + E k . According to that
proposition, we obtain a direct-sum decomposition as soon as we obtain this kind
of sum of linear maps such that E i E j = 0 for i 6= j. The E j ’s will automatically
be projections.
                                                    Q
    The proof of Theorem 5.14 showed that S1 = kj=2 (L − ∏ j I ) has image equal
to the kernel of L − ∏1 I , i.e., equal to the eigenspace for eigenvalue ∏1 . If v
                                      Q
is in this eigenspace, then S1 (v) = kj=2 (∏1 − ∏ j )v. Hence E 1 = c1 S1 , where
         Q
c1−1 = kj=2 (∏1 − ∏ j ). The linear map S1 equals Q 1 (L), where Q 1 (X) =
                               5. Primary Decomposition                             229
Qk
   j=2 (X − ∏ j ). Thus E 1 = c1 Q 1 (L). Similar remarks apply to the other
eigenspaces, and therefore the required decomposition of the identity operator
has to be of the form I = c1 Q 1 (L) + · · · + ck Q k (L) with c1 , . . . , ck equal to
certain scalars.
    The polynomials Q 1 (X), . . . , Q l (X) are at hand from the start, each containing
all but one factor of the minimal polynomial. Moreover, i 6= j implies that
                                  ≥Y
                                   k                ¥≥ Y            ¥
               Q i (L)Q j (L) =          (L − ∏l I )     (L − ∏l I ) .
                                   l=1                l6=i, j

The first factor on the right side is the value of the minimal polynomial of L with
L substituted for X. Hence the right side is 0, and we see that our linear maps
E 1 , . . . , E k have E i E j = 0 for i 6= j.
    As soon as we allow nonconstant coefficients in place of the c j ’s in the above
argument, we obtain a generalization of Theorem 5.14 to the situation that the
minimal polynomial of L is arbitrary. The prime factors of the minimal polyno-
mial need not even be of degree 1. Hence the theorem applies to all L’s even if
K is not algebraically closed.

   Theorem 5.19 (Primary Decomposition Theorem). Let L : V → V be linear
on a finite-dimensional vector space over K, and let M(X) = P1 (X)l1 · · · Pk (X)lk
be the unique factorization of the minimal polynomial M(X) of L into the product
of powers of distinct monic prime polynomials Pj (X). Define U j = ker(Pj (L)l j )
for 1 ≤ j ≤ k. Then
    (a) V = U1 ⊕ · · · ⊕ Uk ,
    (b) the projection E j of V on U j along the sum of the other Ui ’s is of the
         form Tj (L) for some polynomial Tj ,
    (c) each vector subspace U j is invariant under L,
    (d) any linear map from V to itself that commutes with L carries each U j
         into itself,
    (e) any vector subspace W invariant under L has the property that

                      W = (W ∩ U1 ) ⊕ · · · ⊕ (W ∩ Uk ),
                                           Ø
     (f) the minimal polynomial of L j = L ØUj is Pj (X)l j .

  REMARKS. The decomposition in (a) is called the primary decomposition of
V under L, and the vector subspaces U j are called the primary subspaces of V
under L.
  PROOF. For 1 ≤ j ≤ k, define Q j (X) = M(X)/Pj (X)l j . The ideal in
K[X] generated by Q 1 (X), . . . , Q k (X) consists of all products of a single monic
230                    V. Theory of a Single Linear Transformation

polynomial D(X) by arbitrary polynomials, according
                                                Q to Proposition 5.8, and
D(X) has to divide each Q j (X). Since Q j (X) = i6= j Pi (X)li , D(X) cannot
be divisible by any Pj (X), and consequently D(X) = 1. Thus there exist
polynomials R1 (X), . . . , Rk (X) such that

                    1 = Q 1 (X)R1 (X) + · · · + Q k (X)Rk (X).

Define E j = Q j (L)RQ     j (L), so that E 1 + · · · + E k = I . If i 6= j, then
Q i (X)Q j (X) = M(X) r6=i, j Pr (X)lr . Since M(L) = 0, we see that E i E j = 0.
    Proposition 5.17 says that each E j is a projection. Also, it says that if U j
denotes image E j , then V = U1 ⊕ · · · ⊕ Uk , and E j is the projection on U j along
the sum of the other Ui ’s. With this definition of the U j ’s (rather than the one in
the statement of the theorem), we have therefore shown that (a) and (b) hold.
    Let us see that conclusions (c), (d), and (e) follow from (b). Conclusion
(c) holds by Proposition 5.18 since L commutes with Tj (L) whenever Tj is a
polynomial. For (d), if J : V → V is a linear map commuting with L, then
J commutes with each E j since (b) shows that each E j is of the form Tj (L).
From Proposition 5.18 we conclude that each U j is invariant under J . For (e),
the subspace W certainly contains (W ∩ U1 ) ⊕ · · · ⊕ (W ∩ Uk ). For the reverse
containment suppose w is in W . Since E j is of the form Tj (L) and since W
is invariant underP L, E j (w) is in W . But also E j (w) is in U j . Therefore the
expansion w =         j E j (w) exhibits w as the sum of members of the spaces
W ∩ Uj .
    Next let us prove that U j , as we have defined it, is given also by the definition
in the statement of the theorem. In other words, let us prove that

                             image E j = ker(Pj (L)l j ).                          (∗)

We need a preliminary fact. The polynomial Pj (X)l j has the property that
M(X) = Pj (X)l j Q j (X). Hence Pj (L)l j Q j (L) = M(L) = 0. Multiplying
by R j (L), we obtain
                             Pj (L)l j E j = 0.                       (∗∗)
   Now suppose that v is in image E j . Then Pj (L)l j (v) = Pj (L)l j E j (v) = 0
by (∗∗), and hence image E j ⊆ ker(Pj (L)l j ).° Q
                                                 For the reverse¢ inclusion, let v be
             lj                                                 lr             lj
in ker(Pj (L) ). For i 6= j, Q i (X)Ri (X) =      r6=i, j Pr (X) Ri (X)Pj (X) and
hence                     °Q                  ¢
                                           lr              lj
                E i (v) =    r6=i, j Pr (L) Ri (L)Pj (L) (v) = 0.

Writing v = E 1 (v) + · · · + E k (v), we see that v = E j (v). Thus ker(Pj (L)l j ) ⊆
image E j . Therefore (∗) holds, and U j is as in the statement of the theorem. Ø
  Finally let us prove (f). Let M j (X) be the minimal polynomial of L j = L ØUj .
From (∗∗) we see that Pj (L j )l j = 0. Hence M j (X) divides Pj (X)l j . For the
                                 6. Jordan Canonical Form                              231

reverse divisibility we have M j (L j ) = 0. Then certainly M j (L j )Q j (L j )R j (L j ),
which equals M j (L)E j on U j , is 0 on U j . Consider M j (L)E j on Ui = image E i
when i 6= j. Since E j E i = 0, M j (L)E j equals 0 on all Ui other than U j . We
conclude that M j (L)E j equals 0 on V , i.e., M j (L)Q j (L)R j (L) = 0. Since M(X)
is the minimal polynomial of L, M(X) divides
                                                °     P                 ¢
              M j (X)Q j (X)R j (X) = M j (X) 1 −          Q i (X)Ri (X) ,            (†)
                                                       i6= j


and the factor Pj (X)l j of M(X) must divide the right side of (†). On that right
side, Pj (X)l j divides each Q i (X) with
                                       P i 6= j. Since Pj (X) does not divide 1,
Pj (X) does not divide the factor 1 − i6= j Q i (X)Ri (X). Since Pj (X) is prime,
                   P
Pj (X)l j and 1 − i6= j Q i (X)Ri (X) are relatively prime. We know that Pj (X)l j
                                           P
divides the product of M j (X) and 1 − i6= j Q i (X)Ri (X), and consequently
Pj (X)l j divides M j (X). This proves the reverse divisibility and completes the
proof of (f).                                                                  §


                            6. Jordan Canonical Form

Now we can return to the canonical-form problem for similarity of square matrices
and isomorphism of linear maps from a finite-dimensional vector space to itself.
The answer obtained in this section will solve the problem completely if K
is algebraically closed but only partially if K fails to be algebraically closed.
Problems 32–40 at the end of the chapter extend the content of this section to give
a complete answer for general K.
    The present theorem is most easily stated in terms of matrices. A square matrix
is called a Jordan block if it is of the form
                                                        
                           c 1 0 0 ··· 0 0
                              c 1 0 ··· 0 0
                                                        
                                  c 1 ··· 0 0
                                      ..    ..     . .
                                               . .. .. 
                                         .              ,
                                             c     1  0 
                                                        
                                                   c 1
                                                       c

of some size and for some c in K, as in Example 2 of Section 3, with 0 everywhere
below the diagonal. A square matrix is in Jordan form, or Jordan normal form,
if it is block diagonal and each block is a Jordan block. One can insist on grouping
the blocks for which the constant c is the same and arranging the blocks for given
c in some order, but these refinements are inessential.
232                   V. Theory of a Single Linear Transformation

  Theorem 5.20 (Jordan canonical form).
   (a) If the field K is algebraically closed, then every square matrix over K is
similar to a matrix in Jordan form, and two matrices in Jordan form are similar
to each other if and only if their Jordan blocks can be permuted so as to match
exactly.
   (b) For a general field K, a square matrix A is similar to a matrix in Jordan
form if and only if each prime factor of its minimal polynomial has degree 1.
Two matrices in Jordan form are similar to each other if and only if their Jordan
blocks can be permuted so as to match exactly.

   The first step in proving existence of a matrix in Jordan form similar to a
given matrix is to use the Primary Decomposition Theorem (Theorem 5.19). We
think of the matrix A as operating on the space Kn of column vectors in the
usual way. The primary subspaces are uniquely defined vector subspaces of Kn ,
and we introduce an ordered basis, yet to be specified in full detail, within each
primary subspace. The union of these ordered bases gives an ordered basis of
Kn , and we change from the standard basis to this one. The result is that the
given matrix has been conjugated so that its appearance is block diagonal, each
block having minimal polynomial equal to a power of a prime polynomial and the
prime polynomials all being different. Let us call these blocks primary blocks.
The effect of Theorem 5.19 has been to reduce matters to a consideration of each
primary block separately. The hypothesis either that K is algebraically closed
or, more generally, that the prime divisors of the minimal polynomial all have
degree 1 means that the minimal polynomial of the primary block under study
may be taken to be (X − c)l for some c in K and some integer l ∏ 1. In terms
of Jordan form, we have isolated, for each c in K, what will turn out to be the
subspace of Kn corresponding to Jordan blocks with c in every diagonal entry.
   Let us write B for a primary block with minimal polynomial (X − c)l . We
certainly have (B − cI )l = 0, and it follows that the matrix N = B − cI has
N l = 0. A matrix N with N l = 0 for some integer l ∏ 0 is said to be nilpotent.
To prove the existence part of Theorem 5.20, it is enough to prove the following
theorem.

   Theorem 5.21. For any field K, each nilpotent matrix N in Mn (K) is similar
to a matrix in Jordan form.

   The proof of Theorem 5.21 and of the uniqueness statements in Theorem
5.20 will occupy the remainder of this section. It is implicit in Theorem 5.21
that a nilpotent matrix in Mn (K) has 0 as a root of its characteristic polynomial
with multiplicity n, in particular that the only prime polynomials dividing the
characteristic polynomial are the ones dividing the minimal polynomial. We
                                6. Jordan Canonical Form                             233

proved such a fact about divisibility earlier for general square matrices when the
prime factor has degree 1, but we did not give a proof for general degree. We
pause for a moment to give a direct proof in the nilpotent case.


  Lemma 5.22. If N is a nilpotent matrix in Mn (K ), then N has characteristic
polynomial X n and satisfies N n = 0.

   PROOF. If N l = 0, then

(X I − N )(X l−1 I + X l−2 N +· · ·+ X 2 N l−3 + X N l−2 + N l−1 ) = X l I − N l = X l I.

Taking determinants and using Proposition 5.1 in the ring R = K[X], we obtain

                det(X I − N ) det(other factor) = det(X l I ) = X ln .

Thus det(X I − N ) divides X ln . By unique factorization in K[X], det(X I − N ) is
a constant times a power of X. Then we must have det(X I − N ) = X n . Applying
the Cayley–Hamilton Theorem (Theorem 5.9), we obtain N n = 0.                   §


    Let us now prove the uniqueness statements in Theorem 5.20; this step will in
fact help orient us for the proof of Theorem 5.21. In (b), one thing we are to prove
is that if A is similar to a matrix in Jordan form, then every prime polynomial
dividing the minimal polynomial has degree 1. Since characteristic and minimal
polynomials are unchanged under similarity, we may assume that A is itself in
Jordan form. The characteristic and minimal polynomials of A are computed in
the four examples of Section 3. Since the minimal polynomial is the product of
polynomials of degree 1, the only primes dividing it have degree 1.
    In both (a) and (b) of Theorem 5.20, we are to prove that the Jordan form
is unique up to permutation of the Jordan blocks. The matrix A determines
its characteristic polynomial, which determines the roots of the characteristic
polynomial, which are the diagonal entries of the Jordan form. Thus the sizes
of the primary blocks within the Jordan form are determined by A. Within each
primary block, we need to see that the sizes of the various Jordan blocks are
completely determined.
    Thus we may assume that N is nilpotent and that C −1 N C = J is in Jordan
form with 0’s on the diagonal. Although we shall make statements that apply
in all cases, the reader may be helped by referring to the particular matrix J in
Figure 5.1 and its powers in Figure 5.2.
234                      V. Theory of a Single Linear Transformation

                                 0100                            
                                      0010
                                     0001                        
                                                                 
                                     0000                        
                                             010                 
                                                                 
                            J =              001                 .
                                             000                 
                                                   01            
                                                   00
                                                                  
                                                        01
                                                                  
                                                         00
                                                              0

           FIGURE 5.1. Example of a nilpotent matrix in Jordan form.

   Each block of the Jordan form J contributes 1 to the dimension of the kernel
(or null space really) of J via the first column of the block, and hence

                         dim(ker J ) = #{Jordan blocks in J }.

In Figure 5.1 this number is 5.

          0010                                              0001                           
               0001                                               0000
              0000                                             0000                        
                                                                                           
              0000                                             0000                        
                     001                                              000                  
                                                                                           
      J2 =           000                    and   J3 =                000                  
                     000                                              000                  
                           00                                               00             
                           00
                                                                             00
                                                                                              
                                00
                                                                                   00
                                                                                              
                                 00                                                  00
                                      0                                                   0

               FIGURE 5.2. Powers of the nilpotent matrix in Figure 5.1.

When J is squared, the 1’s in J move up and to the right one more step beyond
the diagonal except that blocks of size 2 become 0. When J is cubed, the 1’s in
J move up and to the right one further step except that blocks of size 3 become 0.
Each time J is raised to a new power one higher than before, each block that
is nonzero in the old power contributes an additional 1 to the dimension of the
kernel. Thus we have

            dim(ker J 2 ) − dim(ker J ) = #{Jordan blocks of size ∏ 2}

and        dim(ker J 3 ) − dim(ker J 2 ) = #{Jordan blocks of size ∏ 3};

in the general case,

 dim(ker J k ) − dim(ker J k−1 ) = #{Jordan blocks of size ∏ k}                     for k ∏ 1.
                               6. Jordan Canonical Form                             235

Lemma 5.22 says that J k = 0 when k is ∏ the size of J , and the differences need
not be computed beyond that point.
   For Figure 5.2 the values by inspection are dim(ker J 2 ) = 9 and dim(ker J 3 ) =
11; also J 4 = 0 and hence dim(ker J 4 ) = 12. The numbers of Jordan blocks
of size ∏ k for k = 1, 2, 3, 4 are 5, 4, 2, 1, and these numbers indeed match the
differences 5 − 0, 9 − 5, 11 − 9, 12 − 11, as predicted by the above formula.
   Since C −1 N C = J , we have C −1 N k C = J k and N k C = C J k . The matrix
C is invertible, and therefore dim(ker J k ) = dim(ker C J k ) = dim(ker N k C) =
dim(ker N k ). Hence

 dim(ker N k ) − dim(ker N k−1 ) = #{Jordan blocks of size ∏ k}            for k ∏ 1,

and the number of Jordan blocks of each size is uniquely determined by properties
of N . This completes the proof of all the uniqueness statements in Theorem 5.20.

   Now let us turn to the proof of Theorem 5.21, first giving the idea. The
argument involves a great many choices, and it may be helpful to understand it in
the context of Figures 5.1 and 5.2. Let 6 = (e1 , . . . , e12 ) be the standard ordered
basis of K12 . The matrix J , when operating by multiplication on the left, moves
basis vectors to other basis vectors or to 0. Namely,

                 J e1 = 0, J e2 = e1 , J e3 = e2 , J e4 = e3 ,
                       J e5 = 0, J e6 = e5 , J e7 = e6 ,
                              J e8 = 0, J e9 = e8 ,
                            J e10 = 0, J e11 = e10 ,
                                    J e12 = 0,

with each line describing what happens forµa single      ∂ Jordan block. Let us think
                                                     L
of the given nilpotent matrix N as equal to                for some linear map L. We
                                                   66
want to find a new ordered basis 0 = (v1 , . . . , v12 ) in which
                                                              µ      ∂the matrix of L is
                                                                  I
J . In the expression C −1 N C = J , the matrix C equals               , and its columns
                                                                60
are expressions for v1 , . . . , v12 in the basis 6, i.e., Cei = vi . For each index i,
we have J ei = J ei−1 or J ei = 0. The formula N C = C J , when applied to ei ,
therefore says that
                                          Ω
                                            Cei−1 = vi−1 if J ei = ei−1 ,
            N vi = N Cei = C J ei =
                                            0                if J ei = 0.
Thus we are looking for an ordered basis such that N sends each member of the
basis either into the previous member or into 0. The procedure in this example
236                    V. Theory of a Single Linear Transformation

will be to pick out v4 as a vector not annihilated by N 3 , obtain v3 , v2 , v1 , from
it by successively applying N , pick out v7 as a vector not annihilated by N 2 and
independent of what has been found, obtain v6 , v5 from it by successively applying
N , and so on. It is necessary to check that the appropriate linear independence
can be maintained, and that step will be what the proof is really about.

   The proof of Theorem 5.21 will now be given in the general case. The core of
the argument concerns linear maps and appears as three lemmas. Afterward the
results of the lemmas will be interpreted in terms of matrices. For all the lemmas
let V be an n-dimensional vector space over K, and let N : V → V be linear
with N n = 0. Define K j = ker N j , so that

                      0 = K 0 ⊆ K 1 ⊆ K 2 ⊆ · · · ⊆ K n = V.

   Lemma 5.23. Suppose j ∏ 1 and suppose Sj is any vector subspace of V such
that K j+1 = K j ⊕ Sj . Then N is one-one from Sj into K j and N (Sj ) ∩ K j−1 = 0.
    PROOF. Since N (ker N j+1 ) ⊆ ker N j , we obtain N (Sj ) ⊆ K j ; thus N indeed
sends Sj into K j . To see that N is one-one from Sj into K j , suppose that s is a
member of Sj with N (s) = 0. Then s is in K 1 . Since j ∏ 1, K 1 ⊆ K j . Thus s
is in K j . Since K j ∩ Sj = 0, s is 0. Hence N is one-one from Sj into K j . To see
that N (Sj ) ∩ K j−1 = 0, suppose s is a member of Sj with N (s) in K j−1 . Then
0 = N j−1 (N (s)) = N j (s) shows that s is in K j . Since K j ∩ Sj = 0, s equals 0.
                                                                                  §

  Lemma 5.24. Define Un = Wn = 0. For 0 ≤ j ≤ n − 1, there exist vector
subspaces U j and W j of K j+1 such that
                            K j+1 = K j ⊕ U j ⊕ W j ,
                               U j = N (U j+1 ⊕ W j+1 ),
and                    N : U j+1 ⊕ W j+1 → U j is one-one.

   PROOF. Define Un−1 = N (Un ⊕ Wn ) = 0, and let Wn−1 be a vector subspace
such that V = K n = K n−1 ⊕ Wn−1 . Put Sn−1 = Un−1 ⊕ Wn−1 . Proceeding
inductively downward, suppose that Un , Un−1 , . . . , U j+1 , Wn , Wn−1 , . . . , W j+1
have been defined so that Uk = N (Uk+1 ⊕ Wk+1 ), N : Uk+1 ⊕ Wk+1 → Uk is
one-one, and K k+1 = K k ⊕ Uk ⊕ Wk whenever k satisfies j < k ≤ n − 1. We
put Sk = Uk ⊕ Wk for these values of k, and then Sk satisfies the hypothesis of
Lemma 5.23 whenever k satisfies j < k ≤ n − 1. We now construct U j and W j .
We put U j = N (Sj+1 ). Since Sj+1 satisfies the hypothesis of Lemma 5.23, we
see that U j ⊆ K j+1 , N is one-one from Sj+1 into U j , and U j ∩ K j = 0. Thus
we can find a vector subspace W j with K j+1 = K j ⊕ U j ⊕ W j , and the inductive
construction is complete.                                                             §
                                    6. Jordan Canonical Form                                     237

   Lemma 5.25. The vector subspaces of Lemma 5.24 satisfy

                  V = U0 ⊕ W0 ⊕ U1 ⊕ W1 ⊕ · · · ⊕ Un−1 ⊕ Wn−1 .

   PROOF. Iterated use of Lemma 5.24 gives

                 V = K n = K n−1 ⊕ (Un−1 ⊕ Wn−1 )
                   = K n−2 ⊕ (Un−2 ⊕ Wn−2 ) ⊕ (Un−1 ⊕ Wn−1 )
                   = · · · = K 0 ⊕ (U0 ⊕ W0 ) ⊕ · · · ⊕ (Un−1 ⊕ Wn−1 )
                   = (U0 ⊕ W0 ) ⊕ · · · ⊕ (Un−1 ⊕ Wn−1 ),

the last step holding since K 0 = 0, K 0 being the kernel of the identity function.
                                                                                 §

     PROOF OF THEOREM 5.21. We regard N as acting on V = Kn by multiplication
on the left, and we describe an ordered basis in which the matrix of N is in Jordan
form. For 0 ≤ j ≤ n − 1, form a basis of the vector subspace W j of Lemma
5.24, and let v ( j) be a typical member of this basis. Each v ( j) will be used as the
last basis vector corresponding to a Jordan block of size j + 1. The full ordered
basis for that Jordan block will therefore be N j v ( j) , N j−1 v ( j) , . . . , N v ( j) , v ( j) .
The theorem will be proved if we show that the union of these sets as j and v ( j)
vary is a basis of Kn and that N j+1 v ( j) = 0 for all j and v ( j) .
     From the first conclusion of Lemma 5.24 we see for j ∏ 0 that W j ⊆ K j+1 ,
and hence N j+1 (W j ) = 0. Therefore N j+1 v ( j) = 0 for all j and v ( j) .
     Let us prove by induction downward on j that a basis of U j ⊕ W j consists of all
v ( j) and all N k v ( j+k) for k > 0. The base case of the induction is j = n − 1, and
the statement holds in that case since Un−1 = 0 and since the vectors v (n−1) form
a basis of Wn−1 . The inductive hypothesis is that all v ( j+1) and all N k v ( j+1+k) for
k > 0 together form a basis of U j+1 ⊕ W j+1 . The second and third conclusions
of Lemma 5.24 together show that all N v ( j+1) and all N k+1 v ( j+1+k) for k > 0
together form a basis of U j . In other words, all N k v ( j+k) with k > 0 together
form a basis of U j . The vectors v ( j) by construction form a basis of W j , and
U j ∩ W j = 0. Therefore the union of these separate bases is a basis for U j ⊕ W j ,
and the induction is complete.
     Taking the union of the bases of U j ⊕ W j for all j and applying Lemma 5.25,
we see that we have a basis of V = Kn . This shows that the desired set is a basis
of Kn and completes the proof of Theorem 5.21.                                                   §
238                    V. Theory of a Single Linear Transformation

                      7. Computations with Jordan Form

Let us illustrate the computation of Jordan form and the change-of-basis matrix
with a few examples. We are given a matrix A and we seek J and C with
J = C −1 AC. We regard A as the matrix of some linear L in the standard ordered
      6, and∂we regard J as the matrix of L in some other ordered basis 0. Then
basis µ
          I
C=            , and so the columns of C give the members of 0 written as ordinary
        60
column vectors (in the standard ordered basis).

  EXAMPLE 1. This example will be a nilpotent matrix, and we shall compute J
and C merely by interpreting the proof of Theorem 5.21 in concrete terms. Let
                                       √                    !
                                           −1 1 0
                                A=         −1 1 0               .
                                           −1 1 0

The first step is to compute the characteristic polynomial, which is
                                µ X+1   −1 0
                                                ∂                   ≥            ¥
                                                                        X+1 −1
           det(X I − A) = det      1    X−1 0       = X det              1 X−1
                                                                                     = X 3.
                                   1    −1 X


Then A3 = 0 by the Cayley–Hamilton Theorem (Theorem 5.9), and A is indeed
nilpotent. The diagonal entries of J are thus all 0, and we have to compute the
sizes of the various Jordan blocks. To do so, we compute the dimension of the
kernel of each power of A. The dimension of the kernel of a matrix equals the
number of independent variables when we solve AX = 0 by row reduction. With
the first power of A, the variable x1 is dependent, and x2 and x3 are independent.
Also, A2 = 0. Thus

          dim(ker A0 ) = 0,     dim(ker A) = 2,             and         dim(ker A2 ) = 3.

Hence

      #{Jordan blocks of size ∏ 1} = dim(ker A) − dim(ker A0 ) = 2 − 0 = 2,
      #{Jordan blocks of size ∏ 2} = dim(ker A2 ) − dim(ker A) = 3 − 2 = 1.

From these equalities we see that one Jordan block has size 2 and the other has
size 1. Thus                        µ       ∂
                                            01
                                   J=       00          .
                                                    0
                           7. Computations with Jordan Form                         239

We want to set up vector subspaces as in Lemma 5.24 so that K j+1 = K j ⊕U j ⊕W j
and U j = A(U j+1 ⊕ W j+1 ) for 0 ≤ j ≤ 2. Since K 3 = K 2 , the equations begin
with K 2 = · · · and are

       K 2 = K 1 ⊕ 0 ⊕ W1 ,                    K 1 = K 0 ⊕ U0 ⊕ W0 .
                                    U0 = A(0 ⊕ W1 ),
                                                 µ x1 ∂
Here K 2 = K and K 1 is the subspace of all X = x2 such that AX = 0.
            3
                                                                          x3
The space W1 is to satisfy K 2 = K 1 ⊕ W1 , and we see that W1 is 1-dimensional.
Let {v (1) } be a basis of the 1-dimensional vector subspace W1 . Then U0 is
1-dimensional with basis {Av (1) }. The subspace K 1 is 2-dimensional and contains
U0 . The space W0 is to satisfy K 1 = U0 ⊕W0 , and we see that W0 is 1-dimensional.
Let {v (0) } be a basis of W0 . Then the respective columns of C may be taken to be

                                   Av (1) ,   v (1) ,       v (0) .

Let us compute these vectors.
   If we extend a basis of K 1 to a basis of K 2 , then W1 may be taken to be the
linear span of the added vector. Toµ obtain∂ a basis of K 1 , we compute that the
                                         1 −1 0
reduced row-echelon form of A is         0 0 0 , and the resulting system consists of
                                         0 00
the single equation x1 − x2 = 0.       Thus x1 = x2 , and
                           µ x1 ∂             µ1∂              µ0∂
                              x2      = x2     1       + x3       0   .
                              x3               0                  1

The coefficients of x2 and x3 on the right side form a basis of K 1 , and we are to
choose
µ   ∂ a vector that is not a linear combination of these. Thus we can        v (1) =
                                                                   µ 1 ∂take µ     ∂
  1                                                                             −1
  0 as the basis vector of W1 . Then U0 = A(W1 ) has Av    (1)
                                                               = A 0 = −1
  0                                                                            0   −1
                                       taken
as a basis, and the basis of W0 may be µ  ∂ as any vector in K 1 but not U0 . We
                                                   0
can take this basis to consist of v (0) =          0    .
                                                   1                 µ −1 1 0 ∂
  Lining up our three basis vectors as the columns of C gives us C = −1 0 0 .
                           µ 0 −1 0 ∂                                  −1 0 1
                    −1                                            −1
Computation gives C = 1 −1 0 , and we readily check that C AC = J .
                                   0 −1 1

   EXAMPLE 2. We continue with A and J as in Example 1, but we compute the
columns of C without directly following the proof of Theorem 5.21. The method
starts from the fact that each Jordan block corresponds to a 1-dimensional space
of eigenvectors, and then we backtrack to find vectors corresponding to the other
240                    V. Theory of a Single Linear Transformation

columns. For this particular A, we know that the three columns of C are to be of
the form v1 = Av (1) , v2 = v (1) , and v3 = v (0) . The vectors v1 and v3 together
span the 0 eigenspace of A. We find all the 0 eigenvectors, writing them as a
two-parameter family. Ωµ      eigenspace is just K 1 = ker A, and we found in
                         This∂æ
                            x2
Example 1 that K 1 =        x2. One of these vectors is to be v1 , and it has to
                            x3  µ x2 ∂
equal Av2 . Thus we solve Av2 = x2 . Applying the solution procedure yields
                                        x3

                                 √                          !
                                     1 −1 0         −x2
                                     0 0 0            0             .
                                     0 0 0         x3 −x2


This system has no solutions unless x3 − x2 = 0. If we take x2 = x3 = −1, then
                µ first
we obtain the same  ∂ two columns of C as in Example 1, and any vector in K 1
                  −1
independent of    −1    may be taken as the third column.
                  −1

   EXAMPLE 3. Let                      √                    !
                                            2 1 0
                                 A=        −1 4 0               .
                                           −1 2 2
Direct calculation shows that the characteristic polynomial is det(X I − A) =
X 3 − 8X 2 + 21X − 18 = (X − 2)(X − 3)2 . The possibilities for J are therefore
                         µ3 0 0∂                        µ3 1 0∂
                           030               and            030         ;
                           002                              002

the first one will be correct if the dimension of the eigenspace for the eigenvalue 3
is 2, and the second one will be correct if that dimension is 1.
   The third column of C corresponds to an eigenvector for the eigenvalue      µ ∂ 2,
                                                                                           0
hence to a nonzero solution of (A − 2I )v = 0. The solutions are v = k                     0   ,
                         µ0∂                                                               1

and we can therefore use 0 .
                             1
   For the first two columns of C, we have to find ker(A − 3I ) no matter which of
the methods we use, the one in Example 1 or the one in    µ ∂æ 2. Solving the
                                                       Ω Example            1
system of equations, we obtain all vectors in the space z                   1   . The dimension
                                                                            1
of the space is 1, and the second possibility for the Jordan form is the correct one.
   Following the method of Example 1 to find the columns of C means that we
pick a basis of this kernel and extend it to a basis of ker(A − 3I )2 . A basis of
                                         8. Problems                                      241
                                    µ1∂                         µ0               00
                                                                                       ∂
                                                             2
ker(A − 3I ) consists of the vector  1 . The matrix (A − 3I ) is 0               00     , and
                                          1                                   0 −1 1
the solution procedure leads to the formula
                           µa∂         µ1∂  µ0∂
                              b   =a 0 +c 1
                                  c           0        1
                             µ1∂
for its kernel. The vector    1       arises from a = 1 and c = 1. We are to make an
                              1
independent
µ1∂         choice, say a = 1 and c = 0. Then the second basis vector to use is
  0 . This becomes the second column of C, and the first column then has to be
  0      µ 1 ∂ µ −1 ∂                          µ −1 1 0 ∂
(A − 3I ) 0 = −1 . The result is that C = −1 0 0 .
            0           −1                                 −1 0 1
                                                 µ means
  Following the method of Example 2 for this example ∂   that we retain the
                                                               1
entire kernel of A − 3I , namely all vectors v1 = z            1    , as candidates for the
                                                               1
                µ −1The
first column of C.        µ 1 ∂column is to satisfy (A − 3I )v2 = v1 . Solving
                     ∂ second
leads to v2 = z    0   + c 1 . In contrast to Example 2, there is no potential
                    0             1
contradictory equation. So we choose z and then c. µ        ∂ takeµ z−1=
                                                        If we           ∂ 1 and
                                                          1
c = 0, we find that the first two columns of C are to be 1 and        0 . Then
     µ 1 −1 0 ∂                                           1           0

C = 1 00 .
       1   01

   For any example in which we can factor the characteristic polynomial exactly,
either of the two methods used above will work. The first method appears
complicated but uses numbers throughout; it tends to be more efficient with
large examples involving high-degree minimal polynomials. The second method
appears direct but requires solving equations with symbolic variables; it tends to
be more efficient for relatively simple examples.


                                       8. Problems

In Problems 1–25 all vector spaces are assumed finite-dimensional, and all linear
transformations are assumed defined from such spaces into themselves. Unless
information is given to the contrary, the underlying field K is assumed arbitrary.
1. Let Mmn (C) be the vector space of m-by-n complex matrices. The group
     GL(m, C) × GL(n, C) acts on Mmn (C) by ((g, h), x) 7→ gxh −1 , where gxh −1
     denotes a matrix product. Do the following:
242                      V. Theory of a Single Linear Transformation

      (a) Verify that this is indeed a group action.
      (b) Prove that two members of Mmn (C) lie in the same orbit if and only if they
          have the same rank.
      (c) For each possible rank, give an example of a member of Mmn (C) with that
          rank.

2.    Prove that a member of Mn (K) is invertible if and only if the constant term of its
      minimal polynomial is different from 0.

3.    Suppose that L : V → V is a linear map with minimal polynomial M(X) =
      P1 (X)l1 · · · Pk (X)lk and that V = U ⊕ W with U and W both invariant under
      L. Let P1 (X)r1 · ·Ø · Pk (X)rØk and P1 (X)s1 · · · Pk (X)sk be the respective minimal
      polynomials of L ØU and L ØW . Prove that l j = max(r j , s j ) for 1 ≤ j ≤ k.

4.    (a) If A and B are in Mn (K), if P(X) is a polynomial such that P(AB) = 0,
          and if Q(X) = X P(X), prove that Q(B A) = 0.
      (b) What can be inferred from (a) about the relationship between the minimal
          polynomials of AB and of B A?

5.    (a) Suppose that D and D 0 are in Mn (K), are similar to diagonal matrices, and
          have D D 0 = D 0 D. Prove that there is a matrix C such that C −1 DC and
          C −1 D 0 C are both diagonal.
      (b) Give an example of two nilpotent matrices N and N 0 in Mn (K) with N N 0 =
          N 0 N such that there is no C with C −1 N C and C −1 N 0 C both in Jordan form.

6.    (a) Prove that the matrix of a projection is similar to a diagonal matrix. What
          are the eigenvalues?
      (b) Give a necessary and sufficient condition for two projections involving the
          same V to be given by similar matrices.

7.    Let E : V → V and F : V → V be projections. Prove that E and F have
      (a) the same image if and only if E F = F and F E = E,
      (b) the same kernel if and only if E F = E and F E = F.

8.    Let E : V → V and F : V → V be projections. Prove that E F is a projection
      if E F = F E. Prove or disprove a converse.

9.    An involution on V is a linear map U : V → V such that U 2 = I . Show
      that the equation U = 2E − 1 establishes a one-one correspondence between all
      projections E and all involutions U .

9A. Explain how the proof of the converse half of Theorem 5.14 greatly simplifies
    once the Primary Decomposition Theorem (Theorem 5.19) is available.
                                       8. Problems                                    243

10. Let L : V → V be linear. Prove that there exist vector subspaces U and W of V
    such that
         (i) V = U ⊕ W ,
        (ii) L(U ) ⊆ U and L(W ) ⊆ W ,
       (iii) L is nilpotent on U ,
       (iv) L is nonsingular on W .

11. Prove that the vector subspaces U and W in the previous problem are uniquely
    characterized by (i) through (iv).

12. (Special case of Jordan–Chevalley decomposition) Let L : V → V be a
    linear map, and suppose that its minimal polynomial is of the form M(X) =
    Qk                 lj
       j=1 (X −∏ j ) with the ∏ j distinct. Let V = U1 ⊕· · ·⊕Uk be the corresponding
    primary decomposition of V , and define D : V → V by D = ∏1 E 1 +· · ·+∏k E k ,
    where E 1 , . . . , E k are the projections associated with the primary decomposition.
    Finally put N = L − D. Prove that
    (a) L = D + N ,
    (b) D has a basis of eigenvectors,
    (c) N is nilpotent, i.e., has N dim V = 0,
    (d) D N = N D.
    (e) D and N are given by unique polynomials in L such that each of the
         polynomials is equal to 0 or has degree less than the degree of M(X),
                                                Q
    (f) the minimal polynomial of D is kj=1 (X − ∏ j ),
    (g) the minimal polynomial of N is X max l j .

13. (Special case of Jordan–Chevalley decomposition, continued) In the previous
    problem with L given, prove that a decomposition L = D + N is uniquely
    determined by properties (a) through (d). Avoid using (e) in the argument.

14. (a) Let N 0 be a nilpotent square matrix of size n 0 . Prove for arbitrary c ∈ K that
                                                                  0
        the characteristic polynomial of N 0 + cI is (X − c)n , and deduce that the
        only eigenvalue of N 0 + cI is c.
    (b) Let L = D + N be the decomposition in Problems 12 and 13 of a square ma-
        trix L of size n. Prove that L and D have the same characteristic polynomial.
                                      ≥      ¥
                                        −5 9
15. For the complex matrix A = −4 7 , find a Jordan-form matrix J and an
    invertible matrix C such that J = C −1 AC.
                                  µ 4 1 −1 ∂
16. For the complex matrix A = −8 −2 2 , find a Jordan-form matrix J and an
                                        8   2 −2
    invertible matrix C such that J = C −1 AC.
244                     V. Theory of a Single Linear Transformation

17. For the upper triangular matrix
                                         2 0 0 1 1 0 0
                                              2000   1   1
                                              201   0   0
                                                           
                                     
                                   A=
                                               20   1   2,
                                                2   1   1
                                                     2   1
                                                         3

      find a Jordan-form matrix J and an invertible matrix C such that J = C −1 AC.
18. (a) For M3 (C), prove that any two matrices with the same minimal polynomial
        and the same characteristic polynomial must be similar.
    (b) Is the same thing true for M4 (C)?
19. Suppose that K has characteristic 0 and that J is a Jordan block with nonzero
    eigenvalue and with size > 1. Prove that there is no n ∏ 1 such that J n is
    diagonal.
20. Classify up to similarity all members A of Mn (C) with An = I .
21. How many similarity classes are there of 3-by-3 matrices A with entries in C
    such that A3 = A? Explain.
22. Let n ∏ 2, and let N be a member of Mn (K) with N n = 0 but N n−1 6= 0. Prove
    that there is no n-by-n matrix A with A2 = N .
23. For a Jordan block J , prove that J t is similar to J .
24. Prove that if A is in Mn (C), then At is similar to A.
                                    ≥ ¥
25. Let N be the 2-by-2 matrix 00 10 , and let A and B be the 4-by-4 matrices
        ≥       ¥          ≥      ¥
    A = N0 N0 and B = N0 N      N
                                    . Prove that A and B are similar.

Problems 26–31 concern cyclic vectors. Fix a linear map L : V → V from a finite-
dimensional vector space V to itself. For v in V , let P(v) denote the set of all vectors
Q(L)(v) in V for Q(X) in K[X]; P(v) is a vector subspace and is invariant under
L. If U is an invariant subspace of V , we say that U is a cyclic subspace if there is
some v in U such that P(v) = U ; in this case, v is said to be a cyclic vector for U ,
and U is called the cyclic subspace generated by v. For v in V , let Iv be the ideal
of all polynomials Q(X) in K[X] with Q(L)v = 0. The monic generator of v is the
unique monic polynomial Mv (X) such that Mv (X) divides every member of Iv .
26. For v ∈ V , explain why Iv is nonzero and why Mv (X) therefore exists.
27. For v ∈ V , prove that
    (a) the degree of the monic generator Mv (X) equals the dimension of the cyclic
        subspace P(v),
    (b) the vectors v, L(v), L 2 (v), . . . , L deg Mv −1 (v) form a vector-space basis of
        P(v),
                                      8. Problems                                  245
                                                        Ø
    (c) the minimal and characteristic polynomials of L ØP(v) are both equal to
        Mv (X).
28. Suppose                                        d−1 + X d . Prove that the matrix
        Ø that Mv (X) = c0 + c1 X + · · · + cd−1 X
        Ø
    of L P(v) in a suitable ordered basis is
                                  −c                      
                                     d−1 1 0 ···

                                  −cd−2 0 1            
                                  −cd−3 0 0            
                                     ..     . . . . .. 
                                                . . .
                                      .                .
                                    −c2 0 0  ··· 0 1 0 
                                                       
                                     −c1 0 0 ···    0 01
                                     −c0 0 0 ···      00


29. Suppose that v is in V , that Mv (X) is a power of a prime polynomial P(X),
    and that Q(X) is a nonzero polynomial with deg Q(X) < deg P(X). Prove that
    P(Q(L)(v)) = P(v).
30. Let P(X) be a prime polynomial.
    (a) Prove by induction on dim V that if the minimal polynomial of L is P(X),
        then the characteristic polynomial of L is a power of P(X).
    (b) Prove by induction on l that if the minimal polynomial of L is P(X)l , then
        the characteristic polynomial of L is a power of P(X).
    (c) Conclude that if the minimal polynomial of L is a power of P(X), then
        deg P(X) divides dim V .
31. Prove that every prime factor of the characteristic polynomial of L divides the
    minimal polynomial of L.
Problems 32–40 continue the study of cyclic vectors begun in Problems 26–31, using
the same notation. The goal is to obtain a canonical-form theorem like Theorem 5.20
for L but with no assumption on K or P(X), namely that each primary subspace for
L is the direct sum of cyclic subspaces and the resulting decomposition is unique
up to isomorphism. This result and the Fundamental Theorem of Finitely Generated
Abelian Groups (Theorem 4.56) will be seen in Chapter VIII to be special cases of
a single more general theorem. Still another canonical form for matrices and linear
maps is an analog of the result with elementary divisors mentioned in the remarks
with Theorem 4.56 and is valid here; it is called rational canonical form, but we shall
not pursue it until the problems at the end of Chapter VIII. The proof in Problems
32–40 uses ideas similar to those used for Theorem 5.21 except that the hypothesis
will now be that the minimal polynomial of L is P(X)l with P(X) prime, rather than
just X l . Define K j = ker(P(L) j ) for j ∏ 0, so that K 0 = 0, K j ⊆ K j+1 for all j,
K l = V , and each K j is an invariant subspace under L. Define d = deg P(X).
32. Suppose j ∏ 1, and suppose Sj is any vector subspace of V such that K j+1 =
    K j ⊕ Sj . Prove that P(L) is one-one from Sj into K j and P(L)(Sj ) ∩ K j−1 = 0.
246                    V. Theory of a Single Linear Transformation

33. Define Ul = Wl = 0. For 0 ≤ j ≤ l − 1, prove that there exist vector subspaces
    U j and W j of K j+1 such that
                              K j+1 = K j ⊕ U j ⊕ W j ,
                                 U j = P(L)(U j+1 ⊕ W j+1 ),
                        P(L) : U j+1 ⊕ W j+1 → U j        is one-one.
34. Prove that the vector subspaces of the previous problem satisfy
                    V = U0 ⊕ W0 ⊕ U1 ⊕ W1 ⊕ · · · ⊕ Ul−1 ⊕ Wl−1 .

35. For v 6= 0 in W j , prove that the set of all L r P(L)s (v) with 0 ≤ r ≤ d − 1 and
    0 ≤ s ≤ j is a vector-space basis of P(v).
36. Going back over the construction in Problem 33, prove that each W j can be
                                                       ( j)
    chosen to have a basis consisting of vectors L r (vi ) for 1 ≤ i ≤ (dim W j )/d
    and 0 ≤ r ≤ d − 1.
37. Let the index i used in the previous problem with j be denoted by i j for 1 ≤
    i j ≤ (dim W j )/d. Prove that a vector-space basis of U j ⊕ W j consists of all
                ( j+k)
    L r P(L)k (vi j+k ) for 0 ≤ r ≤ d − 1, k ∏ 0, 1 ≤ i j+k ≤ (dim W j+k )/d.
38. Prove that V is the direct sum of cyclic subspaces under L. Prove specifically
               ( j)
    that each vi j generates a cyclic subspace and that the sum of all these vector
    subspaces, with 0 ≤ j ≤ l and 1 ≤ i j ≤ (dim W j )/d, is a direct sum and
    equals V .
39. In the decomposition of the previous problem, each cyclic subspace generated
               ( j)
    by some vi j has minimal polynomial P(X) j+1 . Prove that
        Ω                                         æ
         direct summands with minimal polynomial
      #                                             = (dim K j+1 − dim K j )/d.
          P(X)k for some k ∏ j + 1

40. Prove that the formula of the previous problem persists for any decomposition
    of V as the direct sum of cyclic subspaces, and conclude from Problem 28 that
    the decomposition into cyclic subspaces is unique up to isomorphism.
Problems 41–46 concern systems of ordinary differential equations with constant
coefficients. The underlying field is taken to be C, and differential calculus is used.
                                                P∞ t k Ak
For A in Mn (C) and t in R, define et A =         k=0 k! . Take for granted that the
                 t A
series defining e converges entry by entry, that the series may be differentiated term
                    d
by term to yield dt   (et A ) = Aet A = et A A, and that es A+t B = es A et B if A and B
commute.
41. Calculate
        ≥     e¥t A for A equal to
           01
    (a) −1 0 ,
                                       8. Problems                                    247
          ≥        ¥
              01
    (b)       10
                    ,
    (c) the diagonal matrix with diagonal entries d1 , . . . , dn .
42. (a) Calculate et J when J is a nilpotent n-by-n Jordan block.
    (b) Use (a) to calculate et J when J is a general n-by-n Jordan block.
43. Let y1 , . . . , yn be unknown functions from R to C, and let y be the vector-valued
    function formed by arranging y1 , . . . , yn in a column. Suppose that A is in
    Mn (C). Prove for each vector v ∈ Cn that y(t) = et A v is a solution of the
    system of differential equations dy dt = Ay(t).
44. With notation as in the previous problem and with v fixed in Cn , use e−t A y(t)
    to show, for each open interval of t’s containing 0, that the only solution of
    dy                                                         tA
     dt = Ay(t) on that interval such that y(0) = v is y(t) = e v.
                                       −1
45. For C invertible, prove that etC AC = C −1 et A C, and deduce a relationship
    between solutions of dy                          dy
                         dt = Ay(t) and solutions of dt = (C
                                                             −1 AC)y(t).
            µ 2 1 0∂
46. Let A = −1 4 0 . Taking into account Example 3 in Section 7 and Problems
                        −1 2 2
                                                                             dy
    42 through 45 above, find all solutions for t in (−1, 1) to the system   dt   = Ay(t)
                     µ1∂
    such that y(0) = 2 .
                                 3
                                     CHAPTER VI

                                Multilinear Algebra



Abstract. This chapter studies, in the setting of vector spaces over a field, the basics concerning
multilinear functions, tensor products, spaces of linear functions, and algebras related to tensor
products.
    Sections 1–5 concern special properties of bilinear forms, all vector spaces being assumed to be
finite-dimensional. Section 1 associates a matrix to each bilinear form in the presence of an ordered
basis, and the section shows the effect on the matrix of changing the ordered basis. It then addresses
the extent to which the notion of “orthogonal complement” in the theory of inner-product spaces
applies to nondegenerate bilinear forms. Sections 2–3 treat symmetric and alternating bilinear forms,
producing bases for which the matrix of such a form is particularly simple. Section 4 treats a related
subject, Hermitian forms when the field is the complex numbers. Section 5 discusses the groups that
leave some particular bilinear and Hermitian forms invariant.
    Section 6 introduces the tensor product of two vector spaces, working with it in a way that does
not depend on a choice of basis. The tensor product has a universal mapping property—that bilinear
functions on the product of the two vector spaces extend uniquely to linear functions on the tensor
product. The tensor product turns out to be a vector space whose dual is the vector space of all
bilinear forms. One particular application is that tensor products provide a basis-independent way
of extending scalars for a vector space from a field to a larger field. The section includes a number
of results about the vector space of linear mappings from one vector space to another that go hand
in hand with results about tensor products. These have convenient formulations in the language of
category theory as “natural isomorphisms.”
    Section 7 begins with the tensor product of three and then n vector spaces, carefully considering
the universal mapping property and the question of associativity. The section defines an algebra
over a field as a vector space with a bilinear multiplication, not necessarily associative. If E is a
vector space, the tensor algebra T (E) of E is the direct sum over n ∏ 0 of the n-fold tensor product
of E with itself. This is an associative algebra with a universal mapping property relative to any
linear mapping of E into an associative algebra A with identity: the linear map extends to an algebra
homomorphism of T (E) into A carrying 1 into 1.
    Sections 8–9 define the symmetric and exterior algebras of a vector space E. The symmetric al-
gebra S(E) is a quotient of T (E) with the following universal mapping property: any linear mapping
of E into a commutative associative algebra A with identity extends to an algebra homomorphism
of S(E) into A carrying 1 into 1. The symmetric algebra is commutative. Similarly the exterior
         V
algebra (E) is a quotient of T (E) with this universal mapping property: any linear mapping l of
E into an associative algebra A with identity such that l(v)2 = 0 for all v ∈ E extends to an algebra
                     V
homomorphism of (E) into A carrying 1 into 1.
    The problems at the end of the chapter introduce some other algebras that are of importance
in applications, and the problems relate some of these algebras to tensor, symmetric, and exterior
algebras. Among the objects studied are Lie algebras, universal enveloping algebras, Clifford
algebras, Weyl algebras, Jordan algebras, and the division algebra of octonions.

                                                 248
                               1. Bilinear Forms and Matrices                                                  249

                          1. Bilinear Forms and Matrices

This chapter will work with vector spaces over a common field of “scalars,” which
will be called K. In Section 6 a field containing K as a subfield will briefly play
a role, and that will be called L.
   If V is a vector space over K, a bilinear form on V is a function from V × V
into K that is linear in each variable when the other variable is held fixed.

   EXAMPLES.
   (1) For general K, take V = Kn . Any matrix A in Mn (K) determines a bilinear
form by the rule hv, wi = v t Aw.
  (2) For K = R, let V be an inner-product space, in the sense of Chapter III,
with inner product ( · , · ). Then ( · , · ) is a bilinear form on V .

    Multilinear functionals on a vector space of row vectors, also called k-linear
functionals or k-multilinear functionals, were defined in the course of working
with determinants in Section II.7, and that definition transparently extends to
general vector spaces. A bilinear form on a general vector space is then just a
2-linear functional. From the point of view of definitions, the words “functional”
and “form” are interchangeable here, but the word “form” is more common in
the bilinear case because of a certain homogeneity that it suggests and that comes
closer to the surface in Corollary 6.12 and in Section 7.
    For the remainder of this section, all vector spaces will be finite-dimensional.
    Bilinear forms, i.e., 2-linear functionals, are of special interest relative to k-
linear functionals for general k because of their relationships with matrices and
linear mappings. To begin with, each bilinear form, in the presence of an ordered
basis, is given by a matrix. In more detail let V be a finite-dimensional vector
space, and let h · , · i be a bilinear form on V . If an ordered basis 0 = (v1 , . . . , vn )
of V is specified, then the bilinear form determines the matrix B with entries
              j i. Conversely we
Bi j = hvi , vP                    Pcan recover the bilinear form from B as follows:
Write v = i ai vi and w = j b j v j . Then
                            ≠P                   P              Æ P
                 hv, wi =          i   ai vi ,       j   b j v j = i, j ai hvi , v j ib j .
                                                                  µ       ∂             µ       ∂
                               t                                      v                     w
In other words, hv, wi = a Bb, where a =                                      and b =               in the notation
                                                                      0                     0
of Section II.3. Therefore
                                                 µ           ∂t       µ     ∂
                                                         v                w
                               hv, wi =                           B          .
                                                         0                0
250                              VI. Multilinear Algebra

Consequently we see that all bilinear forms on a finite-dimensional vector space
reduce to Example 1 above—once we choose an ordered basis.
   Let us examine the effect of a change of ordered basis. Suppose that 0 =
(v1 , . . . , vm ) and 1 = (w1 , . . . , wn ), and let B and C be the matrices of the
                                                                          ∂ i , w j i. Let
bilinear form in these two ordered bases: Bi j = hvi , v j i and Cµi j = hw
                                          P                            I
the two bases be related by w j = i ai j vi , i.e., let [ai j ] =          . Then we
                                                                      01
have
                          ≠P           P        Æ P                   P
     Ci j = hwi , w j i =    aki vk , al j vl = aki al j hvk , vl i = aki Bkl al j .
                           k         l            k,l                  k,l

Translating this formula into matrix form, we obtain the following proposition.

   Proposition 6.1. Let h · , · i be a bilinear form on a finite-dimensional vector
space V , let 0 and 1 be ordered bases of V , and let B and C be the respective
matrices of h · , · i relative to 0 and 1. Then
                                     µ     ∂t µ      ∂
                                        I          I
                                C=           B         .
                                       01         01

   The qualitative conclusion about the matrices may be a little unexpected. It
is not that they are similar but that they are related by C = S t B S for some
nonsingular square matrix S. In particular, B and C need not have the same
determinant.
   Guided by the circle of ideas around the Riesz Representation Theorem for
inner products (Theorem 3.12), let us examine what happens when we fix one
of the variables of a bilinear form and work with the resulting linear map. Thus
again let h · , · i be a bilinear form on V . For fixed u in V , v 7→ hu, vi is a linear
functional on V , thus a member of the dual space V 0 of V . If we write L(u) for this
linear functional, then L is a function from V to V 0 satisfying L(u)(v) = hu, vi.
The formula for L shows that L is in fact a linear function. We define the left
radical, lrad, of h · , · i to be the kernel of L; thus
                       °        ¢
                  lrad h · , · i = {u ∈ V | hu, vi = 0 for all v ∈ V }.

Similarly we let R : V → V 0 be the linear map R(v)(u) = hu, vi, and we define
the right radical, rrad, of h · , · i to be the kernel of R; thus
                    °       ¢
              rrad h · , · i = {v ∈ V | hu, vi = 0 for all u ∈ V }.

   EXAMPLE 1, CONTINUED. The vector space V is the space Kn of n-dimensional
column vectors, the dual V 0 is the space of n-dimensional row vectors, A is
                             1. Bilinear Forms and Matrices                       251

an n-by-n matrix with entries in K, and h · , · i is given by hu, vi = u t Av =
L(u)(v) = R(v)(u) for u and v in Kn . Explicit formulas for L and R are
given by
                               L(u) = u t A = (At u)t
and                            R(v) = (Av)t .

Thus
                         °         ¢
                     lrad h · , · i = ker L = null space(At ),
                         °         ¢
                     rrad h · , · i = ker R = null space(A).

Since A is square and since the row rank and column rank of A are equal, the
dimensions of the null spaces of A and At are equal. Hence
                              °       ¢          °         ¢
                    dim lrad h · , · i = dim rrad h · , · i .

   This equality of dimensions for the case of Kn extends to general V , as is noted
in the next proposition.

   Proposition 6.2. If h · , · i is any bilinear form on a finite-dimensional vector
space V , then                   °       ¢            °       ¢
                     dim lrad h · , · i = dim rrad h · , · i .
    PROOF. We saw above that computations with bilinear forms of V reduce, once
we choose an ordered basis for V , to computations with matrices, row vectors, and
column vectors. Thus the argument just given in the continuation of Example 1
is completely general, and the proposition is proved.                           §

    A bilinear form h · , · i is said to be nondegenerate if its left radical is 0. In
view of the Proposition 6.2, it is equivalent to require that the right radical be 0.
When the radicals are 0, the associated linear maps L and R from V to V 0 are
one-one. Since dim V = dim V 0 , it follows that L and R are onto V 0 . Thus a
nondegenerate bilinear form on V sets up two canonical isomorphisms of V with
its dual V 0 .
    For definiteness let us work with the linear mapping L : V → V 0 given by
L(u)(v) = hu, vi. If U ⊆ V is a vector subspace, define

                    U ⊥ = {u ∈ V | hu, vi = 0 for all v ∈ U }.

It is apparent from the definitions that
                                        °         ¢Ø
                          U ∩ U ⊥ = lrad h · , · i ØU ×U .
252                             VI. Multilinear Algebra

In contrast to the special case that K = R and the bilinear form is an inner
product, U ∩ U ⊥ may be nonzero even if h · , · i is nondegenerate. For example
let V = R2 , define        D≥x ¥ ≥y ¥E
                               1       1
                              x2 , y2      = x1 y1 − x2 y2 ,
                                                                     n≥ x ¥o
                                                                          1
and suppose that U is the 1-dimensional vector subspace U =             x1    . The
                                                             ≥     ¥
                                                               1 0
matrix of the bilinear form in the standard ordered basis is 0 −1 ; since the matrix
       n≥ y ¥o the bilinear form is nondegenerate. Direct calculation shows that
is nonsingular,
   ⊥        1
U =        y1    = U , so that U ∩U ⊥ 6= 0. Nevertheless, in the nondegenerate case
the dimensions of U and U ⊥ behave as if U ⊥ were an orthogonal complement.
The precise result is as follows.

  Proposition 6.3. If h · , · i is a nondegenerate bilinear form on the finite-
dimensional vector space V and if U is a vector subspace of V , then
                            dim V = dim U + dim U ⊥ .
   PROOF. Define ` : V → U 0 by `(v)(u) = hv, ui for v ∈ V and u ∈ U . The
definition of U ⊥ shows that ker ` = U ⊥ . To see that image ` = U 0 , choose a
vector subspace U1 of V with V = U ⊕ U1 , let u 0 be in U 0 , and define v 0 in V 0 by
                                  Ω 0
                                    u      on U,
                             v0 =
                                    0      on U1 .
Since h · , · i is nondegenerate, the linear mapping L : V → V 0 is onto V 0 . Thus
we can choose v ∈ V with L(v) = v 0 . Then
                  `(v)(u) = hv, ui = L(v)(u) = v 0 (u) = u 0 (u)
for all u in U , and hence `(v) = u 0 . Therefore image ` = U 0 , and we conclude
that
dim V = dim(ker `) + dim(image `) = dim U ⊥ + dim U 0 = dim U ⊥ + dim U.
                                                                                   §

    Corollary 6.4. If h · , · i is a nondegenerate bilinear form on the finite-
dimensional vector spaceØ     V and if U is a vector subspace of V , then V = U ⊕U ⊥
if and only if h · , · iØU ×U is nondegenerate.
   PROOF. Corollary 2.29 and Proposition 6.3 together give
         dim(U + U ⊥ ) + dim(U ∩ U ⊥ ) = dim U + dim U ⊥ = dim V.
                                                                     Ø
Thus U + U ⊥ = V if and only if U ∩ U ⊥ = 0, if and only if h · , · iØU ×U is
nondegenerate. The result therefore follows from Proposition 2.30.         §
                              2. Symmetric Bilinear Forms                         253

                         2. Symmetric Bilinear Forms

We continue with the setting in which K is a field and all vector spaces of interest
are defined over K and are finite-dimensional.
    A bilinear form h · , · i on V is said to be symmetric if hu, vi = hv, ui for
all u and v in V , skew-symmetric if hu, vi = −hv, ui for all u and v in V , and
alternating if hu, ui = 0 for all u in V .
    “Alternating” always implies “skew-symmetric.” In fact, if h · , · i is alternat-
ing, then 0 = hu + v, u + vi = hu, ui + hu, vi + hv, ui + hv, vi = hu, vi + hv, ui;
thus h · , · i is skew-symmetric. If K has characteristic different from 2, then the
converse is valid: “skew-symmetric” implies “alternating.” In fact, if h · , · i is
skew-symmetric, then hu, ui = −hu, ui and hence 2hu, ui = 0; thus hu, ui = 0,
and h · , · i is alternating.
    Let us examine further the effect of the characteristic of K. If, on the one hand,
K has characteristic different from 2, the most general bilinear form h · , · i is the
sum of the symmetric form h · , · is and the alternating form h · , · ia given by
                           hu, vis = 12 (hu, vi + hv, ui),
                           hu, via = 12 (hu, vi − hv, ui).
In this sense the symmetric and alternating bilinear forms are the extreme cases
among all bilinear forms, and we shall study the two cases separately.
   If, on the other hand, K has characteristic 2, then “alternating” implies “skew-
symmetric” but not conversely. “Alternating” is a serious restriction, and we
shall be able to deal with it. However, “symmetric” and “skew-symmetric” are
equivalent since 1 = −1, and thus neither condition is much of a restriction; we
shall not attempt to say anything insightful in these cases.
   In this section we study symmetric bilinear forms, obtaining results when K
has characteristic different from 2. From the symmetry it is apparent that the
left and right radicals of a symmetric bilinear form are the same, and we call
this vector subspace the radical of the form. By way of an example, here is a
continuation of Example 1 from the previous section.

    EXAMPLE. Let V = Kn , let A be a symmetric n-by-n matrix (i.e., one with
 t
A = A), and let hu, vi = u t Av. The computation hv, ui = v t Au = (v t Au)t =
u t At v = u t Av = hu, vi shows that the bilinear form h · , · i is symmetric; the
second equality v t Au = (v th Au)t holds since v t Au is a 1-by-1 matrix.

   Again the example is completely general. In fact, if 0 = (v1 , . . . , vn ) is an
ordered basis of a vector space V and if h · , · i is a given symmetric bilinear form
on V , then the matrix of the form has entries Ai j = hvi , v j i, and these evidently
satisfy Ai j = A ji . So A is a symmetric matrix, and computations with the bilinear
form are reduced to those used in the example.
254                                     VI. Multilinear Algebra

    Theorem 6.5 (Principal Axis Theorem). Suppose that K has characteristic
different from 2.
    (a) If h · , · i is a symmetric bilinear form on a finite-dimensional vector space
V , then there exists an ordered basis of V in which the matrix of h · , · i is diagonal.
    (b) If A is an n-by-n symmetric matrix, then there exists a nonsingular n-by-n
matrix M such that M t AM is diagonal.
    REMARKS. Because computations with general symmetric bilinear forms
reduce to computations in the special case of a symmetric matrix and because
Proposition 6.1 tells the effect of a change of ordered basis, (a) and (b) amount
to the same result; nevertheless, we give two proofs of Theorem 6.5—a proof via
matrices and a proof via linear maps. A hint of the validity of the theorem comes
from the case that K = R. For the field R when the bilinear form is an inner
product, the Spectral Theorem (Theorem 3.21) says that there is an orthonormal
basis of eigenvectors and hence that (a) holds. When K = R, the same theorem
says that there exists an orthogonal matrix M with M −1 AM diagonal; since any
orthogonal matrix M satisfies M −1 = M t , the Spectral Theorem is saying that
(b) holds.
   PROOF VIA MATRICES. If A is an n-by-n symmetric matrix, we seek a non-
singular M with M t AM diagonal. We induct on the size of A, the base case of
the induction being n = 1, where there is nothing to prove. Assume the result to
be known
      ≥     ¥ size n − 1, and write the given n-by-n matrix A in block form as
            for
        a b
A = bt d with d of size 1-by-1. If d 6= 0, let x be the column vector −d −1 b.
Then                    ≥ ¥≥         ¥≥      ¥ ≥ ¥
                                a b     I 0
                          I x
                          0 1   bt d    xt 1
                                              = ∗0 d0 ,

and the induction goes through. If d = 0, we argue in a different way. We may
assume that b 6= 0 since otherwise the result is immediate by induction. Say
bi 6= 0 with 1 ≤ i ≤ n − 1. Let y be an (n − 1)-dimensional row vector with i th
entry a member δ of K to be specified and with other entries 0. Then
            ≥         ¥≥ a b¥≥ I   yt
                                        ¥       ≥∗          ∗            ¥       ≥∗        ∗          ¥
                I 0
                y 1
                                            =        yay t +bt y t +yb
                                                                             =        δ 2 aii +2δbi
                                                                                                          .
                         bt 0   0 1              ∗                                ∗

Since K has characteristic different from 2, 2bi is not 0; thus there is some value
of δ for which δ 2 aii + 2δbi 6= 0. Then we are reduced to the case d 6= 0, which
we have already handled, and the induction goes through.                         §
   PROOF VIA LINEAR MAPS. We may assume that the given symmetric bilinear
form is not identically 0, since otherwise
                                    °        ¢any basis will do. Let the radical of
the form be denoted by rad = rad h · , · i . ChooseØ        a vector subspace S of V
such that V = rad ⊕S, and put [ · , · ] = h · , · iØ S×S . Then [ · , · ] is a symmetric
                              2. Symmetric Bilinear Forms                               255

bilinear form on S, and it is nondegenerate. In fact, [u, · ] = 0 means hu, vi = 0
for all v ∈ S; since hu, vi = 0 for v in rad anyway, hu, vi = 0 for all v ∈ V , u is
in rad as well as S, and u = 0.
    Since h · , · i is not identically 0, the subspace S is not 0. Thus the nondegen-
erate symmetric bilinear form [ · , · ] on S is not 0. Since
                                 °                                 ¢
                      [u, v] = 12 [u + v, u + v] − [u, u] − [v, v] ,
                                                                               Ø
it follows that [v, v] 6= 0 for some v in S. Put U1 = Kv. Then [ · , · ]Ø          U1 ×U1
is nondegenerate, and Corollary 6.4 implies that S = U1 ⊕ U1⊥ . Applying
                                                                       Ø        the
converse direction of the same corollary to U1⊥ , we see that [ · , · ]ØU ⊥ ×U ⊥ is
                                                                               1    1
nondegenerate. Repeating this construction with U ⊥ and iterating, we obtain

                             V = rad ⊕U1 ⊕ · · · ⊕ Uk

with hUi , U j i = 0 for i 6= j and with dim Ui = 1 for all i. This completes the
proof.                                                                         §

    Theorem 6.5 fails in characteristic 2. Problem 2 at the end of the chapter
illustrates the failure.
    Let us examine the matrix version of Theorem 6.5 more closely when K is C or
R. The theorem says that if A is n-by-n symmetric, then we can find a nonsingular
M with B = M t AM diagonal. Taking D diagonal and forming C = D t B D,
we see that we can adjust the diagonal entries of B by arbitrary nonzero squares.
Over C, we can therefore arrange that C is of the form diag(1, . . . , 1, 0, . . . , 0).
The number of 1’s equals the rank, and this has to be the same as the rank of the
given matrix A. The form is nondegenerate if and only if there are no 0’s. Thus
we understand everything about the diagonal form.
    Over R, matters are more subtle. We can arrange that C is of the form
diag(±1, . . . , ±1, 0, . . . , 0), the various signs ostensibly not being correlated.
Replacing C by P t C P with P a permutation matrix, we may assume that our
diagonal matrix is of the form diag(+1, . . . , +1, −1, . . . , −1, 0, . . . , 0). The
number of +1’s and −1’s together is again the rank of A, and the form is
nondegenerate if and only if there are no 0’s. But what about the separate numbers
of +1’s and −1’s? The triple given by
                                       °                         ¢
                       ( p, m, z) = #(+1)’s, #(−1)’s, #(0)’s

is called the signature of A when K = R. A similar notion can be defined in the
case of a symmetric bilinear form over R.

  Theorem 6.6 (Sylvester’s Law). The signature of an n-by-n symmetric matrix
over R is well defined.
256                                   VI. Multilinear Algebra

    PROOF. The integer p + m is the rank, which does not change under a trans-
formation A 7→ M t AM if M is nonsingular. Thus we may take z as known. Let
( p0 , m 0 , z) and ( p, m, z) be two signatures for a symmetric matrix A, with p0 ≤ p.
Define the corresponding symmetric bilinear form on Rn by hu, vi = u t Av. Let
(v10 , . . . , vn0 ) and (v1 , . . . , vn ) be ordered bases of Rn diagonalizing the bilinear
form and exhibiting the resulting signature, i.e., having hvi0 , v j0 i = hvi , v j i = 0
for i 6= j and having
                                   
                                    +1          for 1 ≤ j ≤ p0 ,
                        0     0
                     hv j , v j i = −1           for p0 + 1 ≤ j ≤ n − z,
                                   
                                     0           for n − z + 1 ≤ j ≤ n,
                                   
                                    +1          for 1 ≤ j ≤ p,
                     hv j , v j i = −1           for p + 1 ≤ j ≤ n − z,
                                   
                                     0           for n − z + 1 ≤ j ≤ n.

We shall prove that {v1 , . . . , v p , v 0p0 +1 , . . . , vn0 } is linearly independent, and then
we must have p0 ∏ p. Reversing the roles of p and p0 , we see that p0 = p and
m 0 = m, and the theorem is proved. Thus suppose we have a linear dependence:

                     a1 v1 + · · · + a p v p = b p0 +1 v 0p0 +1 + · · · + bn vn0 .

Let v be the common value of the two sides of this equation. Then

                                                                               p
                                                                               X
          hv, vi = ha1 v1 + · · · + a p v p , a1 v1 + · · · + a p v p i =            a j2 ∏ 0
                                                                               j=1
and
                                                                                         n−z
                                                                                         X
hv, vi = hb p0 +1 v 0p0 +1 + · · · + bn vn0 , b p0 +1 v 0p0 +1 + · · · + bn vn0 i = −              b2j ≤ 0.
                                                                                        j= p0 +1

                                      Pp
We conclude that hv, vi = 0, j=1 a j2 = 0, and a1 = · · · = a p = 0. Thus v = 0
and b p0 +1 v 0p0 +1 + · · · + bn vn0 = 0. Since {v 0p0 +1 , . . . , vn0 } is linearly independent,
we obtain also b p0 +1 = · · · = bn = 0. Therefore {v1 , . . . , v p , v 0p0 +1 , . . . , vn0 } is a
linearly independent set, and the proof is complete.                                              §


                              3. Alternating Bilinear Forms

We continue with the setting in which K is a field and all vector spaces of interest
are defined over K and are finite-dimensional.
                                 3. Alternating Bilinear Forms                               257

    In this section we study alternating bilinear forms, imposing no restriction on
the characteristic of K. From the skew symmetry of any alternating bilinear form
it is apparent that the left and right radicals of such a form are the same, and we
call this vector subspace the radical of the form. First let us consider examples
given in terms of matrices. Temporarily let us separate matters according to the
characteristic.

   EXAMPLE 1 OF SECTION 1 WITH K OF CHARACTERISTIC 6= 2. Let V =
K , let A be a skew-symmetric n-by-n matrix (i.e., one with At = −A), and
  n

let hu, vi = u t Av. The computation hv, ui = v t Au = (v t Au)t = u t At v =
−u t Av = −hu, vi shows that the bilinear form h · , · i is skew-symmetric, hence
alternating.

   EXAMPLE 1 OF SECTION 1 WITH K OF CHARACTERISTIC = 2. Let V = Kn , let
A be an n-by-n matrix, and define hu, vi = u t Av. We suppose that A is skew-
symmetric; it is the same to assume that A is symmetric since the characteristic
is 2. In order to have hei , ei i = 0 for each standard basis vector, we shall
assume that Aii =P                          a column vectorP
                        0 for all i. If u isP               with entries u 1 , . . . , u n , then
hu, ui = u t Au = i, j u i Ai j u j = i6= j u i Ai j u j = i< j (Ai j u i u j + A ji u i u j ) =
P
   i< j 2Ai j u i u j = 0. Hence the bilinear form h · , · i is alternating.

    Again the examples are completely general. In fact, if 0 = (v1 , . . . , vn ) is
an ordered basis of a vector space V and if h · , · i is a given alternating bilinear
form, then the matrix of the form has entries Ai j = hvi , v j i that evidently satisfy
Ai j = −A ji and Aii = 0. So A is a skew-symmetric matrix with 0’s on the
diagonal, and computations with the bilinear form are reduced to those used in
the examples. To keep the terminology parallel, let us say that a square matrix is
alternating if it is skew-symmetric and has 0’s on the diagonal.

   Theorem 6.7.
   (a) If h · , · i is an alternating bilinear form on a finite-dimensional vector space
V , then there exists an ordered basis of V in which the matrix of h · , · i has the
form                                                           
                                01
                              −1 0                                           
                                      01
                                                                              
                                                                             
                                     −1 0                                    
                                                                             
                                            ..                               
                                                 .                           .
                                                                             
                                                      01                     
                                                     −1 0
                                                                              
                                                                             
                                                            0                
                                                                ..           
                                                                      .
                                                                          0
258                                  VI. Multilinear Algebra

If h · , · i is nondegenerate, then dim V is even.
    (b) If A is an n-by-n alternating matrix, then there exists a nonsingular n-by-n
matrix M such that M t AM is as in (a).
   PROOF. It is enough to prove (a). Let rad be the radical of the given form h · , · i,
and choose a vector subspace S of V with V = rad ⊕S. The restriction of h · , · i
to S is then alternating and nondegenerate. We may now proceed by induction
on dim V under the assumption that h · , · i is nondegenerate. For dim V = 1, the
form is degenerate. For dim V = 2, we can find u and v with hu, vi 6= 0, and we
can normalize one of the vectors to make hu, vi = 1. Then (u, v) is the required
ordered basis.
   Assuming the result in the nondegenerate case for dimension < n, suppose that
dim V = n. Again choose u and
              Ø                       ¥ hu, vi = 1, and define U = Ku ⊕ Kv.
                               ≥ v with
                                  0 1
Then h · , · iØU ×U has matrix −1 0 and is nondegenerate. By Corollary 6.4,
                  ⊥
V =U     Ø ⊕ U , and an application of the converse of the corollary shows      that
         Ø                                                                ⊥
h · , · i U ⊥ ×U ⊥ is nondegenerate. The induction hypothesis applies to U , and we
obtain the desired matrix for the given form.                                     §


                                    4. Hermitian Forms

In this section the field will be C, and V will be a finite-dimensional vector space
over C.
   A sesquilinear form h · , · i on V is a function from V × V into C that is linear
in the first variable and conjugate linear in the second.1 Sesquilinear forms do
not make sense for general fields because of the absence of a universal analog of
complex conjugation, and we shall consequently work only with the field C in
this section.2
   A sesquilinear form h · , · i is Hermitian if hu, vi = hv, ui for all u and v in
V . The form is skew-Hermitian if instead hu, vi = −hv, ui for all u and v in
V . Hermitian and skew-Hermitian forms are the extreme types of sesquilinear
forms since any sesquilinear form h · , · i is the sum of a Hermitian form h · , · ih
and a skew-Hermitian form h · , · ish given by

                                hu, vih = 12 (hu, vi + hv, ui),
                               hu, vish = 12 (hu, vi − hv, ui).

   1 Some authors, particularly in mathematical physics, reverse the roles of the two variables and

assume the conjugate linearity in the first variable instead of the second.
   2 Sesquilinear forms make sense in number fields like Q
                                                                 £p §
                                                                    2 that have an automorphism of
order 2 (see Section IV.1), but sesquilinear forms in this kind of setting will not concern us here.
                                      4. Hermitian Forms                                      259

In addition, any skew-Hermitian form becomes a Hermitian form simply by
multiplying by i. Specifically if h · , · ish is skew-Hermitian, then ih · , · ish is
sesquilinear and Hermitian, as is readily checked. Consequently the study of
skew-Hermitian forms immediately reduces to the study of Hermitian forms.

  EXAMPLE. Let V = Cn , and let A be a Hermitian matrix, i.e., one with
  ∗
A = A, where A∗ is the conjugate transpose of A. Then it is a simple matter to
check that hu, vi = v ∗ Au defines a Hermitian form on Cn .

    Again the example with a matrix is completely general. In fact, let h · , · i be a
Hermitian form on V , let 0 = (v1 , . . . , vn ) be an ordered basis of V , and define
Ai j = hvi , v j i. Then A is a Hermitian matrix, and hu, vi = u t Av̄, where v̄ is the
entry-by-entry complex conjugate of v.
                                                     P then the formula for changing
    If 1 = (w1 , . . . , wn ) is a second ordered basis,
basis
µ      may
        ∂   be    derived as  follows: Write  w j =      i ci j vi , so that [ci j ] is the matrix
     I                                                        P
         . If Bi j = hwi , w j i, then Bi j = hwi , w j i = kl cki hvk , vl ic̄l j , and hence
  01
                                       µ        ∂t       µ      ∂
                                            I                 I
                                 B=                  A           .
                                           01                01

Thus two Hermitian matrices A and B represent the same Hermitian form in
different bases if and only if B = M ∗ AM for some nonsingular matrix M.

   Proposition 6.8.
   (a) If h · , · i is a Hermitian form on a finite-dimensional vector space V over
C, then there exists an ordered basis of V in which the matrix of h · , · i is diagonal
with real entries.
   (b) If A is an n-by-n Hermitian matrix, then there exists a nonsingular n-by-n
matrix M such that M ∗ AM is diagonal.
   PROOF. The above considerations show that (a) and (b) are reformulations
of the same result. Hence it is enough to prove (b). By the Spectral Theorem
(Theorem 3.21), there exists a unitary matrix U such that U −1 AU is diagonal
with real entries. Since U is unitary, U −1 = U ∗ . Thus we can take M = U to
prove (b).                                                                 §

    Just as with symmetric bilinear forms over R, we can do a little better than
Proposition 6.8 indicates. If B is Hermitian and diagonal with diagonal entries
bi , and if D is diagonal with positive entries di , then C = D ∗ B D is diago-
nal with diagonal entries di2 bi . Choosing D suitably and then replacing C by
P t C P for a suitable permutation matrix P, we may assume that P t C P is of the
260                             VI. Multilinear Algebra

form diag(+1, . . . , +1, −1, . . . , −1, 0, . . . , 0). The number of +1’s and −1’s
together is the rank of A, and the form is nondegenerate if and only if there are
no 0’s. The triple given by
                                     °                          ¢
                      ( p, m, z) = #(+1)’s, #(−1)’s, #(0)’s
is again called the signature of A. A similar notion can be defined in the case of
a Hermitian form, as opposed to a Hermitian matrix.

   Theorem 6.9 (Sylvester’s Law). The signature of an n-by-n Hermitian matrix
is well defined.

   The proof is the same as for Theorem 6.6 except for adjustments in notation.


                5. Groups Leaving a Bilinear Form Invariant

Although it is not logically necessary to do so, we digress in this section to intro-
duce some important groups that are defined by means of bilinear or Hermitian
forms. These groups arise in many areas of mathematics, both pure and applied,
and their detailed structure constitutes a topic in the fields of Lie groups, algebraic
groups, and finite groups that is beyond the scope of this book. Thus the best
place to define them seems to be now.
    We limit our comments on applications to just these: When the underlying
field in the definition of these groups is R or C, the group is quite often a “simple
Lie group,” one of the basic building blocks of the theory of the continuous groups
that so often arise in topology, geometry, differential equations, and mathematical
physics. When the underlying field is a number field in the sense of Example 9
of Section IV.1, the group quite often plays a role in algebraic number theory.
When the underlying field is a finite field, the group is often closely related to a
finite simple group; an example of this relationship occurred in Problems 55–62
at the end of Chapter IV, where it was shown that the group PSL(2, K), built in
an easy way from the general linear group GL(2, K), is simple if the field K has
more than 5 elements. More general examples of finite simple groups produced
by analogous constructions are said to be of “Lie type.” A celebrated theorem
of the late twentieth century classified the finite simple groups—establishing that
the only such groups are the cyclic groups of prime order, the alternating groups
on 5 or more letters, the simple groups of Lie type, and 26 so-called sporadic
simple groups.
    If h · , · i is a bilinear form on an n-dimensional vector space V over a field K,
a nonsingular linear map g : V → V is said to leave the bilinear form invariant
if
                                    hg(u), g(v)i = hu, vi
                         5. Groups Leaving a Bilinear Form Invariant                        261

for all u and v in V . Fix an ordered
                               µ     ∂basis 0 of V , let A be the matrix of the bilinear
                                   g
form in this basis, let g 0 =           be the member of GL(n, K) corresponding
                        µ ∂ 00
                           w
to g, and abbreviate            as w0 for any w in V . To translate the invariance
                           0
condition into one concerning matrices, we use the formula hu, vi = u 0t Av 0 , the
corresponding formula for hg(u), g(v)i, and the formula g(w)0 = g 0 (w0 ) from
Theorem 2.14. Then we obtain u 0t g 0t Ag 0 v 0 = u 0t Av 0 . Taking u to be the i th
member of the ordered basis 0 and v to be the j th member, we obtain equality of
the (i, j)th entry of the two matrices g 0t Ag 0 and A. Thus the matrix form of the
invariance condition is that a nonsingular matrix g 0 satisfy

                                        g 0t Ag 0 = A.

We know that changing the ordered basis 0 amounts to replacing A by M t AM for
some nonsingular matrix M. If g 0 satisfies the invariance condition g 0t Ag 0 = A
relative to A, then M −1 g 0 M satisfies

                     (M −1 g 0 M)t (M t AM)(M −1 g 0 M) = M t AM.

Thus we are led to a conjugate subgroup within GL(n, K). A conjugate subgroup
is not something substantially new, and thus we might as well make a convenient
choice of basis so that A looks particularly special.
    The interesting cases are that the given bilinear form is symmetric or alter-
nating, hence that the matrix A is symmetric or alternating. Let us restrict our
attention to them. The left and right radicals coincide in these cases, and the first
thing to do is to take the two-sided radical into account. Returning to the original
bilinear form, we write V = rad ⊕S, where rad is the radical and S is some
vector subspace of S, and we choose an ordered basis (v1 , . . . , v p , v p+1 , . . . , vn )
such that v1 , . . . , v p are in S and v p+1 , . . . , vn are in rad. Then hvi , v j i = 0 if
i > p or j > p, and consequently A has its only nonzero entries in the upper
left p-by- p block. The same argument as in the proofs of Theorems 6.5 and
6.7 shows that the restriction of the bilinear form to S is nondegenerate, and
consequently the upper left p-by- p block of A is nonsingular. Changing≥notation               ¥
                                                                                       g11 g12
slightly, suppose that g is an n-by-n matrix written in block form as g = g21 g22
                                           ≥ ¥
with g11 of size p-by- p, suppose that A0 00 is another matrix written in the same
   ≥ form,
block            ≥ ¥ that the p-by- p matrix A is nonsingular, and suppose that
         ¥ suppose
  t A 0
g 0 0 g = A0 00 . Making a brief computation, we find that necessary and
                                                                      t
sufficient conditions on g are that g11 be nonsingular and have g11     Ag11 = A,
that g12 = 0, that g22 be arbitrary nonsingular, and that g21 be arbitrary. In other
262                            VI. Multilinear Algebra

                                       t
words, the only interesting condition g11 Ag11 = A is a reflection of what happens
in the nonsingular case.
   Consequently the interesting cases are that the given bilinear form is non-
degenerate, as well as either symmetric or alternating. If A is symmetric and
nonsingular, then the group of all nonsingular matrices g such that g t Ag = A is
called the orthogonal group relative to A. If A is alternating and nonsingular,
then the group of all nonsingular matrices g such that g t Ag = A is called the
symplectic group relative to A.
   For the symplectic case it is customary to invoke Theorem 6.7 and take A to
be                                                     
                                 01
                               −1 0                           
                                                              
                                        01                    
                                       −1 0                   
                       J =                                    ,
                                              ..              
                                                   .          
                                                              
                                                         01
                                                        −1 0

except possibly for a permutation of the rows and columns and possibly for
a multiplication by −1. Two conflicting notations are in common use for the
symplectic group, namely Sp(n, K) and Sp( 12 n, K), and one always has to check
a particular author’s definitions.
   For the orthogonal case the notation is less standardized. Theorem 6.5 says
that we may take A to be diagonal except when K has characteristic 2. But the
theorem does not tell us exactly which A’s are representative of the same bilinear
form. When K = C, we know that we can take A to be the identity matrix I .
The group is known as the complex orthogonal group and is denoted by O(n, C).
When K = R, we can take A to be diagonal with diagonal entries ±1. Sylvester’s
Law (Theorem 6.6) says that the form determines the number of +1’s and the
number of −1’s. The groups are called indefinite orthogonal groups and are
denoted by O( p, q), where p is the number of +1’s and q is the number of −1’s.
When q = 0, we obtain the ordinary orthogonal group of matrices relative to an
inner product.
   A similar analysis applies to Hermitian forms. The field is now C, the invari-
ance condition with the form is still hg(u), g(v)i = hu, vi, and the corresponding
condition with matrices is g t A ḡ = A. The interesting case is that the Hermitian
form is nondegenerate. Proposition 6.8 and Sylvester’s Law (Theorem 6.9)
together show that we may take A to be diagonal with diagonal entries ±1 and
that the Hermitian form determines the number of +1’s and the number of −1’s.
The groups are the indefinite unitary groups and are denoted by U( p, q), where
p is the number of +1’s and q is the number of −1’s. When q = 0, we obtain
the ordinary unitary group of matrices relative to an inner product.
                         6. Tensor Product of Two Vector Spaces                    263

                   6. Tensor Product of Two Vector Spaces

If E is a vector space over K, then the set of all bilinear forms on E is a vector
space under addition and scalar multiplication of the values, i.e., it is a vector
subspace of the set of all functions from E × E into K. In this section we introduce
a vector space called the “tensor product” of E with itself, whose dual, even if E
is infinite-dimensional, is canonically isomorphic to this vector space of bilinear
forms.
    Matters will be clearer if we work initially with something slightly more general
than bilinear forms on a single vector space E. Thus fix a field K, and let E and
F be vector spaces over K. A function from E × F into a vector space U over K
is said to be bilinear if it is linear in each of the two variables when the other one
is held fixed. Such a space of bilinear functions is a vector space over K under
addition and scalar multiplication of the values. The bilinear functions are called
bilinear forms when the range space U is K itself. More generally, if E 1 , . . . , E k
are vector spaces over K, a function from E 1 × · · · × E k into a vector space over
K is said to be k-linear or k-multilinear if it is linear in each of its k variables
when the other k − 1 variables are held fixed. Again the word “form” is used in
the scalar-valued case, and all of these spaces of multilinear functions are vector
spaces over K.
    In this section we shall introduce the tensor product of two vector spaces E
and F over K, ultimately denoting it by E ⊗K F. The dual of this tensor product
will be canonically isomorphic to the vector space of bilinear forms on E × F.
More generally the space of linear functions from the tensor product into a vector
space U will be canonically isomorphic to the vector space of bilinear functions
on E × F with values in U .
    Following the habit encouraged by Chapter IV, we want to arrange that tensor
product is a functor. If V denotes the category of vector spaces over K and if
V × V denotes the category described in Section IV.11 as V S for a two-element
set S, then tensor product is to be a functor from V × V into V. Hence we will
want to examine the effect of tensor products on morphisms, i.e., on linear maps.
    As in similar constructions in Chapter IV, the effect of tensor product on linear
maps is captured by defining the tensor product by means of a universal mapping
property. The appropriate universal mapping property rephrases the statement
above that the space of linear functions from the tensor product into any vector
space U is canonically isomorphic to the vector space of bilinear functions on
E × F with values in U .
    If E and F are vector spaces over K, a tensor product of E and F is a pair
(V, ∂) consisting of a vector space V over K together with a bilinear function
∂ : E × F → V , with the following universal mapping property: whenever b is
a bilinear mapping of E ×F into a vector space U over K, then there exists a unique
264                              VI. Multilinear Algebra

linear mapping B of V into U such that the diagram in Figure 6.1 commutes, i.e.,
such that B∂ = b holds in the diagram. When ∂ is understood, one frequently
refers to V itself as the tensor product. The linear mapping B : V → U is called
the linear extension of b to the tensor product.
                                                b
                                 E × F −−−→ U
                                    
                                    
                                   ∂y      B

                                    V
          FIGURE 6.1. Universal mapping property of a tensor product.

   Theorem 6.10. If E and F are vector spaces over K, then a tensor product
of E and F exists and is unique up to canonical isomorphism in this sense: if
(V1 , ∂1 ) and (V2 , ∂2 ) are tensor products, then there exists a unique linear mapping
B : V2 → V1 with B∂2 = ∂1 , and B is an isomorphism. Any tensor product is
spanned linearly by the image of E × F in it.
   REMARKS. As usual, uniqueness will follow readily from the universal map-
ping property. What is really needed is a proof of existence. This will be carried
out by an explicit construction. Later, in Chapter X, we shall reintroduce tensor
products, taking the basic construction to be that of the tensor product of two
abelian groups, and then the tensor product of two vector spaces will in effect
be obtained in a slightly different way. However, the exact construction does not
matter, only the existence; the uniqueness allows us to match the results of any
two constructions.
                        ∂2                                        ∂1
            E × F −−−→ V2                              E × F −−−→ V1
                                                          
                
             ∂1 y     B2                  and              
                                                        ∂2 y     B1


               V1                                          V2

            FIGURE 6.2. Diagrams for uniqueness of a tensor product.

   PROOF OF UNIQUENESS. Let (V1 , ∂1 ) and (V2 , ∂2 ) be tensor products. Set up
the diagrams in Figure 6.2, and use the universal mapping property to obtain
linear maps B2 : V1 → V2 and B1 : V2 → V1 extending ∂2 and ∂1 . Then
B1 B2 : V1 → V1 has B1 B2 ∂1 = B1 ∂2 = ∂1 , and 1V1 : V1 → V1 has (1V1 )∂1 = ∂1 .
By the assumed uniqueness within the universal mapping property, B1 B2 = 1V1
on V1 . Similarly B2 B1 = 1V2 on V2 . Then B1 : V2 → V1 gives the canonical
isomorphism. Because of the isomorphism the image of E × F will span an
arbitrary tensor product if it spans some particular tensor product.          §
                           6. Tensor Product of Two Vector Spaces                  265
                                       L
   PROOF OF EXISTENCE. Let V1 = (e, f ) K(e, f ), the direct sum being taken
over all ordered pairs (e, f ) with e ∈ E and f ∈ F. Then V1 is a vector space
over K with a basis consisting of all ordered pairs (e, f ). We think of all identities
that the elements of V1 must satisfy to be a tensor product, writing each as some
expression set equal to 0, and then we assemble those expressions into a vector
subspace to factor out from V1 . Namely, let V0 be the vector subspace of V1
generated by all elements of any of the kinds

                           (e1 + e2 , f ) − (e1 , f ) − (e2 , f ),
                                   (ce, f ) − c(e, f ),
                           (e, f 1 + f 2 ) − (e, f 1 ) − (e, f 2 ),
                                   (e, c f ) − c(e, f ),

the understanding being that c is in K, the elements e, e1 , e2 are in E, and the
elements f, f 1 , f 2 are in F. Define V = V1 /V0 , and define ∂ : E × F → V1 /V0
by ∂(e, f ) = (e, f ) + V0 . We shall prove that (V, ∂) is a tensor product of E and
F. The definitions show that the image of ∂ spans V linearly.
   Let b : E × F → U be given as in Figure 6.1. To see that a linear extension
B exists and is unique, define B1 on V1 by
                           ° P                       ¢  P
                      B1               ci (ei , f i ) =   ci b(ei , f i ).
                            (finite)                  (finite)


The bilinearity of b shows that B1 maps V0 to 0. By Proposition 2.25, B1 descends
to a linear map B : V1 /V0 → U , and we have B∂ = b. Hence B exists as required.
   To check uniqueness of B, we observe again that the cosets (e, f ) + V0 within
V1 /V0 span V ; since commutativity of the diagram in Figure 6.1 forces

                     B((e, f ) + V0 ) = B(∂(e, f )) = b(e, f ),

B is unique. This completes the proof.                                              §

   A tensor product of E and F is denoted by (E ⊗K F, ∂), with the bilinear map
∂ given by ∂(e, f ) = e ⊗ f ; the map ∂ is frequently dropped from the notation
when there is no chance of ambiguity. The tensor product that was constructed
in the proof of existence in Theorem 6.10 is not given any special notation to
distinguish it from any other tensor product. The elements e ⊗ f span E ⊗K F,
as was noted in the statement of the theorem. Elements of the form e ⊗ f are
sometimes called pure tensors.
   Not every element need be a pure tensor, but every element in E ⊗K F is a
finite sum of pure tensors. We shall see in Proposition 6.14 that if {u i } is a basis
266                              VI. Multilinear Algebra

of E and {v j } is a basis of F, then the pure tensors u i ⊗ v j form a basis of E ⊗K F.
In particular the dimension of the tensor product is the product of the dimensions
of the factors. We could have defined the tensor product in this way—by taking
bases and declaring that u i ⊗ v j is to be a basis of the desired space. The difficulty
is that we would be forever wedded to our choice of those particular bases, or
we would constantly have to prove that our definitions are independent of bases.
The definition by means of Theorem 6.10 avoids this difficulty.
    To make tensor product (E, F) 7→ E ⊗K F into a functor, we have to describe
the effect on linear mappings. To aid in that discussion, let us reintroduce some
notation first used in Chapter II: if U and V are vector spaces over K, then
HomK (U, V ) is defined to be the vector space of K linear maps from U to V .

   Corollary 6.11. If E, F, and V are vector spaces over K, then the vector space
HomK (E ⊗K F, V ) is canonically isomorphic (via restriction to pure tensors) to
the vector space of all V -valued bilinear functions on E × F.
   PROOF. Restriction is a linear mapping from HomK (E ⊗K F, V ) to the vector
space of all V -valued bilinear functions on E × F, and it is one-one since the
image of E × F in E ⊗K F spans E ⊗K F. It is onto since any bilinear function
from E × F to V has a linear extension to E ⊗K F, by Theorem 6.10.           §

    Corollary 6.12. If E and F are vector spaces over K, then the vector space of
all bilinear forms on E × F is canonically isomorphic to (E ⊗K F)0 , the dual of
the vector space E ⊗K F.
   PROOF. This is the special case of Corollary 6.11 in which V = K.                 §

   Corollary 6.13. If E, F, and V are vector spaces over K, then there is a
canonical K linear isomorphism 8 of left side to right side in

                 HomK (E ⊗K F, V ) ∼
                                   = HomK (E, HomK (F, V ))

such that
                              8(ϕ)(e)( f ) = ϕ(e ⊗ f )
for all ϕ ∈ HomK (E ⊗K F, V ), e ∈ E, and f ∈ F.
  REMARK. This result is just a restatement of Corollary 6.11, but let us prove it
anyway, writing the proof in the language of the statement.
   PROOF. The map 8 is well defined and K linear, and it carries the left side to
the right side. For √ in the right side, define 9(√)(e, f ) = √(e)( f ). Then 9(√)
is a bilinear map from E × F into V , and we let 9(√) e      be the linear extension
from E ⊗K F into V given in Theorem 6.10. Then 9        e is a two-sided inverse to
8, and the corollary follows.                                                     §
                            6. Tensor Product of Two Vector Spaces                             267

   Let us now make (E, F) 7→ E ⊗K F into a covariant functor. If (E 1 , F1 ) and
(E 2 , F2 ) are objects in V × V, i.e., if they are two ordered pairs of vector spaces,
then a morphism from the first to the second is a pair (L , M) of linear maps of the
form L : E 1 → E 2 and M : F1 → F2 . To (L , M), we are to associate a linear
map from E 1 ⊗K F1 into E 2 ⊗K F2 ; this linear map will be denoted by L ⊗ M. We
use Corollary 6.11 to define L ⊗ M as the member of HomK (E 1 ⊗K F1 , E 2 ⊗K F2 )
that corresponds under restriction to the bilinear map (e1 , f 1 ) 7→ L(e1 ) ⊗ M( f 1 )
of E 1 × F1 into E 2 ⊗K F2 . In terms of pure tensors, the map L ⊗ M satisfies
                         (L ⊗ M)(e1 ⊗ f 1 ) = L(e1 ) ⊗ M( f 1 ),
and this formula completely determines L ⊗ M because of the uniqueness of
linear extensions of bilinear maps.
   To check that this definition of the effect of tensor product on pairs of linear
maps makes (E, F) 7→ E ⊗K F into a covariant functor, we have to check the
effect on the identity map and the effect on composition. For the effect on the
identity map (1 E1 , 1 F1 ) when E 1 = E 2 and F1 = F2 , we see from the above
displayed formula that (1 E1 ⊗ 1 F1 )(e1 ⊗ f 1 ) = 1 E1 (e1 ) ⊗ 1 F1 ( f 1 ) = e1 ⊗ f 1 =
1 E1 ⊗K F1 (e1 ⊗ f 1 ). Since elements of the form e1 ⊗ f 1 span E 1 ⊗K F1 , we conclude
that 1 E1 ⊗ 1 F1 = 1 E1 ⊗K F1 .
   For the effect on composition, let (L 1 , M1 ) : (E 1 , F1 ) → (E 2 , F2 ) and
(L 2 , M2 ) : (E 2 , F2 ) → (E 3 , F3 ) be given. Then we have
(L 2 ⊗ M2 )(L 1 ⊗ M1 )(e1 ⊗ f 1 ) = (L 2 ⊗ M2 )(L 1 (e1 ) ⊗ M1 ( f 1 ))
                   = (L 2 L 1 )(e1 ) ⊗ (M2 M1 )( f 1 ) = (L 2 L 1 ⊗ M2 M1 )(e1 ⊗ f 1 ).
Since elements of the form e1 ⊗ f 1 span E 1 ⊗K F1 , we conclude that
                       (L 2 ⊗ M2 )(L 1 ⊗ M1 ) = L 2 L 1 ⊗ M2 M1 .
Therefore (E, F) 7→ E ⊗K F is a covariant functor.
    In particular, E 7→ E ⊗K F and F 7→ E ⊗K F are covariant functors from V
into itself. For these two functors from V into itself, the effect on linear mappings
is especially nice, namely that
                                   Ω
                                      is K linear from HomK (E 1 , E 2 )
             L 1 7→ L 1 ⊗ M1
                                      into HomK (E 1 ⊗K F1 , E 2 ⊗K F2 ),
                                   Ω
                                      is K linear from HomK (F1 , F2 )
             M1 7→ L 1 ⊗ M1
                                      into HomK (E 1 ⊗K F1 , E 2 ⊗K F2 ).
To prove the first of these assertions, for example, we observe that the sum of the
linear extensions of
    (e1 , f 1 ) 7→ L 1 (e1 ) ⊗ M1 ( f 1 )   and       (e1 , f 1 ) 7→ L 01 (e1 ) ⊗ M1 ( f 1 )
268                                VI. Multilinear Algebra

is a linear extension of (e1 , f 1 ) 7→ (L 1 + L 01 )(e1 )⊗ M1 ( f 1 ), and the uniqueness in
the universal mapping property implies that (L 1 +L 01 )⊗M1 = L 1 ⊗M1 +L 01 ⊗M1 .
Similar remarks apply to multiplication by scalars.
    Let us mention some identities satisfied by ⊗K . There is a canonical isomor-
phism
                                    E ⊗K F ∼  = F ⊗K E

given by taking the linear extension of (e, f ) 7→ f ⊗ e as the map from left to
right. The linear extension of ( f, e) 7→ e ⊗ f gives a two-sided inverse. Category
theory has a way of capturing the idea that this isomorphism is systematic, rather
than randomly dependent on E and F. The two sides of the above isomorphism
may be regarded as the values of the covariant functors (E, F) 7→ E ⊗K F and
(E, F) 7→ F ⊗K E. The notion in category theory capturing “systematic” is
called “naturality.” It makes precise the fact that the system of isomorphisms
respects linear maps, as well as the vector spaces. Here is the general definition.
Its usefulness will be examined later in this section.
    Let C and D be two categories, and let 8 : C → D and 9 : C → D
be covariant functors. Suppose that for each X in Obj(C ), a morphism TX
in MorphD (8(X), 9(X)) is given. Then the system {TX } is called a natural
transformation of 8 into 9 if for each pair of objects X 1 and X 2 in C and each
h in MorphC (X 1 , X 2 ), the diagram in Figure 6.3 commutes. If furthermore each
TX is an isomorphism, then it is immediate that the system {TX−1 } is a natural
transformation of 9 into 8, and we say that {TX } is a natural isomorphism.
                                            8(h)
                                8(X 1 ) −−−→ 8(X 2 )
                                              
                                     
                                TX 1 y
                                               T
                                               y X2
                                            9(h)
                                9(X 1 ) −−−→ 9(X 2 )

      FIGURE 6.3. Commutative diagram of a natural transformation {TX }.

   If 8 and 9 are contravariant functors, then the system {TX } is called a natural
transformation of 8 into 9 if the diagram obtained from Figure 6.3 by revers-
ing the horizontal arrows commutes. The system is a natural isomorphism if
furthermore each Tx is an isomorphism.
   In the case we are studying, we have C = V × V and D = V. Objects X in C
are pairs (E, F) of vector spaces, and 8 and 9 are the covariant functors with
8(E, F) = E ⊗K F and 9(E, F) = F ⊗K E. The mapping T(E,F) : E ⊗K F →
F ⊗K E is uniquely determined by the condition that T(E,F) (e ⊗ f ) = f ⊗ e
for all e ∈ E and f ∈ F. A morphism of pairs from (E 1 , F1 ) to (E 2 , F2 ) is of
                          6. Tensor Product of Two Vector Spaces                         269

the form h = (L , M) with L ∈ HomK (E 1 , E 2 ) and M ∈ HomK (F1 , F2 ). Our
constructions above show that

               8(L , M) = L ⊗ M ∈ HomK (E 1 ⊗K F1 , E 2 ⊗K F2 )
and            9(L , M) = M ⊗ L ∈ HomK (F1 ⊗K E 1 , F2 ⊗K E 2 ).

In Figure 6.3 the two routes from top left to bottom right in the diagram have

   T(E2 ,F2 ) 8(L , M)(e1 ⊗ f 1 ) = T(E2 ,F2 ) (L ⊗ M)(e1 ⊗ f 1 )
                                  = T(E2 ,F2 ) (L(e1 ) ⊗ M( f 1 )) = M( f 1 ) ⊗ L(e1 )

and

      9(L , M)T(E1 ,F1 ) (e1 ⊗ f 1 ) = 9(L , M)( f 1 ⊗ e1 )
                                     = (M ⊗ L)( f 1 ⊗ e1 ) = M( f 1 ) ⊗ L(e1 ).

The results are equal, and therefore the diagram commutes. Consequently the
isomorphism
                              E ⊗K F ∼ = F ⊗K E
is natural in the pair (E, F).
   Another canonical isomorphism of interest is

                                     E ⊗K K ∼
                                            = E.

Here the map from left to right is the linear extension of (e, c) 7→ ce, while
the map from right to left is e 7→ e ⊗ 1. In view of the previous canonical
isomorphism, we have K ⊗K E ∼    = E also. Each of these isomorphisms is natural
in E.
   Next let us consider how ⊗K interacts with direct sums. The result is that
tensor product distributes over direct sums, even infinite direct sums:
                              °M ¢ M
                       E ⊗K         Fs ∼ =     (E ⊗K Fs ).
                                   s∈S         s∈S

The map from left to right is the linear extension of the bilinear map (e, { f s }s∈S ) 7→
{e ⊗ f s }s∈S . For the definition of the inverse, the constructions of Section II.6
show that we have only to define the map on each E ⊗K Fs L         , where it is the linear
extension of (e, f s ) 7→ e ⊗ {i s ( f s ))}s∈S ; here i s0 : Fs0 → s Fs is the one-one
linear map carrying the s0th vector space into the direct sum. Once again it is
possible to prove that the isomorphism is natural; we omit the details.
   It follows from the displayed isomorphism and the isomorphism E ⊗K K ∼              =E
that if {xi } is a basis of E and {yj } is a basis of F, then {xi ⊗ yj } is a basis of
E ⊗K F. This proves the following result.
270                                 VI. Multilinear Algebra

   Proposition 6.14. If E and F are vector spaces over K, then
                           dim(E ⊗K F) = (dim E)(dim F).

P{yj } is a basis of F, then the most general member of E ⊗K F is of the form
If
   j e j ⊗ y j with all e j in E.

   We turn to a consideration of HomK from the point of view of functors. In
the examples in Section IV.11, we saw that V 7→ HomK (U, V ) is a covariant
functor from V to itself and that U 7→ HomK (U, V ) is a contravariant functor
from V to itself. If we are not squeamish about mixing the two types—covariant
and contravariant—then we can consider (U, V ) 7→ HomK (U, V ) as a functor3
from V × V to V. At any rate if L is in HomK (U1 , U2 ) and M is in HomK (V1 , V2 ),
then Hom(L , M) carries HomK (U2 , V1 ) into HomK (U1 , V2 ) and is given by
                Hom(L , M)(h) = Mh L                for h ∈ HomK (U2 , V1 ).
It is evident that the result is K linear as a function of h, and hence
                                          °                             ¢
          Hom(L , M)        is in HomK HomK (U2 , V1 ), HomK (U1 , V2 ) .
   When we look for analogs for the functor HomK of the identity E ⊗K K ∼      =E
for the functor ⊗K , we are led to two identities. One is just the definition of the
dual of a vector space:
                               HomK (U, K) = U 0 .
The other is the natural isomorphism
                                     HomK (K, V ) ∼
                                                  = V.
In the proof of the latter identity, the mapping from left to right is given by sending
a linear h : K → V to h(1), and the mapping from right to left is given by sending
v in V to h with h(c) = cv.

   Next let us consider how HomK interacts with direct sums and direct products.
The construction HomK (U, V ) distributes over finite direct sums in each variable,
but the situation with infinite direct sums or direct products is more subtle. Valid
identities are
                            °M           ¢ Y
                    HomK           Us , V ∼
                                          =      HomK (Us , V )
                                  s∈S               s∈S
                             ° Y ¢ Y
and                      HomK U, Vs ∼
                                    = HomK (U, Vs ),
                                        s∈S         s∈S

   3 Readers  who care about this point can regard U as in the category V opp defined in Problems
78–80 at the end of Chapter IV. Then (U, V ) 7→ HomK (U, V ) is a covariant functor from V opp × V
into V.
                         6. Tensor Product of Two Vector Spaces                    271

and these are natural isomorphisms. Proofs of these identities for all S and
counterexamples related to them when S is infinite appear in Problems 7–8 at the
end of the chapter.
   We have already checked that the isomorphism E ⊗K F ∼     = F ⊗K E is natural in
(E, F), and we have asserted naturality in some other situations in which it is easy
to check. The next proposition asserts naturality for the identity of Corollary 6.13,
which combines ⊗K and HomK in a nontrivial way. After the proof of the result,
we shall digress for a moment to indicate the usefulness of natural isomorphisms.

   Proposition 6.15. Let E, F, V , E 1 , F1 , and V1 be vector spaces over K, and
let L E1 : E 1 → E, L F1 : F1 → F, and L V : V → V1 be K linear maps. Then
the isomorphism 8 of Corollary 6.13 is natural in the sense that the diagram

                                        8
            HomK (E ⊗K F, V ) −−−→ HomK (E,                 HomK (F, V ))
                                                            
                           
     Hom(L E1 ⊗L F1 , L V )y
                                                             Hom(L ,Hom(L ,L ))
                                                             y     E1     F1 V


                                        8
          HomK (E 1 ⊗K F1 , V1 ) −−−→ HomK (E 1 , HomK (F1 , V1 ))

commutes.

   REMARKS. Observe that the first two linear maps L E1 and L F1 go in the
opposite direction to the two vertical maps, while L V goes in the same direction
as the vertical maps. This is a reflection of the fact that both sides of the identity
in Corollary 6.13 are contravariant in the first two variables and covariant in the
third variable.

   PROOF. For ϕ in HomK (E ⊗K F, V ), e1 in E 1 , and f 1 in F1 , we have

      (Hom(L E1 , Hom(L F1 ,L V )) ◦ 8)(ϕ)(e1 )( f 1 )
                            = (Hom(L F1 , L V ) ◦ 8(ϕ) ◦ L E1 )(e1 )( f 1 )
                            = (Hom(L F1 , L V ) ◦ (8(ϕ) ◦ L E1 ))(e1 )( f 1 )
                            = L V (8(ϕ)(L E1 (e1 ))(L F1 ( f 1 )))
                            = L V (ϕ(L E1 (e1 ) ⊗ L F1 ( f 1 )))
                            = (L V ◦ ϕ ◦ (L E1 ⊗ L F1 ))(e1 ⊗ f 1 )
                            = (Hom(L E1 ⊗ L F1 , L V )(ϕ))(e1 ⊗ f 1 )
                            = 8(Hom(L E1 ⊗ L F1 , L V ) ◦ ϕ)(e1 )( f 1 ).

This proves the proposition.                                                        §
272                             VI. Multilinear Algebra

   Let us now discuss naturality in a wider context. In a general category D, if
we have two objects U and U 0 such that Morph(U, V ) and Morph(U 0 , V ) have
the same cardinality for each object V , then we cannot really say anything about
the relationship between U and U 0 . But under a hypothesis that the isomorphism
of sets has a certain naturality to it, then, according to Proposition 6.16 below,
U and U 0 are isomorphic objects. Thus naturality of a system of weak-looking
set-theoretic isomorphisms can lead to a much stronger-looking isomorphism.
Corollary 6.17 goes on to make a corresponding assertion about functors. The
assertion about functors in the corollary is a helpful tool for establishing natural
isomorphisms of functors, and an example appears below in Proposition 6.200 .

   Proposition 6.16. Let D be a category, and suppose that U and U 0 are objects
in D with the following property: to each object V in D corresponds a one-one
onto function
                     TV : Morph(U, V ) → Morph(U 0 , V )
with the system {TV } natural in V in the sense that whenever σ is in Morph(V, V 0 ),
then the diagram

                                         TV
                      Morph(U, V ) −−−→ Morph(U 0 , V )
                                            
                               
                     left-by-σ y
                                             left-by-σ
                                             y
                                         TV 0
                     Morph(U, V 0 ) −−−→ Morph(U 0 , V 0 )

commutes. Then U is isomorphic to U 0 as an object in D, an isomorphism from
U to U 0 being the member TU−1                      0
                             0 (1U 0 ) of Morph(U, U ).


    REMARKS.
    (1) Another way of formulating this result is as follows: Let D be any category,
let S be the category of sets, and let U and U 0 be objects in D. Define a covariant
functor HU : D → S by HU (V ) = MorphD (U, V ) and HU (σ ) = left-by-σ
for σ ∈ MorphD (V, V 0 ), and define HU 0 similarly. If HU and HU 0 are naturally
isomorphic functors, then U and U 0 are isomorphic objects in D.
    (2) A similar result is valid when HU and HU 0 are contravariant functors,
HU being defined by HU (V ) = HomD (V, U ) and HU (σ ) = right-by-σ for
σ ∈ MorphD (V, V 0 ). The result in this case follows immediately by applying
Proposition 6.16 to the opposite category D opp of D as defined in Problems 78–80
at the end of Chapter IV.
   PROOF. Let ϕ be the element TU−1                         0
                                    0 (1U 0 ) of Morph(U, U ), and let √ be the
                           0
element TU (1U ) of Morph(U , U ). To prove the proposition, it is enough to show
that ϕ√ = 1U 0 and √ϕ = 1U .
                        6. Tensor Product of Two Vector Spaces                  273

   For σ in Morph(V, V 0 ), form the commutative diagram in the statement of the
proposition. The commutativity says that

                 σ TV (h) = TV 0 (σ h)         for h ∈ Morph(U, V ).            (∗)

Taking V = U , V 0 = U 0 , σ = ϕ, and h = 1U in (∗) proves the second equality
of the chain

                 ϕ√ = ϕTU (1U ) = TU 0 (ϕ1U ) = TU 0 (ϕ) = 1U 0 .

Taking V = U 0 , V 0 = U , σ = √, and h = ϕ in (∗) proves the first equality of
the chain
                 TU (√ϕ) = √ TU 0 (ϕ) = √1U 0 = √ = TU (1U );
Applying TU−1 , we obtain √ϕ = 1U , as required.                                 §

   Corollary 6.17. Let C and D be categories, and let F : C → D and
G : C → D be covariant functors. Suppose that to each pair of objects (A, V ) in
C × D corresponds a one-one onto function

                 TA,V : Morph(F(A), V ) → Morph(G(A), V )

with the system {TA,V } natural in (A, V ). Then the functors F and G are naturally
isomorphic.
  REMARKS. A similar result is valid if TA,V carries Morph(V, F(A)) to
Morph(V, G(A)) and/or if F and G are contravariant. To handle these situations,
we apply the corollary to the opposite categories D opp and/or C opp , as defined in
Problems 78–80 at the end of Chapter IV, instead of to the categories D and/or C.
                                                                      −1
   PROOF. By Proposition 6.16 and the hypotheses, the member TA,G(A)        (1G(A) )
of MorphD (F(A), G(A)) is an isomorphism. We are to prove that the system
{TA,G(A) } is natural in A. If σ in MorphC (A, A0 ) is given, then the naturality of
TA,V in the V variable implies that the diagram
                                         T A,G(A)
            MorphD (F(A), G(A)) −−−→ MorphD (G(A), G(A))
                                             
                          
             left-by-G(σ )y
                                              left-by-G(σ )
                                              y
                                         T A,G(A0 )
            MorphD (F(A), G(A0 )) −−−−→ MorphD (G(A), G(A0 ))
                          −1
commutes. Evaluating at TA,G(A) (1G(A) ) ∈ MorphD (F(A), G(A)) the two equal
compositions in the diagram, we obtain
                                           °       −1
                                                                ¢
            G(σ ) = G(σ )1G(A) = TA,G(A0 ) G(σ )TA,G(A) (1G(A) ) .       (∗)
274                              VI. Multilinear Algebra

With σ as above, the naturality of TA,V in the A variable implies that the diagram

                                        T A0 ,G(A0 )
           MorphD (F(A0 ), G(A0 )) −−−−→ MorphD (G(A0 ), G(A0 ))
                                                 
                         
           right-by-F(σ )y
                                                  right-by-G(σ )
                                                  y
                                         T A,G(A0 )
           MorphD (F(A), G(A0 )) −−−−→ MorphD (G(A), G(A0 ))

commutes. Evaluating at TA−1                                   0      0
                             0 ,G(A0 ) (1G(A0 ) ) ∈ MorphD (F(A ), G(A )) the two

equal compositions in the diagram, we obtain
                                             °                            ¢
            G(σ ) = 1G(A0 ) G(σ ) = TA,G(A0 ) TA−1
                                                 0 ,G(A0 ) (1G(A0 ) )F(σ ) .    (∗∗)

Equations (∗) and (∗∗), together with the fact that TA,G(A0 ) is invertible, say that

                        −1
                  G(σ )TA,G(A) (1G(A) ) = TA−1
                                             0 ,G(A0 ) (1G(A0 ) )F(σ ).



In other words, the isomorphism T                                    eA =
                                   eA ∈ MorphD (F(A), G(A)) given by T
  −1
TA,G(A) (1G(A) ) makes the diagram

                                            TeA
                                F(A) −−−→ G(A)
                                          
                                   
                              F(σ )y
                                           G(σ )
                                           y
                                           TeA0
                               F(A0 ) −−−→ G(A0 )

commute. Thus F is naturally isomorphic to G.                                      §

   Tensor product provides a device for converting a real vector space canonically
into a complex vector space, so that a basis over R in the original space becomes a
basis over C in the new space. If E is the given real vector space, then the complex
vector space, called the complexification of E, is the space E C = E ⊗R C with
multiplication by a complex number c in E C defined to be 1 ⊗ (z 7→ cz).
   This construction works more generally when we have any inclusion of fields
K ⊆ L. In this situation, L becomes a vector space over K if scalar multiplication
K × L → L is defined as the restriction of the multiplication L × L → L within
L. For any vector space E over K, we define E L = E ⊗K L, initially as a vector
space over K. For c ∈ L, we then define

       (multiplication by c in E ⊗K L) = 1 ⊗ (multiplication by c in L).
                          6. Tensor Product of Two Vector Spaces                    275

The above identities concerning tensor products of linear maps allow one easily
to prove the following identities:

                                 c1 (c2 v) = (c1 c2 )v,
                                c(u + v) = cu + cv,
                              (c1 + c2 )v = c1 v + c2 v,
                                       1v = v.

Together these identities say that E L = E ⊗K L, with its vector-space addition
and the above definition of multiplication by scalars in L, is a vector space over
L. The further identity

                 c(e ⊗ 1) = ce ⊗ 1          if c is in K and e is in E

shows that its scalar multiplication is consistent with scalar multiplication in E
when the scalars are in K and E is identified with the subset E ⊗ 1 of E L .
   Let us say that the pair (E L , ∂), where ∂ : E → E L is the mapping e 7→ e ⊗ 1,
is obtained by extension of scalars. This construction is characterized by a
universal mapping property as follows.

   Proposition 6.18. Let K ⊆ L be an inclusion of fields, and let E be a vector
space over K.
    (a) If (E L , ∂) is formed by extension of scalars, then (E L , ∂) has the following
universal mapping property: whenever U is a vector space over L and ϕ : E → U
is a K linear map, there exists a unique L linear map 8 : E L → U such that
8 ∂ = ϕ.
    (b) Suppose that (V, j) is any pair in which V is a vector space over L and
 j : E → V is a K linear function such that the following universal mapping
property holds: whenever U is a vector space over L and ϕ : E → U is a K
linear map, there exists a unique L linear map 8 : V → U such that 8j = ϕ.
Then there exists a unique isomorphism 9 : E L → V of L vector spaces such that
9 ∂ = j.
   PROOF. In (a), for the uniqueness of 8, we must have 8(e ⊗c) = c8(e ⊗1) =
c(8 ∂)(e) = cϕ(e). Hence 8 is determined by ϕ on pure tensors in E ⊗K L and
therefore everywhere.
   For existence let 8 : E ⊗K L → U be the K linear extension of the K bilinear
function of E × L into U given by

                     (e, c) 7→ cϕ(e)        for e ∈ E and c ∈ L.
276                            VI. Multilinear Algebra

In the L vector space E ⊗K L, multiplication by a member c0 of L is defined to
be 1 ⊗ (multiplication by c0 ). On a pure tensor e ⊗ c, we therefore have

      8(c0 (e ⊗ c)) = 8(e ⊗ c0 c) = (c0 c)ϕ(e) = c0 (cϕ(e)) = c0 (8(e ⊗ c)).

Since E ⊗K L is generated by pure tensors, 8 is L linear. By the construction of
8, ϕ(e) = 8(e ⊗ 1) = (8 ∂)(e). Thus 8 has the required properties.
   In (b), let (V, j) have the same universal mapping property as (E L , ∂). We
apply the universal mapping property of (E L , ∂) to the K linear map j : E → V
to obtain an L linear 8 : E L → V with 8 ∂ = j, and we apply the universal
mapping property of (V, j) to the K linear map ∂ : E → E L to obtain an L linear
80 : V → E L with 80 j = ∂. From (80 8) ∂ = 80 j = ∂ and 1 E L ∂ = ∂, the
uniqueness in the universal mapping property for (E L , ∂) implies 80 8 = 1 E L .
Arguing similarly, we obtain 880 = 1V . Thus 8 is an isomorphism with the
required properties.
   If 9 : E L → V is another isomorphism with 9 ∂ = j, then the argument just
given shows that 80 9 = 1 E L and 980 = 1V . Hence 9 = (80 )−1 = 8, and 9
is unique.                                                                    §

   To make E 7→ E L into a covariant functor from vector spaces over K to vector
spaces over L, we must examine the effect on linear maps. The tool is Proposition
6.18a. Thus let E and F be two vector spaces over K, and let M : E → F be
a K linear map between them. We extend scalars for E and F. The proposition
applies to the composition E → F → F L and shows that the composition
extends uniquely to an L linear map from E L to F L . A quick look at the proof
shows that this L linear map is M ⊗ 1. Actually, we can see directly that M ⊗ 1 is
indeed linear over L and not just over K: we just use our identity for compositions
of tensor products to write

  (M ⊗ 1)(I ⊗ (multiplication by c)) = M ⊗ (multiplication by c)
                                     = (I ⊗ (multiplication by c))(M ⊗ 1).

In any event, the explicit form of the extended linear map as M ⊗ 1 shows
immediately that the identity linear map goes to the identity and that compositions
go to compositions. Thus E 7→ E L is a covariant functor.
   In the special case that the vector spaces are Kn and Km , extension of scalars
has a particularly simple interpretation. The new spaces may be viewed as Ln
and Lm . Thus column vectors with entries in K get replaced by column vectors
with entries in L. What happens with linear mappings is even more transparent.
A linear map M : E → F is given by an m-by-n matrix A with entries in K, and
the linear map M ⊗ 1 : E L → F L is the one given by the same matrix A. Now
the entries of A are to be regarded as members of the larger field L. Viewed this
                                  7. Tensor Algebra                               277

way, extension of scalars might look as if it is dependent on choices of bases, but
the tensor-product formalism shows that it is not.
    A related notion to extension of scalars is that of restriction of scalars. Again
with an inclusion K ⊆ L of fields, a vector space E over the larger field L
becomes a vector space E K over the smaller field K by ignoring unnecessary
scalar multiplications. Although this notion is related to extension of scalars, it
is not inverse to it. For example, if the two fields are R and C and if we start with
an n-dimensional vector space E over R, then E C is a complex vector space of
dimension n and (E C )R is a real vector space of dimension 2n. We thus do not
get back to the original space E.



                                7. Tensor Algebra

Just as polynomial rings are often used in the construction of more general
commutative rings, so “tensor algebras” are often used in the construction of
more general rings that may not be commutative. In this section we construct the
“tensor algebra” of a vector space as a direct sum of iterated tensor products of
the vector space with itself, and we establish its properties. We shall proceed with
care, in order to provide a complete proof of the associativity of the multiplication.
   Let A, B, and C be vector spaces over a field K. A triple tensor product V =
A ⊗K B ⊗K C is a vector space over K with a 3-linear map ∂ : A × B × C → V
having the following universal mapping property: whenever t is a 3-linear map-
ping of A × B ×C into a vector space U over K, then there exists a linear mapping
T of V into U such that the diagram in Figure 6.4 commutes.
                                                      t
                              A× B ×C           −−−→ U
                                 
                                 
                                ∂y                        T

                         V = A ⊗K B ⊗K C

          FIGURE 6.4. Commutative diagram of a triple tensor product.

   The usual argument with universal mapping properties shows that there is at
most one triple tensor product up to a well-determined isomorphism, and one can
give an explicit construction of it that is similar to the one for ordinary tensor
products E ⊗K F. We shall not need that particular proof of existence since
Proposition 6.19a below will give us an alternative argument. Once we have that
statement, we shall use the uniqueness of triple tensor products to establish in
Proposition 6.19b an associativity formula for ordinary iterated tensor products.
278                             VI. Multilinear Algebra

A shorter proof of Proposition 6.19b, which avoids Proposition 6.19a and uses
naturality, will be given after the proof of Proposition 6.20.

   Proposition 6.19. If K is a field and A, B, C are vector spaces over K, then
      (a) (A ⊗K B) ⊗K C and A ⊗K (B ⊗K C) are triple tensor products.
      (b) there exists a unique K isomorphism 8 from left to right in

                       (A ⊗K B) ⊗K C ∼
                                     = A ⊗K (B ⊗K C)

         such that 8((a ⊗ b) ⊗ c) = a ⊗ (b ⊗ c) for all a ∈ A, b ∈ B, and c ∈ C.

    PROOF. In (a), consider (A ⊗K B) ⊗K C. Let t : A × B × C → U be
3-linear. For c ∈ C, define tc : A × B → U by tc (a, b) = t (a, b, c). Then tc
is bilinear and hence extends to a linear Tc : A ⊗K B → U . Since t is 3-linear,
tc1 +c2 = tc1 +tc2 and txc = xtc for scalar x; thus uniqueness of the linear extension
forces Tc1 +c2 = Tc1 + Tc2 and Txc = x Tc . Consequently

                             t 0 : (A ⊗K B) × C → U

given by t 0 (d, c) = Tc (d) is bilinear and therefore extends to a linear
T : (A ⊗K B) ⊗K C → U . This T proves existence of the linear extension of the
given t. Uniqueness is trivial, since the elements (a ⊗b)⊗c span (A ⊗K B)⊗K C.
So (A ⊗K B) ⊗K C is a triple tensor product. In a similar fashion, A ⊗K (B ⊗K C)
is a triple tensor product.
    For (b), set up the diagram of the universal mapping property for a triple tensor
product, using V = (A ⊗K B) ⊗K C, U = A ⊗K (B ⊗K C), and t (a, b, c) =
a ⊗ (b ⊗ c). We have just seen in (a) that V is a triple tensor product with
∂(a, b, c) = (a ⊗b)⊗c. Thus there exists a linear T : V → U with T ∂(a, b, c) =
t (a, b, c). This equation means that T ((a ⊗ b) ⊗ c) = a ⊗ (b ⊗ c). Interchanging
the roles of (A ⊗K B) ⊗K C and A ⊗K (B ⊗K C), we obtain a two-sided inverse
for T . Thus T will serve as 8 in (b), and existence is proved. Uniqueness is
trivial, since the elements (a ⊗ b) ⊗ c span (A ⊗K B) ⊗K C.                        §

   When there is no danger of confusion, Proposition 6.19 allows us to write a
triple tensor product without parentheses as A ⊗K B ⊗K C. The same argument
as in Corollaries 6.11 and 6.12 shows that the vector space of 3-linear forms on
A × B ×C is canonically isomorphic to the dual of the vector space A ⊗K B ⊗K C.
   Just as with Corollary 6.13 and Proposition 6.15, the result of Proposition 6.19
can be improved by saying that the isomorphism is natural in the variables A, B,
and C, as follows.
                                 7. Tensor Algebra                             279

   Proposition 6.20. Let A, B, C, A1 , B1 , and C1 be vector spaces over a field
K, and let L A : A → A1 , L B : B → B1 , and L C : C → C1 be linear maps.
Then the isomorphism 8 of Proposition 6.19b is natural in the triple (A, B, C)
in the sense that the diagram
                                        8
                  (A ⊗K B) ⊗K C      −−−→       A ⊗K (B ⊗K C)
                                                     
                             
             (L A ⊗L B )⊗L C y
                                                       L ⊗(L ⊗L )
                                                      y A B C
                                        8
                (A1 ⊗K B1 ) ⊗K C1 −−−→ A1 ⊗K (B1 ⊗K C1 )
commutes.
  PROOF. We have
      ((L A ⊗ (L B ⊗ L C )) ◦ 8)((a ⊗ b) ⊗ c)
                                 = (L A ⊗ (L B ⊗ L C ))(a ⊗ (b ⊗ c))
                                 = L A a ⊗ (L B ⊗ L C )(b ⊗ c)
                                 = L A a ⊗ (L B b ⊗ L C c)
                                 = 8((L A a ⊗ L B b) ⊗ L C c)
                                 = 8((L A ⊗ L B )(a ⊗ b) ⊗ L C c)
                                 = (8 ◦ ((L A ⊗ L B ) ⊗ L C ))((a ⊗ b) ⊗ c),
and the proposition follows.                                                    §
   The treatment of Propositions 6.19 and 6.20 can be shortened if we are willing
to bypass the notion of a triple tensor product and use what was proved about
naturality in the previous section. The result and the proof are as follows.
   Proposition 6.200 . Let A, B, and C be vector spaces over a field K. Then
there is an isomorphism 8 : (A ⊗K B) ⊗K C → A ⊗K (B ⊗K C) that is natural
in the triple (A, B, C) and satisfies 8(a ⊗ (b ⊗ c)) = a ⊗ (b ⊗ c).
   PROOF. Writing =   ∼ for “naturally isomorphic in all variables” and applying
Proposition 6.15 and other natural isomorphisms of the previous section repeat-
edly, we have
          °                    ¢          °                       ¢
  HomK (A ⊗K B) ⊗K C, V ∼        = HomK A ⊗K B, HomK (C, V )
                                          °                            ¢
                                 ∼
                                 = HomK B, HomK (A, HomK (C, V ))
                                          °                       ¢
                                 ∼
                                 = HomK B, HomK (A ⊗K C, V )
                                          °                       ¢
                                 ∼
                                 = HomK B, HomK (C ⊗K A, V )
                                          °                    ¢
                                 ∼
                                 = HomK (C ⊗K B) ⊗K A, V          by symmetry
                                          °                    ¢
                                 ∼
                                 = HomK A ⊗K (C ⊗K B), V
                                          °                    ¢
                                 ∼
                                 = HomK A ⊗K (B ⊗K C), V .
280                                    VI. Multilinear Algebra

Then the existence of the natural isomorphism follows from Corollary 6.17. Using
the explicit formula for the isomorphism in Proposition 6.16 and tracking matters
down, we see that 8(a ⊗ (b ⊗ c)) = a ⊗ (b ⊗ c).                                §

   There is no difficulty in generalizing matters to n-fold tensor products by
induction. An n-fold tensor product is to be universal for n-multilinear maps.
Again it is unique up to canonical isomorphism, as one proves by an argument
that runs along familiar lines. A direct construction of an n-fold tensor product
is possible in the style of the proof for ordinary tensor products, but such a
construction will not be needed. Instead, we can form an n-fold tensor product
as the (n − 1)-fold tensor product of the first n − 1 spaces, tensored with the n th
space. Proposition 6.19b allows us to regroup parentheses (inductively) in any
fashion we choose, and the same argument as in Corollaries 6.11 and 6.12 yields
the following proposition.

   Proposition 6.21. If E 1 , . . . , E n , and V are vector spaces over K, then the
vector space HomK (E 1 ⊗K · · · ⊗K E n , V ) is canonically isomorphic (via restric-
tion to pure tensors) to the vector space of all V -valued n-multilinear functions
on E 1 × · · · × E n . In particular the vector space of all n-multilinear forms on
E 1 × · · · × E n is canonically isomorphic to (E 1 ⊗K · · · ⊗K E n )0 .

   Iterated application of Proposition 6.20 shows that we get also a well-defined
notion of a linear map L 1 ⊗ · · · ⊗ L n , the tensor product of n linear maps. Thus
(E 1 , . . . , E n ) 7→ E 1 ⊗K · · · ⊗K E n is a functor. There is no need to write out the
details.

   We turn to the question of defining a multiplication operation on tensors. If K
is a field, an algebra4 over K is a vector space V over K with a multiplication
or product operation V × V → V that is K bilinear. The additive part of the K
bilinearity means that the product operation satisfies the distributive laws

 a(b + c) = ab + ac              and        (b + c)a = ba + ca             for all a, b, c in V,

and the scalar-multiplication part of the K bilinearity means that

               (ka)b = k(ab) = a(kb)               for all k in K and a, b in V.

   Within the text of the book, we shall work mostly just with associative
algebras, i.e., those algebras satisfying the usual associative law

                          a(bc) = (ab)c            for all a, b, c in V.
   4 Some   authors use the term “algebra” to mean what we shall call an “associative algebra.”
                                   7. Tensor Algebra                               281

An associative algebra is therefore a ring and a vector space, the scalar multipli-
cation and the ring multiplication being linked by the requirement that (ka)b =
k(ab) = a(kb) for all scalars k. Some commutative examples of associative alge-
bras over K are any field L containing K, the polynomial algebra K[X 1 , . . . , X n ],
and the algebra of all K-valued functions on a nonempty set S. Two noncommu-
tative examples of associative algebras over K are the matrix algebra Mn (K), with
matrix multiplication as its product, and HomK (V, V ) for any vector space V ,
with composition as its product. The division ring H of quaternions (Example 10
in Section IV.1) is another example of a noncommutative associative algebra
over R.
    Despite our emphasis on algebras that are associative, certain kinds of nonasso-
ciative algebras are of great importance in applications, and consequently several
problems at the end of the chapter make use of nonassociative algebras. A
nonassociative algebra is determined by its vector-space structure and the mul-
tiplication table for the members of a K basis. There is no restriction on the
multiplication table; all multiplication tables define algebras. Perhaps the best-
known nonassociative algebra is the 3-dimensional algebra over R determined by
vector product in R3 . A basis is {i, j, k}, the multiplication operation is denoted
by ×, and the multiplication table is
                 i × i = 0,          i × j = k,            i × k = −j,
                 j × i = −k,         j × j = 0,            j × k = i,
                 k × i = j,          k × j = −i,           k × k = 0.
Since i × (i × k) = i × (−j) = −k and (i × i) × k = 0, vector product is not
associative. The vector-product algebra is a special case of a Lie algebra; Lie
algebras are defined in Problems 31–35 at the end of the chapter.
   Tensor algebras, which we shall now construct, will be associative algebras.
Fix a vector space E over K, and for integers n ∏ 1, let T n (E) be the n-fold
tensor product of E with itself. In the case n = 0, we let T 0 (E) be the field K.
Define, initially as a vector space, T (E) to be the direct sum
                                          ∞
                                          M
                                T (E) =         T n (E).
                                          n=0

The elements that lie in one or another T n (E) are called homogeneous. We
define a bilinear multiplication on homogeneous elements

                          T m (E) × T n (E) → T m+n (E)

to be the restriction of the canonical isomorphism

                          T m (E) ⊗K T n (E) → T m+n (E)
282                                   VI. Multilinear Algebra

resulting from iterating Proposition 6.19b. This multiplication, denoted by ⊗, is
associative, as far as it goes, because the restriction of the K isomorphism
         T l (E) ⊗K (T m (E) ⊗K T n (E)) → (T l (E) ⊗K T m (E)) ⊗K T n (E)
to T l (E) × (T m (E) × T n (E)) factors through the map
             T l (E) × (T m (E) × T n (E)) → (T l (E) × T m (E)) × T n (E)
given by (r, (s, t)) 7→ ((r, s), t).
    This much tells how to multiply homogeneous elements in T (E).           PnSince each
element t in T (E) has a unique expansion as a finite sum t =                    k=0 tk with
                                                                                 P0
tk ∈ T (E), we can define the product of this t and the element t = nk=0 tk0 to
        k                                                                 0
                         P  n+n 0 P                  0
                                                                         P                  0
be the element t ⊗ t 0 = l=0         k+k 0 =l (tk ⊗ tk ); the expression    k+k 0 =l (tk ⊗ tk )
is the component of the product in T l (E).
    Multiplication is thereby well defined in T (E), and it satisfies the distributive
laws and is associative. Thus T (E) becomes an associative algebra with a
(two-sided) identity, namely the element 1 in T 0 (E). In the presence of the
identification ∂ : E → T 1 (E), T (E) is known as the tensor algebra of E. The
pair (T (E), ∂) has the universal mapping property given in Proposition 6.22
and pictured in Figure 6.5.
                                                    l
                                           E    −−−→ A
                                           
                                           
                                          ∂y            L

                                        T (E)

            FIGURE 6.5. University mapping property of a tensor algebra.
   Proposition 6.22. The pair (T (E), ∂) has the following universal mapping
property: whenever l : E → A is a linear map from E into an associative alge-
bra with identity, then there exists a unique associative algebra homomorphism
L : T (E) → A with L(1) = 1 such that the diagram in Figure 6.5 commutes.
   PROOF. Uniqueness is clear, since E and 1 generate T (E) as an algebra. For
existence we define L (n) on T n (E) to be the linear extension of the n-multilinear
map
                      (v1 , v2 , . . . , vn ) 7→ l(v1 )l(v2 ) · · · l(vn ),
                 L (n)
and we let L =       L in obvious notation. Let u 1 ⊗ · · · ⊗ u m be in T m (E) and
v1 ⊗ · · · ⊗ vn be in T n (E). Then we have
                             L (m) (u 1 ⊗ · · · ⊗ u m ) = l(u 1 ) · · · l(u m ),
                               L (n) (v1 ⊗ · · · ⊗ vn ) = l(v1 ) · · · l(vn ),
      L (m+n) (u 1 ⊗ · · · ⊗ u m ⊗ v1 ⊗ · · · ⊗ vn ) = l(u 1 ) · · · l(u m )l(v1 ) · · · l(vn ).
                                      8. Symmetric Algebra                                      283

Hence

L (m) (u 1 ⊗ · · · ⊗ u m )L (n) (v1 ⊗ · · · ⊗ vn ) = L (m+n) (u 1 ⊗ · · · ⊗ u m ⊗ v1 ⊗ · · · ⊗ vn ).

Taking linear combinations, we see that L is a homomorphism.                                     §

   Proposition 6.22 allows us to make E 7→ T (E) into a functor from the category
of vector spaces over K to the category of associative algebras with identity over
K. To carry out the construction, we suppose that ϕ : E → F is a linear map
between two vector spaces over K. If i : E → T (E) and j : F → T (F) are the
inclusion maps, then jϕ is a linear map from E into T (F), and Proposition 6.22
produces a unique algebra homomorphism 8 : T (E) → T (F) carrying 1 to 1
and satisfying 8i = jϕ. Then the tensor-product functor is defined to carry the
linear map ϕ to the homomorphism 8 of associative algebras with identity.
   For the situation in which R is a commutative ring with identity, Section
IV.5 introduced the ring R[X 1 , . . . , X n ] of polynomials in n commuting inde-
terminates with coefficients in R. This ring was characterized by a universal
mapping property saying that if a ring homomorphism of R into a commutative
ring with identity were given and if n elements t1 , . . . , tn were given, then the
ring homomorphism of R could be extended uniquely to a ring homomorphism
of R[X 1 , . . . , X n ] carrying X j into t j for each j.
   Proposition 6.22 yields a noncommutative version of this result, except that the
ring of coefficients is assumed this time to be a field K. To arrange for X 1 , . . . , X n
to be noncommuting indeterminates,L            we form a vector space with {X 1 , . . . , X n }
as a basis. Thus we let E = nj=1 KX j . If t1 , . . . , tn are arbitrary elements of an
associative algebra A with identity, then the formulas l(X j ) = t j for 1 ≤ j ≤ n
define a linear map l : E → A. The associative-algebra homomorphism
L : T (E) → A produced by the proposition extends the inclusion of K into
the subfield K1 of A and carries each X j to t j .


                                  8. Symmetric Algebra

We continue to allow K to be an arbitrary field. Let E be a vector space over
K, and let T (E) be the tensor algebra. We begin by defining the symmetric
algebra S(E). This is to be a version of T (E) in which the elements, which are
called symmetric tensors, commute with one another. It will not be canonically
an algebra of polynomials, as we shall see presently, and thus we make no use of
polynomial rings in the construction.
   Just as the vector space of n-multilinear forms E ×· · ·× E → K is canonically
the dual of T n (E), so the vector space of symmetric n-multilinear forms will be
284                              VI. Multilinear Algebra

canonically the dual of S n (E). Here “symmetric” means that f (x1 , . . . , xn ) =
 f (xτ (1) , . . . , xτ (n) ) for every permutation τ in the symmetric group Sn .
    Since tensor algebras are supposed to be universal devices for constructing
associative algebras over K, whether commutative or not, we seek to form S(E)
as a quotient of T (E). If q is the quotient homomorphism, we want to have
q(u ⊗ v) = q(v ⊗ u) in S(E) whenever u and v are in ∂(E) = T 1 (E). Hence
every element u ⊗ v − v ⊗ u is to be in the kernel of the homomorphism. On the
other hand, we do not want to impose any unnecessary conditions on our quotient,
and so we factor out only what the elements u ⊗ v − v ⊗ u force us to factor out.
Thus we define the symmetric algebra by

                                  S(E) = T (E)/I,
                        √                                        !
                            two-sided ideal generated by all
where             I =       u ⊗ v − v ⊗ u with u and v               .
                            in T 1 (E)
Then S(E) is an associative algebra with identity.
   Let us see that the fact that the generators of the ideal I are homogeneous
elements (all being in T 2 (E)) implies that
                                      ∞
                                      M
                                I =         (I ∩ T n (E)).
                                      n=0

In fact, each I ∩ T n (E) is contained in I , and hence I contains the right side.
On the other hand, if x is any element of I , then x is a sum of terms of the form
a ⊗ (u ⊗ v − v ⊗ u) ⊗ b, and we may assume that each a and b is homogeneous.
Any individual term a ⊗ (u ⊗ v − v ⊗ u) ⊗ b is in some I ∩ T n (E), and x is
exhibited as a sum of members of L the various intersections I ∩ T n (E).
   An ideal with the property I = ∞              n
                                     n=0 (I ∩ T (E)) is said to be homogeneous.
Since I is homogeneous,
                                  ∞
                                  M
                        S(E) =          T n (E)/(I ∩ T n (E)).
                                  n=0

We write S n (E) for the n th summand on the right side, so that
                                            ∞
                                            M
                                 S(E) =            S n (E).
                                             n=0

Since I ∩ T 1 (E) = 0, the map of E → T 1 (E) → S 1 (E) into first-order elements
is one-one onto. The product operation in S(E) is written without a product sign,
                                    8. Symmetric Algebra                                 285

the image in S n (E) of v1 ⊗ · · · ⊗ vn in T n (E) being written as v1 · · · vn . If a is in
S m (E) and b is in S n (E), then ab is in S m+n (E). Moreover, S n (E) is generated
by elements v1 · · · vn with all v j in S 1 (E) ∼ = E, since T n (E) is generated by
corresponding elements v1 ⊗ · · · ⊗ vn . The defining relations for S(E) make
vi v j = v j vi for vi and v j in S 1 (E), and it follows that the associative algebra
S(E) is commutative.                                                                      §
   Proposition 6.23. Let E be a vector space over the field K.
   (a) Let ∂ be the n-multilinear function ∂(v1 , . . . , vn ) = v1 · · · vn of E × · · · × E
into S n (E). Then (S n (E), ∂) has the following universal mapping property:
whenever l is any symmetric n-multilinear map of E × · · · × E into a vector
space U , then there exists a unique linear map L : S n (E) → U such that the
diagram
                                                    l
                                E × · · · × E −−−→ U
                                     
                                     
                                    ∂y           L

                                    S n (E)

commutes.
   (b) Let ∂ be the one-one linear function that embeds E as S 1 (E) ⊆ S(E).
Then (S(E), ∂) has the following universal mapping property: whenever l is
any linear map of E into a commutative associative algebra A with identity, then
there exists a unique algebra homomorphism L : S(E) → A with L(1) = 1 such
that the diagram
                                                l
                                       E      −−−→ A
                                       
                                       
                                      ∂y            L

                                    S(E)

commutes.
    PROOF. In both cases uniqueness is trivial. For existence we use the universal
mapping properties of T n (E) and T (E) to produce e  L on T n (E) or T (E). If we
can show that L annihilates the appropriate subspace so as to descend to S n (E)
                e
or S(E), then the resulting map can be taken as L, and we are done. For (a), we
have eL : T n (E) → U , and we are to show that e  L(T n (E) ∩ I ) = 0, where I is
generated by all u ⊗Pv − v ⊗ u with u and v in T 1 (E). A member of T n (E) ∩ I
is thus of the form    ai ⊗ (u i ⊗ vi − vi ⊗ u i ) ⊗ bi with each term in T n (E).
Each term here is a sum of pure tensors
 x1 ⊗· · ·⊗ xr ⊗u i ⊗vi ⊗ y1 ⊗· · ·⊗ ys − x1 ⊗· · ·⊗ xr ⊗vi ⊗u i ⊗ y1 ⊗· · ·⊗ ys (∗)
286                             VI. Multilinear Algebra

with r + 2 + s = n. Since l by assumption takes equal values on

                     x1 × · · · × xr × u i × vi × y1 × · · · × ys

and                  x1 × · · · × xr × vi × u i × y1 × · · · × ys ,

e                                      L(T n (E) ∩ I ) = 0.
L vanishes on (∗), and it follows that e
   For (b) we are to show that e  L : T (E) → A vanishes on I . Since ker e   L
is an ideal, it is enough to check that eL vanishes on the generators of I . But
e
L(u ⊗ v − v ⊗ u) = l(u)l(v) − l(v)l(u) = 0 by the commutativity of A, and thus
L(I ) = 0.                                                                    §

   Corollary 6.24. If E and F are vector spaces over the field K, then the
vector space HomK (S n (E), F) is canonically isomorphic (via restriction to pure
tensors) to the vector space of all F-valued symmetric n-multilinear functions on
E × · · · × E.
   PROOF. Restriction is linear and one-one. It is onto by Proposition 6.23a. §

   Corollary 6.25. If E is a vector space over the field K, then the dual (S n (E))0
of S n (E) is canonically isomorphic (via restriction to pure tensors) to the vector
space of symmetric n-multilinear forms on E × · · · × E.
   PROOF. This is a special case of Corollary 6.24.                                 §

   If ϕ : E → F is a linear map between vector spaces, then we can use
Proposition 6.23b to define a corresponding homomorphism 8 : S(E) → S(F)
of associative algebras with identity. In this way, we can make E 7→ S(E) into a
functor from the category of vector spaces over K to the category of commutative
associative algebras with identity over K. The details appear in Problem 14 at
the end of the chapter.
   Next we shall identify a basis for S n (E) as a vector space. The union of such
bases as n varies will then be a basis of S(E). Let {u i }i∈A be a basis of E, possibly
infinite. As noted in Section A5 of the appendix, a simple ordering on the index
set A is a partial ordering in which every pair of elements is comparable and in
which a ≤ b and b ≤ a together imply a = b.

    Proposition 6.26. Let E be a vector space over the field K, let {u i }i∈A be a
basis of E, and suppose that a simple ordering has been imposed on the index set
                                    j           j                            P
A. Then the set of all monomials u i11 · · · u ikk with i 1 < · · · < i k and m jm = n
is a basis of S n (E).
                                    8. Symmetric Algebra                              287

    REMARK. In particular if E is finite-dimensional with (u 1 , . . . , u N ) as an
                                    j          j
ordered basis, then the monomials u 11 · · · u NN of total degree n form a basis of
S n (E).
    PROOF. Since S(E) is commutative and since n-fold products of elements ∂(u i )
in T 1 (E) span T n (E), the indicated set of monomials spans S n (E). Let us see that
the
P set is linearlyPindependent. Take any finite subset F ⊆ A of indices. The map
   i∈A  ci u i →
               7   i∈F ci X i of E into the polynomial algebra K[{X i }i∈F ] is linear
into a commutative algebra with identity. Its extension via Proposition 6.23b maps
all monomials in the u i for i ∈ F into distinct monomials in K[{X i }i∈F ], which
are necessarily linearly independent. Hence any finite subset of the monomials in
the statement of the proposition is linearly independent, and the whole set must
be linearly independent. Therefore our spanning set is a basis.                     §

   The proof of Proposition 6.26 shows that S(E) may be identified with poly-
nomials in indeterminates identified with members of E once a basis has been
chosen, but this identification depends on the choice of basis. Indeed, if we think
of E as specified in advance, then the isomorphism was set up by mapping the set
{X i }i∈A to the specified basis of E, and the result certainly depended on what basis
was used. Nevertheless, if E is finite-dimensional, there is still an isomorphism
that is independent of basis; it is between S(E 0 ), where E 0 is the dual of E, and
a natural basis-free notion of “polynomials” on E. We return to this point after
one application of Proposition 6.26.

   Corollary 6.27. Let E be a finite-dimensional vector space over K of dimen-
sion N . Then         µ            ∂
              n         n+ N −1
    (a) dim S (E) =                   for 0 ≤ n < ∞,
                          N −1
          n
    (b) S (E ) is canonically isomorphic to S n (E)0 in such a way that
             0


                                                        n
                                                       XY
                   ( f 1 · · · f n )(w1 · · · wn ) =               f j (wτ ( j)) ),
                                                       τ ∈Sn j=1


        for any f 1 , . . . , f n in E 0 and any w1 , . . . , wn in E, provided K has
        characteristic 0; here Sn is the symmetric group on n letters.

   PROOF. For (a), a basis has been described in Proposition 6.26. To see its
cardinality, we recognize that picking out N − 1 objects from n + N − 1 to label
                                          µ to the u j∂’s in an ordered basis; thus
as dividers is a way of assigning exponents
                                           n+ N −1
the cardinality of the indicated basis is                .
                                            N −1
288                                       VI. Multilinear Algebra

   For (b), let f 1 , . . . , f n be in E 0 and w1 , . . . , wn be in E, and define
                                                               n
                                                              XY
                         l f1 ,..., fn (w1 , . . . , wn ) =               f j (wτ ( j)) ).
                                                              τ ∈Sn j=1


Then l f1 ,..., fn is symmetric n-multilinear from E × · · · × E into K and extends
by Proposition 6.23a to a linear L f1 ,..., fn : S n (E) → K. Thus l( f 1 , . . . , f n ) =
L f1 ,..., fn defines a symmetric n-multilinear map of E 0 × · · · × E 0 into S n (E)0 . Its
linear extension L maps S n (E 0 ) into S n (E)0 .
     To complete the proof, we shall show that L carries basis to basis. Let
u 1 , . . . , u N be an ordered basis of E, and let u 01 , . . P . , u 0N be the dual basis. Part
(a) shows that the elements (u 01 ) j1 · · · (u 0N ) jN with Pm jm = n form a basis of
S n (E 0 ) and that the elements (u 1 )k1 · · · (u N )k N with m km = n form a basis of
S n (E). We show that L of the basis of S n (E 0 ) is the dual basis of the basis of
S n (E), except for positive-integer factors. Thus let all of f 1 , . . . , f j1 be u 01 , let
all of f j1 +1 , . . . , f j1 + j2 be u 02 , and so on. Similarly let all of w1 , . . . , wk1 be u 1 ,
let all of wk1 +1 , . . . , wk1 +k2 be u 2 , and so on. Then

        L((u 01 ) j1 · · · (u 0N ) jN )((u 1 )k1 · · · (u N )k N ) = L( f 1 · · · f n )(w1 · · · wn )
                                                                   = l( f 1 , . . . , f n )(w1 · · · wn )
                                                                      XY       n
                                                                   =                 f i (wτ (i)) ).
                                                                     τ ∈Sn i=1


For given τ , the product on the right side is 0 unless, for each index i, an inequality
 jm−1 + 1 ≤ i ≤ jm implies that km−1 + 1 ≤ τ (i) ≤ km . In this case the product
is 1; so the right side counts the number of such τ ’s. For given τ , obtaining a
nonzero product forces km = jm for all m. And when km = jm for all m, the
choice τ = 1 does lead to product 1. Hence the members of L of the basis are
positive-integer multiples of the members of the dual basis, as asserted.             §

    Let us return to the question of introducing a basis-free notion of polynomials
on the vector space E under the assumption that E is finite-dimensional. We take
a cue from Corollary 4.32, which tells us that the evaluation homomorphism
carrying K[X 1 , . . . , X n ] to the algebra of K-valued polynomial functions of
(t1 , . . . , tn ) is one-one if K is an infinite field. We regard the latter as the algebra
of polynomial functions on Kn , and we check what happens when we identify
the vector space E with Kn by fixing a basis. Let 0 = {x1 , . . . , xn } be a basis of
E, and let 0 0 = {x10 , . . . , xn0 } be the dual basis of E 0 . If e = t1 x1 + · · · + tn xn is
the expansion of a member of E in terms of 0, then we have x j0 (e) = t j . Thus the
polynomial functions t j are given by the members of the dual basis. The vector
                                 8. Symmetric Algebra                             289

space of all homogeneous first-degree polynomial functions is the set of linear
combinations of the t j ’s, and these are given by arbitrary linear functionals on E.
Thus the vector space of homogeneous first-degree polynomial functions on E is
just the dual space E 0 , and this conclusion does not depend on the choice of basis.
The algebra of all polynomial functions on E is then the algebra of all K-valued
functions on E generated by E 0 and the constant functions.
   This discussion tells us unambiguously what polynomial functions on E are
to be, and we want to backtrack to handle abstract polynomials on E. Although
the evaluation homomorphism from K[X 1 , . . . , X n ] to the algebra of polynomial
functions on Kn may fail to be one-one if K is a finite field, its restriction to
homogeneous first-degree polynomials is one-one. Thus, whatever we might
mean by the vector space of homogeneous first-degree polynomials on E, the
evaluation mapping should exhibit this space as isomorphic to E 0 .
   Armed with these clues, we define the polynomial algebra P(E) on E to be
the symmetric algebra S(E 0 ) if E is finite-dimensional. We need an evaluation
mapping for each point e of E, and we obtain this from the universal mapping
property of symmetric algebras (Proposition 6.23b): With e fixed, we have a
linear map l from the vector space E 0 to the commutative associative algebra
K given with l(e0 ) = e0 (e). The universal mapping property gives us a unique
algebra homomorphism L : S(E 0 ) → K that extends l and carries 1 to 1. The
algebra homomorphism L is then a multiplicative linear functional on P(E) =
S(E 0 ) that carries 1 to 1 and agrees with evaluation at e on homogeneous first-
degree polynomials. We write this homomorphism as p 7→ p(e), and we define
P n (E) = S n (E 0 ); this is the vector space of homogeneous n th -degree polynomials
on E. A confirmation that P(E) is indeed to be regarded as the algebra of abstract
polynomials on E comes from the following.

   Proposition 6.28. If E is a finite-dimensional vector space over the field
K, then the system of evaluation homomorphisms P(E) → K on polynomials
given by p 7→ { p(e)}e∈E is an algebra homomorphism of P(E) onto the algebra
of K-valued polynomial functions on E that carries the identity to the constant
function 1, and it is one-one if K is an infinite field.
   PROOF. Certainly p 7→ { p(e)}e∈E is an algebra homomorphism of P(E) into
the algebra of K-valued polynomial functions on E, and it carries the identity to
the constant function 1. We have seen that the image of P 1 (E) is exactly E 0 , and
hence the image of P(E) is the algebra of K-valued functions on E generated
by E 0 and the constants. This is exactly the algebra of all K-valued polynomial
functions, and hence the mapping is onto.
   Suppose that K is infinite. The restriction of p 7→ { p(e)}e∈E to the finite-
dimensional subspace P n (E) of P(E) maps into the finite-dimensional subspace
of all polynomial functions on E homogeneous of degree n, and this restriction
290                                VI. Multilinear Algebra

must therefore be onto. We can read off the dimension of the space of all
polynomial functions on E homogeneous of degree n from Corollary 4.32 and
Corollary 6.27a. This dimension matches the dimension of P n (E), according to
Corollary 6.27a. Since the mapping is onto and the finite dimensions match, the
restricted mapping is one-one. Hence p 7→ { p(e)}e∈E is one-one.             §

   We have defined the symmetric algebra S(E) as a quotient of the tensor algebra
T (E). Now let us suppose that K has characteristic 0. With this hypothesis we
shall be able to identify an explicit vector subspace of T (E) that maps one-one
onto S(E) during the passage to the quotient. This subspace of T (E) can therefore
be viewed as a version of S(E) for some purposes.
   We define an n-multilinear function from E × · · · × E into T n (E) by

                                           1 X
                   (v1 , . . . , vn ) 7→           vτ (1) ⊗ · · · ⊗ vτ (n) ,
                                           n! τ ∈S
                                                    n



and let σ : T n (E) → T n (E) be its linear extension. We call σ the symmetrizer
operator. The image of σ in T (E) is denoted by e  S n (E), and the members of this
subspace are called symmetrized tensors.

   Proposition 6.29. Let the field K have characteristic 0, and let E be a vector
space over K. Then the symmetrizer operator σ satisfies σ 2 = σ . The kernel of
σ on T n (E) is exactly T n (E) ∩ I , and therefore

                                    S n (E) ⊕ (T n (E) ∩ I ).
                          T n (E) = e

  REMARK. In view of this corollary, the quotient map T n (E) → S n (E) carries
S (E) one-one onto S n (E). Thus e
en                                 S n (E) can be viewed as a copy of S n (E)
                                   n
embedded as a direct summand of T (E).
     PROOF. We have

                                      1    X
           σ 2 (v1 ⊗ · · · ⊗ vn ) =               vρτ (1) ⊗ · · · ⊗ vρτ (n)
                                    (n!)2 ρ,τ ∈S
                                                n

                                      1 X X
                                  =                     vω(1) ⊗ · · · ⊗ vω(n)
                                    (n!)2 ρ∈S ω∈S ,
                                                        n        n
                                                            (ω=ρτ )
                                      1 X
                                  =          σ (v1 ⊗ · · · ⊗ vn )
                                      n! ρ∈S
                                                n

                                  = σ (v1 ⊗ · · · ⊗ vn ).
                                  9. Exterior Algebra                             291

Hence σ 2 = σ . Thus σ fixes any member of image σ , and it follows that
image σ ∩ ker σ = 0. Consequently T n (E) is the direct sum of image σ and
ker σ . We are left with identifying ker σ as T n (E) ∩ I .
   The subspace T n (E) ∩ I is spanned by elements

x1 ⊗ · · · ⊗ xr ⊗ u ⊗ v ⊗ y1 ⊗ · · · ⊗ ys − x1 ⊗ · · · ⊗ xr ⊗ v ⊗ u ⊗ y1 ⊗ · · · ⊗ ys

with r + 2 + s = n, and the symmetrizer σ certainly vanishes on such elements.
Hence T n (E) ∩ I ⊆ ker σ . Suppose that the inclusion is strict, say with t in
ker σ but t not in T n (E) ∩ I . Let q be the quotient map T n (E) → S n (E).
The kernel of q is T n (E) ∩ I , and thus q(t) 6= 0. From Proposition 6.26 the
T (E) monomials in basis elements from E with increasing indices map onto a
basis of S(E). Since K has characteristic 0, the symmetrized versions of these
monomials map to nonzero multiples of the images of the initial monomials.
Consequently q carries e                                                     S n (E)
                           S n (E) = image σ onto S n (E). Thus choose t 0 ∈ e
         0                   0                    n
with q(t ) = q(t). Then t − t is in ker q = T (E) ∩ I ⊆ ker σ . Since σ (t) = 0,
we see that σ (t 0 ) = 0. Consequently t 0 is in ker σ ∩ image σ = 0, and we obtain
t 0 = 0 and q(t) = q(t 0 ) = 0, contradiction.                                    §


                                9. Exterior Algebra

We turn to a discussion of the exterior algebra. Let K be an arbitrary field, and
let E be a vectorVspace over K. The construction, results, and proofs for the
exterior algebra
             V      (E) are similar to those for the symmetric algebra S(E). The
elements of (E) are to be all the alternating tensors (= skew-symmetric if K
has characteristic 6= 2), and so we want to force v ⊗ v = 0. Thus we define the
exterior algebra by
                                 V
                                  (E) = T (E)/I 0 ,
                        µ                                      ∂
                   0        two-sided ideal generated by all
where             I =                                              .
                            v ⊗ v with v in T 1 (E)
      V
Then (E) is an associative algebra with identity.
                                             L∞            n
  It is clear that I 0 is homogeneous: I 0 =           0
                                                n=0 (I ∩ T (E)). Thus we can
write                  V       L
                          (E) = ∞      n      0      n
                                  n=0 T (E)/(I ∩ T (E)).
          V
We write n (E) for the n th summand on the right side, so that
                              V     L    Vn
                               (E) = ∞
                                     n=0    (E).
292                                VI. Multilinear Algebra
                                                                          V
Since I 0 ∩ T 1 (E) = 0, the mapV      of E into first-order elements 1 (E) is one-one
onto.
Vn The product operation         n
                                    in (E) is denoted by ∧ rather than ⊗, the image    Vm in
    (E) of vV1 ⊗   · · · vn in T   (E) being  denoted
                                            Vm+n       by v1 ∧ · · · ∧ v
                                                                       Vn . If a is in   (E)
                n                                                         n
and b is in       (E), then a ∧ b is in            (E). Moreover,           (E) is generated
                                                 V
by elements v1 ∧ · · · ∧ vn with all v j in 1 (E) ∼      = E, since T n (E) V    is generated
by corresponding elements v1 ⊗ · · · ⊗ vn . The defining relations for (E) make
                                           V
vi ∧ v j = −v j ∧ vi for vi and v j in 1 (E), and it follows that
                                                     V                    V
           a ∧ b = (−1)mn b ∧ a              for a ∈ m (E) and b ∈ n (E).

   Proposition 6.30. Let E be a vector space over the field K.
   (a)VLet ∂ be the n-multilinear
                       V          function ∂(v1 , . . . , vn ) = v1 ∧· · ·∧vn of E×· · ·×E
into n (E). Then ( n (E), ∂) has the following universal mapping property:
whenever l is any alternating n-multilinear map   V of E ×· · ·× E into a vector space
U , then there exists a unique linear map L : n (E) → U such that the diagram
                                                    l
                                E × · · · × E −−−→ U
                                     
                                     
                                    ∂y           L
                                  Vn
                                      (E)

commutes.                                     V         V            V
   (b) Let ∂ be the function that embeds E as 1 (E) ⊆ (E). Then ( (E), ∂)
has the following universal mapping property: whenever l is any linear map of
                                                           2
                                                     V l(v) = 0 for all v ∈ E,
E into an associative algebra A with identity such that
then there exists a unique algebra homomorphism L : (E) → A with L(1) = 1
such that the diagram
                                                l
                                      E −−−→ A
                                      
                                      
                                     ∂y     L
                                    V
                                      (E)

commutes.
   PROOF. The proof is completely analogous to the proof of Proposition 6.23. §

   Corollary 6.31. V   If E and F are vector spaces over the field K, then the
vector space HomK ( n (E), F) is canonically isomorphic (via restriction to pure
tensors) to the vector space of all F-valued alternating n-multilinear functions on
E × · · · × E.
   PROOF. Restriction is linear and one-one. It is onto by Proposition 6.30a. §
                                        9. Exterior Algebra                                293
                                                                           V
  VCorollary 6.32. If E is a vector space over the field K, then the dual ( n (E))0
of n (E) is canonically isomorphic (via restriction to pure tensors) to the vector
space of alternating n-multilinear forms on E × · · · × E.
   PROOF. This is a special case of Corollary 6.31.                                         §

   If ϕ : E → F is a linear map between vector spaces, then      V we can  V use
Proposition 6.30b to define a corresponding homomorphism 8 : (E)      V →     (F)
of associative algebras with identity. In this way, we can make E 7→ (E) into a
functor from the category of vector spaces over K to the category of commutative
associative algebras with identity over K. We omit the details, which are similar
to those for symmetric tensors.       V
   Next we shall identify a basis for n (E)V as a vector space. The union of such
bases as n varies will then be a basis of (E).

    Proposition 6.33. Let E be a vector space over the field K, let {u i }i∈A be a
basis of E, and suppose that a simple ordering has been imposed on the index set
A. Then the set of all monomials u i1 ∧ · · · ∧ u in with i 1 < · · · < i n is a basis of
V  n
     (E).
                                         V
    PROOF
      Vm . Since multiplication
                        Vn           in (E) satisfies a ∧ b = (−1)mn b ∧ a for
a ∈ V(E) and b ∈           (E) and since monomials span T n (E), the indicated set
          n
spans       (E). Let us see that the set is linearly independent. For i ∈ A, let u i0 be
the member of E 0 with u i0 (u j ) equal to 1 for j = i and equal to 0 for j 6= i. Fix
r1 < · · · < rn , and define

             l(w1 , . . . , wn ) = det{u r0 i (w j )}       for w1 , . . . , wn in E.

Then l is alternating n-multilinear
                         Vn         from E × · · · × E into K and extends by
Proposition 6.30a to L :    (E) → K. If k1 < · · · < kn , then

               L(u k1 ∧ · · · ∧ u kn ) = l(u k1 , . . . , u kn ) = det{u r0 i (u k j )},

                                                                Vn case it is 1. This
and the right side is 0 unless r1 = k1 , . . . , rn = kn , in which
proves that the u r1 ∧ · · · ∧ u rn are linearly independent in   (E).             §

   Corollary 6.34. Let E be a finite-dimensional vector space over K of dimen-
sion N . Then          µ ∂
            V            N
    (a) dim n (E) =           for 0 ≤ n ≤ N and = 0 for n > N ,
        Vn 0             n                  V
    (b)    (E ) is canonically isomorphic to n (E)0 by

                    ( f 1 ∧ · · · ∧ f n )(w1 , . . . , wn ) = det{ f i (w j )}.
294                                VI. Multilinear Algebra

   PROOF. Part (a) is an immediate consequence of Proposition 6.33, and (b) is
proved in the same way as Corollary 6.27b, using Proposition 6.30a as a tool. The
“positive-integer multiples” that arise in the proof of Corollary 6.27b are all 1 in
the current proof, and hence no restriction on the characteristic of K is needed. §

   Now let us suppose that K has characteristic 0. We define an n-multilinear
function from E × · · · × E into T n (E) by

                                       1 X
               (v1 , . . . , vn ) 7→           (sgn τ )vτ (1) ⊗ · · · ⊗ vτ (n) ,
                                       n! τ ∈S
                                                n


and let σ 0 : T n (E) → T n (E) be its linear extension. We call σ 0 the antisym-
                                                                 Vn
metrizer operator. The image of σ 0 in T (E) is denoted by e (E), and the
members of this subspace are called antisymmetrized tensors.

   Proposition 6.35. Let the field K have characteristic 0, and let E be a vector
space over K. Then the antisymmetrizer operator σ 0 satisfies σ 02 = σ 0 . The
kernel of σ 0 on T n (E) is exactly T n (E) ∩ I 0 , and therefore
                                    Vn
                          T n (E) = e (E) ⊕ (T n (E) ∩ I 0 ).
                                                                V
   REMARK. In view of this corollary, the quotient map T n (E) → n (E) carries
e n (E) one-one onto Vn (E). Thus V
V                                   e n (E) can be viewed as a copy of Vn (E)
embedded as a direct summand of T n (E).
  PROOF. We have
                                  1    X
       σ 02 (v1 ⊗ · · · ⊗ vn ) =              (sgn ρτ )vρτ (1) ⊗ · · · ⊗ vρτ (n)
                                (n!)2 ρ,τ ∈S
                                            n

                                  1 X X
                              =                    (sgn ω)vω(1) ⊗ · · · ⊗ vω(n)
                                (n!)2 ρ∈S ω∈S ,
                                                    n        n
                                                        (ω=ρτ )
                                   1 X 0
                              =           σ (v1 ⊗ · · · ⊗ vn )
                                   n! ρ∈S
                                            n
                                       0
                              = σ (v1 ⊗ · · · ⊗ vn ).

Hence σ 02 = σ 0 . Consequently T n (E) is the direct sum of image σ 0 and ker σ 0 ,
and we are left with identifying ker σ 0 as T n (E) ∩ I 0 .
  The subspace T n (E) ∩ I 0 is spanned by elements

                      x1 ⊗ · · · ⊗ xr ⊗ v ⊗ v ⊗ y1 ⊗ · · · ⊗ ys
                                      10. Problems                                    295

with r +2+s = n, and the antisymmetrizer σ 0 certainly vanishes on such elements.
Hence T n (E) ∩ I 0 ⊆ ker σ 0 . Suppose that the inclusion is strict, say V with t in
ker σ 0 but t not in T n (E) ∩ I 0 . Let q be the quotient map T n (E) → n (E). The
kernel of q is T n (E) ∩ I 0 , and thus q(t) 6= 0. From PropositionV6.33 the T (E)
monomials with strictly increasing indices map onto a basis of (E). Since K
has characteristic 0, the antisymmetrized versions of these monomials map to
nonzero multiples of the images of the initial monomials. Consequently q carries
e n (E) = image σ 0 onto Vn (E). Thus choose t 0 ∈ V
V                                                           e n (E) with q(t 0 ) = q(t).
Then t 0 − t is in ker q = T n (E) ∩ I 0 ⊆ ker σ 0 . Since σ 0 (t) = 0, we see that
σ 0 (t 0 ) = 0. Consequently t 0 is in ker σ 0 ∩ image σ 0 = 0, and we obtain t 0 = 0
and q(t) = q(t 0 ) = 0, contradiction.                                                §



                                    10. Problems

1.   Let V be a vector space over a field K, and let h · , · i be a nondegenerate bilinear
     form on V .
     (a) Prove that every member v 0 of V is of the form v 0 (w) = hv, wi for one and
         only one member v of V .
     (b) Suppose that ( · , · ) is another bilinear form on V . Prove that there is some
         linear function L : V → V such that (v, w) = hL(v), wi for all v and w
         in V .
                       ≥ ¥
2.   The matrix A = 01 10 with entries in F2 is symmetric. Prove that there is no
     nonsingular M with M t AM diagonal.
3.   This problem shows that one possible generalization of Sylvester’s Law to other
     fields is not ≥
                   valid. Over
                           ¥   the ≥field¥F3 , show that there is a nonsingular matrix
                     −1 0        t 1 0 M. Conclude that the number of squares in
     M such that      0 −1
                             = M      01
     K× among the diagonal entries of the diagonal form in Theorem 6.5 is not an
     invariant of the symmetric matrix.
4.   Let V be a complex n-dimensional vector space, let ( · , · ) be a Hermitian form on
     V , let VR be the 2n-dimensional real vector space obtained from V by restricting
     scalar multiplication to real scalars, and define h · , · i = Im( · , · ). Prove that
     (a) h · , · i is an alternating bilinear form on VR ,
     (b) hJ (v1 ), J (v2 )i = hv1 , v2 i for all v1 and v2 if J : VR → VR is what
           multiplication by i becomes when viewed as a linear map from VR to itself,
     (c) h · , · i is nondegenerate on VR if and only if ( · , · ) is nondegenerate on V .
5.   Let W be a 2n-dimensional real vector space, and let h · , · i be a nondegenerate
     alternating bilinear form on W . Suppose that J : W → W is a linear map such
296                                   VI. Multilinear Algebra

      that J 2 = −I and hJ (w1 ), J (w2 )i = hw1 , w2 i for all w1 and w2 in W . Prove
      that W equals VR for some n-dimensional complex vector space V possessing a
      Hermitian form whose imaginary part is h · , · i.
6.    This problem sharpens the result of Theorem 6.7 in the nondegenerate case. Let
      h · , · i be a nondegenerate alternating bilinear form on a 2n-dimensional vector
      space V over K. A vector subspace S of V is called an isotropic subspace if
      hu, vi = 0 for all u and v in S. Prove that
      (a) any isotropic subspace of V that is maximal under inclusion has dimension
             n,
      (b) for any maximal isotropic subspace S1 , there exists a second maximal
             isotropic subspace S2 such that S1 ∩ S2 = 0.
      (c) if S1 and S2 are maximal isotropic subspaces of V Ø such that S1 ∩ S2 = 0,
             then the linear map S2 → S10 given by s2 7→ h · , s2 iØ S1 is an isomorphism of
             S2 onto the dual space S10 .
      (d) if S1 and S2 are maximal isotropic subspaces of V such that S1 ∩ S2 = 0,
             then there exist bases { p1 , . . . , pn } of S1 and {q1 , . . . , qn } of S2 such that
             h pi , p j i = hqi , q j i = 0 and h pi , q j i = δi j for all i and j. (The resulting
             basis { p1 , . . . , pn , q1 , . . . , qn } of V is called a Weyl basis of V .)
7.    Let S be a nonempty set, and let K be a field. For s in S, let Us and Vs be vector
      spaces over K, and let °U and V be two¢ further  vector spaces over K.
                                L                Q
      (a) Prove that HomK ° s∈S Us , V¢ ∼     =Q s∈S  Hom   K (Us , V ).
                                   Q         ∼
      (b) Prove that HomK U, s∈S Vs = s∈S HomK (U, Vs ).
      (c) Give examples to show that neither isomorphism in (a) and (b) need remain
          valid if all three direct products are changed to direct sums.
8.    This problem continues Problem 1 at the end of Chapter V, which established
      a canonical-form theorem for an action of G L(m, K) × G L(n, K) on m-by-
      n matrices. For the present problem, the group G L(n, K) acts on Mn (K) by
      (g, x) 7→ gxg t .
      (a) Verify that this is indeed a group action and that the vector subspaces Ann (K)
           of alternating matrices and Snn (K) of symmetric matrices are mapped into
           themselves under the group action.
      (b) Prove that two members of Ann (K) lie in the same orbit if and only if they
           have the same rank, and that the rank must be even. For each even rank ≤ n,
           find an example of a member of Ann (K) with that rank.
      (c) Prove that two members of Snn (C) lie in the same orbit if and only if they
           have the same rank, and for each rank ≤ n, find an example of a member of
           Snn (C) with that rank.
9.    Let U and V be vector spaces over K, and let U 0 be the dual of U . The bilinear
      map (u 0 , v) 7→ u 0 ( · )v of U 0 × V into HomK (U, V ) extends to a linear map
      TU V : U 0 ⊗K V → HomK (U, V ). Do the following:
                                    10. Problems                                  297

    (a) Prove that TU V is one-one.
    (b) Prove that TU V is onto HomK (U, V ) if U is finite-dimensional.
    (c) Give an example for which TU V is not onto HomK (U, V ).
    (d) Let C be the category of all vector spaces over K, and let 8 and 9 be the
        functors from C × C into C whose effects on objects are 8(U, V ) = U 0 ⊗K V
        and 9(U, V ) = HomK (U, V ). Prove that the system {TU V } is a natural
        transformation of 8 into 9.
    (e) In view of (c), can the system {TU V } be a natural isomorphism?
10. Let K ⊆ L be an inclusion of fields, and let VK and VL be the categories of
    vector spaces over K and L. Section 6 of the text defined extension of scalars as
    a covariant functor 8(E) = E ⊗K L. Another definition of extension of scalars
    is 9(E) = HomK (L, E) with (lϕ)(l 0 ) = ϕ(ll 0 ). Verify that 9(E) is a vector
    space over L and that 9 is a functor.
11. A linear map L : E → F between finite-dimensional complex vector spaces
    becomes a linear map L R : E R → FR when we restrict attention to real scalars.
    Explain how to express a matrix for L R in terms of a matrix for L.
12. (Kronecker product of matrices) Let L : E 1 → E 2 and M : F1 → F2 be
    linear maps between finite-dimensional vector spaces over K, let 01 and 02 be
    ordered bases of E 1 and E 2 , and let≥ 11 and
                                                 ¥ 12 be ordered
                                                           ≥      bases
                                                                  ¥     of F1 and F2 .
                                             L                M
    Define matrices A and B by A = 0 0 and B = 1 1 . Use 01 , 02 , 11 ,
                                             2 1              2 1
    and 12 to define ordered bases  ≥ ƒ1 ¥  and ƒ2 of E 1 ⊗K F1 and E 2 ⊗K F2 , and
                                     L⊗M
    describe how the matrix C = ƒ ƒ is related to A and B.
                                      2   1

13. Let K be a field, and let E be the vector space KX ⊕KY . Prove that the subalgebra
     of T (E) generated by 1, Y , and X 2 + X Y + Y 2 is isomorphic as an algebra with
     identity to T (F) for some vector space F.
                                                                                  V
Problems 14–17 concern the functors E 7→ T (E), E 7→ S(E), and E 7→ E
defined for vector spaces over a field K.
14. If ϕ : E → F is a linear map between vector spaces over K, Section 8 of the text
    indicated how to define a corresponding homomorphism 8 : S(E) → S(F) of
    associative algebras with identity over K, using Proposition 6.23b.
    (a) Fill in the details of this application of Proposition 6.23b.
    (b) Establish the appropriate conditions on mappings that complete the proof
         that E 7→ S(E) is a functor.
    (c) Verify that 8 carries S n (E) linearly into S n (F) for all integers n ∏ 0.
15. Suppose that a linear map ϕ : E → E is given. Let 8 : S(E) → S(E) and
    e : T (E) → T (E) be the associated algebra homomorphisms of S(E) into itself
    8
    and of T (E) into itself, and let q : T (E) → S(E) be the quotient homomorphism
    appearing in the definition of S(E). These mappings are related by the equation
               e
    8q(x) = q 8(x)     for x in T (E). Proposition 6.29 shows for each n ∏ 0 that
298                                 VI. Multilinear Algebra

      T n (E) = eS n (E) ⊕ (T n (E) ∩ I ), where eS n (E) is the image of T n (E) under the
      symmetrizer mapping. The remark with the proposition observes that q carries
      S nØ(E) one-one onto
      e                        n                     e          en
                          Ø S (E). Prove that 8 carries S (E) into itself and             that
      eØ n
      8         matches  8Ø n     in the sense that q  e
                                                       8(x)  =  8q(x) for all  x in e
                                                                                    S n (E).
          e
          S (E)            S (E)

16. With VE finite-dimensional
                    V            let ϕ : E → E be a linear mapping, and define V
    8 :     E →       E to be the corresponding
                                            Vn      algebra homomorphism of E
    sending 1 into 1. This carries each         E into itself. Prove that 8 acts as
                                                                  V
    multiplication by the scalar det ϕ on the 1-dimensional space dim E (E).
17. Suppose that G is a group, that the vector space E over K is finite-dimensional,
    and that ϕ : G → GL(E)V    is a representation of G on E. The functors E 7→ T (E),
    E 7→ S(E), and E 7→ E yield, forVeach ϕ(g), algebra homomorphisms of
    T (E) into itself, S(E) into itself, and E into itself.
    (a) Show that as g varies, the result in each case is a representation of G.
    (b) Suppose that E = Kn . Give a formula for the representation of G on a
         member of P(Kn ) = S((Kn )0 ).

Problems 18–22 concern universal mapping properties. Let A and V be two cat-
egories, and let F : A → V be a covariant functor. (In practice, F tends to be a
relatively simple functor, such as one that simply ignores some of the structure of
A.) Let E be in Obj(V ). A pair (S, ∂) with S in Obj(A) and ∂ in MorphV (E, F(S))
is said to have the universal mapping property relative to E and F if the following
condition is satisfied: whenever A is in Obj(A) and a member l of MorphV (E, F(A))
is given, there exists a unique member L of MorphA (S, A) such that F(L) ∂ = l.

18. (a) By suitably specializing A, V, F, etc., show that the universal mapping
        property of the symmetric algebra of a vector space over K is an instance of
        what has been described.
    (b) How should the answer to (a) be adjusted so as to account for the universal
        mapping property of the exterior algebra of a vector space over K?
    (c) How should the answer to (a) be adjusted so as to account for the universal
        mapping property of the coproduct of {X j } j∈J in a category C, the universal
        mapping property being as in Figure 4.12? (Educational note: For the
        product of {X j } j∈J in C, the above description does not apply directly because
        the morphisms go the wrong way. Instead, one applies the above description
        to the opposite categories Aopp and V opp , defined as in Problems 78–80 at
        the end of Chapter IV.)
19. If (S, ∂) and (S 0 , ∂0 ) are two pairs that each have the universal mapping property
    relative to E and F, prove that S and S 0 are canonically isomorphic as objects
    in A. More specifically prove that there exists a unique L in MorphA (S, S 0 ) such
    that F(L)∂ = ∂0 and that L is an isomorphism whose inverse L 0 in MorphA (S 0 , S)
    has F(L 0 )∂0 = ∂.
                                       10. Problems                                   299

20. Suppose that the pair (S, ∂) has the universal mapping property relative to E
    and F. Let S be the category of sets, and define functors F : A → S and
    G : A → S by F(A) = MorphA (S, A), F(ϕ) equals composition on the left
    by ϕ for ϕ ∈ MorphA (A, A0 ), G(A) = MorphV (E, F(A)), and G(ϕ) equals
    composition on the left by F(ϕ). Let T A : MorphA (S, A) → MorphV (E, F(A))
    be the one-one onto map given by the universal mapping property. Show that the
    system {T A } is a natural isomorphism of F into G.
21. Suppose that (S 0 , ∂) is a second pair having the universal mapping property relative
    to E and F. Define F 0 : A → S by F 0 (A) = MorphA (S 0 , A). Combining the
    previous problem and Proposition 6.16, obtain a second proof (besides the one
    in Problem 19) that S and S 0 are canonically isomorphic.
22. Suppose that for each E in Obj(V ), there is some pair (S, ∂) with the universal
    mapping property relative to E and F. Fix such a pair (S, ∂) for each E, calling
    it (S(E), ∂ E ). Making an appropriate construction for morphisms and carrying
    out the appropriate verifications, prove that E 7→ S(E) is a functor.

Problems 23–28 introduce the Pfaffian of a (2n)-by-(2n) alternating matrix X = [xi j ]
with entries in a field K. This is the polynomial in the entries of X with integer
coefficients given by
                                    X                    n
                                                         Y
                     Pfaff(X) =               (sgn τ )         xτ (2k−1),τ (2k) ,
                                  some τ ’s              k=1
                                   in S2n

where the sum is taken over those permutations τ such that τ (2k − 1) < τ (2k) for
1 ≤ k ≤ n and such that τ (1) < τ (3) < · · · < τ (2n − 1). It will be seen that det X
is the square of this polynomial. Examples of Pfaffians are
                                            0 a b c
            ≥      ¥
               0 x                            −a 0 d e
       Pfaff −x 0 = x        and     Pfaff  −b −d 0 f  = a f − be + cd.
                                                    −c −e − f          0

The problems in this set will be continued at the end of Chapter VIII.
23. For the matrix J in Section 5, show that Pfaff(J ) = 1.
                               P                 Q2n
24. In the expansion det X = σ ∈S2n (sgn σ ) l=1       xl,σ (l) , prove that the value of
    the right side with X as above is not changed if the sum is extended only over
    those σ ’s whose expansion in terms of disjoint cycles involves only cycles of
    even length (and in particular no cycles of length 1).
25. Define σ ∈ S2n to be “good” if its expansion in terms of disjoint cycles involves
    only cycles of even length. If σ is good, show that there uniquely exist two
    disjoint subsets A and B of n elements each in {1, . . . , 2n} such that A contains
    the smallest-numbered index in each cycle and such that σ maps each set onto
    the other.
300                                   VI. Multilinear Algebra

26. In the notation of the previous problem with σ good, let y(σ ) be the product
    of the monomials xab such that a is in A and b = σ (a). For each factor xi j of
    y(σ ) with i > j, replace the factor by −x ji . In the resulting product, arrange
    the factors in order so that their first subscripts are increasing, and denote this
    expression by sxi1 i2 xi3 i4 · · · xi2n−1 i2n , where s is a sign. Let τ be the permutation
    that carries each r to ir , and define s(τ ) to be the sign s. Similarly let z(σ )
    be the product of the monomials xba such that b is in B and a = σ (b). For
    each factor xi j of z(σ ) with i > j, replace the factor by −x ji . In the resulting
    product, arrange the factors in order so that their first subscripts are increasing,
    and denote this expression by s 0 x j1 j2 x j3 j4 · · · x j2n−1 j2n , where s 0 is a sign. Let τ 0
    be the permutation that carries each r to jr , and define s 0 (τ 0 ) to be the sign s 0 .
    Prove, apart from signs, that the σ th term in the expansion of det X matches the
    product of the τ th term of Pfaff(X) and the τ 0th term of Pfaff(X).
27. In the previous problem, take the signs s(τ ) and s 0 (τ 0 ) into account and show
    that the signs of σ , τ , and τ 0 work out so that the σ th term in the expansion of
    det X is the product of the τ th and τ 0th terms of Pfaff(X).
28. Show that every term of the product of Pfaff(X) with itself is accounted for once
    and only once by the construction in the previous three problems, and conclude
    that the alternating matrix X has det X = (Pfaff(X))2 .

Problems 29–30 concern filtrations and gradings. A vector space V over K is said
to be filtered when an increasing sequence of subspaces V0 ⊆ V1 ⊆ V2 ⊆ · · · is
specified with union V . In this case we put V−1 = 0 by convention. The space V is
graded if a sequence of subspaces V 0 , V 1 , V 2 , . . . is specified such that
                                                 ∞
                                                 M
                                          V =          V n.
                                                 n=0
                                                                  L
When V is graded, there is a natural filtration of V given by Vn = nk=0 V k . Examples
of graded vectorV  spaces are any tensor algebra V = T (E), symmetric algebra S(E),
exterior algebra (E), and polynomial algebra P(E), the n th subspace of the grading
consisting of those elements that are homogeneous of degree n. Any polynomial
algebra K[X 1 , . . . , X n ] is another example of a graded vector space, the grading
being by total degree.
29. When V is a filtered vector space as in (A.34), the associated graded vector
                       L
    space is gr V = ∞                                 #
                          n=0 Vn /Vn−1 . Let V and V be two filtered vector spaces,
    and let ϕ be a linear map between them such that ϕ(Vn ) ⊆ Vn# for all n. Since
                                                   # , this restriction induces a linear
    the restriction of ϕ to Vn carries Vn−1 into Vn−1
            n                          #   #
    map gr ϕ : (Vn /Vn−1 ) → (Vn /Vn−1 ). The direct sum of these linear maps
    is then a linear map gr ϕ : gr V → gr V # called the associated graded map
    for ϕ. Prove that if gr ϕ is a vector-space isomorphism, then ϕ is a vector-space
    isomorphism.
                                       10. Problems                                     301

30. Let A be an associative algebra over K with identity. If A has a filtration
    A0 , A1 , . . . of vector subspaces with 1 ∈ A0 such that Am An ⊆ Am+n for
    all m and n, then one says L thatn A is a filtered associative   algebra; similarly
    if A is graded as A = ∞      n=0 A  in such  a way  that A m An ⊆ Am+n for all m

    and n, then one says that A is a graded associative algebra. If A is a filtered
    associative algebra with identity, prove that the graded vector space gr A acquires
    a multiplication in a natural way, making it into a graded associative algebra with
    identity.
Problems 31–35 concern Lie algebras and their universal enveloping algebras. If K
is a field, a Lie algebra g over K is a nonassociative algebra whose product, called
the Lie bracket and written [x, y], is alternating as a function of the pair (x, y) and
satisfies the Jacobi identity [x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0 for all x, y, z in
g. The universal enveloping algebra U (g) of g is the quotient T (g)/I 00 , where I 00
is the two-sided ideal generated by all elements x ⊗ y − y ⊗ x − [x, y] with x and
y in T 1 (g). The grading for T (g) makes U (g) into a filtered associate algebra with
identity. The product of x and y in U (g) is written x y.
31. If A is an associative algebra over K, prove that A becomes a Lie algebra if the
     Lie bracket is defined by [x, y] = x y − yx. In particular, observe that Mn (K)
     becomes a Lie algebra in this way.
32. Fix a matrix A ∈ Mn (K), and let g be the vector subspace of all members x of
    Mn (K) with x t A + Ax = 0.
    (a) Prove that g is closed under the bracket operation of the previous problem
        and is therefore a Lie subalgebra of Mn (K).
    (b) Deduce as a special case of (a) that the vector space of all skew-symmetric
        matrices in Mn (K) is a Lie subalgebra of Mn (K).
33. Let g be a Lie algebra over K, and let ∂ be the linear map obtained as the
    composition of g → T 1 (g) and the passage to the quotient U (g). Prove that
    (U (g), ∂) has the following universal mapping property: whenever l is any linear
    map of g into an associative algebra A with identity satisfying the condition of
    being a Lie algebra homomorphism, namely l[x, y] = l(x)l(y) − l(y)l(x) for
    all x and y in g, then there exists a unique associative algebra homomorphism
    L : U (g) → A with L(1) = 1 such that L ◦ ∂ = l.
34. Let g be a Lie algebra over K, let {u i }i∈A be a vector-space basis of g, and suppose
    that a simple ordering has been imposed on the index set A. Prove that the set of
                      j          j                            P
    all monomials u i11 · · · u ikk with i 1 < · · · < i k and m jm arbitrary is a spanning
    set for U (g).
35. For a Lie algebra g over K, the Poincaré–Birkhoff–Witt Theorem says that the
    spanning set for U (g) in the previous problem is actually a basis. Assuming this
    theorem, prove that gr U (g) is isomorphic as a graded algebra to S(g).
Problems 36–40 introduce Clifford algebras. Let K be a field of characteristic 6= 2,
302                                  VI. Multilinear Algebra

let E be a finite-dimensional vector space over K, and let h · , · i be a symmetric
bilinear form on E. The Clifford algebra Cliff(E, h · , · i) is the quotient T (E)/I 00 ,
where I 00 is the two-sided ideal generated by all elements5 v ⊗ v + hv, vi with v in
E. The grading for T (E) makes Cliff(E, h · , · i) into a filtered associative algebra
with identity. Products in Cliff(E, h · , · i) are written as ab with no special symbol.
36. Let ∂ be the composition of the inclusion E ⊆ T 1 (E) and the passage to the
     quotient modulo I 00 . Prove that (Cliff(E, h · , · i), ∂) has the following universal
     mapping property: whenever l is any linear map of E into an associative algebra
     A with identity such that l(v)2 = −hv, vi1 for all v ∈ E, then there exists a
     unique algebra homomorphism L : Cliff(E, h · , · i) → A with L(1) = 1 and
     such that L ◦ ∂ = l.
37. Let {u 1 , . . . , u n } be a basis of E. Prove that the 2n elements of Cliff(E, h · , · i)
    given by u i1 u i2 · · · u ik with i 1 < · · · < i k form a spanning set of Cliff(E, h · , · i).
38. Using the Principal Axis Theorem, fix a basis {e1 , . . . , en } of E such that
    hei , e j i = di δi j for all j. Introduce an algebra C over K of dimension 2n with
    generators e1 , . . . , en and with a basis parametrized by subsets of {1, . . . , n} and
    given by all elements
                           ei1 ei2 · · · eik with    i1 < i2 < · · · < ik ,
    with the multiplication that is implicit in the rules
                       ei2 = −di        and      ei e j = −e j ei if i 6= j,
      namely, to multiply two monomials ei1 ei2 · · · eik and e j1 e j2 · · · e jl , put them end
      to end, replace any occurrence of two ek ’s by the scalar −dk , and then permute
      the remaining ek ’s until their indices are in increasing order, introducing a minus
      sign each time two distinct ek ’s are interchanged. Prove that the algebra C is
      associative.
39. Prove that the associative algebra C of the previous problem is isomorphic as an
    algebra to Cliff(E, h · , · i).
                                                                          V
40. Prove that gr Cliff(E, h · , · i) is isomorphic as a graded algebra to (E).
Problems 41–48 introduce finite-dimensional Heisenberg Lie algebras and the corre-
sponding Weyl algebras. They make use of Problems 31–35 concerning Lie algebras
and universal enveloping algebras. Let V be a finite-dimensional vector space over
the field K, and let h · , · i be a nondegenerate alternating bilinear form on V × V .
Write 2n for the dimension of V . Introduce an indeterminate X 0 . The Heisenberg
Lie algebra H (V ) on V is a Lie algebra whose underlying vector space is KX 0 ⊕ V
and whose Lie bracket is given by [(cX 0 , u), (d X 0 , v)] = hu, viX 0 . Let U (H (V )) be
its universal enveloping algebra. The Weyl algebra W (V ) on V is the quotient of the
tensor algebra T (V ) by the two-sided ideal generated by all u ⊗ v − v ⊗ u − hu, vi1
with u and v in V ; as such, it is a filtered associative algebra.
5 Some authors factor out the elements v ⊗ v − hv, vi instead. There is no generally accepted
convention.
                                            10. Problems                                      303

41. Verify when the field is K = R that an example of a 2n-dimensional V with its
    nondegenerate alternating bilinear form h · , · i is V = Cn with hu, vi = Im(u, v),
    where ( · , · ) is the usual inner product on Cn . For this V , exhibit a Lie-algebra
    isomorphism of H (Vµ) with the  ∂ Lie algebra of all complex (n + 1)-by-(n + 1)
                                0 z̄ t ir
     matrices of the form       0 0 z       with z ∈ Cn and r ∈ R.
                                0 0 0

42. In the general situation show that the linear map ∂(cX 0 , v) = c1+v is a Lie algebra
    homomorphism of H (V ) into W (V ) and that its extension to an associative
    algebra homomorphisme      ∂ : U (H (V )) → W (V ) is onto and has kernel equal to
    the two-sided ideal in U (H (V )) generated by X 0 − 1.
43. Prove that W (V ) has the following universal mapping property: whenever
    ϕ : H (V ) → A is a Lie algebra homomorphism of H (V ) into an associative
    algebra A with identity such that ϕ(X 0 ) = 1, then there exists a unique associative
    algebra homomorphism e    ϕ of W (V ) into A such that ϕ = e  ϕ ◦ ∂.
                                                                                              k2n
44. Let v1 , . . . , v2n be any vector space basis of V . Prove that the elements v1k1 · · · v2n
    with integer exponents ∏ 0 span W (V ).
                                                                                                2
45. For K = R, let S be the vector space of all real-valued functions P(x)e−π|x| ,
    where P(x) is a polynomial in n real variables. Show that S is mapped into itself
    by the linear operators @/@ xi and m j = (multiplication by x j ).
46. With K = R, let { p1 , . . . , pn , q1 , . . . , qn } be a Weyl basis of V in the terminology
    of Problem 6. In the notation of Problem 45, let ϕ : V → HomR (S, S ) be the
    linear map given by ϕ( pi ) = @/@ xi and ϕ(q j ) = m j . Use Problem 43 to extend
    ϕ to an algebra homomorphism e            ϕ : W (V ) → HomR (S, S ) with e          ϕ (1) = 1,
    and use Problem 42 to obtain a representation of H (V ) on S. Prove that this
    representation of H (V ) is irreducible in the sense that there is no proper nonzero
    vector subspace carried to itself by all members of e          ϕ (H (V )).
47. In Problem 46 with K = R, prove that the associative algebra homomorphism
    ϕ : W (V ) → HomR (S, S ) is one-one. Conclude for K = R that the elements
    e
                k2n
    v1k1 · · · v2n  of Problem 44 form a vector-space basis of W (V ).
48. For K = R, prove that gr W (V ) is isomorphic as a graded algebra to S(V ).

Problems 49–51 deal with Jordan algebras. Let K be a field of characteristic 6= 2. An
algebra J over K with multiplication a · b is called a Jordan algebra if the identities
a · b = b · a and a 2 · (b · a) = (a 2 · b) · a are always satisfied; here a 2 is an abbreviation
for a · a.
49. Let A be an associative algebra, and define a · b = 12 (ab + ba). Prove that A
    becomes a Jordan algebra under this new multiplication.
304                               VI. Multilinear Algebra

50. In the situation of the previous problem, suppose that a 7→ a t is a one-one linear
    mapping of A onto itself such that (ab)t = bt a t for all a and b. (For example,
    a 7→ a t could be the transpose mapping if A = Mn (K).) Prove that the vector
    subspace of all a with a t = a is carried to itself by the Jordan product a · b and
    hence is a Jordan algebra.
51. Let V be a finite-dimensional vector space over K, and let h · , · i be a symmetric
    bilinear form on V . Define A = K1 ⊕       ° V as a vector space, and  ¢ define a
    multiplication in A by (c1, x) · (d1, y) = (cd + hx, yi)1, cy + dx . Prove that
    A is a Jordan algebra under this definition of multiplication.

Problems 52–56 deal with the algebra O of real octonions, sometimes known as
the Cayley numbers. This is a certain 8-dimensional nonassociative algebra with
identity over R with an inner product such that kabk = kakkbk for all a and b and
such that the left and right multiplications by any element a 6= 0 are always invertible.
52. Let A be an algebra over R. Let [a, b] = ab − ba and [a, b, c] = (ab)c − a(bc).
    (a) The 3-multilinear function (a, b, c) 7→ [a, b, c] from A× A× A to A is called
        the associator in A. Observe that it is 0 if and only if A is associative. Show
        that it is alternating if and only if A always satisfies the limited associativity
        laws

                   (aa)b = a(ab),        (ab)a = a(ba),         (ba)a = b(aa).

          In this case, A is said to be alternative.
      (b) Show that A is alternative if the first and third of the limited associativity
          laws in (a) are always satisfied.
53. (Cayley–Dickson construction) Suppose that A is an algebra over R with a
    two-sided identity 1, and suppose that there is an R linear function ∗ from A to
    itself (called “conjugation”) such that 1∗ = 1, a ∗∗ = a, and (ab)∗ = b∗ a ∗ for all
    a and b in A. Define an algebra B over R to have the underlying real vector-space
    structure of A ⊕ A and to have multiplication and conjugation given by

          (a, b)(c, d) = (ac − db∗ , a ∗ d + cb)       and      (a, b)∗ = (a ∗ , −b).

      (a) Prove that (1, 0) is a two-sided identity in B and that the operation ∗ in B
          satisfies the required properties of a conjugation.
      (b) Prove that if a ∗ = a for all a ∈ A, then A is commutative.
      (c) Prove that if a ∗ = a for all a ∈ A, then B is commutative.
      (d) Prove that if A is commutative and associative, then B is associative.
      (e) Verify the following outcomes of the above construction A → B:
               (i) A = R yields B = C,
              (ii) A = C yields B = H, the algebra of quaternions.
                                     10. Problems                                   305

54. Suppose that A is an algebra over R with an identity and a conjugation as in the
    previous problem. Say that A is nicely normed if
         (i) a + a ∗ is always of the form r1 with r real and
        (ii) aa ∗ always equals a ∗ a and for a 6= 0, is of the form r1 with r real and
             positive.
    (a) Prove that if A is nicely normed, then so is the algebra B of the previous
        problem.
    (b) Prove that if A is nicely normed, then (a, b) = 12 (ab∗ + ba ∗ ) is an inner
        product on A with norm kak = (aa ∗ )1/2 = (a ∗ a)1/2 .
    (c) Prove that if A is associative and nicely normed, then the algebra B of the
        previous problem is alternative.
55. Starting from the real algebra A = H, apply the construction of Problem 53,
    and let the resulting 8-dimensional real algebra be denoted by O, the algebra of
    octonions.
    (a) Prove that O is an alternative algebra and is nicely normed.
    (b) Prove that (x x ∗ )y = x(x ∗ y) and x(yy ∗ ) = (x y)y ∗ within O.
    (c) Prove that kabk2 a = kak2 kbk2 a within O.
    (d) Conclude from (c) that the operations of left and right multiplication by any
         a 6= 0 within O are invertible.
    (e) Show that the inverse operators are left and right multiplication by kak−2 a ∗ .
    (f) Denote the usual basis vectors of H by 1, i, j, k. Write down a multiplication
         table for the eight basis vectors of O given by (x, 0) and (0, y) as x and y
         run through the basis vectors of H.
56. What prevents the construction of Problem 53, when applied with A = O, from
    yielding a 16-dimensional algebra B in which kabk2 = kak2 kbk2 and therefore
    in which the operations of left and right multiplication by any a 6= 0 within B
    are invertible?
                                      CHAPTER VII

                             Advanced Group Theory



Abstract. This chapter continues the development of group theory begun in Chapter IV, the main
topics being the use of generators and relations, representation theory for finite groups, and group
extensions. Representation theory uses linear algebra and inner-product spaces in an essential way,
and a structure-theory theorem for finite groups is obtained as a consequence. Group extensions
introduce the subject of cohomology of groups.
    Sections 1–3 concern generators and relations. The context for generators and relations is that of
a free group on the set of generators, and the relations indicate passage to a quotient of this free group
by a normal subgroup. Section 1 constructs free groups in terms of words built from an alphabet
and shows that free groups are characterized by a certain universal mapping property. This universal
mapping property implies that any group may be defined by generators and relations. Computations
with free groups are aided by the fact that two reduced words yield the same element of a free group
if and only if the reduced words are identical. Section 2 obtains the Nielsen–Schreier Theorem that
subgroups of free groups are free. Section 3 enlarges the construction of free groups to the notion
of the free product of an arbitrary set of groups. Free product is what coproduct is for the category
of groups; free groups themselves may be regarded as free products of copies of the integers.
    Sections 4–5 introduce representation theory for finite groups and give an example of an important
application whose statement lies outside representation theory. Section 4 contains various results
giving an analysis of the space C(G, C) of all complex-valued functions on a finite group G. In this
analysis those functions that are constant on conjugacy classes are shown to be linear combinations
of the characters of the irreducible representations. Section 5 proves Burnside’s Theorem as an
application of this theory—that any finite group of order pa q b with p and q prime and with a +b > 1
has a nontrivial normal subgroup.
    Section 6 introduces cohomology of groups in connection with group extensions. If N is to be
a normal subgroup of G and Q is to be isomorphic to G/N , the first question is to parametrize the
possibilities for G up to isomorphism. A second question is to parametrize the possibilities for G if
G is to be a semidirect product of N and Q.



                                         1. Free Groups

This section and the next two introduce some group-theoretic notions that in
principle apply to all groups but in practice are used with countable groups, often
countably infinite groups that are nonabelian. The material is especially useful in
applications in topology, particularly in connection with fundamental groups and
covering spaces. But the formal development here will be completely algebraic,
not making use of any definitions or theorems from topology.
                                                  306
                                          1. Free Groups                                         307

    In the case of abelian groups, every abelian group G is a quotient of a suitable
free abelian group, i.e., a suitable direct sum of copies of the additive group Z
of integers.1 Recall the discussion of Section IV.9: We introduce a copy Zg of
                                       L
Z for each g in G, define G     e =
                                          g∈G Zg , let i g : Zg → G be the standard
                                                                   e
embedding, and let ϕg : Zg → G be the group homomorphism written additively
as ϕg (n) = ng. The universal mapping property of direct sums that was stated
as Proposition 4.17 produces a unique group homomorphism ϕ : G          e → G such
that ϕ ◦ i g = ϕg for all g, and ϕ is the required homomorphism of a free abelian
group onto G.
    The goal in this section is to carry out an analogous construction for groups that
are not necessarily abelian. The constructed groups, to be called “free groups,”
are to be rather concrete, and the family of all of them is to have the property that
every group is the quotient of some member of the family.
    If S is any set, we construct a “free group F(S) on the set S.” Let us speak
of S as a set of “symbols” or as the members of an “alphabet,” possibly infinite,
with which we are working. If S is empty, the group F(S) is taken to be the
one-element trivial group, and we shall therefore now assume that S is not empty.
If a is a symbol in S, we introduce a new symbol a −1 corresponding to it, and we
let S −1 denote the set of all such symbols a −1 for a ∈ S. Define S 0 = S ∪ S −1 .
A word is a finite string of symbols from S 0 , i.e., an ordered n-tuple for some
n of members of S 0 with repetitions allowed. Words that are n-tuples are said
to have length n. The empty word, with length 0, will be denoted by 1. Other
words are usually written with the symbols juxtaposed and all commas omitted,
as in abca −1 cb−1 . The set of words will be denoted by W (S 0 ). We introduce a
multiplication W (S 0 )× W (S 0 ) → W (S 0 ) by writing end-to-end the words that are
to be multiplied: (abca −1 , cb−1 ) 7→ abca −1 cb−1 . The length of a product is the
sum of the lengths of the factors. It is plain that this multiplication is associative
and that 1 is a two-sided identity. It is not a group operation, however, since most
elements of W (S 0 ) do not have inverses: multiplication never decreases length,
and thus the only way that 1 can be a product of two elements is as the product
11. To obtain a group from W (S 0 ), we shall introduce an equivalence relation in
W (S 0 ).
    Two words are said to be equivalent if one of the words can be obtained
from the other by a finite succession of insertions and deletions of expressions
aa −1 or a −1 a within the word; here a is assumed to be an element of S. It will be
convenient to refer to the pairs aa −1 and a −1 a together; therefore when b = a −1 is
in S −1 , let us define b−1 = (a −1 )−1 to be a. Then two words are equivalent if one
of the words can be obtained from the other by a finite succession of insertions
and deletions of expressions of the form bb−1 with b in S 0 . This definition is
    1 Direct sum here is what coproduct, in the sense of Section IV.11, amounts to in the category of

all abelian groups.
308                                VII. Advanced Group Theory

arranged so that “equivalent” is an equivalence relation. We write x ∼ y if x and
y are words that are equivalent. The underlying set for the free group F(S) will
be taken to be the set of equivalence classes of members of W (S 0 ).

   Theorem 7.1. If S is a set and W (S 0 ) is the corresponding set of words built
from S 0 = S ∪ S −1 , then the product operation defined on W (S 0 ) descends in a
well-defined fashion to the set F(S) of equivalence classes of members of W (S 0 ),
and F(S) thereby becomes a group. Define ∂ : S → F(S) to be the composition
of the inclusion into words of length one followed by passage to equivalence
classes. Then the pair (F(S), ∂) has the following universal mapping property:
whenever G is a group and ϕ : S → G is a function, then there exists a unique
group homomorphism e    ϕ : F(S) → G such that ϕ = e   ϕ ◦ ∂.
    REMARK. The group F(S) is called the free group on S. Figure 7.1 illustrates
its universal mapping property. The brief form in words of the property is that
any function from S into a group G extends uniquely to a group homomorphism
of F(S) into G. This universal mapping property actually characterizes F(S), as
will be seen in Proposition 7.2.
                                                    ϕ
                                          S     −−−→ G
                                          
                                          
                                         ∂y              ϕe

                                        F(S)
              FIGURE 7.1. Universal mapping property of a free group.
    PROOF. Let us denote equivalence classes by brackets. We want to define
multiplication in F(S) by [w1 ][w2 ] = [w1 w2 ]. To see that this formula makes
sense in F(S), let x1 , x2 , and y be words, and let b be in S 0 . Define x = x1 x2 and
x 0 = x1 bb−1 x2 , so that x 0 ∼ x. Then it is evident that x 0 y ∼ x y and yx 0 ∼ yx.
Iteration of this kind of relationship shows that w10 ∼ w1 and w20 ∼ w2 implies
w10 w20 ∼ w1 w2 , and hence multiplication of equivalence classes is well defined.
    Since multiplication in W (S 0 ) is associative, we have [w1 ]([w2 ][w3 ]) =
[w1 ][w2 w3 ] = [w1 (w2 w3 )] = [(w1 w2 )w3 ] = [w1 w2 ][w3 ] = ([w1 ][w2 ])[w3 ].
Thus multiplication is associative in F(S). The class [1] of the empty word 1 is a
two-sided identity. If b1 , . . . , bn are in S 0 , then bn−1 · · · b2−1 b1−1 b1 b2 · · · bn is equiv-
alent to 1, and so is b1 b2 · · · bn bn−1 · · · b2−1 b1−1 . Consequently [bn−1 · · · b2−1 b1−1 ] is
a two-sided inverse of [b1 b2 · · · bn ], and F(S) is a group.
    Now we address the universal mapping property, first proving the stated unique-
ness of the homomorphism. Every member of F(S) is the product of classes [b]
with b in S 0 . In turn, if b is of the form a −1 with a in S, then [b] = [a]−1 . Hence
F(S) is generated by all classes [a] with a in S, i.e., by ∂(S). Any homomorphism
                                         1. Free Groups                                309

of a group is determined by its values on the members of a generating set, and
uniqueness therefore follows from the formula e          ϕ (∂(a)) = ϕ(a).
                                               ϕ ([a]) = e
   For existence we begin by defining a function 8 : W (S 0 ) → G such that

                    8(a) = ϕ(a)                   for a in S,
                       −1           −1
                 8(a        ) = ϕ(a)              for a −1 in S −1 ,
               8(w1 w2 ) = 8(w1 )8(w2 ) for w1 and w2 in W (S 0 ).

We use the formulas 8(a) = ϕ(a) for a in S and 8(a −1 ) = ϕ(a)−1 for a −1 in S −1
as a definition of 8(b) for b in S 0 . Any member of W (S 0 ) can be written uniquely
as b1 · · · bn with each b j in S 0 , and we set 8(b1 · · · bn ) = 8(b1 ) · · · 8(bn ). (If
n = 0, the understanding is that 8(1) = 1.) Then 8 has the required properties.
   Let us show that w0 ∼ w implies 8(w0 ) = 8(w). If b1 , . . . , bn are in S 0 and
b is in S 0 , then the question is whether

                                                 ?
              8(b1 · · · bk bb−1 bk+1 · · · bn ) = 8(b1 · · · bk bk+1 · · · bn ).

If g and g 0 denote the elements 8(b1 ) · · · 8(bk ) and 8(bk+1 ) · · · 8(bn ) of G, then
the two sides of the queried formula are

                             g8(b)8(b−1 )g 0         and        gg 0 .

Thus the question is whether 8(b)8(b−1 ) always equals 1 in G. If b = a is in S,
this equals ϕ(a)ϕ(a)−1 = 1, while if b = a −1 is in S −1 , it equals ϕ(a)−1 ϕ(a) = 1.
We conclude that w0 ∼ w implies 8(w0 ) = 8(w).
                           ϕ ([w]) = 8(w) for [w] in F(S). Since e
   We may therefore define e                                            ϕ ([w][w0 ]) =
        0            0               0                    0
ϕ ([ww ]) = 8(ww ) = 8(w)8(w ) = e
e                                          ϕ ([w])eϕ ([w ]), e ϕ is a homomorphism
of F(S) into G. For a in S, we have e  ϕ ([a]) = 8(a) = ϕ(a). In other words,
ϕ (∂(a)) = ϕ(a). This completes the proof of existence.
e                                                                                    §

   Proposition 7.2. Let S be a set, F be a group, and ∂0 : S → F be a func-
tion. Suppose that the pair (F, ∂0 ) has the following universal mapping property:
whenever G is a group and ϕ : S → G is a function, then there exists a unique
group homomorphism e    ϕ : F → G such that ϕ = e       ϕ ◦ ∂0 . Then there exists a
unique group homomorphism 8 : F(S) → F such that ∂0 = 8 ◦ ∂, and it is a
group isomorphism.
   REMARKS. Chapter VI is not a prerequisite for the present chapter. However,
readers who have been through Chapter VI will recognize that Proposition 7.2 is
a special case of Problem 19 at the end of that chapter.
310                            VII. Advanced Group Theory

   PROOF. We apply the universal mapping property of (F(S), ∂), as stated in
Theorem 7.1, to the group G = F and the function ϕ = ∂0 , obtaining a group
homomorphism 8 : F(S) → F such that ∂0 = 8 ◦ ∂. Then we apply the given
universal mapping property of (F, ∂0 ) to the group G = F(S) and the function
ϕ = ∂, obtaining a group homomorphism 9 : F → F(S) such that ∂ = 9 ◦ ∂0 .
   The group homomorphism 9 ◦ 8 : F(S) → F(S) has the property that
(9 ◦8)◦∂ = 9 ◦(8◦∂) = 9 ◦∂0 = ∂, and the identity 1 F(S) has this same property.
By the uniqueness of the group homomorphism in Theorem 7.1, 9 ◦ 8 = 1 F(S) .
   Similarly the group homomorphism 8 ◦ 9 : F → F has the property that
(8 ◦ 9) ◦ ∂0 = ∂0 , and the identity 1 F has this same property. By the uniqueness
of the group homomorphism in the assumed universal mapping property of F,
8 ◦ 9 = 1F .
   Therefore 8 is a group isomorphism. We know that ∂(S) generates F(S). If
80 : F(S) → F is another group isomorphism with ∂0 = 80 ◦ ∂, then 80 and 8
agree on ∂(S) and therefore have to agree everywhere. Hence 8 is unique.        §

    Proposition 7.2 raises the question of recognizing candidates for the set T =
∂0 (S) in a given group F so as to be in a position to exhibit F as isomorphic to the
free group F(S). Certainly T has to generate F. But there is also an independence
condition. The idea is that if we form words from the members of T , then two
words are to lead to equal members of F only if they can be transformed into one
another by the same rules that are allowed with free groups.
    What this problem amounts to in the case that F = F(S) is that we want a
decision procedure for telling whether two given words are equivalent. This is
the so-called word problem for the free group. If we think about the matter for a
moment, not much is instantly obvious. If a1 and a2 are two members of S and if
they are considered as words of length 1, are they equivalent? Equivalence allows
for inserting pairs bb−1 with b in S 0 , as well as deleting them. Might it be possible
to do some complicated iterated insertion and deletion of pairs to transform a1
into a2 ? Although the negative answer can be readily justified in this situation by
a parity argument, it can be justified even more easily by the universal mapping
property: there exist groups G with more than one element; we can map a1 to
one element of G and a2 to another element of G, extend to a homomorphism
ϕ : F(S) → G, see that e
e                          ϕ (∂(a1 )) 6= e ϕ (∂(a2 )), and conclude that ∂(a1 ) 6= ∂(a2 ).
But what about the corresponding problem for two more-complicated words in a
free group? Fortunately there is a decision procedure for the word problem in a
free group. It involves the notion of “reduced” words. A word in W (S 0 ) is said
to be reduced if it contains no consecutive pair bb−1 with b in S 0 .

   Proposition 7.3 (solution of the word problem for free groups). Let S be a set,
let S 0 = S ∪ S −1 , and let W (S 0 ) be the corresponding set of words. Then each
word in W (S 0 ) is equivalent to one and only one reduced word.
                                          1. Free Groups                               311

   REMARK. To test whether two words are equivalent, the proposition says to
delete pairs bb−1 with b ∈ S 0 as much as possible from each given word, and to
check whether the resulting reduced words are identical.
   PROOF. Removal of a pair bb−1 with b ∈ S 0 decreases the length of a word
by 2, and the length has to remain ∏ 0. Thus the process of successively removing
such pairs has to stop after finitely many steps, and the result is a reduced word.
This proves that each equivalence class contains a reduced word.
   For uniqueness we shall associate to each word a finite sequence of reduced
words such that the last member of the sequence is unchanged when we insert
or delete within the given word any expression bb−1 with b ∈ S 0 . Specifically if
w = b1 · · · bn , with each bi in S 0 , is a given word, we associate to w the sequence
of words x0 , x1 , . . . , xn defined inductively by

            x0 = 1,
            x1 = b1 ,
                 Ω
                    xi−1 bi          if i ∏ 2 and xi−1 does not end in bi−1 ,
            xi =                                                                       (∗)
                    yi−2             if i ∏ 2 and xi−1 = yi−2 bi−1 ,

and we define r(w) = xn . Let us see, by induction on i ∏ 0, that xi is reduced.
The base cases i = 0 and i = 1 are clear from the definition. Suppose that i ∏ 2
and that x0 , . . . , xi−1 are reduced. If xi−1 = yi−2 bi−1 for some yi−2 , then xi−1
reduced forces yi−2 to be reduced, and hence xi = yi−2 is reduced. If xi−1 does
not end in bi−1 , then the last two symbols of xi = xi−1 bi do not cancel, and no
earlier pair can cancel since xi−1 is assumed reduced; hence xi is reduced. This
completes the induction and shows that xi is reduced for 0 ≤ i ≤ n.
   If the word w = b1 · · · bn is reduced, then each xi for i ∏ 2 is determined by
the first of the two choices in (∗), and hence xi = b1 · · · bi for all i. Consequently
r(w) = w if w is reduced. If we can prove for a general word b1 · · · bn that

                     r(b1 · · · bn ) = r(b1 · · · bk bb−1 bk+1 · · · bn ),            (∗∗)

then it follows that every word w0 equivalent to a word w has r(w0 ) = r(w). Since
r(w) = w for w reduced, there can be only one reduced word in an equivalence
class.
   To prove (∗∗), let x0 , . . . , xn be the finite sequence associated with b1 · · · bn ,
                        0
and let x00 , . . . , xn+2 be the sequence associated with b1 · · · bk bb−1 bk+1 · · · bn .
                      0                             0        0
Certainly xi = xi for i ≤ k. Let us compute xk+1        and xk+2 . From (∗) we see that
                               Ω
                     0             xk b     if xk does not end in b−1 ,
                    xk+1   =
                                   y        if xk = yb−1 .
312                            VII. Advanced Group Theory

                               0                                          0
In the first of these cases, xk+1 ends in b, and (∗) says therefore that xk+2  = xk .
In the second of the cases, the fact that xk is reduced implies that y does not end
                             0                                    0
in b; hence (∗) says that xk+2   = yb−1 = xk . In other words, xk+2     = xk in both
cases. Since the inductive definition of any xi depends only on xi−1 , and similarly
                       0
for xi0 , we see that xk+2+i = xk+i for 0 ≤ i ≤ n − k. Therefore xn+2 0
                                                                          = xn , and
(∗∗) follows. This proves the proposition.                                        §

   Let us return to the problem of recognizing candidates for the set T = ∂0 (S)
in a given group F so that the subgroup generated by T is a free group. Using
the universal mapping property for the free group F(T ), we form the group
homomorphism of F(T ) into F that extends the identity mapping on T . We want
this homomorphism to be one-one, i.e., to have the property that the only way a
word in F built from the members of T can equal the identity is if it comes from
the identity. Because of Proposition 7.3 the only reduced word in F(T ) that yields
the identity is the empty word. Thus the condition that the homomorphism be
one-one is that the only image in F of a reduced word in F(T ) that can equal the
identity is the image of the empty word. Making this condition into a definition,
we say that a subset S = {gt | t ∈ T } of F not containing 1 is free if no nonempty
product h 1 h 2 · · · h m in which each h i or h i−1 is in S and each h i+1 is different
from h i−1 can be the identity. A free set in F that generates F is called a free
basis for F.

    EXAMPLE. Within the free group F({x, y}) on two generators x and y, consider
the subgroup generated by u = x 2 , v = y 2 , and w = x y. The claim is that
the subset {u, v, w} is free, so that the subgroup generated by u, v, and w is
isomorphic to a free group F({u, v, w}) on three generators. We are to check that
no nonempty reduced word in u, v, w, u −1 , v −1 , w−1 can reduce to the empty
word after substitution in terms of x and y. We induct on the length of the u, v, w
word, the base case being length 0. Suppose that v = y 2 occurs somewhere
in our reduced u, v, w word that collapses to the empty word after substitution.
Consider what is needed for the left-hand factor of y in the y 2 to cancel. The
cancellation must result from the presence of some y −1 . Suppose that this y −1
occurs to the left of y 2 . Since passing to a reduced word need involve only
deletions and not insertions of pairs, everything between y −1 and y 2 must cancel.
If the y −1 has resulted from w−1 = y −1 x −1 , then the number of x, y symbols
between y −1 and y 2 is odd, and an odd number of factors can never cancel. So
the y −1 must arise from the right-hand y −1 in a factor v −1 = y −2 . The symbols
between y −2 and y 2 come from some reduced u, v, w word, and induction shows
that this word must be trivial. Then y −2 and y 2 are adjacent, contradiction. Thus
the left factor of y 2 must cancel because of some y −1 on the right of y 2 . If the y −1
is part of w−1 = y −1 x −1 or is the left y −1 in v −1 = y −2 , then the number of x, y
                                     1. Free Groups                                 313

symbols between the left y and the y −1 is odd, and we cannot get cancellation. So
the y −1 must be the right-hand y −1 in a factor y −2 . Then we have an expression
y(y · · · y −1 )y −1 in which the symbols in parentheses cancel. The symbols · · ·
must cancel also; since these represent some reduced u, v, w word, induction
shows that · · · is empty. We conclude that y 2 and y −2 are adjacent, contradiction.
Thus our reduced u, v, w word contains no factor v. Similarly examination of the
right-hand factor x in an occurrence of x 2 shows that our reduced u, v, w word
contains no factor u. It must therefore be a product of factors w or a product of
factors w−1 . Substitution of w = x y leads directly without any cancellation to
an x, y reduced word, and we conclude that the u, v, w word is empty. Thus the
subset {u, v, w} is free.

   If G is any group, the commutator subgroup G 0 of G is the subgroup generated
by all elements x yx −1 y −1 with x ∈ G and y ∈ G.

   Proposition 7.4. If G is a group, then the commutator subgroup is normal,
and G/G 0 is abelian. If ϕ : G → H is any homomorphism of G into an abelian
group H , then ker ϕ ⊇ G 0 .
   PROOF. The computation

            ax yx −1 y −1 a −1 = (axa −1 )(aya −1 )(axa −1 )−1 (aya −1 )−1

shows that G 0 is normal. If √ : G → G/G 0 is the quotient homomorphism, then
√(x)√(y) = x yG 0 = x y(y −1 x −1 yx)G 0 = yx G 0 = √(y)√(x), and therefore
G/G 0 is abelian. Finally if ϕ : G → H is a homomorphism of G into an abelian
group H , then the computation ϕ(x yx −1 y −1 ) = ϕ(x)ϕ(y)ϕ(x)−1 ϕ(y)−1 =
ϕ(x)ϕ(x)−1 ϕ(y)ϕ(y)−1 = 1 shows that G 0 ⊆ ker ϕ.                          §

    Corollary 7.5. If F is the free group on a set S and if F 0 is theLcommutator
subgroup of F, then F/F 0 is isomorphic to the free abelian group s∈S Zs .
                        L
    PROOF. Let H = s∈S Zs , and let ϕ : S → H be the function with ϕ(s) = 1s ,
i.e., ϕ(s) is to be the member of H that is 1 in the s th coordinate and is 0 elsewhere.
Application of the universal mapping property of F as given in Theorem 7.1
yields a group homomorphism e       ϕ : F → H such that e       ϕ ◦ ∂ = ϕ. Since the
elements ϕ(s), with s in S, generate H , e   ϕ carries F onto H . Since H is abelian,
Proposition 7.4 shows that ker e   ϕ ⊇ F 0 . Proposition 4.11 shows that e   ϕ descends
to a homomorphism e      ϕ0 : F/F 0 → H , and e ϕ0 has to be onto H .
    To complete the proof, we show that e    ϕ0 is one-one. Let x be a member of F.
Since the products of the elements ∂(s) and their inverses generate F and since
                                              j      j
F/F 0 is abelian, we can write x F 0 = si11 · · · sinn F 0 , where si1 occurs a total of
 j1 times in x, . . . , and sin occurs a total of jn times in x; it is understood that
314                             VII. Advanced Group Theory

an occurrence of si−1 1
                          is to contribute −1 toward j1 . Then we have e       ϕ0 (x F 0 ) =
                                        0
                                 ϕ0 (x F ) = 0, we obtain j1 ϕ(si1 )+· · ·+ jn ϕ(sin ) = 0,
j1 ϕ(si1 )+· · ·+ jn ϕ(sin ). If e
and then j1 = · · · = jn = 0 since the elements ϕ(si1 ), . . . , ϕ(sin ) are members
of a Z basis of H . Hence x F 0 = F 0 , x is in F 0 , and e
                                                          ϕ0 is one-one.                   §

   Corollary 7.6. If F1 and F2 are isomorphic free groups on sets S1 and S2 ,
respectively, then S1 and S2 have the same cardinality.

   PROOF. Corollary 7.5 shows that an L   isomorphism of LF1 with F2 induces an
isomorphism of the free abelian groups s∈S1 Zs1 and s∈S2 Zs2 . The rank of a
free abelian group is a well-defined cardinal, and the result follows—almost.
   We did not completely prove this fact about the rank of a free abelian group
in Section IV.9. Theorem 4.53 did prove, however, that rank is well defined for
finitely generated free abelian groups. Thus the corollary follows if S1 and S2 are
finite. If S1 or S2 is uncountable, then the cardinality of the corresponding free
abelian group matches the cardinality of its Z basis; hence the corollary follows
if S1 or S2 is uncountable. The only remaining case to eliminate is that one of
S1 and S2 , say the first of them, has a countably infinite Z basis and the other
has finite rank n. The first of the groups then has a linearly independent set of
n + 1 elements, and Lemma 4.54 shows that the span of these elements cannot
be isomorphic to a subgroup of a free abelian group of rank n. This completes
the proof in all cases.                                                          §

    Because of Corollary 7.6, it is meaningful to speak of the rank of a free group;
it is the cardinality of any free basis. We shall see in the next section that any
subgroup of a free group is free. In contrast to the abelian case, however, the rank
may actually increase in passing from a free group to one of its subgroups: the
example earlier in this section exhibited a free group of rank 3 as a subgroup of
a free group of rank 2.
    We turn to a way of describing general groups, particularly groups that are at
most countable. The method uses “generators,” which we already understand,
and “relations,” which are defined in terms of free groups. Let S be a set, let
R be a subset of F(S), and let N (R) be the smallest normal subgroup of F(S)
containing R. The group G = F(S)/N (R) is sometimes written as G = hS; Ri
or as
                       G = helements of S; elements of Ri,

with the elements of S and R listed rather than grouped as a set. Either of these
expressions is called a presentation of G. The set S is a set of generators, and
the set R is the corresponding set of relations. The following result implicit in
the universal mapping property of Theorem 7.1 shows the scope of this definition.
                                     1. Free Groups                                   315

   Proposition 7.7. Each group G is the homomorphic image of a free group.

   PROOF. Let S be a set of generators for G; for example, S can be taken to
be G itself. Let ϕ : S → G be the inclusion of the set of generators into G,
        ϕ : F(S) → G be the group homomorphism of Theorem 7.1 such that
and let e
                                             ϕ is a subgroup of G that contains
ϕ (∂(s)) = ϕ(s) for all s in S. The image of e
e
                                                              ϕ is the required
the generating set S and is therefore equal to all of G. Thus e
homomorphism.                                                                §

    If G is any group and e  ϕ : F(S) → G is the homomorphism given in Propo-
sition 7.7, then the subgroup R = ker e     ϕ has the property that G ∼      = hS; Ri.
Consequently every group can be given by generators and relations.
    For example the proof of the proposition shows that one possibility is to take
S = G and R equal to the set of all members of the multiplication table, but with
the multiplication table entry ss 0 = s 00 rewritten as the left side ss 0 (s 00 )−1 of an
equation ss 0 (s 00 )−1 = 1 specifying a combination of generators that maps to 1.
This is of course not a very practical example. Generators and relations are most
useful when S and R are fairly small. One says that G is finitely generated if S can
be chosen to be finite, finitely presented if both S and R can be chosen to be
finite.

   A frequently used device in working with generators and relations is the
following simple proposition.

   Proposition 7.8. Let G = hS; Ri be a group given by generators and relations,
let G 0 be a second group, let ϕ be a one-one function ϕ from S onto a set of
generators for G 0 , and let 8 : F(S) → G 0 be the extension of ϕ to a group
homomorphism. If 8(r) = 1 for every member r of R, then 8 descends to a
homomorphism of G onto G 0 . In particular, if G = hS; Ri and G 0 = hS; R 0 i
are groups given by generators and relations with R ⊆ R 0 , then the natural
homomorphism of F(S) onto G 0 descends to a homomorphism of G onto G 0 .

   PROOF. The proposition follows immediately from the universal mapping
property in Theorem 7.1 in combination with Proposition 4.11.         §

   Now let us consider some examples of groups given by generators and relations.
The case of one generator is something we already understand: the group has to
be cyclic. A presentation of Z is as ha; i, and a presentation of Cn is as ha; a n i.
But other presentations are possible with one generator, such as ha; a 6 , a 9 i for
C3 . Here is an example with two generators.
316                            VII. Advanced Group Theory
                                              ≠                     Æ
   EXAMPLE. Let us prove that Dn ∼        = x, y; x n , y 2 , (x y)2 , where Dn is the
dihedral group of order 2n. ≥Concretely let us work
                                                  ¥   ≥ D¥n as the group of 2-by-2
                                                     with
                              cos 2π/n − sin 2π/n       1 0
real matrices generated by sin 2π/n cos 2π/n and 0 −1 . The generated group
indeed has order 2n. If we identify
                   ≥                     ¥                         ≥      ¥
                     cos 2π/n − sin 2π/n                              1 0
           x with sin 2π/n cos 2π/n              and    y with 0 −1 ,

then y 2 = 1, and the formula
                  ≥                        ¥k       ≥                           ¥
                     cos 2π/n − sin 2π/n                cos 2πk/n − sin 2πk/n
                     sin 2π/n cos 2π/n
                                                =       sin 2πk/n cos 2πk/n
                                                    ≥           ¥
                                                        cos 2π/n sin 2π/n
shows that x n = 1. In addition, x y =                           , and the square of
                                                        sin 2π/n − cos 2π/n
this  is the identity. Æ By Proposition 7.8, Dn is a homomorphic image of D    en =
≠         n   2      2
  x, y; x , y , (x y) . To complete the identification, it is enough to show that the
order of D  en is ≤ 2n because the homomorphism of D       en onto Dn must then be
               ≠        n   2     2
                                    Æ                  −1
one-one. In x, y; x , y , (x y) , we compute that y = y and that x(yx)y = 1
implies yx = x −1 y −1 = x −1 y. Induction then yields yx k = x −k y for k > 0.
Multiplying left and right by y gives yx −k = x k y for k > 0. So yx l = x −l y for
every integer l. This means that every element is of the form x m or x m y, and we
may take 0 ≤ m ≤ n − 1. Hence there are at most 2n elements.

   Without trying to be too precise, let us mention that the word problem for
finitely presented groups is to give an algorithm for deciding whether two words
represent the same element of the group. It is known that there is no such
algorithm applicable to all finitely presented groups. Of course, there can be
such an algorithm for certain special classes of presentations. For example, if
there are no relations in the presentation, then the group is a free group, and
Proposition 7.3 gives a solution in this case. There tends to be a solution for a
class of groups if the groups all correspond rather concretely to some geometric
situation, such as a tiling of Euclidean space or some other space. The example
above with Dn is of this kind.
   By way of a concrete
                     ≠                    Æ one can identify any doubly generated
                          class of examples,
group of the form x, y; x a , y b , (x y)c if a, b, c are integers > 1, and one can
describe what words represent what elements in these groups. These groups all
correspond to tilings in 2 dimensions. In fact, let ∞ = a −1 + b−1 + c−1 . If ∞ > 1,
the tiling is of the Riemann sphere, and the group is finite. If ∞ = 1, the tiling is
of the Euclidean plane R2 , and the group is infinite. If ∞ < 1, the tiling is of the
hyperbolic plane, and the group is infinite. In all cases one starts from a triangle in
the appropriate geometry with angles π/a, π/b, and π/c, and a basic tile consists
of the double of this triangle obtained by reflecting the triangle about any of its
                             2. Subgroups of Free Groups                         317

sides. The group elements x, y, and x y are rotations, suitably oriented, about the
vertices of the triangle through respective angles 2π/a, 2π/b, and 2π/c. Further
information about the cases ∞ > 1 and ∞ = 1 is obtained in Problems 37–46 at
the end of the chapter.
   We conclude with one further example of a presentation whose group we can
readily identify concretely.

   Proposition 7.9. Let S be a set, and let R = {sts −1 t −1 | s ∈ S, t ∈ S}. Then
the smallest normal subgroup of the free group F(S) containing R is the com-
                       0
      Lsubgroup F(S) , and therefore hS; Ri is isomorphic to the free abelian
mutator
group s∈S Zs .
   PROOF. The members of R are in F(S)0 , the product of two members of F(S)0
is in F(S)0 , and any conjugate of a member of F(S)0 is in F(S)0 . Therefore
the smallest normal subgroup N (R) containing R has N (R) ⊆ F(S)0 . Let
ϕ : F(S) → F(S)/N (R) be the quotient homomorphism. Elements of the
quotient F(S)/N (R) may be expressed as words in the elements ϕ(s) and ϕ(s)−1
for s in S, and the factors commute because of the definition of R. Therefore
F(S)/N (R) is abelian. By Proposition 7.4, N (R) ⊇ F(S)0 . Therefore N (R) =
F(S)0 . This proves the first conclusion, and the second conclusion follows from
Corollary 7.5.                                                                 §


                         2. Subgroups of Free Groups

The main result of this section is that any subgroup of a free group is a free group.
An example in the previous section shows that the rank can actually increase in
the process of passing to the subgroup.
   The proof of the main result is ostensibly subtle but is relatively easy to under-
stand in topological terms. Although we shall give the topological interpretation,
we shall not pursue it further, and the proof that we give may be regarded as a
translation of the topological proof into the language of algebra, combined with
some steps of beautification.
   For purposes of the topological argument, let us think of the given free group
for the moment as finitely generated, and let us suppose that the subgroup has
finite index. A free group on n symbols is the fundamental group of a bouquet
of n circles, all joined at a single point, which we take as the base point. By the
theory of covering spaces, any subgroup of index k is the fundamental group of
some k-sheeted covering space of the bouquet of circles. This covering space is
a 1-dimensional simplicial complex, and one can prove with standard tools that
the fundamental group of any 1-dimensional simplicial complex is a free group.
The theorem follows.
318                          VII. Advanced Group Theory

   If the special hypotheses are dropped that the given free group is finitely
generated and the subgroup has finite index, then the same proof is applicable as
long as one allows a suitable generalization of the notion of simplicial complex.
Thus the topological argument is completely general.
   The theorem then is as follows.

    Theorem 7.10 (Nielsen–Schreier Theorem). Every subgroup of a free group
is a free group.
   REMARKS. The algebraic proof will occupy the remainder of the section but
will occasionally be interrupted by comments about the example in the previous
section.

   Let the given free group be F, let the subgroup be H , and form the right cosets
Hg in F. Let C be a set of representatives for these cosets, with 1 chosen as
the representative of the identity coset; we shall impose further conditions on C
shortly.

   EXAMPLE, CONTINUED. For the example in the previous section, we were
given a free group F with two generators x, y, and the subgroup H is taken to
have generators x 2 , x y, y 2 . In fact, one readily checks that H is the subgroup
formed from all words of even length, and we shall think of it that way. The set
C of coset representatives may be taken to be {1, x} in this case. The argument
we gave that H is free has points of contact with the proof we give of Theorem
7.10 but is not an exact special case of it. One point of contact is that within
each generator of H that we identify, there is some particular factor that does
not cancel when that generator appears in a word representing a member of the
subgroup.

   We define a function ρ : F → C by taking ρ(x) to be the coset representative
of the member x of F. This function has the property that ρ(hx) = ρ(x) for all
h in H and x in F. Also, x 7→ xρ(x)−1 is a function from F to H , and it is the
identity function on H . The first lemma shows that a relatively small subset of
the elements xρ(x)−1 is a set of generators of H .

   Lemma 7.11. Let S be the set of generators of F, and let S 0 = S ∪ S −1 .
Every element of H is a product of elements of the form gbρ(gb)−1 with g in
C and b in S 0 . Furthermore the element g 0 = ρ(gb) of C has the properties
                                                            °               ¢−1
that g = ρ(g 0 b−1 ) and that gb−1 ρ(gb−1 )−1 is of the form g 0 bρ(g 0 b)−1 .
Consequently the elements gaρ(ga)−1 with g in C and a in S form a set of
generators of H .
                               2. Subgroups of Free Groups                                319

   EXAMPLE, CONTINUED. In the example, we are taking C = {1, x} and S =
{x, y}. The elements gbρ(gb)−1 obtained with g=1 and b equal to x, y, x −1 , y −1
are 1, yx −1 , x −1 x −1 , and y −1 x −1 . The elements gbρ(gb)−1 obtained with g = x
and b equal to x, y, x −1 , y −1 are x x, x y, 1, and x y −1 . The lemma says that 1,
yx −1 , x x, and x y form a set of generators of H and that the elements x −1 x −1 ,
y −1 x −1 , 1, and x y −1 are inverses of these generators in some order.
   REMARK. The lemma needs no hypothesis that F is free. A nontrivial ap-
plication of the lemma with F not free appears in Problem 43 at the end of the
chapter.
     PROOF. Any h in F can be written as a product h = b1 · · · bn with each b j in
S 0 . Define r0 = 1 and rk = ρ(b1 · · · bk ) for 1 ≤ k ≤ n. Then

                   hrn−1 = (r0 b1r1−1 )(r1 b2r2−1 ) · · · (rn−1 bn rn−1 ).                (∗)

Since
   rk = ρ(b1 · · · bk ) = ρ(b1 · · · bk−1 bk ) = ρ(ρ(b1 · · · bk−1 )bk ) = ρ(rk−1 bk ),

we have rk−1 bk rk−1 = gbρ(gb)−1 with g = rk−1 and b = bk . Thus (∗) exhibits
hrn−1 as a product of elements as in the first conclusion of the lemma. Since
rn = ρ(b1 · · · bn ) = ρ(h), rn = 1 if h is in H . Therefore in this case, h itself is
a product of elements as in the statement of that conclusion, and that conclusion
is now proved.
   For the other conclusion, let gb−1 ρ(gb−1 )−1 be given, and put g 0 = ρ(gb−1 ),
so that gb−1 g 0−1 = h is in H . This equation implies that g 0 b = h −1 g. Hence
ρ(g 0 b) = ρ(h −1 g) = ρ(g) = g, and it follows that gb−1 ρ(gb−1 )−1 = gb−1 g 0−1
                    °             ¢−1
= (g 0 bg −1 )−1 = g 0 bρ(g 0 b)−1 . This proves the lemma.                         §

    Lemma 7.12. With F free it is possible to choose the set C of coset represen-
tatives in such a way that all of its members have expansions in terms of S 0 as
g = b1 · · · bn in which
     (a) g = b1 b2 · · · bn is a reduced word as written,
     (b) b1 b2 · · · bn−1 is also a member of C.
    REMARKS. It is understood from the case of n = 1 in (b) that 1 is the
representative of the identity coset. When C is chosen as in this lemma, C is
said to be a Schreier set. In the example, C = {1, x} is a Schreier set. So is
C = {1, y}, and hence the selection of a Schreier set may involve a choice.
   PROOF. If S 0 is finite or countably infinite, we enumerate it. In the uncountable
case (which is of less practical interest), we introduce a well ordering in S 0 by
means of Zermelo’s Well-Ordering Theorem as in Section A5 of the appendix.
320                              VII. Advanced Group Theory

    The ordering of S 0 will be used to define a lexicographic ordering of the set of
all reduced words in the members of S 0 . If

                        x = b1 · · · bm       and        y = b10 · · · bn0                   (∗)

are reduced words with m ≤ n, we say that x < y if any of the following hold:
      (i) m < n,
     (ii) m = n and b1 < b10 ,
    (iii) m = n, and for some k < m, b1 = b10 , . . . , bk = bk0 , and bk+1 < bk+1     0
                                                                                             .
With this definition the set of reduced words is well ordered, and hence any
nonempty subset of reduced words has a least element.
    Let us observe that if x, y, z are reduced words with x < y and if yz is reduced
as written, then x z < yz after x z has been reduced. In fact, let us assume that x
and y are as in (∗) and that the length of z is r. The assumption is that yz has
length n + r, and the length of x z is at most m + r. If m < n, then certainly
x z < yz. If m = n and x z fails to be reduced, then the length of x z is less than the
length of yz, and x z < yz. If m = n and x z is reduced, then the first inequality
bk < bk0 with x and y shows that x z < yz.
    To define the set C of coset representatives, let the representative of Hg be
the least member of the set Hg, each element being written as a reduced word.
Since the length of the empty word is 0, the representative of the identity coset
H is 1 under this definition. Thus all we have to check is that an initial segment
of a member of C is again in C.
    Suppose that b1 · · · bn is in C, so that b1 · · · bn is the least element of H b1 · · · bn .
Denote the least element of H b1 · · · bn−1 by g. If g = b1 · · · bn−1 , we are done.
Otherwise g < b1 · · · bn−1 , and then the fact that b1 · · · bn is reduced implies
that gbn < b1 · · · bn . But gbn is in H b1 · · · bn , and this inequality contradicts
the minimality of b1 · · · bn in that coset. Thus we conclude that g = b1 · · · bn−1 .
This proves the lemma.                                                                         §

   For the remainder of the proof of Theorem 7.10, we assume, as we may by
Lemma 7.12, that the set C of coset representatives is a Schreier set. Typical
elements of S will be denoted by a, and typical elements of S 0 = S ∪ S −1 will be
denoted by b. Let us write u for a typical element gaρ(ga)−1 with g in C, and let
us write v for a typical element gbρ(gb)−1 with g in C. The elements u generate
H by Lemma 7.11, and each element v is either an element u or the inverse of an
element u, according to the lemma. We shall prove that the elements u not equal
to 1 are distinct and form a free basis of H .
   First we prove that each of the elements v = gbρ(gb)−1 either is reduced as
written or is equal to 1. Put g 0 = ρ(gb), so that v = gbg 0−1 . Since g and g 0 are in
the Schreier set C, they are reduced as written, and hence so are g and g 0−1 . Thus
                                  2. Subgroups of Free Groups                                  321

the only possible cancellation in v occurs because the last factor of g is b−1 or the
last factor of g 0 is b. If the last factor of g is b−1 , then gb is an initial segment of
g and hence is in the Schreier set C; thus ρ(gb) = gb and v = gbρ(gb)−1 = 1.
Similarly if the last factor of g 0 is b, then g 0 b−1 is an initial segment of g 0 and
hence is in the Schreier set C; thus ρ(g 0 b−1 ) = g 0 b−1 , and Lemma 7.11 gives
         °             ¢−1
v −1 = gbρ(gb)−1            = g 0 b−1 ρ(g 0 b−1 )−1 = 1. Thus v = gbρ(gb)−1 either is
reduced as written or is equal to 1.
    Next let us see that the elements v other than 1 are distinct. Suppose that
v = gbρ(gb)−1 = g 0 b0 ρ(g 0 b0 )−1 is different from 1. Remembering that each of
these expressions is reduced as written, we see that if g is shorter than g 0 , then gb
is an initial segment of g 0 . Since C is a Schreier set, gb is in C and ρ(gb) = gb;
thus v = gbρ(gb)−1 equals 1, contradiction. Similarly g 0 cannot be shorter than
g. So g and g 0 must have the same length l. In this case the first l + 1 factors
must match in the two equal reduced words, and we conclude that g = g 0 and
b = b0 . This proves the uniqueness.
    We know that each v is either some u or some u 0−1 , and this uniqueness shows
that it cannot be both unless v = 1. Therefore the nontrivial u’s are distinct, and
the nontrivial v’s consist of the u’s and their inverses, each appearing once.
    Since an element v not equal to 1 therefore determines its g and b, let us refer
to the factor b of v = gbρ(gb)−1 as the significant factor of v. This is the part
that will not cancel out when we pass from a product of v’s to its reduced form.
    Specifically suppose that we have v = gbρ(gb)−1 and v̄ = ḡ b̄ρ(ḡ b̄)−1 , that
neither of these is 1, and that v̄ 6= v −1 . Put g 0 = ρ(gb) and ḡ 0 = ρ(ḡ b̄). The
claim is that the cancellation in forming v v̄ = gbg 0−1 ḡ b̄ ḡ 0−1 does not extend
to either of the significant factors b and b̄. If it does, then one of three things
happens:
     (i) the b in bg 0−1 gets canceled because the last factor of g 0 is b, in which
         case g 0 b−1 is an initial segment of g 0 , g 0 b−1 = ρ(g 0 b−1 ) = g, and
         v = gbg 0−1 = 1, or
    (ii) the b̄ in ḡ b̄ gets canceled because the last factor of ḡ is b̄−1 , in which case
         ḡ b̄ is an initial segment of ḡ, ḡ b̄ = ρ(ḡ b̄) = ḡ 0 , and v̄ = ḡ b̄ ḡ 0−1 = 1, or
   (iii) g 0−1 ḡ = 1 and bb̄ = 1, in which case ḡ = g 0 , b̄ = b−1 , and the middle
         conclusion of Lemma 7.11 allows us to conclude that v̄ = v −1 .
All three of these possibilities have been ruled out by our assumptions, and
therefore neither of the significant factors in v v̄ cancels.
   As a consequence of this noncancellation, we can see that in any product
v1 · · · vm of v’s in which no vk is 1 and no vk+1 equals vk−1 , none of the significant
factors cancel. In fact, the previous paragraph shows that the significant factors
of v1 and v2 survive in forming v1 v2 , the significant factors of v2 and v3 survive
in right multiplying by v3 , and so on. Since the nontrivial u’s are distinct and
322                          VII. Advanced Group Theory

the nontrivial v’s consist of the u’s and their inverses, each appearing once, we
conclude that the set of nontrivial u’s is a free subset of F. Lemma 7.11 says that
the u’s generate H , and therefore the set of nontrivial u’s is a free basis of H .


                                3. Free Products

The free abelian group on an index set S, as constructed in Section IV.9, has a
universal mapping property that allows arbitrary functions from S into any target
abelian group to be extended to homomorphisms of the free abelian group into
the target group. The construction of free groups in Section 1 was arranged to
adapt the construction so that the target group in the universal mapping property
could be any group, abelian or nonabelian.
     In this section we make a similar adaptation of the construction of a direct sum
of abelian groups so that the result is applicable in a context of arbitrary groups.
Proposition
L               4.17 gave the universal mapping property of the external direct sum
    s∈S sG   of   L of abelian groups with associated embedding homomorphisms
                a set
i s0 : G s0 →         s∈S G s . The statement is that if H is any abelian group and
{ϕs | s ∈ S} is a system of group homomorphismsL          ϕs : G s → H , then there
exists a unique group homomorphism ϕ : s∈S G s → H such that ϕ ◦ i s0 = ϕs0
for all s0 ∈ S. Example 2 of coproducts in Section IV.11 shows that direct sum
is therefore the coproduct functor in the L  category of all abelian groups.
     This universal mapping property of s∈S G s fails when H is a nonabelian
group such as the symmetric group S3 . In fact, S3 has an element of order 2 and
an element of order 3 and hence admits nontrivial homomorphisms ϕ2 : C2 → S3
and ϕ3 : C3 → S3 . But there is no homomorphism ϕ : C2 ⊕ C3 → S3 such
that ϕ ◦ i 2 = ϕ2 and ϕ ◦ i 3 = ϕ3 because the image of ϕ has to be abelian but the
images of ϕ2 and ϕ3 do not commute. Consequently direct sum cannot extend to
a coproduct functor in the category of all groups.
     Instead, the appropriate group constructed from C2 and C3 for this kind of
universal mapping property is the “free product” of C2 and C3 , denoted by
C2 ∗ C3 . In this section we construct the free product of any set of groups,
finite or infinite. Also, we establish its universal mapping property and identify
it in terms of generators and relations. The prototype of a free product is the free
group F(S), which equals a free product of copies of Z indexed by S. A free
product is always an infinite group if at least two of the factors are not 1-element
groups.
     An important application of free products occurs in the theory of the fundamen-
tal group in topology: if X is a topological space for which the theory of covering
spaces is applicable, and if A and B are open subsets of X with X = A ∪ B such
that A ∩ B is nonempty, connected, and simply connected, then the fundamental
                                    3. Free Products                                323

group of X is the free product of the fundamental group of A and the fundamental
group of B. This result, together with a generalization that no longer requires
A ∩ B to be simply connected, is known as the Van Kampen Theorem.
    Let S be a nonempty set of groups G s for s in S. The set S is allowed to be
infinite, but in practice it often has just two elements. We shall describe the group
                                            *
defined to be the free product G = s∈S G s . We start from the set W ({G s }) of
all words built from the groups G s . This consists of all finite sequences g1 · · · gn
with each gi in some G s depending on i. The length of a word is the number of
factors in it. The empty word is denoted by 1. We multiply two words by writing
them end to end, and the resulting operation of multiplication is associative. A
word is said to be equivalent to a second word if the first can be obtained from
the second by a finite sequence of steps of the following kinds and their inverses:
     (i) drop a factor for which gi is the identity element of the group in which it
         lies,
    (ii) collapse two factors gi gi+1 to a single one gi∗ if gi and gi+1 lie in the same
         G s and their product in that group is gi∗ .
The result is an equivalence relation, and the set of equivalence classes is the
                   *
underlying set of s∈S G s .

   Theorem 7.13. If S is a nonempty set of groups G s and W ({G s }) is the set
of all words from the groups G s , then the product operation defined on W ({G s })
                                                      *
descends in a well-defined fashion to the set s∈S G s of equivalence classes of

                              *
members of W ({G s }), and s∈S G s thereby becomes a group. For each s0 in

                        *
S, define i s0 : G s0 → s∈S G s to be the group homomorphism obtained as the
composition of the inclusion of G s0 into
                                       ° words of length ¢      1 followed by passage
to equivalence classes. Then the pair
                                             *G
                                          s∈S s  , {i s }  has the following universal
mapping property: whenever H is a group and {ϕs | s ∈ S} is a system of group
homomorphisms ϕs : G s → H , then there exists a unique group homomorphism
   *
ϕ : s∈S G s → H such that ϕ ◦ i s0 = ϕs0 for all s0 ∈ S.
                                                 ϕs
                                     G s0       −−−→ H
                                       
                                       
                                  i s0 y              ϕ


                                *  s∈S G s

           FIGURE 7.2. Universal mapping property of a free product.

                            *
   REMARKS. The group s∈S G s is called the free product of the groups G s .
Figure 7.2 illustrates its universal mapping property. This universal mapping
                                  *
property actually characterizes s∈S G s , as will be seen in Proposition 7.14. One
324                                VII. Advanced Group Theory

often writes G 1 ∗ · · · ∗ G n when the set S is finite; the order of listing the groups is
immaterial. The proof of Theorem 7.13 is rather similar to the proof of Theorem
7.1, and we shall skip some details.
  PROOF. Let us write ∼ for the equivalence relation on words, and let us denote
equivalence classes by brackets. We want to define multiplication in s∈S G s by
                                                                                        * let x, x ,
                                                                                                  0
[w1 ][w2 ] = [w1 w2 ]. To see that this formula makes sense in
                                                               *0
                                                                                s∈S G s ,
and y be words in W ({G }), and suppose that x and x differ by only one operation
                              s
                                                       0
of type (i) or type (ii) as above. Then x ∼ x , and it is evident that x 0 y ∼ x y and
yx 0 ∼ yx. Iteration of this kind of relationship shows that w10 ∼ w1 and w20 ∼ w2
implies w10 w20 ∼ w1 w2 , and hence multiplication is well defined.
   The associativity of multiplication in W ({G s }) implies that multiplication in
* s∈S G s is associative, and [1] is a two-sided identity. We readily check that if

g = g1 · · · gn is a word, then the word g −1 = gn−1 · · · g1−1 has the property that
[g −1 ] is a two-sided inverse to [g]. Therefore s∈S G s is a group.
                                                           *
   The uniqueness of the homomorphism ϕ in the universal mapping property
is no problem since all words are products of words of length 1 and since the
                                              *
subgroups i s0 (G s0 ) together generate s∈S G s .
   For existence of ϕ, we begin by defining a function 8 : W ({G s }) → H such
that

   8(gs ) = ϕs (gs )      for gs in G s when viewed as a word of length 1,
 8(w1 w2 ) = 8(w1 )8(w2 ) for w1 and w2 in W ({G s }).

We take the formulas 8(gs ) = ϕ(gs ) for gs in G s as a definition of 8 on words
of length 1. Any member of W ({G s }) can be written uniquely as g1 · · · gn with
each gi in G si , and we set 8(g1 · · · gn ) = 8(g1 ) · · · 8(gn ). (If n = 0, the
understanding is that 8(1) = 1.) Then 8 has the required properties.
   Let us show that w0 ∼ w implies 8(w0 ) = 8(w). The questions are whether
     (i) if g1 , . . . , gn are in various G s ’s with gi equal to the identity 1si of G si ,
         then
                                                  ?
               8(g1 · · · gi−1 1si gi+1 · · · gn ) = 8(g1 · · · gi−1 gi+1 · · · gn ),

      (ii) if g1 , . . . , gn are in various G s ’s with G si = G si+1 and if gi gi+1 = gi∗ in
           G si , then

                                                   ?
           8(g1 · · · gi−1 gi gi+1 gi+2 · · · gn ) = 8(g1 · · · gi−1 gi∗ gi+2 · · · gn ).
                                      3. Free Products                           325

In the case of (i), the question comes down to whether a certain h8(1si )h 0 in H
equals hh 0 , and this is true because 8(1si ) = ϕsi (1si ) is the identity of H . In
the case of (ii), the question comes down to whether h8(gi )8(gi+1 )h 0 equals
h8(gi∗ )h 0 if G si = G si+1 and gi gi+1 = gi∗ in G si , and this is true because
8(gi )8(gi+1 ) = ϕsi (gi )ϕsi (gi+1 ) = ϕsi (gi gi+1 ) = ϕsi (gi∗ ) = 8(gi∗ ). We
conclude that w0 ∼ w implies 8(w0 ) = 8(w).
    We may therefore define ϕ([w]) = 8(w) for [w] in F({G s }), and ϕ is a
homomorphism of F({G s }) into H as a consequence of the property 8(w1 w2 ) =
8(w1 )8(w2 ) of 8 on W ({G s }). For gs in G s , we have ϕ([gs ]) = 8(gs ) =
ϕs (gs ), i.e., ϕ(i(gs )) = ϕs (gs ). This completes the proof of existence.       §

   Proposition 7.14. Let S be a nonempty set of groups G s . Suppose that G 0 is
a group and that i s0 : G s → G 0 for s ∈ S is a system of group homomorphisms
with the following universal mapping property: whenever H is a group and
{ϕs | s ∈ S} is a system of group homomorphisms ϕs : G s → H , then there
exists a unique group homomorphism ϕ : G 0 → H such that ϕ ◦ i s0 = ϕs for all
s ∈ S. Then there exists a unique group homomorphism 8 : s∈S G s → G 0
                                                                    *
such that i s0 = 8 ◦ i s for all s ∈ S. Moreover, 8 is a group isomorphism, and the
homomorphisms i s0 : G s → G 0 are one-one.
    REMARKS. As was true with Proposition 7.2, readers who have been through
Chapter VI will recognize that Proposition 7.14 is a special case of Problem 19
at the end of that chapter.
   PROOF. Put G =
7.13, let H = G   0
                         *s∈S G s .
                      and ϕ       0
                                      In the universal mapping property of Theorem
                                   and let 8 : G → G 0 be the homomorphism ϕ
                          s = is ,
produced by that theorem. Then 8 satisfies 8 ◦ i s = i s0 for all s. Reversing the
roles of G and G 0 , we obtain a homomorphism 80 : G 0 → G with 80 ◦ i s0 = i s
for all s. Therefore (80 ◦ 8) ◦ i s = 80 ◦ i s0 = i s .
   Comparing 80 ◦ 8 with the identity 1G and applying the uniqueness in the
universal mapping property for G, we see that 80 ◦ 8 = 1G . Similarly the
uniqueness in the universal mapping property of G 0 gives 8 ◦ 80 = 1G 0 . Thus 8
is a group isomorphism. It is uniquely determined by the given properties since
the various subgroups i s (G s ) generate G. Since i s0 = 8 ◦ i s and since 8 and i s
are one-one, i s0 is one-one.                                                     §
   As was the case for free groups, we want a decision procedure for telling
whether two given words in W ({G s }) are equivalent. This is the so-called word
problem for the free product. Solving it allows us to use free products concretely,
just as Proposition 7.3 allowed us to use free groups concretely. A word in
W ({G s })) is said to be reduced if it
     (i) contains no factor for which gi is the identity element of the group G s in
         which it lies,
326                            VII. Advanced Group Theory

      (ii) contains no two consecutive factors gi and gi+1 taken from the same
           group G s .

    Proposition 7.15. (solution of the word problem for free products). If S is a
nonempty set of groups G s and W ({G s }) is the set of all words from the groups
G s , then each word in W ({G s }) is equivalent to one and only one reduced word.
   EXAMPLE. Consider the free product C2 ∗C2 of two cyclic groups, one with x as
generator and the other with y as generator. Words consist of a finite sequence of
factors of x, y, the identity of the first factor, and the identity of the second factor.
A word is reduced if no factor is an identity and if no two x’s are adjacent and no
two y’s are adjacent. Thus the reduced words consist of finite sequences whose
terms are alternately x and y. Those of length ≤ 3 are 1, x, y, x y, yx, x yx, yx y,
and in general there are two of each length > 0. The proposition tells us that all
these reduced words give distinct members of C2 ∗ C2 . In particular, the group is
infinite.
   REMARK. More generally, to test whether two words are equivalent, the
proposition says to eliminate factors of the identity and multiply consecutive
factors in each word when they come from the same group, and repeat these steps
until it is no longer possible to do either of these operations on either word. Then
each of the given words has been replaced by a reduced word, and the two given
words are equivalent if and only if the two reduced words are identical. Problems
37–46 at the end of the chapter concern C2 ∗C3 , and some of these problems make
use of the result of this proposition—that distinct reduced words are inequivalent.
   PROOF OF PROPOSITION 7.15. Both operations—eliminating factors of the
identity and multiplying consecutive factors in each word when they come from
the same group—reduce the length of a word. Since the length has to remain
∏ 0, the process of successively carrying out these two operations as much as
possible has to stop after finitely many steps, and the result is a reduced word.
This proves that each equivalence class of words contains a reduced word.
   For uniqueness of the reduced word in an equivalence class, we proceed
somewhat as with Proposition 7.3, associating to each word a finite sequence
of reduced words such that the last member of the sequence is unchanged when
we apply an operation to the word that preserves equivalence. However, there are
considerably more details to check this time.
   If w = g1 · · · gn is a given word with each gi in G si , then we associate to w
the sequence of reduced words x0 , x1 , . . . , xn defined inductively by

                  x0 = 1,
                       Ω
                          g1       if g1 is not the identity of G s1 ,
                  x1 =
                          1        if g1 is the identity of G s1 ,
                                            3. Free Products                                           327

and the following formula for i ∏ 2 if xi−1 is of the reduced form h 1 · · · h k with
h j in G tj :
        
             h 1 · · · h k gi   if G si 6= G tk and gi is not the identity 1G si of G si ,
        
        
         h ···h                 if gi is the identity 1G si of G si ,
                1         k
  xi =
        
             h 1 · · · h k−1    if G tk = G si with h k gi = 1G si ,
        
                              ∗
              h 1 · · · h k−1 gi if G tk = G si with h k gi = gi∗ 6= 1G si .
Put r(w) = xn . We check inductively for i ∏ 0 that each xi is reduced. In fact, xi
for i ∏ 2 begins in every case with h 1 · · · h k−1 , which is assumed reduced. The
only possible reduction for xi thus comes from factors that are adjoined or from
interference with h k−1 , and all possibilities are addressed in the above choices.
Thus r(w) = xn is necessarily reduced for each word w.
    If g1 · · · gn is reduced as given, then xi is determined by the first possible choice
h 1 · · · h k gi every time, and hence xi = g1 · · · gi for all i. Therefore we obtain
r(w) = w if w is reduced.
    Now consider the equivalent words

         w = g1 · · · g j g j+1 · · · gn        and        w0 = g1 · · · g j 1G s g j+1 · · · gn .
                                              0
Form x0 , . . . , xn for w and x00 , . . . , xn+1 for w0 . Then we have x j0 = x j ; let
h 1 · · · h k be a reduced form of x j0 . The formula for x j+1 0
                                                                    is governed by the
                                         0                              0
second choice in the display, and x j+1 = h 1 · · · h k = x j . Then x j+i+1 = x j+i for
                                    0
1 ≤ i ≤ n − j as well. Hence xn+1          = xn , and r(w0 ) = r(w).
    Next suppose that g j∗ = g j g j+1 in G sj , and consider the equivalent words

  w = g1 · · · g j−1 g j∗ g j+2 · · · gn       and         w0 = g1 · · · g j−1 g j g j+1 g j+2 · · · gn .

As above, form x0 , . . . , xn for w and x00 , . . . , xn+1     0
                                                                     for w0 . Then we have x j−1 =
  0
x j−1 , and we let h 1 · · · h k be a reduced form of x j−1 . There are cases, subcases,
and subsubcases.
     First assume G tk 6= G sj . Then x j equals h 1 · · · h k g j∗ or h 1 · · · h k in the two
subcases g j∗ 6= 1G sj and g j∗ = 1G sj . In the first subcase, we have g j∗ 6= 1G sj and
x j = h 1 · · · h k g j∗ . Then x j0 equals h 1 · · · h k g j or h 1 · · · h k in the two subsubcases
                                                                            0
g j 6= 1G sj and g j = 1G sj . In the first subsubcase, x j+1                     = h 1 · · · h k g j∗ = x j
                                                                                ∗
whether or not g j+1 = 1G sj . In the second subsubcase, g j = g j g j+1 cannot be
                             0
1G sj , and therefore x j+1     = h 1 · · · h k g j∗ = x j .
     In the second subcase of the case G tk 6= G sj , we have g j∗ = 1G sj and x j =
x j−1 = h 1 · · · h k . Then x j0 equals h 1 · · · h k g j or h 1 · · · h k in the two subsubcases
                                                                  0                                    0
g j 6= 1G sj and g j = 1G sj . In both subsubcases, x j+1                = h 1 · · · h k , so that x j+1 =
xj .
328                                   VII. Advanced Group Theory

     Now assume G tk = G sj . Then x j equals h 1 · · · h k−1 h ∗k or h 1 · · · h k−1 in the
two subcases h k g j∗ = h ∗k 6= 1G sj and h k g j∗ = 1G sj . In the first subcase, we
have h k g j∗ = h k∗ 6= 1G sj and x j = h 1 · · · h k−1 h ∗k . Then x j0 equals h 1 · · · h k−1 h 0k or
h 1 · · · h k−1 in the two subsubcases h k g j = h 0k 6= 1G sj and h k g j = 1G sj . In the first
subsubcase, h 0k g j+1 = h k g j g j+1 = h k g j∗ = h ∗k implies x j+1         0
                                                                                  = h 1 · · · h k−1 h ∗k =
                                                                ∗
x j . In the second subsubcase, we know that h k cannot be 1G si and hence that
g j+1 = h k g j g j+1 = h k g j∗ = h k∗ cannot be 1G sj ; thus x j+1     0
                                                                               = h 1 · · · h k−1 h ∗k = x j .
     In the second subcase of the case G tk = G sj , we have h k g j∗ = 1G sj and x j =
h 1 · · · h k−1 . Then x j0 equals h 1 · · · h k−1 h k∗ 0 or h 1 · · · h k−1 in the two subsubcases
h k g j = h ∗k 0 6= 1G sj and h k g j = 1G sj . In the first subsubcase, g j+1 cannot be
1G sj but h ∗k 0 g j+1 = h k g j g j+1 = h k g j∗ = 1G sj ; hence x j+1      0
                                                                                 = h 1 · · · h k−1 = x j .
                                      0                                                               0
In the second subsubcase, x j = h 1 · · · h k−1 and g j+1 = 1G sj , so that x j+1                          =
h 1 · · · h k−1 = x j .
                             0                                             0
     We conclude that x j+1        = x j in all cases. Hence x j+i+1             = x j+i for 0 ≤ i ≤
             0                      0
n − j, xn+1 = xn , and r(w ) = r(w). Consequently the only reduced word that
is equivalent to w is r(w).                                                                                §

   Proposition 7.16. Let S be a nonempty set of groups G s , and suppose that
hSs ; Rs i is a ≠presentation
                 S           S of G s ,Æ the sets Ss being understood to be disjoint for
s ∈ S. Then         s∈S Ss ;  s∈S Rs is a presentation of the free product     s∈S G s .
                                                                                              *
    REMARK. One effect of this proposition is to make Proposition 7.8 available
as a tool for use with free products. Using Proposition 7.8 may be easier than
appealing to the universal mapping property in Theorem 7.13.
                     S                    S
    PROOF. Put S = s∈S Ss and R = s∈S Rs , and define G to be a group given
by generators and relations as G = hS; Ri. Consider the function from Ss into
the quotient group G = F(S)/N (R) given by carrying x in Ss into the word
x in S and then passing to F(S) and its quotient G. Because of the universal
mapping property of free groups, this function extends to a group homomorphism
e
i s : F(Ss ) → G. If r is a reduced word relative to Ss representing a member
of Rs , then r is carried by e
                             i s into a member of the larger set R and then into
the identity of G. Since kere   i s is normal in F(Ss ), kere
                                                            i s contains the smallest
normal subgroup N (Rs ) in F(Ss ) that contains Rs . Proposition 4.11 shows that
e
i s descends to a group homomorphism i s : G s → G.
    We shall prove that G and the system {i s } have the universal mapping property
of Proposition 7.14 that characterizes a free product. Then it will follow from
that proposition that G ∼
                                   *
                         = s∈S G s , and the proof will be complete.
    Thus let H be a group, and let {ϕs | s ∈ S} be a system of group homo-
morphisms ϕs : G s → H . We are to produce a homomorphism 8 : G → H
such that 8 ◦ i s = ϕs for all s, and we are to prove that such a homomorphism
                               4. Group Representations                          329

is unique. Let qs : F(Ss ) → G s be the quotient homomorphism, and define
ϕs : F(Ss ) → H by e
e                         ϕs = ϕs ◦ qs . Now define 8   e : S → H as follows: if
x is in S, then x is in a set Ss for a unique s and thereby defines a member
                               e
of F(Ss ) for that unique s; 8(x)                    ϕs (x). The universal mapping
                                      is taken to be e
property of the free group F(S) allows us to extend 8  e to a group homomorphism,
                              e
which we continue to call 8, of F(S) into H . Let r be a nontrivial relation in
R ⊆ F(S). Then r, by hypothesis of disjointness for the sets Ss , lies in a unique
            e
Rs . Hence 8(r)   =e ϕs (r) = ϕs (qs (r)) = ϕs (1s ) = 1 H . Consequently the kernel
   e contains the smallest normal subgroup N (R) of F(S) containing R, and 8
of 8                                                                              e
descends to a homomorphism 8 : G → H . This 8 satisfies
                                              Ø
                  8 ◦ i s ◦ qs = 8 ◦ eis = 8eØ      =eϕs = ϕs ◦ qs .
                                             F(Ss )
Since the quotient homomorphism qs is onto G s , we obtain 8 ◦ i s = ϕs , and
existence of the homomorphism 8 is established.
   For uniqueness, we observe that the identities 8 ◦ i s = ϕs imply that 8 is
uniquely determined on the subgroup of G generated by the images of all i s .
Since qs is onto G s , this subgroup is the same as the subgroup generated by the
images of all ei s . This subgroup contains the image in G of every generator of
F(S) and hence is all of G. Thus 8 is uniquely determined.                     §


                           4. Group Representations

Group representations were defined in Section IV.6 as group actions on vector
spaces by invertible linear functions. The underlying field of the vector space
will be taken to be C in this section and the next, and the theory will then be
especially tidy. The subject of group representations is one that uses a mix of
linear algebra and group theory to reveal hidden structure within group actions. It
has broad applications to algebra and analysis, but we shall be most interested in
an application to finite groups known as Burnside’s Theorem that will be proved
in the next section.
   Let us begin with the abelian case, taking G for the moment to be a finite abelian
group. A multiplicative character of G is a homomorphism χ : G → S 1 ⊆ C×
of G into the multiplicative group of complex numbers of absolute value 1. The
multiplicative characters form an abelian group G b under pointwise multiplication
of their complex values: (χχ )(g) = χ(g)χ 0 (g). The identity of G
                                 0                                          b is the
multiplicative character that is identically 1 on G, and the inverse of χ is the
complex conjugate of χ.
   The notion of multiplicative character adapts to the case of a finite group the
familiar exponential functions x 7→ einx on the line, which can be regarded as
multiplicative characters of the additive group R/2πZ of real numbers modulo
2π. These functions have long been used to resolve a periodic function of
330                               VII. Advanced Group Theory

time into its component frequencies: The device is the Fourier series of the
function f . RIf f is periodic of period 2π, then the Fourier coefficients of f
            1    π        −inx
are cn = 2π     −π f (x)e      dx, and the Fourier series of f is the infinite series
P∞            inx
   n=−∞ cn e      . A portion of the subject of Fourier series looks for senses in
which f (x) is actually equal to the sum of its Fourier series. This is the problem
of Fourier inversion.
    A similar problem can be formulated when R/2πZ is replaced by the finite
abelian group G. The exponential functions are replaced by the multiplicative
characters. One can form an analog of Fourier coefficients for the vector space
C(G, C) of complex-valued functions2 defined on G, and then one can form the
analog of the Fourier series of the function. The problem of Fourier inversion
becomes one of linear algebra, once we take into account the known structure of
all finite abelian groups (Theorem 4.56). The result is as follows.

   Theorem 7.17 (Fourier inversion formula for finite abelian groups). Let G be a
finite abelian group, and introduce an inner product on the complex vector space
C(G, C) of all functions from G to C by the formula
                                        X
                            hF, F 0 i =    F(g)F 0 (g),
                                              g∈G

the corresponding norm being kFk = hF, Fi1/2 . Then the members of G
                                                                   b form an
orthogonal basis of C(G, C), each χ in G
                                       b satisfying kχk2 = |G|. Consequently
 b = |G|, and any function F : G → C is given by the “sum of its Fourier
|G|
series”:
                             1 X≥X                  ¥
                    F(g) =                F(h)χ(h) χ(g).
                           |G| b h∈G
                                        χ∈G

   REMARKS. This theorem is one of the ingredients in the proof in Chapter I of
Advanced Algebra of Dirichlet’s theorem that if a and b are positive relatively
prime integers, then there are infinitely many primes of the form an + b. In
applications to engineering, the ordinary Fourier transform on the line is often
approximated, for computational purposes, by a Fourier series on a large cyclic
group, and then Theorem 7.17 is applicable. Such a Fourier series can be com-
puted with unexpected efficiency using a special grouping of terms; this device
    2 The notation C(G, C) is to be suggestive of what happens for G = S 1 and for G = R1 , where

one works in part with the space of continuous complex-valued functions vanishing off a bounded
set. In any event, pointwise multiplication makes C(G, C) into a commutative ring. Later in the
section we introduce a second multiplication, called “convolution,” that makes C(G, C) into a ring
in a different way. In Chapter VIII we shall introduce the “complex group algebra” CG of G. The
vector space C(G, C) is the dual vector space of CG. However, C(G, C) and CG are canonically
isomorphic because they have distinguished bases, and the isomorphism respects the multiplication
structures—convolution in C(G, C) and the group-algebra multiplication in CG.
                                4. Group Representations                            331

is called the fast Fourier transform and is described in Problems 29–31 at the
end of the chapter.
                                                                b and put
  PROOF. For orthogonality let χ and χ 0 be distinct members of G,
 00    0      0−1                        00
χ = χχ = χχ . Choose g0 in G with χ (g0 ) 6= 1. Then
                     °P            ¢ P                     P
           χ 00 (g0 )        00
                        g∈G χ (g) =
                                                00
                                           g∈G χ (g0 g) =
                                                                  00
                                                             g∈G χ (g),
                                          P
so that                   [1 − χ 00 (g0 )] g∈G χ 00 (g) = 0
                                P          00
and therefore                        g∈G χ (g) = 0.
                           P                       P
Consequently hχ, χ 0 i = g∈G χ(g)χ 0 (g) = g∈G χ 00 (g) = 0.

   The orthogonality implies that the members of G  b are linearly independent,
                                                                P
and we obtain |G| ≤ dim C(G, C) = |G|. Certainly kχk2 = g∈G |χ(g)|2 =
                 b
P
   g∈G 1 = |G|.
   To see that the members of G b are a basis of C(G, C), we write G as a direct
sum of cyclic groups, by Theorem 4.56. A summand Z/mZ has at least m distinct
multiplicative characters, given by j mod m 7→ e2πi jr/m for 0 ≤ r ≤ m − 1, and
these characters extend to G as 1 on the other direct summands of G. Taking
products of such multiplicative characters from the different summands of G,
             b ∏ |G|. Therefore |G|
we see that |G|                      b = |G|, and G b is an orthogonal basis by
Corollary 2.4. The formula for F(g) in the statement of the theorem follows by
applying Theorem 3.11c.                                                       §

    Now suppose that the finite group G is not necessarily abelian. Since S 1 is
abelian, Proposition 7.4 shows that χ takes the value 1 on every member of the
commutator subgroup G 0 of G. Consequently there is no way that the multiplica-
tive characters can form a basis for the vector space C(G, C) of complex-valued
functions on G. The above analysis thus breaks down, and some adjustment is
needed in order to extend the theory.
    The remedy is to use representations, as defined in Section IV.6, on complex
vector spaces of dimension > 1. We shall assume in the text that the vector space
is finite-dimensional. The sense in which representations extend the theory of
multiplicative characters is that any multiplicative character χ gives a represen-
tation R on the 1-dimensional vector space C by R(g)(z) = χ(g)z for g in G
and z in C. Conversely any 1-dimensional representation gives a multiplicative
character: if R is the representation on the 1-dimensional vector space V and if
v0 6= 0 is in V , then χ(g) is the scalar such that R(g)v0 = χ(g)v0 . It is enough
to observe that the only elements of finite order in the multiplicative group C×
are certain members of the circle S 1 , and then it follows that χ takes values in S 1 .
332                                  VII. Advanced Group Theory

    In the higher-dimensional case, the analog of the multiplicative character χ
in passing to a 1-dimensional representation R is a “matrix representation.” A
                                                           Pn G into invertible
matrix representation of G is a function g 7→ [ρ(g)i j ] from
square matrices of some given size such that ρ(g1 g2 )i j = k=1 ρ(g1 )ik ρ(g2 )k j .
If a representation R acts on the finite-dimensional complex vector space V , then
the choice of an ordered basis 0 for V leads to a matrix representation by the
formula                                     µ      ∂
                                              R(g)
                               [ρ(g)i j ] =          .
                                              00
Conversely if a matrix representation g 7→ [ρ(g)i j ] and an ordered basis 0 of V
are given, then the same formula may be used to obtain a representation R of G
on V .
    In contrast to the 1-dimensional case, the matrices that occur with a matrix rep-
resentation of dimension > 1 need not be unitary. The correspondence between
unitary linear maps and unitary matrices was discussed in Chapter III. When
the finite-dimensional vector space V has an inner product, a linear map was
defined to be unitary if it satisfies the equivalent conditions of Proposition 3.18.
A complex square matrix A was defined to be unitary if A∗ A = I . The matrix
of a unitary linear map relative to an ordered orthonormal basis is unitary, and
conversely when a unitary matrix and an ordered orthonormal basis are given, the
associated linear map is unitary. We can thus speak of unitary representations
and unitary matrix representations.
    Some examples of representations appear in Section IV.6. One further pair
of examples will be of interest to us. With the finite group G fixed but not
necessarily abelian, we continue to let C(G, C) be the complex vector space of
all functions f : G → C. We define two representations of G on C(G, C): the
left regular representation ` given by (`(g) f )(x) = f (g −1 x) and the right
regular representation r given by (r(g) f )(x) = f (xg). The reason for the
presence of an inverse in one case and not the other was discussed in Section
IV.6. Relative to the inner product
                                         X
                             ( f1, f2) =      f 1 (x) f 2 (x),
                                                  x∈G

both ` and r are unitary. The argument for ` is that
                           X                                X
   (`(g) f 1 ,`(g) f 2 ) =   (`(g) f 1 )(x)(`(g) f 2 )(x) =   f 1 (g −1 x) f 2 (g −1 x)
                              x∈G                                        x∈G

                  under   y=g −1 x   X
                          =                f 1 (y) f 2 (y) = ( f 1 , f 2 ),
                                     y∈G

and the argument for r is completely analogous.
                               4. Group Representations                          333

   It will be convenient to abbreviate “representation R on V ” as “representa-
tion (R, V ).” Let (R, V ) be a representation of the finite group G on a finite-
dimensional complex vector space. An invariant subspace U of V is a vector
subspace such that R(g)U ⊆ U for all g in G. The representation is irreducible
if V 6= 0 and if V has no invariant subspaces other than 0 and V .
   Two representations (R1 , V1 ) and (R2 , V2 ) on finite-dimensional complex vec-
tor spaces are equivalent if there exists a linear invertible function A : V1 → V2
such that AR1 (g) = R2 (g)A for all g in G. In the terminology of Section
IV.11, “equivalent” is the notion of “is isomorphic to” in the category of all
finite-dimensional representations of G.
   In more detail a morphism from (R1 , V1 ) to (R2 , V2 ) in this category is an
intertwining operator, namely a linear map A : V1 → V2 such that AR1 (g) =
R2 (g)A for all g in G. The condition for this equality to hold is that the diagram
in Figure 7.3 commute.
                                          A
                                   V1 −−−→      V2
                                               
                                    
                              R1 (g)y
                                                 R (g)
                                                y 2
                                          A
                                  V1 −−−→ V2
FIGURE 7.3. An intertwining operator for two representations, i.e., a morphism
         in the category of finite-dimensional representations of G.

   An example of a pair of representations that are equivalent is the left and right
regular representations of G on C(G, C): in fact, if we define (A f )(x) = f (x −1 ),
then
  (`(g)A f )(x) = (A f )(g −1 x) = f (x −1 g) = (r(g) f )(x −1 ) = (Ar(g) f )(x).

   Proposition 7.18 (Schur’s Lemma). If (R1 , V1 ) and (R2 , V2 ) are irreducible
representations of the finite group G on finite-dimensional complex vector spaces
and if A : V1 → V2 is an intertwining operator, then A is invertible (and hence
exhibits R1 and R2 as equivalent) or else A = 0. If (R1 , V1 ) = (R2 , V2 ) and
A : V1 → V2 is an intertwining operator, then A is scalar.
   REMARK. The conclusion that A is scalar makes essential use of the fact that
the underlying field is C.
   PROOF. The equality R2 (g)Av1 = AR1 (g)v1 shows that ker A and image A
are invariant subspaces. By the assumed irreducibility, ker A equals 0 or V1 , and
image A equals 0 or V2 . The first statement follows. When (R1 , V1 ) = (R2 , V2 ),
the identity I : V1 → V2 is an intertwining operator. If ∏ is an eigenvalue of A,
then A − ∏I is another intertwining operator. Since A − ∏I is not invertible when
∏ is an eigenvalue of A, A − ∏I must be 0.                                      §
334                           VII. Advanced Group Theory

  Corollary 7.19. Every irreducible finite-dimensional representation of a finite
abelian group G is 1-dimensional.
   PROOF. If (R, V ) is given, then the linear map A = R(g) satisfies AR(x) =
R(gx) = R(xg) = R(x)A for all x in G. By Schur’s Lemma (Proposition 7.18),
A = R(g) is scalar. Since g is arbitrary, every vector subspace of V is invariant.
Irreducibility therefore implies that V is 1-dimensional.                      §

  Let R be a representation of the finite group G on the finite-dimensional
complex vector space V , let ( · , · )0 be any inner product on V , and define
                                       X
                       (v1 , v2 ) =          (R(x)v1 , R(x)v2 )0 .
                                       x∈G

Then we have
                             P
      (R(g)v1 , R(g)v2 ) =         (R(x)R(g)v1 , R(x)R(g)v2 )0
                             x∈G
                             P
                        =          (R(xg)v1 , R(xg)v2 )0
                             x∈G
                             P
                        =          (R(y)v1 , R(y)v2 )0       by the change y = xg
                             y∈G
                        = (v1 , v2 ).

With respect to the inner product ( · , · ), the representation (R, V ) is therefore
unitary. In other words, we are always free to introduce an inner product to
make a given finite-dimensional representation unitary. The significance of this
construction is noted in the following proposition.

   Proposition 7.20. If (R, V ) is a finite-dimensional representation of the finite
group G and if an inner product is introduced in V that makes the representation
unitary, then the orthogonal complement of an invariant subspace is invariant.
   PROOF. Let U be an invariant subspace. If u is in U and u ⊥ is in U ⊥ , then
(R(g)u ⊥ , u) = (R(g)−1 R(g)u ⊥ , R(g)−1 u) = (u ⊥ , R(g)−1 u) = 0. Thus u ⊥ in
U ⊥ implies R(g)u ⊥ is in U ⊥ .                                              §

    Corollary 7.21. Any finite-dimensional representation of the finite group G
is a direct sum of irreducible representations.
   REMARK. That is, we can find a system of invariant subspaces such that the
action of G is irreducible on each of these subspaces and such that the whole
vector space is the direct sum of these subspaces.
                                   4. Group Representations                                  335

   PROOF. This is immediate by induction on the dimension. For dimension 0,
the representation is the empty direct sum of irreducible representations. If the
decomposition is known for dimension < n and if U is an invariant subspace
under R of smallest possible dimension ∏ 1, then U is irreducible under R, and
Proposition 7.20 says that the subspace U ⊥ , which satisfies V = U ⊕ U ⊥ , is
invariant. It is therefore enough to decompose U ⊥ , and induction achieves such
a decomposition.                                                               §

   Proposition 7.22 (Schur orthogonality). For finite-dimensional representa-
tions of a finite group G in which inner products have been introduced to make
the representations unitary,
    (a) if (R1 , V1 ) and (R2 , V2 ) are inequivalent and irreducible, then
    X
        (R1 (x)v1 , v10 )(R2 (x)v2 , v20 ) = 0 for all v1 , v10 ∈ V1 and v2 , v20 ∈ V2 .
    x∈G

    (b) if (R, V ) is irreducible, then
  X                                    |G|(v1 , v2 )(v10 , v20 )
      (R(x)v1 , v10 )(R(x)v2 , v20 ) =                             for v1 , v2 , v10 , v20 ∈ V.
  x∈G
                                             dim V

   REMARKS. If G is abelian, then V1 and V2 in (a) are 1-dimensional, and the
conclusion of (a) reduces to the statement that the multiplicative characters are
orthogonal. Conclusion (b) in this case reduces to a trivial statement.
   PROOF. For (a), let l : V2 → V1 be any linear map, and form the linear map
                                 P
                             L=     R1 (x)l R2 (x −1 ).
                                       x∈G

Multiplying on the left by R1 (g) and on the right by R2 (g −1 ) and changing vari-
ables in the sum, we obtain R1 (g)L R2 (g −1 ) = L, so that R1 (g)L = L R2 (g) for
all g ∈ G. By Schur’s Lemma (Proposition 7.18) and the assumed irreducibility
and inequivalence, L = 0. Thus (Lv20 , v10 ) = 0. For the particular choice of l as
l(w2 ) = (w2 , v2 )v1 , we have
                      P
  0 = (Lv20 , v10 ) =     (R1 (x)l R2 (x −1 )v20 , v10 )
                      x∈G
       P °                                      ¢     P
    =        R1 (x)(R2 (x −1 )v20 , v2 )v1 , v10 =       (R1 (x)v1 , v10 )(R2 (x −1 )v20 , v2 ),
       x∈G                                          x∈G

and (a) results since (R2 (x −1 )v20 , v2 ) = (R2 (x)v2 , v20 ).
  For (b), we proceed in the same way, starting from l : V → V , and we obtain
L = ∏I from Schur’s Lemma. Taking the trace of both sides, we find that
336                                 VII. Advanced Group Theory

                        ∏ dim V = Tr L = |G| Tr l.
                       ±
Therefore ∏ = |G|(Tr l) dim V . Since L = ∏I ,

                                                   |G| Tr l 0 0
                                 (Lv20 , v10 ) =           (v1 , v2 ).
                                                    dim V
Again we make the particular choice of l as l(w2 ) = (w2 , v2 )v1 . Since Tr l =
(v1 , v2 ), we obtain

              (v1 , v2 )(v10 , v20 )    Tr l
                                     =       (v 0 , v 0 ) = |G|−1 (Lv20 , v10 )
                    dim V              dim V 1 2
                                             P
                                     = |G|−1      (R(x)l R(x −1 )v20 , v10 )
                                               x∈G
                                               P °                                       ¢
                                    = |G|−1            R(x)(R(x −1 )v20 , v2 )v1 , v10
                                               x∈G
                                               P
                                    = |G|−1          (R(x)v1 , v10 )(R(x −1 )v20 , v2 ),
                                               x∈G


and (b) results since (R(x −1 )v20 , v2 ) = (R(x)v2 , v20 ).                                      §

    Let us interpret Proposition 7.22 as a statement about the left and right regular
representations ` Pand r of G on the inner-product space C(G, C), the inner product
being h f, f 0 i = g∈G f (g) f 0 (g). Let R be an irreducible representation of G
on the finite-dimensional vector space V , and introduce an inner product to make
it unitary. A member of C(G, C) of the form g 7→ (R(g)v, v 0 ) is called a matrix
coefficient of R. Let v1 , . . . , vn be an orthonormal basis of V . The matrix
representation of G that corresponds to R and this choice of orthonormal basis
has ρ(g)i j = (R(g)v j , vi ), and hence the entries of [ρ(g)i j ], as functions on G,
provide examples of matrix coefficients. These particular matrix coefficients are
orthogonal, according to Proposition 7.22b, with

 X                     X                                        |G|(v j , v j )(vi , vi )    |G|
       |ρ(g)i j |2 =         (R(g)v j , vi )(R(g)v j , vi ) =                             =       .
 g∈G                   g∈G
                                                                      dim V                 dim V
                            p
Thus the functions |G|−1 dim V ρ(x)i j form an orthonormal basis of an
n 2 -dimensional subspace VR of C(G, C), where n = dim V . The vector subspace
VR has the following properties:
                                                                              P
      (i) All matrix coefficients of R are in VR , as is seen by expanding v = j c j v j
                         P                                      P
          and v 0 = i di vi and obtaining (R(g)v, v 0 ) = i, j c j d̄i (R(g)v j , vi ) =
          P
            i, j c j d̄i ρ(g)i j .
                                 4. Group Representations                          337

   (ii) VR is invariant under ` and r because
           `(g)(R( · )v, v 0 )(x) = (R(g −1 x)v, v 0 ) = (R(x)v, R(g)v 0 ),
           r(g)(R( · )v, v 0 )(x) = (R(xg)v, v 0 ) = (R(x)R(g)v, v 0 ).
   (iii) Any representation R 0 equivalent to R has VR 0 = VR .
   Let us see how VR decomposes into irreducible subspaces under r. The com-
putation with r in (ii) above shows, for each i, that the vector space of all functions
x 7→ (R(x)v, vi ) for v ∈ V is invariant under r. This is the linear span of the
matrix coefficients obtained from the i th row of [ρ(x)i j ]. Define a linear map A
from V into this vector space by Av = (R( · )v, vi ). It is evident that A is one-one
onto, and moreover AR(g)v = (R( · )R(g)v, vi ) = r(g)(R( · )v, vi ) = r(g)Av.
Thus A exhibits this space, with r as representation, as equivalent to (R, V ). The
space VR is the direct sum of these spaces on i, and the summands are orthogonal,
according to Proposition 7.22b. Thus VR decomposes under r as the direct sum
of dim V irreducible subspaces, each one equivalent to (R, V ).
   One can make a similar analysis with `, using columns in place of rows.
However, this analysis is a little more subtle since VR , acted upon by `, is the
direct sum of dim V copies of the “contragredient” of (R, V ), rather than (R, V )
itself. The details are left to Problems 32–36 at the end of the chapter.
   As R varies over inequivalent representations, these vector spaces VR are
orthogonal, according to Proposition 7.22a. The claim is that their direct sum is
the space C(G, C) of all functions on G. We argue by contradiction. The sum is
invariant under r, and if it is not all of C(G, C), then we can find a nonzero vector
subspace U = { f ( · )} of C(G, C) orthogonal to all the spaces VR such that U is
invariant and irreducible under r. Let u 1 , . . . , u m be an orthonormal basis of U .
Then each function x 7→ (r(x)u j , u i ) is orthogonal to U by construction, i.e.,
                         X
                   0=         (r(x)u j , u i ) f (x)   for all f in U .
                        x∈G

Applying the Riesz Representation Theorem (Theorem 3.12), choose a member
e of U such that f (1) = ( f, e) for all f in U . By definition of r(x) and e, we
find that
                       u(x) = (r(x)u)(1) = (r(x)u, e)
for all u in U . Substitution and use once more of Proposition 7.22b gives
                    X                                    |G|(u j , u)(u i , e)
               0=         (r(x)u j , u i )(r(x)u, e) =
                    x∈G
                                                              dim U
for all i and j. Since we can take u = u j = u 1 and since i is arbitrary, this
equation forces e = 0 and gives a contradiction. We conclude that the sum of all
the spaces VR is all of C(G, C). Let us state the result as a theorem.
338                               VII. Advanced Group Theory

   Theorem 7.23. For the finite group G, let {(Rα , Uα )} be a complete set of
inequivalent irreducible finite-dimensional representations of G, and let VRα be
the linear span of the matrix coefficients of Rα . Then
    (a) the spaces VRα are mutually orthogonal and are invariant under the left
         and right regular representations ` and r,
    (b) the representation (r, VRα ) is equivalent to the direct sum of dim Uα copies
         of (Rα , Uα ),
    (c) the direct sum of the spaces VRα is the space C(G, C) of all complex-
         valued functions on G.
Moreover,
    (d) the number of Rα ’s is finite,
    (e) dim VRα = (dim Uα )2 ,
     (f) any irreducible subspace of (r, C(G, C)) that is equivalent to (Rα , Uα ) is
         contained in VRα .

    Corollary 7.24. Let {(Rα , Uα )} be a complete set of inequivalent irreducible
finite-dimensional representations of the finite group G, and let dα = dim Uα . In
each
© (α) Uα , introduce
                  (α) ™
                         an inner product making (Rα , Uα ) unitary. For each α, let
  u 1 , . . . , u dα be an orthonormal basis of Uα . Then the functions in C(G, C)
              p           °               ¢
given by |G|−1 dα Rα (x)v j(α) , vi(α) form an orthonormal basis of C(G, C).
Consequently every f in C(G, C) satisfies
                     1 X X≥X                °                     ¢ ¥°                      ¢
      f (x) =             dα           f (y) Rα (y)v j(α) , vi(α)      Rα (x)v j(α) , vi(α)
                   |G| α      i, j y∈G

and
           X                    1 X X ØØ X            °                    ¢ØØ2
                 | f (x)|2 =         dα     Ø    f (y) Rα (y)v j(α) , vi(α) Ø .
           x∈G
                               |G| α    i, j y∈G

   REMARKS. The first displayed formula is the Fourier inversion formula
for an arbitrary finite group G and generalizes Theorem 7.17, which gives the
           ° abelian case; in¢ the abelian case all the dimensions dα equal 1, and the
result in the
functions Rα (x)v j(α) , vi(α) are just the multiplicative characters of G. The second
displayed formula is known as the Plancherel formula, a result incorporating
the conclusion about norms in Parseval’s equality (Theorem 3.11d).
  PROOF. This follows form (a), (c), and (e) in Theorem 7.23, together with
Theorem 3.11 and the remarks made before the statement of Theorem 7.23. §

   Corollary 7.25. Let {(Rα , Uα )} be a complete set of inequivalent irreducible
finite-dimensional
       P           representations of the finite group G, and let dα = dim Uα .
Then α dα2 = |G|.
                                         4. Group Representations                                    339

   PROOF. This follows by counting the number of members listed in the
orthonormal basis of C(G, C) given in Corollary 7.24.               §

   We shall make use of a second multiplication on the vector space C(G, C)
besides the pointwise multiplication that itself makes C(G, C) into a ring. The
new multiplication is called convolution and is defined by
                              X                        X
           ( f 1 ∗ f 2 )(x) =   f 1 (y) f 2 (y −1 x) =   f 1 (x y −1 ) f 2 (y),
                                       y∈G                          y∈G

the two expressions on the right being equal by a change of P        variables. The first of
the expressions on the right equals the value of the function y∈G f 1 (y)`(y) f 2 at
x and shows that the convolution is an average of the left translates of f 2 weighted
by f 1 . Convolution is associative because
                             X                                X
 ( f 1 ∗ ( f 2 ∗ f 3 ))(x) =   f 1 (y)( f 2 ∗ f 3 )(y −1 x) =   f 1 (y) f 2 (y −1 x z −1 ) f 3 (z)
                                   y                                  y,z
                            X
                          =  ( f 1 ∗ f 2 )(x z −1 ) f 3 (z) = (( f 1 ∗ f 2 ) ∗ f 3 )(x),
                                   z

and one readily checks that C(G, C) becomes a ring when convolution is used as
the multiplication.
   For anyPfinite-dimensional representation (R, V ) and any v in V , let us define
R( f )v = x∈G f (x)R(x)v. Convolution has the property that
                                       R( f 1 ∗ f 2 ) = R( f 1 )R( f 2 )
because
                      P                                   P
  R( f 1 ∗ f 2 )v =       x ( f1   ∗ f 2 )(x)R(x)v =          x,y f 1 (x y −1 ) f 2 (y)R(x)v
                      P                                      P                     °P                 ¢
                 =        x,y   f 1 (x) f 2 (y)R(x y)v =         x f 1 (x)R(x)         y f 2 (y)R(y)v
                      P
                 =        x   f 1 (x)R(x)R( f 2 )v = R( f 1 )R( f 2 )v.
    We shall combine the notion of convolution with the notion of a “character.” If
(R, V ) is a finite-dimensional representation of G, then the character of (R, V )
is the function χ R given by
                                             χ R (x) = Tr R(x),
with Tr denoting the trace. Equivalent representations have the same character
since Tr(AR(x)A−1 ) = Tr R(x) if A is invertible. Characters have the additional
properties that
     (i) χ R (gxg −1 ) = χ R (x) because Tr R(gxg −1 ) = Tr(R(g)R(x)R(g)−1 ) =
         Tr R(x),
    (ii) χ R1 ⊕···⊕Rn = χ R1 + · · · + χ Rn since the trace of a block-diagonal matrix
         is the sum of the traces of the blocks.
340                               VII. Advanced Group Theory

The character of a 1-dimensional representation is the associated multiplicative
character. Here is an example of a character for a representation on a space of
dimension more than 1; its values are not all in S 1 .

    EXAMPLE. The dihedral group Dn with 2n elements, defined in Section IV.1,
is isomorphic to the matrix group generated by
                   ≥                     ¥            ≥     ¥
                     cos 2π/n − sin 2π/n                1 0
              x = sin 2π/n cos 2π/n        and  y = 0 −1 .

The map carrying each matrix of the group to itself is a representation of Dn on
C2 . The value of the character of this representation is 2 cos 2πk/n on x k for
0 ≤ k ≤ n − 1, and the value of the character is 0 on y and on the remaining
n − 1 elements of the group.

    Computations with characters are sometimes aided by the use of inner products.
If an inner product is imposed on a finite-dimensional complex vector space V
               an orthonormal basis, then the trace of a linear A : V → V is given
and if {vi } isP
by Tr A =P i (Avi , vi ). If R is a representation on V , we consequently have
χ R (x) = i (R(x)vi , vi ).

   Proposition 7.26. Let R, R1 , and R2 be irreducible finite-dimensional repre-
sentations of a finite group G. Then their characters satisfy
        P
    (a)        |χ (x)|2 = |G|,
        Px∈G R
    (b)    x∈G χ R1 (x)χ R2 (x) = 0 if R1 and R2 are inequivalent.

   PROOF. These follow from Schur orthogonality (Proposition 7.22): For (a),
let R act on the vector space V , let d = dim V , introduce an inner product with
respect to which R is unitary, and let {vi } be an orthonormal basis of V . Then
Proposition 7.22b gives
           P                      P °P                           ¢° P                   ¢
               x   |χ R (x)|2 =       x        i   (R(x)vi , vi )     j (R(x)v j , v j )
                                  P        P
                            =       i, j       x (R(x)vi , vi )(R(x)v j , v j )
                                  P                              P
                            =       i, j   |G|d −1 δi j δi j =      i   |G|d −1 = |G|.

Part (b) is proved in the same fashion, using Proposition 7.22a.                            §

   Let us now bring together the notions of convolution and character. A class
function on G is a function f in C(G, C) with f (gxg −1 ) = f (x) for all g and
x in G. That is, class functions are the ones that are constant on each conjugacy
class of the group. Every character is an example of a class function. The class
                                            4. Group Representations                            341

functions form a vector subspace of C(G, C), and the dimension of this vector
subspace equals the number of conjugacy classes in G. Class functions are closed
under convolution because if f 1 and f 2 are class functions, then
                         P                                  P
 ( f 1 ∗ f 2 )(gxg −1 ) = y f 1 (gxg −1 y −1 ) f 2 (y) = y f 1 (xg −1 y −1 g) f 2 (g −1 yg)
                         P
                        = z f 1 (x z −1 ) f 2 (z) = ( f 1 ∗ f 2 )(x).
On an abelian group every member of C(G, C) is a class function.

   Theorem 7.27 (Fourier inversion formula for class functions). For the finite
group G, let {(Rα , Uα )} be a complete set of inequivalent irreducible finite-
dimensional representations of G. If f is a class function on G, then
                           1 X≥X                      ¥
                  f (x) =                f (y)χ Rα (y) χ Rα (x).
                          |G| α y∈G

  REMARK. This result may be regarded as a second way (besides the one in
Corollary 7.24) of generalizing Theorem 7.17 to the nonabelian case.
   PROOF. Using the result and notation of Corollary 7.24, we have
                  P P≥ P                                      ¥
    f (x) = |G|−1    dα           f (y)(Rα (y)vi(α) , v j(α) ) (Rα (x)vi(α) , v j(α) ).
                              α      i, j   y∈G

Replace f (y) by f (gyg −1 ) since f is a class function, and then change variables
and sum over g in G to see that |G| f (x) is equal to
        P P≥P                                                  ¥
  |G|−1     dα         f (y)(Rα (y)Rα (g)vi(α) , Rα (g)v j(α) ) (Rα (x)vi(α) , v j(α) ).
           α         i, j     g,y

Within this expression we have
     P
        (Rα (y)Rα (g)vi(α) , Rα (g)v j(α) )
       g
               P°                                                         ¢
           =           Rα (y)(Rα (g)vi(α) , vk(α) )vk(α) , Rα (g)v j(α)
               g,k
               P
           =         (Rα (g)vi(α) , vk(α) )(Rα (g)v j(α) , Rα (y)vk(α) )
               g,k
                     P
           =   |G|
                dα          (v j(α) , vi(α) )(Rα (y)vk(α) , vk(α) )    by Schur orthogonality
                      k
               |G|     (α) (α)
           =    dα (v j , vi )χ Rα (y)
               |G|
           =    dα δi j χ Rα (y).

Substituting, we obtain the formula of the theorem.                                              §
342                          VII. Advanced Group Theory

   Corollary 7.28. If G is a finite group, then the number of irreducible finite-
dimensional representations of G, up to equivalence, equals the number of con-
jugacy classes of G.
   PROOF. Theorem 7.27 shows that the irreducible characters span the vector
space of class functions. Proposition 7.26b shows that the irreducible characters
are orthogonal and hence are linearly independent. Thus the number of irreducible
characters equals the dimension of the space of class functions, which equals the
number of conjugacy classes.                                                   §

   EXAMPLE. The above information already gives us considerable control over
finding a complete set of inequivalent irreducible finite-dimensional representa-
tions of elementary groups. We know that the number of such representations
equals the number of conjugacy classes and that the sum of the squares of their
dimensions equals |G|. For the symmetric group S3 of order 6, for example, the
conjugacy classes are given by the cycle structures of the possible permutations,
namely the cycle structures of (1), (1 2), and (1 2 3). Hence there are three
inequivalent irreducible representations. The sum of the squares of the three
dimensions is to be 6; thus we have two of dimension 1 and one of dimension 2.
The multiplicative characters 1 and sgn are the two of dimension 1, and the one
of dimension 2 can be taken to be the 2-dimensional representation of D3 whose
character was computed in the example preceding Proposition 7.26.

   One final constraint on the dimensions of the irreducible representations of a
finite group G is as follows.

  Proposition 7.29. If G is a finite group and (R, V ) is an irreducible finite-
dimensional representation of G, then dim V divides |G|.

    For example, if |G| = p2 with p prime, then it follows from Propositions
7.29 and 7.25 that every irreducible finite-dimensional representation of G has
dimension 1, and one can easily conclude from this fact that G is abelian. (See
Problem 14 at the end of the chapter.) Thus we recover as an immediate conse-
quence the conclusion of Corollary 4.39 that groups of order p2 are abelian.
    The proof of Proposition 7.29 is surprisingly subtle. We shall obtain the
theorem as a consequence of Theorem 7.31 below, a theorem that will be used
also in the proof of Burnside’s Theorem in the next section. Theorem 7.31 gives a
little taste of the usefulness of algebraic number theory, and we shall see more of
this usefulness in Chapter IX. The application to Burnside’s Theorem will use the
Fundamental Theorem of Galois Theory, whose proof is deferred to Chapter IX.
    An algebraic integer is any complex number  p that is a rootpof a monic poly-
nomial with coefficients in Z. For example, 2 and 12 (1 + i 3) are algebraic
                                  4. Group Representations                           343

integers because they are roots of X 2 − 2 and X 2 − X + 1, respectively. Any
root of unity is an algebraic integer, being a root of some polynomial X n − 1.
The set of algebraic integers will be denoted in this chapter by O. Before stating
Theorem 7.31, let us establish two elementary facts about O.

   Lemma 7.30. The set O of algebraic integers is a ring, and O ∩ Q = Z.
   PROOF. Suppose that x and y are complex numbers satisfying the polynomial
equations x m +am−1 x m−1 +· · ·+a1 x+a0 = 0 and y n +bn−1 y n−1 +· · ·+b1 y+b0 =
0, each with integer coefficients. Form the subset of C given by
                                        P n−1
                                        m−1 P
                                   M=               Zx k y l .
                                         k=0 l=0
This is a finitely generated subgroup of the abelian group C under addition. It
satisfies
                    m n−1
                    P  P                 n−1
                                         P l m
            xM =          Zx k y l ⊆ M +     Zy x
                   k=1 l=0                    l=0
                         n−1
                         P
                =M+             Zy l (−am−1 x m−1 − · · · − a1 x − a0 ) ⊆ M,
                          l=0
and similarly y M ⊆ M. Hence (x ± y)M ⊆ M and x y ⊆ M.
     To prove that O is a ring, it is enough to show that if N is a nonzero finitely
generated subgroup of the abelian group C under addition and if z is a complex
number with z N ⊆ N , then z is an algebraic integer. By Theorem 4.56, N is a
direct sum of cyclic groups. Since every nonzero member of C has infinite order
additively, these cyclic groups must be copies of Z. So N is free abelian. Let
z 1 , . . . , z n be a Z basis of N . Here n > 0. Since z N ⊆ N , we can find unique
integers ci j such that
                                     Pn
                              zz i =    ci j z j for 1 ≤ i ≤ n.
                                  j=1
                                                  √ z1 !
                                                     .
This equation says that the matrix C = [ci j ] has .. as an eigenvector with
                                                                 zn
eigenvalue z. Therefore the matrix z I − C is singular, and det(z I − C) = 0.
Since det(z I −C) is a monic polynomial expression in z with integer coefficients,
z is an algebraic integer.
   To see that O∩Q = Z, let p and q be relatively prime integers with q > 0, and
suppose that p/q is a root of X n + an−1 X n−1 + · · · + a1 X + a0 with an−1 , . . . , a0
in Z. Substituting p/q for X, setting the expression equal to 0, and clearing
fractions, we obtain pn + an−1 pn−1 q + · · · + a1 pq n−1 + a0 q n = 0. Since q
divides every term here after the first, we conclude that q divides pn . Since
GCD( p, q) = 1, we conclude that q = 1. Thus p/q is in Z.                              §
344                              VII. Advanced Group Theory

    Lemma 7.30 allows us to see that if G is a finite group and χ is the irreducible
character corresponding to an irreducible finite-dimensional representation R,
then χ(x) is an algebraic integer for each x in G. In fact, the subgroup H of ØG
generated by x is cyclic and is in particular abelian. Corollary 7.21 says that R Ø H
is the direct sum of irreducible representations of H , and Corollary 7.19 says that
  Ø such irreducible representation is 1-dimensional. Thus in a suitable basis,
each
R Ø H is diagonal. The diagonal entries must be roots of unity (in fact, N th roots
of unity if x has order N ), and χ(x) is thus a sum of roots of unity. By Lemma
7.30, χ(x) is an algebraic integer.

   Theorem 7.31. Let G be a finite group, (R, V ) be an irreducible finite-
dimensional representation of G, χ be the character of R, and C be a conjugacy
class in G. Denote
              ±    by χ(C) the constant value of χ on the conjugacy class C.
Then |C|χ(C) dim V is an algebraic integer.
   PROOF. If f is any class function
                             P       on G, then R( f ) commutes with each R(x)
for x in G because R( f ) = y f (y)R(y) yields
                                P                                            P
      R(x)R( f )R(x)−1 =             f (y)R(x)R(y)R(x)−1 =                        f (y)R(x yx −1 )
                                 y                                            y
                                P           −1
                                                                   P
                            =        f (x        zx)R(z) =               f (z)R(z) = R( f ).
                                 z                                   z


By Schur’s Lemma (Proposition 7.18), R( f ) is scalar. If C is a conjugacy class,
then the function IC that is 1 on C and is 0 elsewhere is a class function, and hence
R(IC ) is a scalar ∏C . As C varies, the functions IC form a vector-space
                                                                 P                  basis of
the space of class functions. The formula (IC ∗ IC 0 )(x) = y IC (y)IC 0 (y −1 x)
shows that IC ∗ IC 0 is integer-valued, and we have seen that P    the convolution of
two class functions is a class function. Therefore IC ∗ IC 0 P  = C 00 n CC 0 C 00 IC 00 for
suitable integers n CC 0 C 00 . Application of R gives ∏C ∏C 0 = C 00 n CC 0 C 00 ∏C 00 . If we
fix C and let A be the square matrix with entries AC 0 C 00 = n CC 0 C 00 , we obtain
                                                 X
                                 ∏C ∏C 0 =               AC 0 C 00 ∏C 00 .
                                                  C 00


This equation says that the matrix A has the column vector with entries ∏C 00 as
an eigenvector with eigenvalue ∏C . Therefore the matrix ∏C I − A is singular,
and det(∏C I − A) = 0. Since det(∏C I − A) is a monic polynomial expression
                                    is an algebraic integer. Taking the trace of the
in ∏C with integer coefficients, ∏C P
equation R(IC ) = ∏C I , we obtain x∈C χ(x) = ∏C dim V . Since χ(x) = χ(C)
for x in C, the result is that |C|χ(C)/ dim V = ∏C . Since ∏C is an algebraic
integer, |C|χ(C)/ dim V is an algebraic integer.                                  §
                                  5. Burnside’s Theorem                                345

   PROOF THAT THEOREM 7.31 IMPLIES PROPOSITION 7.29. Proposition 7.26a
gives
            P              P P
     |G|      x∈G |χ(x)|
                        2
                                    |χ(x)|2 X ≥ |C|χ(C) ¥
          =               = C x∈C           =                χ(C).
    dim V      dim V            dim V          C
                                                     dim V
Each term in parentheses on the right side is an algebraic integer, according to
Theorem 7.31, and therefore Lemma 7.30 shows that |G|/ dim V is an algebraic
integer. Since |G|/ dim V is in Q, Lemma 7.30 shows that |G|/ dim V is in Z. §


                               5. Burnside’s Theorem

The theorem of this section is as follows.

   Theorem 7.32 (Burnside’s Theorem). If G is a finite group of order pa q b with
p and q prime and with a + b > 1, then G has a nontrivial normal subgroup.

   The argument will use the result Theorem 7.31 from algebraic number the-
ory, and also it will make use of a special case of the Fundamental Theorem
of Galois Theory, whose proof is deferred to Chapter IX. That special case is
the following statement, whose context was anticipated in Section IV.1, where
groups of automorphisms of certain fields were discussed briefly. Since the set
{1, e2πi/n , e2·2πi/n , e3·2πi/n , . . . } is linearly dependent over Q, Proposition 4.1 in
that section implies that the subring Q[e2πi/n ] of C generated by Q and e2πi/n is a
subfield and is a finite-dimensional vector space over Q. According to Example 9
of that section, the group 0 = Gal(Q[e2πi/n ]/Q) of automorphisms of Q[e2πi/n ]
fixing every element of Q is a finite group.

   Proposition 7.33 (special case of the Fundamental Theorem of Galois Theory).
Let n > 0 be an integer, and put K = Q[e2πi/n ]. Let 0 be the finite group of
field automorphisms of K fixing every element of Q. Then the only members β
of K such that σ (β) = β for every σ in 0 are the members of Q.

   Lemma 7.34. Let G be a finite group, (R, V ) be an irreducible finite-
dimensional representation of G, χ be the character of R, and C be a conjugacy
class in G. If GCD(|C|, dim V ) = 1 and if x is in C, then either R(x) is scalar
or χ(x) = 0.
   PROOF. Define χ(C) to be the constant value of χ on C, and put α =
χ(x)/ dim V = χ(C)/ dim V . Since GCD(|C|, dim V ) = 1, we can choose
integers m and n with m|C| + n dim V = 1. Multiplication by α yields
                              m|C|χ(C)
                                       + nχ(C) = α.
                               dim V
346                            VII. Advanced Group Theory

Theorem 7.31 shows that the coefficients |C|χ(C)
                                               dim V and χ(C) of m and n on the
left side are algebraic integers, and therefore α is an algebraic integer. As we
observed toward the end of the previous section, χ(x) = χ(C) is the sum of
dim V roots of unity. Since α = χ(C)/ dim V , we see that |α| ≤ 1 with equality
only if all the roots of unity are equal, in which case R(x) is scalar. In view of
the hypothesis, we may assume that |α| < 1. We shall show that α = 0.
    Let K = Q[e2πi/|G| ] be the smallest subfield of C containing Q and the
complex number e2πi/|G| , and let 0 be the group of field automorphisms of K
that fix every element of Q. We know that K is finite-dimensional over Q and
that 0 is a finite group, and Proposition 7.33 shows that the only members of K
fixed by every element of 0 are the members of Q.
    Our element x of G has x |G| = 1. Thus every root of unity contributing
to χ(x) is a |G|th root of unity and is in K . Therefore the algebraic integer α
is in K . If σ is in 0, each of the |G|th roots of unity is mapped by σ to some
complex number x satisfying x |G| = 1, and hence the member σ (α) of K satisfies
|σ (α)| ≤ 1. Also, σ (α) is an algebraic integer, as we see by applying σ to the
monic equation with integer coefficients
                               Q           satisfied by α, and we are assuming that
|α| < 1. Consequently β = σ ∈0 σ (α) is an algebraic integer and has absolute
value < 1. A change of variables in the product shows that β is fixed by every
member of 0, and we see from the previous paragraph that β is in Q. By Lemma
7.30, β is in Z. Being of absolute value less than 1, it is 0. Thus α = 0, and
χ(x) = 0.                                                                        §

   Lemma 7.35. Let G be a finite group, and let C be a conjugacy class in G
such that |C| = pk for some prime p and some integer k > 0. Then there exists
an irreducible finite-dimensional representation R 6= 1 of G with R(x) scalar for
every x in C. Consequently G is not simple.

   PROOF. The conjugacy class C cannot be {1} because |{1}| 6= pk with k > 0.
Let χreg be the character of the right regular representation r of G on C(G, C). If
Ig denotes the function that is 1 at g and is 0 elsewhere, thenP   the functions Ig form
an orthonormal basis of C(G, C), and therefore χreg (x) = g∈G (r(x)Ig , Ig ) =
P
   g∈G (I gx −1 , I g ). Every term on the right side is 0 if x 6= 1, and thus Theorem
7.23 gives
                                          X
                       0 = χreg (x) = 1 +     dχ χ(x)       for x ∈ C,                (∗)
                                        χ6=1

the sum being taken over all irreducible characters other than 1, with dχ being
the dimension of an irreducible representation corresponding to χ. Let Rχ be an
irreducible representation with character χ. Any χ such that p does not divide
dχ has GCD(|C|, dχ ) = 1 since |C| is assumed to be a power of p. Arguing by
                                6. Extensions of Groups                          347

contradiction, we may assume that no such χ has Rχ (x) scalar, and then Lemma
7.34 says that χ(x) = 0 for all such χ. Hence (∗) simplifies to
                                X
                 0=1+                   dχ χ(x)     for x ∈ C.            (∗∗)
                           χ6=1, p divides dχ
Since χ(x) is an algebraic integer, Lemma 7.30 shows that this equation is of the
form 1 + pβ = 0, where β is an algebraic integer. Then β = −1/ p shows that
−1/ p is an algebraic integer. Since −1/ p is in Q, Lemma 7.30 shows that it
must be in Z, and we have arrived at a contradiction. Thus there must have been
some χ with Rχ (x) scalar for x in C.
   The set of g in G for which this Rχ has Rχ (g) scalar is a normal subgroup of
G that contains x and cannot therefore be {1}. Assume by way of contradiction
that G is simple. Then Rχ (g) is scalar for all g in G. Since Rχ is irreducible,
Rχ is 1-dimensional. Then the commutator subgroup G 0 of G is contained in the
kernel of Rχ . Since Rχ 6= 1, G 0 is not all of G. Since G 0 is normal, G 0 = {1},
and we conclude that G is abelian. But the given G has a conjugacy class with
more than one element, and we have arrived at a contradiction.                  §
   PROOF OF THEOREM 7.32. Corollary 4.38 shows that a group of prime-power
order has a center different from {1}, and we may therefore assume that p 6= q,
a > 0, and b > 0. Let H be a Sylow q-subgroup. Applying Corollary 4.38,
let x be a member of the center Z H of H other than 1. The centralizer Z G ({x})
                                                          0
is a subgroup containing H , and it therefore has order pa q b . If a 0 = a, then
x is in the center of G, and the powers of x form the desired proper normal
subgroup of G. Thus a 0 < a. By Proposition 4.37 the conjugacy class C of x has
        0          0
|G|/ pa q b = pa−a elements with a − a 0 > 0. By Lemma 7.35, G is not simple.
                                                                               §

                            6. Extensions of Groups

In Section IV.8 we examined composition series for finite groups. For a given
finite group, a composition series consists of a decreasing sequence of subgroups
starting with the whole group and ending with {1}, each normal in the next larger
one, such that the successive quotient groups are simple. The Jordan–Hölder
Theorem (Corollary 4.50) assured us that the set of successive quotients, up to
isomorphism, is independent of the choice of composition series. This theorem
raises the question of reconstructing the whole group from data of this kind.
Consider a single step of the process. If we know the normal subgroup and the
simple quotient that it yields at a certain stage, what are the possibilities for the
next-larger subgroup? We study this question and some of its ramifications in
this section, dropping any hypotheses that are not helpful in the analysis. Here is
an example that we shall carry along.
348                               VII. Advanced Group Theory

   EXAMPLE. Suppose that the normal subgroup is the cyclic group C4 and that
the quotient is the cyclic group C2 . The whole group has to be of order 8, and
the classification of groups of order 8 done in Problems 39–44 at the end of
Chapter IV tells us that there are four different possibilities for the whole group:
the abelian groups C4 × C2 and C8 , the dihedral group D4 , and the quaternion
group H8 .

   Let us establish a framework for the general problem. We start with a group E,
a normal subgroup N , and the quotient G = E/N . We seek data that determine
the group law in E in terms of N and G. For each member u of G, fix a coset
representative ū in E such that ū N = u. Since N is normal, the element ū of E
yields an automorphism ( · )u of N defined by x u = ūx ū −1 . In addition, the fact
that G is a group says that any two of our representatives ū and v̄ have

                  ū v̄ = a(u, v)uv        for some unique a(u, v) in N .

The set of all elements a(u, v) for this choice of coset representatives is called a
factor set, and E is called a group extension of N by the group3 G.
   The automorphisms and the factor set constructed above have to satisfy two
compatibility conditions, as follows:
     (i) (x v )u = a(u, v)x uv a(u, v)−1 because (x u )v = ū(x v )ū −1 = ū v̄x v̄ −1 ū −1
         = (a(u, v)uv)x(a(u, v)uv)−1 = a(u, v)x uv a(u, v)−1 ,
    (ii) a(v, w)u a(u, vw) = a(u, v)a(uv, w) because (ū v̄)w̄ = a(u, v)uv w̄
         = a(u, v)a(uv, w)uvw and ū(v̄ w̄) = ūa(v, w)vw = a(v, w)u ūvw =
         a(v, w)u a(u, vw)uvw.
Then the multiplication law in E is given in terms of the automorphisms and the
factor set by the formula
   (iii) (x ū)(y v̄) = x y u a(u, v)uv by the computation (x ū)(y v̄) = x y u ū v̄ =
         x y u a(u, v)uv.
Conversely, according to the proposition below, such data determine a group E
with a normal subgroup isomorphic to N and a quotient E/N isomorphic to G.

   Proposition 7.36 (Schreier). Let two groups N and G be given, along with
a family of automorphisms x 7→ x u of N parametrized by u in G, as well as a
function a : G × G → N such that
    (a) (x v )u = a(u, v)x uv a(u, v)−1 for all u and v in G,
    (b) a(v, w)u a(u, vw) = a(u, v)a(uv, w) for all u, v, w in G.
Then the set N × G becomes a group E under the multiplication
    (c) (x, u)(y, v) = (x y u a(u, v), uv),
   3 Warning:   Some authors say “group extension of G by N .”
                                      6. Extensions of Groups                                       349

and this group has a normal subgroup isomorphic to N with quotient group
isomorphic to G. More particularly, the identity of E is (a(1, 1)−1 , 1), the map
x 7→ (xa(1, 1)−1 , 1) of N into E is a one-one homomorphism that exhibits N as a
normal subgroup of E, and the map (x, u) 7→ u of E onto G is a homomorphism
that exhibits G as isomorphic to E/N .
   PROOF. Reverting to the earlier notation, let us write x ū in place of (x, u) for
elements of E. Associativity of multiplication follows from the computation
                       °               ¢
    (x ū y v̄)(z w̄) = x y u a(u, v)uv z w̄                                              by (c)
                           u           uv
                    = x y a(u, v)z a(uv, w)uvw                                            by (c)
                           u           uv          −1
                    = x y a(u, v)z a(u, v) a(u, v)a(uv, w)uvw
                    = x y u a(u, v)z uv a(u, v)−1 a(v, w)u a(u, vw)uvw                    by (b)
                        °              ¢u
                    = x yz v a(v, w) a(u, vw)uvw                                          by (a)
                             °                ¢
                    = (x ū) yz v a(v, w)vw                                               by (c)
                    = (x ū)(y v̄z w̄)                                                    by (c).

   The identity is to be 1̄a(1, 1)−1 . Before checking this assertion, we prove three
preliminary identities. Setting u = v = 1 in (a) and replacing x 1 by x gives4

                        x 1 = a(1, 1)xa(1, 1)−1              for all x ∈ N .                        (∗)

Setting v = w = 1 in (b) gives a(1, 1)u a(u, 1) = a(u, 1)a(u, 1) and hence

                           a(1, 1)u = a(u, 1)            for all u ∈ G.                             (†)

Meanwhile, setting u = v = 1 in (b) gives a(1, w)1 a(1, w) = a(1, 1)a(1, w)
and hence a(1, w)1 = a(1, 1) for all w ∈ G. The left side a(1, w)1 of this last
equality is equal to a(1, 1)a(1, w)a(1, 1)−1 by (∗); canceling a(1, 1) yields

                           a(1, w) = a(1, 1)             for all w ∈ G.                          (††)

Using these identities, we check that a(1, 1)−1 1̄ is a two-sided identity by making
the computations

                (x ū)(a(1, 1)−1 1̄) = x(a(1, 1)−1 )u a(u, 1)ū                by (c)
                                                      −1 u           u
                                       = x(a(1, 1) ) a(1, 1) ū                by (†)
                                       = x ū
    4 The effect of the automorphism x 7→ x 1 is not necessarily trivial since the coset representative

1̄ of 1 is not assumed to be the identity. Thus we must distinguish between x 1 and x.
350                             VII. Advanced Group Theory

and

                (a(1, 1)−1 1̄)(y v̄) = a(1, 1)−1 y 1 a(1, v)v̄        by (c)
                                                −1
                                    = ya(1, 1) a(1, v)v̄              by (∗)
                                    = y v̄                            by (††).
                                                                                 −1
   Let us check that a left inverse for x ū is a(1, 1)−1 a(u −1 , u)−1 (x u )−1 u −1 .
In fact,
  °                            −1       ¢
   a(1, 1)−1 a(u −1 , u)−1 (x u )−1 u −1 (x ū)
                                                     −1          −1
                      = a(1, 1)−1 a(u −1 , u)−1 (x u )−1 x u a(u −1 , u)1̄            by (c)
                      = a(1, 1)−1 1̄,

as required. Thus multiplication is associative, there is a two-sided identity, and
every element has a left inverse. It follows that E is a group.
   The map x ū 7→ u of E into G is a homomorphism by (c), and it is certainly
onto G. Its kernel is evidently the subgroup of all elements xa(1, 1)−1 1̄ in E.
Since
  °                ¢°            ¢
      xa(1, 1)−1 1̄ ya(1, 1)−1 1̄ = xa(1, 1)−1 (ya(1, 1)−1 )1 a(1, 1)1̄           by (c)
                                                −1                      −1
                                    = xa(1, 1) a(1, 1)(ya(1, 1) )1̄               by (∗)
                                    = x ya(1, 1)−1 1̄,

the one-one map x 7→ xa(1, 1)−1 1̄ of N onto the kernel respects the group
structures and is therefore an isomorphism. In other words, the embedded version
of N is the kernel. Being a kernel, it is a normal subgroup.                  §

   EXAMPLE, CONTINUED. Let N = C4 = {1, r, r 2 , r 3 } and G = C2 = {1, u 0 }
with u 20 = 1. The group N has two automorphisms, the nontrivial one fixing 1
and r 2 while interchanging r and r 3 . The automorphism of N from 1 ∈ G has to
be trivial, while the automorphism of N from u 0 ∈ G can be trivial or nontrivial.
In fact,
                           Ω
                             trivial         for E = C4 × C2 and E = C8 ,
      the automorphism is
                             nontrivial      for E = D4 and E = H8 .
In each case the automorphism does not depend on the choice of coset represen-
tatives. The factor sets do depend on the choice of representatives, however. Let
us fix 1̄ as the identity of E and make a particular choice of u 0 for each E. Then
                                    6. Extensions of Groups                                 351

the definition of factor set shows that a(1, 1) = a(u 0 , 1) = a(1, u 0 ) = 1, and
the only part of the factor set yet to be determined is a(u 0 , u 0 ). Let us consider
matters group by group. For C4 × C2 , we can take u 0 to be the generator of the C2
factor; this has square 1, and hence a(u 0 , u 0 ) = 1. For C8 = {1 θ, θ 2 , . . . , θ 7 },
let us think of N as embedded in E with r = θ 2 . The element u 0 can be any odd
power of θ; if we take u 0 = θ, then (u 0 )2 = θ 2 = r, and hence a(u 0 , u 0 ) = r.
For E = D4 , the example following Proposition 7.8 shows that we may view the
elements as the rotations 1, r, r 2 , r 3 and the reflections s, rs, r 2 s, r 3 s for particular
choices of r and s. We can take u 0 to be any of the reflections, and then (u 0 )2 = 1
and a(u 0 , u 0 ) = 1. Finally for E = H8 = {±1, ±i, ±j, ±k}, let us say that N
is embedded as {±1, ±i}. Then u 0 can be any of the four elements ±j and ±k.
Each of these has square −1, and hence a(u 0 , u 0 ) = −1. For the choices we
have made, we therefore have
                              
                              1           for E = C4 × C2 and E = D4 ,
               a(u 0 , u 0 ) = r           for E = C8 ,
                              
                                −1         for E = H8 .
The formula of Proposition 7.36a reduces to (x v )u = x uv since N is abelian, and
it is certainly satisfied. The formula for Proposition 7.36b is a(v, w)u a(u, vw) =
a(u, v)a(uv, w). This is satisfied for E = C4 × C2 and E = D4 since a( · , · ) is
identically 1. For the other two cases the values of a( · , · ) lie in the 2-element
subgroup of N that is fixed by the nontrivial automorphism, and hence a(v, w)u =
a(v, w) in every case. The formula to be checked reduces to a(v, w)a(1, 1) =
a(1, 1)a(v, w) by (††) if u = 1, to a(1, 1)a(u, w) = a(1, 1)a(u, w) by (†) and
(††) if v = 1, and to a(1, 1)a(u, v) = a(u, v)a(1, 1) by (†) if w = 1. Thus all
that needs checking is the case that u = v = w = u 0 , and then the formula in
question reduces to a(u 0 , u 0 )a(1, 1) = a(u 0 , u 0 )a(1, 1) by (†) and (††).

   Let us examine for a particular extension the dependence of the automorphisms
and factor set on the choice of coset representatives. Returning to our original
construction, suppose that we change the coset representatives of the members
of G, associating a member e     u to u ∈ G in place of ū. We then obtain a new
                                                                        ∗
automorphism of N corresponding to u, and we write it as x 7→ x u = e           u −1
                                                                             u xe
instead of x 7→ x u = ūx ū −1 . To quantify matters, we observe that e
                                                                       u lies in the
same coset of N as does ū. Thus e   u = α(u)ū for some function α : G → N , and
the function α can be absolutely arbitrary. In terms of this function α, the two
automorphisms are related by
                  ∗
               xu = e  u −1 = α(u)ūx ū −1 α(u)−1 = α(u)x u α(u)−1 .
                    u xe
                                  u } of coset representatives is denoted by
If the factor set for the system {e
{b(u, v)}, then we have b(u, v)α(uv)uv = b(u, v)fuv = euev = α(u)ūα(v)v̄ =
352                            VII. Advanced Group Theory

α(u)α(v)u a(u, v)uv. Equating coefficients of uv, we obtain

                        b(u, v) = α(u)α(v)u a(u, v)α(uv)−1 .

Accordingly we say that a group extension of N by G determined by automor-
phisms x 7→ x u and a factor set a(u, v) is equivalent, or isomorphic, to a group
                                                                ∗
extension of N by G determined by automorphisms x 7→ x u and a factor set
b(u, v) if there is a function α : G → N such that
        ∗
      x u = α(u)x u α(u)−1        and       b(u, v) = α(u)α(v)u a(u, v)α(uv)−1

for all u and v in G. It is immediate that equivalence of group extensions is an
equivalence relation.

   Proposition 7.37. Suppose that E 1 and E 2 are group extensions of N by G
with respective inclusions i 1 : N → E 1 and i 2 : N → E 2 and with respective
quotient homomorphisms ϕ1 : E 1 → G and ϕ2 : E 2 → G. If there exists a group
isomorphism 8 : E 1 → E 2 such that the two squares in Figure 7.4 commute, then
the two group extensions are equivalent. Conversely if the two group extensions
are equivalent, then there exists a group isomorphism 8 : E 1 → E 2 such that the
two squares in Figure 7.4 commute.
                                     i1            ϕ1
                              N −−−→ E 1 −−−→             G
                              ∞                          ∞
                              ∞                          ∞
                              ∞     8y                    ∞
                                     i2            ϕ2
                              N −−−→ E 2 −−−→ G

                     FIGURE 7.4. Equivalent group extensions.

   REMARKS. The commutativity of the squares is important. Just because two
group extensions of N by G are isomorphic as groups does not imply that they
are equivalent group extensions. An example is given in Problem 19 at the end
of the chapter.
     PROOF. For the direct part, suppose that 8 exists. For each u in G, select ū in
E 1 with ϕ1 (ū) = u. Then we can form the extension data {x 7→ x u } and {a(u, v)}
for E 1 relative to the normal subgroup i 1 (N ) and the system {ū | u ∈ G} of coset
representatives. When reinterpreted in terms of N , E 1 , and G, these data become
{i 1−1 (x) 7→ i 1−1 (x u )} and {i 1−1 (a(u, v))}.
     Application of 8 to the coset i 1 (N )ū yields i 2 (N )8(ū) since 8 i 1 = i 2 , and
8(ū) is a member of E 2 with ϕ2 (8(ū)) = ϕ1 (ū) = u. Setting e            u = 8(ū), we
see that 8(i 1 (N )ū) is the coset i 2 (N )e  u of i 2 (N ) in E 2 . Thus we can determine
                                     6. Extensions of Groups                                   353

extension data for E 2 relative to i 2 (N ) and the system {e u | u ∈ G}, and we can
transform them by i 2−1 to obtain data relative to N , E 2 , and G.
    The claim is that the data relative to N , E 2 , and G match those for N , E 1 , and
                                                                            ∗
G. The automorphisms of N from E 2 are the maps i 2−1 (x 0 ) 7→ i 2−1 (x 0u ), where
    ∗
x 0u = e     u −1 . From i 2 = 8 i 1 and the fact that each of these maps is one-one,
        u x 0e
we obtain i 2−1 = i 1−1 8−1 on i 2 (N ). Substitution shows that the automorphisms
of N from E 2 are
                                                    ∗
               i 1−1 (8−1 (x 0 )) 7→ i 1−1 (8−1 (x 0u )) = i 1−1 (8−1 (e    u −1 ))
                                                                       u x 0e
                                = i 1−1 (ū8−1 (x 0 )ū −1 ) = i 1−1 ((8−1 (x 0 ))u ).

If we set x 0 = 8(x) with x in i 1 (N ), then the automorphisms of N from E 2 take
the form i 1−1 (x) 7→ i 1−1 (x u ). Thus they match the automorphisms of N from E 1 .
     In the case of the factor sets, we have ū v̄ = a(u, v)uv. Application of 8 gives
ue
e  v = 8(a(u, v))f     u v. Thus the factor set for E 2 relative to N is {i 2−1 8(a(u, v))}.
          −1         −1
Since i 2 8 = i 1 , this matches the factor set for E 1 relative to N .
     We turn to the converse part. Suppose that the multiplication law in E 1 is
(i 1 (x)ū)(i 1 (y)v̄) = i 1 (x)i 1 (y)u i 1 (a(u, v))uv for x and y in N , and that the
                                                                         ∗
multiplication law in E 2 is (i 2 (x)e              v ) = i 2 (x)i 2 (y)u i 2 (b(u, v))f
                                         u )(i 2 (y)e                                  u v. Here ū
and v̄ are preimages of u and v under ϕ1 , ande        u ande  v are preimages of u and v under
                                                                                ∗              ∗
ϕ2 . Define automorphisms of N by x u = i 1−1 (i 1 (x)u ) and x u = i 2−1 (i 2 (x)u ).
We can then rewrite the multiplication laws as

                         (i 1 (x)ū)(i 1 (y)v̄) = i 1 (x y u a(u, v))uv
                                                          ∗
and                                         v ) = i 2 (x y u b(u, v))f
                                 u )(i 2 (y)e
                         (i 2 (x)e                                   u v.

The assumption that E 1 is equivalent to E 2 as an extension of N by G means that
there exists a function α : G → N such that
        ∗
      x u = α(u)x u α(u)−1           and        b(u, v) = α(u)α(v)u a(u, v)α(uv)−1

for all u and v in G. Define 8 : E 1 → E 2 by

                                8(i 1 (x)ū) = i 2 (xα(u)−1 )e
                                                             u.

Certainly 8 is one-one onto. It remains to check that 8 is a group homomorphism
and that the squares commute in Figure 7.4.
  To check that 8 : E 1 → E 2 is a group homomorphism, we compare

      8(i 1 (x)ūi 1 (y)v̄) = 8(i 1 (x y u a(u, v))uv = i 2 (x y u a(u, v)α(uv)−1 )f
                                                                                   uv
354                                VII. Advanced Group Theory

with the product
                                                      u i 2 (yα(v)−1 )e
             8(i 1 (x)ū)8(i 1 (y)v̄) = i 2 (xα(u)−1 )e               v
                                                                        ∗
                                        = i 2 (xα(u)−1 (yα(v)−1 )u b(u, v))f
                                                                           u v.
Since
                          ∗                                     ∗
  α(u)−1 (yα(v)−1 )u b(u, v) = α(u)−1 (yα(v)−1 )u α(u)α(v)u a(u, v)α(uv)−1
                                     = (yα(v)−1 )u α(v)u a(u, v)α(uv)−1
                                     = y u a(u, v)α(uv)−1 ,
these expressions are equal, and 8 is a group homomorphism. Thus 8 is a group
isomorphism.
   Now we check the commutativity of the squares. The computation
                 ϕ2 8(i 1 (x)ū) = ϕ2 (i 2 (xα(u)−1 )e
                                                     u ) = u = ϕ1 (i 1 (x)ū)
shows that the right-hand square commutes.
     For the left-hand square we use the fact recorded in the statement of Proposition
7.36 that i 1 (a(1, 1)−1 )1̄ is the identity of E 1 and i 2 (b(1, 1)−1 )e
                                                                        1 is the identity of
E 2 . Therefore 8 i 1 (x) = 8(i 1 (xa(1, 1)−1 )1̄) = i 2 (xa(1, 1)−1 α(1)−1 )e     1. Since
i 2 (x) = xb(1, 1)−1e  1, the left-hand square commutes if b(1, 1) = α(1)a(1, 1).
This formula follows from (∗) in the proof of Proposition 7.36 by the computation
 b(1, 1) = α(1)α(1)1 a(1, 1)α(1)−1 = α(1)a(1, 1)α(1)α(1)−1 = α(1)a(1, 1),
and thus the left-hand square indeed commutes.                                                     §

   For the remainder of this section, let us assume that N is abelian. In this
case Proposition 7.36a reduces to the identity (x v )u = x uv for all u and v in
G independently of the choice of representatives, just as it does in the example
we studied with N = C4 and G = C2 . In the terminology of Section IV.7, G
acts on N by automorphisms.5 Suppose we fix such an action τ : G → Aut N
by automorphisms and consider all extensions of N by G built from τ . In our
example we are thus to consider E equal to C4 × C2 or C8 , which are built with
the trivial τ , or else E equal to D4 or H8 , which are built with the nontrivial τ (in
which the nontrivial element of G acts by the nontrivial automorphism of N ).
   Since N is abelian, let us switch to additive notation for N and to ordinary
function notation for τ (w), rewriting the formula of Proposition 7.36b as
                    τ (u)a(v, w) + a(u, vw) = a(u, v) + a(uv, w).
    5 The formula (x v )u = x uv correctly corresponds to a group action with the group on the left as

in Section IV.7.
                                  6. Extensions of Groups                                355

This condition is preserved under addition of factor sets as long as τ does not
change, it is satisfied by the 0 factor set, and the negative of a factor set is again
a factor set. Therefore the factor sets for this τ form an abelian group.
   Two factor sets for this τ are equivalent (in the sense of yielding equivalent
group extensions) if and only if their difference is equivalent to 0, and a(u, v) is
equivalent to 0 if and only if

                         a(u, v) = α(uv) − α(u) − τ (u)α(v)

for some function α : G → N . The set of factor sets for this τ that are equivalent
to 0 is thus a subgroup,6 and we arrive at the following result.

   Proposition 7.38. Let G and N be groups with N abelian, and suppose that
τ : G → Aut N is a homomorphism. Then the set of equivalence classes of
group extensions of N by G corresponding to the action τ : G → Aut N is
parametrized by the quotient of the abelian group of factor sets by the subgroup
of factor sets equivalent to 0.

   The extension E corresponding to the 0 factor set is of special interest. In
this case the multiplication law for the coset representatives is ū v̄ = uv since
the member a(u, v) = 0 of N is to be interpreted multiplicatively in this product
formula. Consequently the map u 7→ ū of G into E is a group homomorphism,
necessarily one-one, and we can regard G as a subgroup of E. Proposition 4.44
allows us to conclude that E is the semidirect product G ×τ N . The multiplication
law for general elements of E, with multiplicative notation used for N , is

                               (x ū)(y v̄) = x(τ (u)y)uv.

   It is possible also to describe explicitly the extension one obtains from the
sum of two factor sets corresponding to the same τ , but we leave this matter
to Problems 20–23 at the end of the chapter. The operation on extensions that
corresponds to addition of factor sets in this way is called Baer multiplication.
What we saw in the previous paragraph says that the group identity under Baer
multiplication is the semidirect product.

   The two conditions, the compatibility condition on a factor set given in Proposi-
tion 7.36b and the condition with α in it for equivalence to 0, are of a combinatorial
type that occurs in many contexts in mathematics and is captured by the ideas
of “homology” and “cohomology.” For the current situation the notion is that of
cohomology of groups, and we shall define it now. The subject of homological
   6 One  can legitimately ask whether an arbitrary α : G → N leads to a factor set under the
definition a(u, v) = α(uv) − τ (v)α(u) − α(v), and one easily checks that the answer is yes.
Alternatively, one can refer to the case n = 2 in the upcoming Proposition 7.39.
356                                VII. Advanced Group Theory

algebra, which is developed in Chapter IV of Advanced Algebra, puts cohomology
of groups in a wider context and explains some of its mystery.
   We fix an abelian group N , a group G, and a group action τ of G on N
by automorphisms. It is customary to suppress τ in the notation for the group
action, and we shall follow that convention. For integers n ∏ 0, one begins with
the abelian group C n (G, N ) of n-cochains of G with coefficients in N . This is
defined by
                             Ω
                 n              N                          if n = 0,
               C (G, N ) = ©           Qn             ™
                                   f : k=1 G → N           if n > 0.
            n
In words, C (G, N ) is the set of all functions into N from the n-fold direct product
of G with itself. The coboundary map δn : C n (G, N ) → C n+1 (G, N ) is the
homomorphism of abelian groups defined by
                                    (δ0 f )(g1 ) = g1 f − f
and by
  (δn f )(g1 , . . . , gn+1 ) = g1 ( f (g2 , . . . , gn+1 ))
                                   Pn
                                +       (−1)i f (g1 , . . . , gi−1 , gi gi+1 , gi+2 , . . . , gn+1 )
                                   i=1
                                + (−1)n+1 f (g1 , . . . , gn )
for n > 0. We postpone to the end of this section the proof of the following result.

   Proposition 7.39. δn δn−1 = 0 for all n ∏ 1.

   It follows from Proposition 7.39 that image δn−1 ⊆ ker δn for all n ∏ 1. Thus
if we define abelian groups by
                     Z n (G, N ) = ker δn ,
                                   Ω
                       n             0                for n = 0,
                     B (G, N ) =
                                     image δn−1       for n > 0,
then B n (G, N ) ⊆ Z n (G, N ) for all n, and it makes sense to define the abelian
groups
                H n (G, N ) = Z n (G, N )/B n (G, N )      for n ∏ 0.
The elements of Z n (G, N ) are called n-cocycles, the elements of B n (G, N ) are
called n-coboundaries, and H n (G, N ) is called the n th cohomology group of G
with coefficients in N .

  EXAMPLES IN LOW DEGREE.
  DEGREE 0. Here (δ0 f )(u) = u f − f with f in N and u in G. The cocycle
condition is that this is 0 for all u. Thus f is to be fixed by G. We say that an f
                                 6. Extensions of Groups                             357

fixed by G is an invariant of the group action. The space of invariants is denoted
by N G . By convention above, we are taking B 0 (G, N ) = 0. Thus
                                  H 0 (G, N ) = N G .
   DEGREE 1. Here (δ1 f )(u, v) = u( f (v)) − f (uv) + f (u) with f a function
from G to N . The cocycle condition is that
                  f (uv) = f (u) + u( f (v))         for all u, v ∈ G.
A function f satisfying this condition is called a crossed homomorphism of G
into N . A coboundary is a function f : G → N of the form f (u) = (δ0 x)(u) =
ux − x for some x ∈ N . Then H 1 (G, N ) is the quotient of the group of crossed
homomorphisms by this subgroup. In the special case that the action of G on N is
trivial, the crossed homomorphisms reduce to ordinary homomorphisms of G into
N , and every coboundary is 0. Thus H 1 (G, N ) is the group of homomorphisms
of G into N if G acts trivially on N .
    DEGREE 2. Here f is a function from G × G into N , and
       (δ2 f )(u, v, w) = u( f (v, w)) − f (uv, w) + f (u, vw) − f (u, v).
The cocycle condition is that
    u( f (v, w)) + f (u, vw) = f (uv, w) + f (u, v)           for all u, v, w ∈ G.
This is the same as the condition that { f (u, v)} be a factor set for extensions of
N by G relative to the given action of G on N by automorphisms. A coboundary
is a function f : G × G → N of the form
 f (u, v) = (δ0 α)(u, v) = u(α(v)) − α(uv) + α(u)             for some α : G → N .
This is the same as the condition that {− f (u, v)} be a factor set equivalent to 0.
Thus we can restate Proposition 7.38 as follows.

   Proposition 7.40. Let G and N be groups with N abelian, and suppose that
τ : G → Aut N is a homomorphism. Then the set of equivalence classes of
group extensions of N by G corresponding to the action τ : G → Aut N is
parametrized by H 2 (G, N ).

    Since group extensions have such a nice interpretation in terms of cohomology
groups H 2 , it is reasonable to look for a nice interpretation for H 1 as well. Indeed,
H 1 has an interpretation in terms of uniqueness up to inner isomorphisms for
semidirect-product decompositions. We continue with the abelian group N , a
group G, and a group action τ of G on N by automorphisms. A semidirect product
E = G ×τ N is an allowable extension. Since G embeds as a subgroup of E, we
are given a one-one group homomorphism u 7→ ū of G into E. The construction
at the beginning of this section works with the set ū of coset representatives, and
they have ū v̄ = uv.
358                              VII. Advanced Group Theory

   Suppose that the semidirect product can be formed by a second one-one group
homomorphism u 7→ e      u of G into E. If we write e   u = α(u)ū for a function
α : G → N , then we know from earlier in the section that the extensions formed
                    u } are equivalent. Because G maps homomorphically into E
from {ū} and from {e
for both systems, the factor sets are 0 in both cases. Consequently the function α
must satisfy
                          α(uv) − α(u) − τ (u)α(v) = 0.
This is exactly the condition that α : G → N be a 1-cocyle. Thus the group
Z 1 (G, N ) parametrizes all ways that we can embed G as a complementary
subgroup to N in the semidirect product E = G ×τ N .
    A relatively trivial way to construct a one-one group homomorphism u 7→ e          u
                                                                              −1
from u 7→ ū is to form, in the usual multiplicative notation, e       u = x0 ūx0 for
some x0 ∈ N . Then e   u = x0−1 ūx0 1̄ = x0−1 (τ (u)(x0 ))ū, and the additive notation
for α(u) has α(u) = τ (u)(x0 ) − x0 . Referring to our earlier computations in
degree 1, we see that α is in the group B 1 (G, N ) of coboundaries.
    The conclusion is that H 1 (G, N ) parametrizes all ways, modulo relatively
trivial ways, that we can embed G as a complementary subgroup to N in the
semidirect product E = G ×τ N .
    As promised, we now return to the proof of Proposition 7.39.

   PROOF OF PROPOSITION 7.39. For n = 1, we have
             (δ1 δ0 f )(u, v) = u((δ0 f )(v)) − (δ0 f )(uv) + (δ0 f )(u)
                              = u(v f − f ) − (uv f − f ) + (u f − f ) = 0.
For n > 1, we begin with
   (δn δn−1 f )(g1 , . . . , gn+1 ) = g1 ((δn−1 f )(g2 , . . . , gn+1 ))
                                         Pn
                                      + (−1)i (δn−1 f )(g1 , . . . , gi gi+1 , . . . , gn+1 )
                                          i=1
                                     + (−1)n+1 (δn−1 f )(g1 , . . . , gn )
                                  = I + II + III.
Here
                                         n
                                         P
 I = g1 g2 ( f (g3 , . . . , gn+1 )) +         (−1)i−1 g1 ( f (g2 , . . . , gi gi+1 , . . . , gn+1 ))
                                         i=2
        + (−1)n g1 ( f (g2 , . . . , gn )) = IA + IB + IC,
                                                n
                                                P
II = −(δn−1 f )(g1 g2 , g3 , . . . , gn )+          (−1)i (δn−1 f )(g1 , . . . , gi gi+1 , . . . , gn+1 )
                                                i=2
      = IIA + IIB,
                                           6. Extensions of Groups                                         359

III = (−1)n+1 g1 ( f (g2 , . . . , gn )) + (−1)n+1 (−1) f (g1 g2 , g3 , . . . , gn )
                       n−1
                       P
      + (−1)n+1              (−1)i f (g1 , . . . , gi gi+1 , . . . , gn )
                       i=2
      + (−1)n+1 (−1)n f (g1 , . . . , gn−1 )
   = IIIA + IIIB + IIIC + IIID.

Terms IIA and IIB decompose further as

IIA = −g1 g2 ( f (g3 , . . . , gn+1 )) + f (g1 g2 g3 , g4 , . . . , gn+1 )
       Pn
      − (−1)i+1 f (g1 g2 , . . . , gi gi+1 , . . . , gn+1 ) − (−1)n f (g1 g2 , g3 , . . . , gn )
            i=3

     = IIAa + IIAb + IIAc + IIAd,
        n
        P
IIB =         (−1)i g1 ( f (g2 , . . . , gi gi+1 , . . . , gn+1 ))
        i=2
        + (−1)2 (−1) f (g1 g2 g3 , g4 , . . . , gn+1 )
          Pn
        +    (−1)i (−1) f (g1 g2 , . . . , gi gi+1 , . . . , gn+1 )
            i=3
            n
            P             i−2
                          P
        +         (−1)i         (−1) j f (g1 , . . . , g j g j+1 , . . . , gi gi+1 , . . . , gn+1 )
            i=2           j=2
            n
            P
        +         (−1)i (−1)i−1 f (g1 , . . . , gi−1 gi gi+1 , . . . , gn+1 )
            i=3
            n−1
            P
        +         (−1)i (−1)i f (g1 , . . . , gi gi+1 gi+2 , . . . , gn+1 )
            i=2
            n−2
            P                n
                             P
        +         (−1)i            (−1) j−1 f (g1 , . . . , gi gi+1 , . . . , g j g j+1 , . . . , gn+1 )
            i=2           j=i+2
            n−1
            P
        +         (−1)i (−1)n f (g1 , . . . , gi gi+1 , . . . , gn )
            i=2
        + (−1)n (−1)n f (g1 , . . . , gn−1 )
     = IIBa + IIBb + IIBc + IIBd + IIBe + IIBf + IIBg + IIBh + IIBi.

Inspection shows that we have cancellation between term IA and term IIAa, term
IB and term IIBa, term IC and term IIIA, term IIAb and term IIBb, term IIAc and
term IIBc, term IIAd and term IIIB, term IIBd and term IIBg, term IIBe and term
IIBf, term IIBh and term IIIC, and term IIBi and term IIID. All the terms cancel,
and we conclude that δn δn−1 f = 0.                                           §
360                                VII. Advanced Group Theory

                                          7. Problems

1.    Using Burnside’s Theorem and Problem 34 at the end of Chapter IV, show that
      60 is the smallest possible order of a nonabelian simple group.
2.    A commutator in a group is any element of the form x yx −1 y −1 .
      (a) Prove that the inverse of a commutator is a commutator.
      (b) Prove that any conjugate of a commutator is a commutator.
3.    Let a and b be elements of a group G. Prove that the subgroup generated by a
      and b is the same as the subgroup generated by bab2 and bab3 .
4.    A subgroup H of a group G is said to be characteristic if it is carried into itself
      by every automorphism of G.
      (a) Prove that characteristic implies normal.
      (b) Prove that the center Z G of G is a characteristic subgroup.
      (c) Prove that the commutator subgroup G 0 of G is a characteristic subgroup.
5.    In the terminology of the previous problem, which subgroups of the quaternion
      subgroup H8 are characteristic?
6.    Is every finite group finitely presented? Why or why not?
7.    Let G = SL(2, R), and let G 0 ≥be the
                                          ¥ commutator subgroup.
                                            1 t
      (a) Prove that every element          01
                                                  is in G 0 .
                     G 0 = G.¥
      (b) Prove that ≥
                       −1 0
      (c) Prove that     0 −1
                               is not a commutator even though it is in G 0 .
8.    Problem 53 at the end of Chapter IV produced a group G of order 27 generated
      by two elements a and b satisfying a 9 = b3 = b−1 aba −4 = 1. Prove that G is
      given by generators and relations as
                                      ≠                          Æ
                                  G = a, b; a 9 , b3 , b−1 aba −4 .

9.    Let G n be given by generators and a single relation as
                       ≠                                                                 Æ
                  G n = x1 , y1 , . . . , xn , yn ; x1 y1 x1−1 y1−1 · · · xn yn xn−1 yn−1 .

      Prove that G n /G 0n is free abelian of rank 2n, and conclude that the groups G n are
      mutually nonisomorphic as n varies. (Educational note related to topology: The
      group G n may be shown to be the fundamental group of a compact orientable
      2-dimensional manifold without boundary and with n handles.)
10. Prove that a free group of finite rank n cannot be generated by fewer than n
    elements.
                                         7. Problems                                       361

11. Let F be the free group on generators a, b, c, and let H be the subgroup generated
    by all words of length 2.
    (a) Find coset representatives g such that G is the disjoint union of the cosets
         Hg.
    (b) Find a free basis of H .
12. For the free group on generators x and y, prove that the elements y, x yx −1 ,
    x 2 yx −2 , x 3 yx −3 , . . . , constitute a free basis of the subgroup that they generate.
    Conclude that a free group of rank 2 has a free subgroup of infinite rank.
13. Let G = C2 ∗ C2 . Prove that the only quotient groups of G, up to isomorphism,
    are G itself, {1}, C2 , C2 × C2 , and the dihedral groups Dn for n ∏ 3.
14. Prove that if every irreducible finite-dimensional representation of a finite group
    G is 1-dimensional, then G is abelian.
15. Let G be a finitely generated group, and let H be a subgroup of finite index.
    Prove that H is finitely generated.
16. Let N be an abelian group, let G be a group, let τ be an action of G on N by
    automorphisms, and let n > 0 be an integer.
    (a) Prove that if every element of N has finite order dividing an integer m, then
        every member of H n (G, N ) has finite order dividing m.
    (b) Suppose that G is finite and that f is an n-cocycle. Define an (n −1)-cochain
        F by                                    P
                        F(g1 , . . . , gn−1 ) =    f (g1 , . . . , gn−1 , g).
                                                  g∈G

          By summing the cocycle condition for f over the last variable, express
          |G| f (g1 , . . . , gn ) in terms of F, and deduce that |G| f is a coboundary.
          Conclude that every member of H n (G, N ) has order dividing |G|.
17. Let G be a finite group. Suppose that G has a normal abelian subgroup N , and
    suppose that GCD(|N |, |G/N |) = 1. Prove that there exists a subgroup H of G
    such that G is the semidirect product of H and N .
18. Let N be the cyclic group C2 , and let G be an arbitrary group of order 4. Identify
    up to equivalence all group extensions of N by G.
                               L∞
19. Let N = C2 , and let E = n=1      (C2 ⊕ C4 ). Regard E as an extension of N in
    two ways—first by embedding N as one of the summands C2 of E and then by
    embedding N as a subgroup of one of the summands C4 of E. Show that the
    quotient groups E/N in the two cases are isomorphic, that E/N acts trivially on
    N in both cases, and that the two group extensions are not equivalent.
Problems 20–23 concern Baer multiplication of extensions. Let N be an abelian
group, let G be a group, let τ be an action of G on N by automorphisms, and let
E 1 and E 2 be two extensions of N by G relative to τ . Write ϕ1 : E 1 → G and
ϕ2 : E 2 → G for the quotient mappings. Let (E, E 0 ) denote the subgroup of all
362                             VII. Advanced Group Theory

members (e1 , e2 ) of E 1 × E 2 for which ϕ1 (e1 ) = ϕ2 (e2 ). Writing the operation in
N multiplicatively, let Q = {(x, x −1 ) ∈ E 1 × E 2 | x ∈ N }. The Baer product of E 1
and E 2 is defined to be the quotient (E 1 , E 2 )/Q. A typical coset of the Baer product
will be denoted by (e1 , e2 )Q.
20. Prove that the homomorphism x 7→ (x, 1)Q is one-one from N into (E 1 , E 2 )/Q,
    that the homomorphism ϕ : (E 1 , E 2 ) → G defined by ϕ(e1 , e2 ) = ϕ1 (e1 ) has
    image G and descends to the quotient (E 1 , E 2 )/Q, and that the kernel of the
    descended ϕ is the embedded copy of N . (Therefore (E 1 , E 2 )/Q is an extension
    of N by G, evidently relative to τ .)
21. For each u ∈ G, select ū ∈ E 1 and e     u ∈ E 2 with ϕ1 (ū) = u = ϕ2 (e   u ), and
    define a(u, v) and b(u, v) for u and v in G by (x ū)(y v̄) = a(u, v)uv and
    (xe     v ) = b(u, v)e
       u )(ye            b(u, v). Show that (ū, e                u )Q) = u and that the
                                                  u )Q has ϕ((ū, e
    associated 2-cocyle for (E 1 , E 2 )/Q is a(u, v)b(u, v) if the group operation in N
    is written multiplicatively.
22. Prove that Baer multiplication descends to a well-defined multiplication of equiv-
    alence classes of extensions of N by G relative to τ , in the following sense:
    Suppose that E 1 and E 10 are equivalent extensions and that E 2 and E 20 are equiv-
    alent extensions. Let (E 1 , E 2 )/Q and (E 10 , E 20 )/Q 0 be the Baer products. Then
    (E 1 , E 2 )/Q is equivalent to (E 10 , E 20 )/Q 0 . Conclude that if Baer multiplication
    is imposed on equivalence classes of extensions of N by G relative to τ , then the
    correspondence stated in Proposition 7.40 of equivalence classes to members of
    H 2 (G, N ) is a group isomorphism.

Problems 23–24 derive the Poisson summation formula for finite abelian groups. If G
                              b is its group of multiplicative characters, then the Fourier
is a finite abelian group and G                                    P
coefficient at χ ∈ G of a function f in C(G, C) is b
                      b                                   f (χ) = g∈G f (g)χ(g). The
                                                                        P
Fourier inversion formula in Theorem 7.17 says that f (g) = |G|−1 χ∈b            b
                                                                              G f (χ)χ(g).

23. Let G be a finite abelian group, let H be a subgroup, and let G/H be the quotient
                                 .
    group. If t is in G, write t for the coset of t in G/H . Let f be in C(G, C)
                   .      P
    and define F(t) = h∈H f (t + h) as a function on G/H . Suppose that χ is a
    member of G  b that is identically 1 on H , so that χ descends to a member χ. of
                                    .
    [ . Prove that b
    G/H                         b χ).
                        f (χ) = F(
24. (Poisson summation formula) With f and F as in the previous problem, apply
    the Fourier inversion formula for G/H to the function F, and derive the formula
                       X                     1          X
                             f (t + h) =                            b
                                                                    f (ω)ω(t).
                       h∈H
                                           |G/H |
                                                    ω∈b
                                                      G , ω| H =1


                P note: This formula
      (Educational                   is often applied with t = 0, in which case it
                                1 P                  b
      reduces to h∈H f (h) = |G/H |      G , ω| H =1 f (ω).)
                                       ω∈b
                                         7. Problems                                      363

Problems 25–28 continue the introduction to error-correcting codes begun in Problems
63–73 at the end of Chapter IV, combining those results with the Poisson summation
formula in the problems above and with notions from Section VI.1. Let F be the field
Z/2Z, and form                          n                                              n
             Pn the Hamming space F n. Define a nondegenerate bilinear form on F
by (a, c) = i=1 ai ci for a and c in F . Recall from Chapter IV that a linear code
C is a vector subspace of Fn . For such a C, let C ⊥ as in Section VI.1 be the set of all
a ∈ Fn such that (a, c) = 0 for all c ∈ C; the linear code C ⊥ is called the dual code.
A linear code is self dual if C ⊥ = C.
25. (a) Show that the codes 0 and Fn are dual to each other.
    (b) Show that the repetition code and the parity-check code are dual to each
        other.
    (c) Show that the Hamming code of order 8 is self dual.
    (d) Show that any self-dual linear code C has dim C = n/2, and conclude that
        the Hamming code of order 2r with r > 3 is not self dual.
    (e) Show that any member c of a self-dual linear code C has even weight.
    (f) Show that if a linear code C has C ⊆ C ⊥ and if every member c of C has
        even weight, then c 7→ 12 wt(c) mod 2 is a group homomorphism of C into
        Z/2Z. Here wt(c) denotes the weight of c.
26. Regard Fn as an additive group G to which the Fourier inversion formula of
    Section 4 can be applied.
    (a) Show that one can map G           b to Fn by χ 7→ aχ with χ(c) = (−1)(aχ ,c) and
        that the result is a group isomorphism. (Therefore if f is in C(Fn , C), we
        can henceforth regard b        f as a function on Fn .)
    (b) Show under the identification in (a) that if f is in C(Fn , C), then b         f (a) =
        P                                        n
             c∈Fn f (c)(−1)
                              (a,c)  for a in F .
    (c) Suppose that the function f ∈ C(Fn , C) is of the special form f (c) =
        Qn
             i=1 f i (ci ) whenever c = (c1 , . . . , cn ). HereQn
                                                                  each f i is a function on
        the 2-element group F. Prove that b            f (a) = i=1    b
                                                                      f i (ai ) whenever a =
        (a1 , . . . , aP
                       n ). Here  bf i  is given  by the formula of (b)   for the case n = 1:
        bf i (ai ) = ci ∈F f i (ci )(−1)ai ci .
27. Fix two complex numbers x and y. Define f 0 : F → C to be the function
    with f 0 (0)
             Qn = x and f 0 (1)   = y. Define f : F → C to be the function with
     f (c) = i=1  f 0 (ci ) = x n−wt(c) y wt(c) where wt(c) is the weight of c.
    (a) Show that b f 0 (0) = x + y and b   f 0 (1) = x − y.
                    b
    (b) Show that f (a) = (x + y)     n−wt(a)    (x − y)wt(a) .
28. Let C be a linear code in Fn . Take G to be the additive group of Fn and H to be
    the additive group of C. Regard C ⊥ as an additive group also.
    (a) Map G/H[ to C ⊥ by χ 7→ aχ with χ(c) = (−1)(aχ ,c) . Show that this
         mapping is a group isomorphism.
364                            VII. Advanced Group Theory

      (b) Applying the Poisson summation formula of Problem 24, prove that
                                P            1 P b
                                    f (h) = ⊥         f (a)
                                h∈C        |C | a∈C ⊥

          for all f in C(Fn , C).
                                                          Pn            n−k Y k , where
      (c) (MacWilliams identity) Let WC (X, Y ) =           k=0 Nk (C)X
          Nk (C) is the number of members of C with weight k, be the weight-
          enumerator polynomial of C, and let WC ⊥ (X, Y ) be defined similarly.
          By applying (b) to the function f in the previous problem, prove that
                              −1
          WC (x, y) = |C ⊥ | WC ⊥ (x + y, x − y) for each x and y. Conclude from
          Corollary 4.32 that weight-enumerator polynomials satisfy WC (X, Y ) =
                −1
          |C ⊥ | WC ⊥ (X + Y, X − Y ).
      (d) The polynomials WC (X, Y ) were seen in Chapter IV to be X n for the 0
          code, (X + Y )n for the code Fn , X n + Y n for the repetition code,
          1          n           n                                 8      4 4      8
          2 ((X +Y ) +(X −Y ) ) for the parity-check code, and X +14X Y +Y for
          the Hamming code of order 8. Using relationships established in Problem 25,
          verify the result of (c) for each of these codes.
      (e) Suppose that C is a self-dual linear code. Applying (c) in this case, exhibit
          WC (X, Y ) as being invariant under a copy of the dihedral group D8 of
          order 16. (Educational note: If the polynomial WC (X, Y ) is invariant also
          under X 7→ i X, as is true for the Hamming code of order 8, then WC (X, Y )
          is invariant under the group generated by D8 and this transformation, which
          can be shown to have order 192.)

Problems 29–31 concern an unexpectedly fast method of computation of Fourier
coefficients in the context of finite abelian groups, particularly in the context of cyclic
groups. They show for a cyclic group of order m = pq that the use of the idea
behind the Poisson summation formula of Problem 24 makes it possible to compute
the Fourier coefficients of a function in about pq( p +q) steps rather than the expected
m 2 = p2 q 2 steps. This savings may be iterated in the case of a cyclic group of order
2n so that the Fourier coefficients are computed in about n2n steps rather than the
expected 22n steps. An organized algorithm to implement this method of computation
is known as the fast Fourier transform. Write the cyclic group Cm as the set
{0, 1, 2, . . . , m−1} of integers modulo m under addition, and let ≥m = e2πi/m . For k
in Cm define a multiplicative character χn of Cm by χn (k) = (≥mn )k . The resulting m
multiplicative characters satisfy χn χn 0 = χn+n 0 , and they exhaust C  c m since distinct
multiplicative characters are orthogonal. It will be convenient to identify χn with
χn (1) = ≥mn .
29. In the setting of Problem 23, suppose that G = Cm with m = pq; here p and q
    need not be relatively prime. Let H = {0, q, 2q, . . . , ( p−1)q} be the subgroup
    of G isomorphic to C p , so that G/H = {0, 1, 2, . . . , q − 1} is isomorphic to
                                                                  p   2p           (q−1) p
    Cq . Prove that the characters χ of G identified with ≥m0 , ≥m , ≥m , . . . , ≥m
                                           7. Problems                                          365

    are the ones that are identically 1 on H and therefore descend to characters
                                                                .
    of G/H . Verify that the descended characters χ are the ones identified with
                               q−1                                             .
    ≥q0 , ≥q1 , ≥q2 , . . . , ≥q . Consequently the formula b              b χ)
                                                                  f (χ) = F(      of Problem 23
                                              b     0   p  2p         (q−1) p
    provides a way of computing f at ≥m , ≥m , ≥m , . . . , ≥m                from the values of
    b Show that if F
    F.                         b is computed from the definition of Fourier coefficients, then
    the number of steps involved in its computation is about q 2 , apart from a constant
    factor. Show therefore that the total number of steps in computing b               f at these
    special values of χ is therefore on the order of q 2 + pq.
30. In the previous problem show for each k with 0 ≤ k ≤ p−1 that the value of b              f at
      k     p+k        2 p+k          (q−1) p+k
    ≥m , ≥m , ≥m , . . . , ≥m                   can be handled in the same way with a different
    F by replacing f by a suitable variant of f . Doing so for each k requires p times
    the number of steps detected in the previous problem, and therefore all of b            f can
    be computed in about p(q 2 + pq) = pq( p + q) steps.
31. Show how iteration of this process to compute the Fourier coefficients of each F,
    together with further iteration of this process, allows one to compute the Fourier
    coefficients for a function on Cm 1 m 2 ···m r in about m 1 m 2 · · · m r (m 1 +m 2 +· · ·+m r )
    steps.
Problems 32–36 concern contragredient representations and the decomposition of the
left regular representation of a finite group G. They make use of Problems 24–28 in
Chapter III, which introduce the complex conjugate V of a complex vector space V . In
the case that V is an inner-product space, those problems define (u, v)V = (v, u)V ,
and they show that if `v ∈ V 0 is given by `v (u) = (u, v)V = (v, u)V , then the
mapping `v ↔ v is an isomorphism of V 0 with V .
32. Show that the definition (`v1 , `v2 )V 0 = (v1 , v2 )V makes the isomorphism of V 0
      with V preserve inner products.
33. If R is a unitary representation of G on the finite-dimensional complex vector
    space V , define the contragredient representation R c of G on V 0 by R c (x) =
    R(x −1 )t . Prove that R c (x)`v = ` R(x)v and that R c is unitary on V 0 .
34. Show that the matrix coefficients of R c are the complex conjugates of those of
    R and that the characters satisfy χ R c = χ R .
35. Give an example of an irreducible representation of a finite group G that is not
    equivalent to its contragredient.
36. Let ` be the left regular representation of G on C(G, C), and let V R be the linear
    span in C(G, C) of the matrix coefficients of an irreducible representation R of
    dimension d. Prove that the representation (`, V R ) of G is equivalent to the direct
    sum of d copies of the contragredient R c .
Problems 37–46 concern the free product C2 ∗ C3 and its quotients. The problems
make use of the group of matrices SL(2, Z/mZ) of determinant 1 over the com-
mutative ring Z/mZ, as discussed in Section V.2. One of the quotients of C2 ∗ C3
366                               VII. Advanced Group Theory

will be PSL(2, Z) = SL(2, Z)/{scalar matrices}, and these problems show that the
quotient mapping can be arranged to be an isomorphism. Other quotients will be
the groups G m = hX, Y ; X 2 , Y 3 , (X Y )m i with m ∏ 2. These arise in connection
with tilings in 2-dimensional geometry. The isomorphism C2 ∗ C3 ∼        = PSL(2, Z)
leads to a homomorphism that will be called σm carrying G m onto PSL(2, Z/mZ) =
SL(2, Z/mZ)/{scalar matrices}, the image group being finite. The problems show
that the homomorphism σm : G m → PSL(2, Z/mZ) is an isomorphism for the cases
in which G m arises from spherical geometry, namely for 2 ≤ m ≤ 5, and that the
homomorphism is not an isomorphism for m = 6, the case in which G m arises from
Euclidean geometry.
                               ≥      ¥      ≥      ¥
                                 0 −1           0 1
37. Show that the elements 1 0 and −1 −1 generate SL(2, Z) by arguing as
     follows: if the subgroup 0 of≥SL(2,   ¥ Z) generated by these two elements is not
                                       a b
     SL(2, Z), choose an element c d outside 0 having max(|a|, |b|) as small as
     possible, and derive a contradiction by showing that a suitable right multiple of
     it by elements of 0 is in 0.
                                 ≥      ¥                           ≥      ¥
                                   0 −1                                0 1
38. By mapping X 7→ x = 1 0 mod ±I and Y 7→ y = −1 −1 mod ±I ,
      produce a group homomorphism 8 of C2 ∗C3 = hX, Y ; X 2 , Y 3 i onto PSL(2, Z).
39. Let x, y, and 8 : C2 ∗≥C3 →¥ PSL(2, Z) be as in the previous ≥≥
                                                                 problem.
                                                                       ¥  ¥
                           a b
    (a) For any member c d mod ±I of PSL(2, Z), define µ ac db mod ±I
                                ≥≥ ¥           ¥
        = max(|a|, |b|) and ∫ ac db mod ±I = max(|c|, |d|). Prove that if
              ≥ ¥
        z = ac db mod ±I in PSL(2, Z) has ab ≤ 0, then µ(zyx) ∏ µ(z) and
            µ(zy −1 x) ∏ µ(z), while if cd ≤ 0, then ∫(zyx) ∏ ∫(z) and ∫(zy −1 x) ∏
            ∫(z).
      (b)   Prove that µ(zx) = µ(z) and ∫(zx) = ∫(z) for all z in PSL(2, Z).
      (c)   Show that there are only 10 members z of PSL(2, Z) for which the two
            conditions µ(z) = 1 and ∫(z) = 1 both hold.
      (d)   A reduced word in C2 ∗ C3 is a finite sequence of factors X, Y , and Y −1 ,
            with no two consecutive factors equal and with no two consecutive factors
            Y Y −1 or Y −1 Y . Prove for any reduced word a1 · · · an in C2 ∗ C3 , where
            each a j is one of X, Y , and Y −1 , that µ(8(a1 · · · an )) ∏ µ(8(a1 · · · an−1 ))
            and that ∫(8(a1 · · · an )) ∏ ∫(8(a1 · · · an−1 )).
      (e)   Deduce that the homomorphism 8 is an isomorphism.
40. Let 0(m) be the group of all matrices M in SL(2, Z) such that every entry of
    M − I is divisible by m.
    (a) Prove that passage from a matrix in SL(2, Z) to the same matrix with its
        entries considered modulo m gives a homomorphism e    σm : SL(2, Z) →
        SL(2, Z/mZ) with ker eσm = 0(m).
                                     7. Problems                                   367

    (b) Prove that if α, β, and m are positive integers with GCD(α, β, m) = 1,
        then there exists an integer r such that GCD(α + mr, β) = 1. (One
        way of proceeding is to use Dirichlet’s theorem on primes in arithmetic
        progressions.)
    (c) Prove that imageeσm = SL(2, Z/mZ), i.e., eσm is onto.

41. Let 8m : C2 ∗C3 → G m be the homomorphism defined by the conditions X 7→ X
    and Y 7→ Y . Let Hm be the smallest normal subgroup of PSL(2, Z) containing
    (x y)m mod ±I . Let eσm : SL(2, Z) → SL(2, Z/mZ) be the homomorphism of
    the previous problem.
    (a) Why is 8m well defined?
    (b) Why is Hm = 8(ker 8m )?
    (c) Define PSL(Z/mZ) = SL(2, Z/mZ)/{scalar matrices}. Why does the
         composition of eσm followed by passage to the quotient descend to a ho-
         momorphism σm of PSL(2, Z) onto PSL(2, Z/mZ)?
    (d) If K ⊆ PSL(2, Z) is the kernel of σm , why is Hm ⊆ K m ?
    (e) Show that if t is≥any integer,
                              ¥        then≥the following
                                                 ¥        members
                                                            ≥     of K m¥ lie in the
                                               1 0              1+tm tm
         subgroup Hm : 10 tm mod ±I ,         tm 1
                                                     mod ±I ,    −tm 1−tm
                                                                             mod ±I ,
             ≥          ¥ 1
               1+tm −tm
         and tm 1−tm mod ±I .

42. With G m defined as above, exhibit homomorphisms of various groups G m onto
    the following finite groups:
    (a) S3 when m = 2 by sending X 7→ (1 2) and Y 7→ (1 2 3).
    (b) A4 when m = 3 by sending X 7→ (1 2)(3 4) and Y 7→ (1 2 3).
    (c) S4 when m = 4 by sending X 7→ (1 2) and Y 7→ (2 3 4).
    (d) A5 when m = 5 by sending X 7→ (1 2)(3 4) and Y 7→ (1 3 5).

43. This problem shows how to prove that Hm = K m for 2 ≤ m ≤ 5, and it asks
    that the steps be carried out for m = 2 and m = 3. Recall from the remark
    with Lemma 7.11 that Lemma 7.11 is valid for all groups in determining a set
    of generators of a subgroup from generators of the whole group and a system of
    coset representatives. The lemma is to be applied to the group PSL(2, Z) and
    the subgroup K m . Generators of PSL(2, Z) are taken as b1 = x mod ±I and
    b2 = y mod ±I .
    (a) For the case m = 2, find members g1 , . . . , g6 of PSL(2, Z) such that the six
         cosets of PSL(2, Z)/K 2 are exactly K 2 g1 , . . . , K 2 g6 .
    (b) Still for the case m = 2, find g j bi ρ(g j bi )−1 for 1 ≤ i ≤ 2 and 1 ≤ j ≤ 6.
         Lemma 7.11 says that these 12 elements generate K 2 .
    (c) Using Problem 41e and any necessary variations of it, show that each of
         the 12 generators of K 2 in (b) lies in the subgroup H2 , and conclude that
         H2 = K 2 .
368                           VII. Advanced Group Theory

      (d) Repeat steps (a), (b), and (c) for m = 3. There are 12 cosets K 3 g j of
          PSL(2, Z)/K 3 . (Educational note: There are 24 cosets for PSL(2, Z)/K 4
          and 60 cosets for PSL(2, Z)/K 5 .)
44. Take for granted that Hm = K m for 2 ≤ m ≤ 5. Deduce the isomorphisms
            = PSL(2, Z/2Z) ∼
    (a) G 2 ∼                = S3 .
    (b) G 3 ∼
            =  PSL(2,  Z/3Z) ∼
                             = A4 . (This group is called the tetrahedral group.)
            ∼
    (c) G 4 = PSL(2, Z/4Z) ∼ = S4 . (This group is called the octahedral group.)
            = PSL(2, Z/5Z) ∼
    (d) G 5 ∼                = A5 . (This group is called the icosahedral group.)
45. A translation in the Euclidean plane R2 is any function T(a,b) (x, y) =
    (a + x, b + y), the rotation about the origin
                                                ≥      clockwise¥ through the angle θ
                                                  cos θ − sin θ
    is the linear map Rθ given by the matrix sin θ cos θ , and the rotation about
    (x0 , y0 ) clockwise through the angle θ is the linear map given by (x, y) 7→
    Rθ (x − x0 , y − y0 ) + (x0 , y0 ).
    (a) Prove that Rθ T(a,b) Rθ−1 = TRθ (a,b) .
    (b) Prove that the union of the set of translations and all the sets of rotations
          about points of R2 is a group by showing that it is the semidirect product
          of the subgroup of rotations about the origin and the normal subgroup of
          translations.
46. Fix a triangle T in the Euclidean plane with vertices arranged counterclockwise
    at a, b, c and with angles π/2 at a, π/3 at b, and π/6 at c. Let ra be rotation
    clockwise through π at a, rb be rotation clockwise through 2π/3 at b, and rc be
    rotation counterclockwise through π/3 at c.
    (a) Show that ra2 = 1, rb3 = 1, rc6 = 1, and rc = ra rb .
    (b) Show that the member rb ra rb ra rb of the group generated by ra and rb is a
         nontrivial translation and therefore that the generated group is infinite.
    (c) Conclude that G 6 ¿ PSL(2, Z/6Z). (Educational note: If T        e denotes the
         union of T and the reflection of T in one of the sides of T , it can be shown
         that the group generated by ra and rb is isomorphic to G 6 and tiles the plane
         with copies of Te.)
Problems 47–52 establish a harmonic analysis for arbitrary representations of finite
groups on complex vector spaces, whether finite-dimensional or infinite-dimensional.
Let G be a finite group, and let V beP   a complex vector space. For any representation
R of G on V , one defines R( f )v = x∈G f (x)R(x)v for f in C(G, C) and v in V ,
just as in the case that V is finite-dimensional. The same computation as in Section
VII.4 shows that the formula R( f 1 ∗ f 2 ) = R( f 1 )R( f 2 ) remains valid when V is
infinite-dimensional.
47. Let (R1 , V1 ) and (R2 , V2 ) be irreducible finite-dimensional representations of G
     on complex vector spaces, and let χ R1 and χ R2 be their characters. Using Schur
     orthogonality, prove that
     (a) χ R1 ∗ χ R2 = 0 if R1 and R2 are inequivalent,
                                      7. Problems                                    369

    (b) χ R1 ∗ χ R1 = |G|d R−1
                             1
                               χ R1 , where d R1 = dim V R .
48. With (R, V ) given, let (Rα , Vα ) be any irreducible finite-dimensional represen-
    tation of G, and define E α : V → V by E α = |G|−1 dα R(χα ), where χα is the
    character of Rα and where dα = dim Vα .
    (a) Prove that E α2 = E α .
    (b) Prove that E α E β = E β E α = 0 if (Rβ , Vβ ) is an irreducible finite-
         dimensional representation of G such that Rα and Rβ are inequivalent.
49. Observe for each v in V that {R(x)v | x ∈ G} spans a finite-dimensional invariant
    subspace of V . By Corollary 7.21, each v in V lies in a finite direct sum of finite-
    dimensional invariant subspaces of V on each of which R acts irreducibly. Using
    Zorn’s Lemma, prove that V is the direct sum of finite-dimensional subspaces
    on each of which R acts irreducibly. (If V is infinite-dimensional, there will of
    course be infinitely many such subspaces.)
                                                                                      Ø
50. Suppose that V0 is a finite-dimensional invariant subspace of V such that R ØV0
    is equivalent to some Rα , where Rα is as in Problem 48. Prove that E α is the
    identity on V0 .
51. Deduce that if {(Rβ , Vβ )} is a maximal collection      P of inequivalent finite-
    dimensional irreducible representations of G, then β E β = I on V and the
    image of E α is the set of all sums of vectors in V lying in some finite-dimensional
                                             Ø
    invariant subspace V0 of V such that R ØV0 is equivalent to Rα . (Educational note:
    Consequently V is exhibited as the finite direct sum of the spaces image E α ,
    each space image E α is the direct sum of finite-dimensional irreducible invariant
    subspaces, and the restriction of R to any finite-dimensional irreducible invariant
    subspace of image E α is equivalent with Rα .
52. Suppose that (Rα , Vα ) is a 1-dimensional representation of G given by a multi-
    plicative character ω. Prove that the image of E α consists of all vectors v in V
    such that R(x)v = ω(x)v for all x in G.
                                     CHAPTER VIII

               Commutative Rings and Their Modules



Abstract. This chapter amplifies the theory of commutative rings that was begun in Chapter IV,
and it introduces modules for any ring. Emphasis is on the topic of unique factorization.
    Section 1 gives many examples of rings, some commutative and some noncommutative, and
introduces the notion of a module for a ring.
    Sections 2–4 discuss some of the tools related to questions of factorization in integral domains.
Section 2 defines the field of fractions for an integral domain and gives its universal mapping property.
Section 3 defines prime and maximal ideals and relates quotients of them to integral domains and
fields. Section 4 introduces principal ideal domains, which are shown to have unique factorization,
and it defines Euclidean domains as a special kind of principal ideal domain for which greatest
common divisors can be obtained constructively.
    Section 5 proves that if R is an integral domain with unique factorization, then so is the polynomial
ring R[X]. This result is a consequence of Gauss’s Lemma, which addresses what happens to the
greatest common divisor of the coefficients when one multiplies two members of R[X]. Gauss’s
Lemma has several other consequences that relate factorization in R[X] to factorization in F[X],
where F is the field of fractions of R. Still another consequence is Eisenstein’s irreducibility criterion,
which gives a sufficient condition for a member of R[X] to be irreducible.
    Section 6 contains the theorem that every finitely generated unital module over a principal ideal
domain is a direct sum of cyclic modules. The cyclic modules may be assumed to be primary in a
suitable sense, and then the isomorphism types of the modules appearing in the direct-sum decom-
position, together with their multiplicities, are uniquely determined. The main results transparently
generalize the Fundamental Theorem for Finitely Generated Abelian Groups, and less transparently
they generalize the existence and uniqueness of Jordan canonical form for square matrices with
entries in an algebraically closed field.
    Sections 7–11 contain foundational material related to factorization for the two subjects of
algebraic number theory and algebraic geometry. Both these subjects rely heavily on the theory of
commutative rings. Section 7 is a section of motivation, showing the analogy between a situation
in algebraic number theory and a situation in algebraic geometry. Sections 8–10 introduce Noe-
therian rings, integral closures, and localizations. Section 11 uses this material to establish unique
factorization of ideals for Dedekind domains, as well as some other properties.



                           1. Examples of Rings and Modules

Sections 4–5 of Chapter IV introduced rings and fields, giving a small number of
examples of each. In the present section we begin by recalling those examples
and giving further ones. Although Chapters VI and VII are not prerequisite for
                                                   370
                                1. Examples of Rings and Modules                                 371

the present chapter, our list of examples will include some rings and fields that
arose in those two chapters.
    The theory to be developed in this chapter is intended to apply to commutative
rings, especially to questions related to unique factorization in such rings. Despite
this limitation it seems wise to include examples of noncommutative rings in the
list below.
    In the conventions of this book, a ring need not have an identity. Many rings
that arise only in the subject of algebra have an identity, but there are important
rings in the subject of real analysis that do not. From the point of view of category
theory, one therefore distinguishes between the category of all rings, with ring
homomorphisms as morphisms, and the category of all rings with identity, with
ring homomorphisms carrying 1 to 1 as morphisms. In the latter case one may
want to exclude the zero ring from being an object in the category under certain
circumstances.

   EXAMPLES OF RINGS.
   (1) Basic commutative rings from Chapter IV. All of the structures Z, Q, R,
C, Z/mZ, and 2Z are commutative rings. All but the last have an identity. Of
these, Q, R, and C are fields, and so is F p = Z/ pZ if p is a prime number. The
others are not fields.
   (2) Polynomial rings. Let R be a nonzero commutative ring with identity.
In Section IV.5 we defined the commutative ring R[X 1 , . . . , X n ] of polynomials
over R in n indeterminates. It has a universal mapping property with respect to
substitution for the indeterminates and use of a homomorphism on the coefficients.
Making substitutions from R itself and mapping the coefficients by the identity ho-
momorphism, we are led to the ring of all functions (r1 , . . . , rn ) 7→ f (r1 , . . . , rn )
for r1 , . . . , rn in R and f (X 1 , . . . , X n ) in R[X 1 , . . . , X n ]; this is called the ring
of all polynomial functions in n variables on R. Polynomials may be considered
also in infinitely many variables, but we did not treat this case in any detail.
     (3) Matrix rings over commutative rings. Let R be a nonzero commutative
ring with identity. The set Mn (R) of all n-by-n matrices with entries in R is a ring
under entry-by-entry
               P           addition and the usual definition of matrix multiplication:
(AB)i j = nk=1 Aik Bk j . It has an identity, namely the identity matrix I with
Ii j = δi j . In this setting, Section V.2 introduced a theory of determinants, and it
was proved that a matrix has a one-sided inverse if and only if it has a two-sided
inverse, if and only if its determinant is a member of the group R × of units in
R, i.e., elements of R invertible under multiplication. The matrix ring Mn (R) is
always noncommutative if n > 1.
   (4) Matrix rings over noncommutative rings. If R is any ring, we can still make
the set Mn (R) of all n-by-n matrices with entries in R into a ring. However, if
372                        VIII. Commutative Rings and Their Modules

R has no identity, Mn (R) will have no identity. The theory of determinants does
not directly apply if R is noncommutative or if R fails to have an identity,1 and as
a consequence, questions about the invertibility of matrices are more subtle than
with the previous example.
    (5) Spaces of linear maps from a vector space into itself. Let V be a vector
space over a field K. The vector space EndK (V ) = HomK (V, V ) of all K linear
maps from V to itself is initially a vector space over K. Composition provides a
multiplication that makes EndK (V ) into a ring with identity. In fact, associativity
of multiplication is automatic for any kind of function, and so is the distributive law
(L 1 + L 2 )L 3 = L 1 L 3 + L 2 L 3 . The distributive law L 1 (L 2 + L 3 ) = L 1 L 2 + L 1 L 3
follows from the fact that L 1 is linear. This ring is isomorphic as a ring to Mn (K)
if V is n-dimensional, an isomorphism being determined by specifying an ordered
basis of V .
    (6) Associative algebras over fields. These were defined in Section VI.7,
knowledge of which is not being assumed now. Thus we repeat the definition. If
K is a field, then an associative algebra over K, or associative K algebra, is a ring
A that is also a vector space over K such that the multiplication A × A → A is
K-linear in each variable. The conditions of linearity concerning multiplication
have two parts to them: an additive part saying that the usual distributive laws
are valid and a scalar-multiplication part saying that

              (ka)b = k(ab) = a(kb)                for all k in K and a, b in A.

If A has an identity, the displayed condition says that all scalar multiples of
the identity lie in the center of A, i.e., commute with every element of A. In
Examples 2 and 3, when R is a field K, the polynomial rings and matrix rings
over K provide examples of associative algebras over K; scalar multiplication is
to be done in entry-by-entry fashion. Example 5 is an associative algebra as well.
If L is any field such that K is a subfield, then L may be regarded as an associative
algebra over K. An interesting commutative associative algebra over C without
identity is the algebra Ccom (R) of all continuous complex-valued functions on R
that vanish outside a bounded interval; the vector-space operations are the usual
pointwise operations, and the operation of multiplication is given by convolution
                                         Z
                         ( f ∗ g)(x) =       f (x − y)g(y) dy.
                                               R

Section VII.4 worked with an analog C(G, C) of this algebra in the context that
R is replaced by a finite group G.
    1 A limited theory of determinants applies in the noncommutative case, but it will not be helpful

for our purposes.
                                1. Examples of Rings and Modules                                  373

   (7) Division rings. A division ring is a nonzero ring with identity such that every
element has a two-sided inverse under multiplication. A commutative division
ring is just a field. The ring H of quaternions is the only explicit noncommutative
division ring that we have encountered so far. It is an associative algebra over R.
More generally, if A is a division ring, then we can easily check that the center
K of A is a field and that A is an associative algebra over K.2
   (8) Tensor, symmetric, and exterior algebras. If E is a vector space over a field
K, Chapter VI defined the tensor, symmetric, and exterior algebras of E over K, as
well as the polynomial algebra on E in the case that E is finite-dimensional. These
are all associative algebras with identity. Symmetric algebras and polynomial
algebras are commutative. None of these algebras will be discussed further in
this chapter.
   (9) A field of 4 elements. This was constructed in Section IV.4. Further finite
fields beyond the field of 4 elements and the fields F p = Z/ pZ with p prime will
be constructed in Chapter IX.
   (10) Algebraic number fields Q[θ]. These were discussed in Sections IV.1
and IV.4. In defining Q[θ], we assume that θ is a complex number and that
there exists an integer n > 0 such that the complex numbers 1, θ, θ 2 , . . . , θ n
are linearly dependent over Q. The set Q[θ] is defined to be the subset of C
obtained by substitution of θ into all members of Q[X]. It coincides with the
linear span over Q of 1, θ, θ 2 , . . . , θ n−1 . Proposition 4.1 shows that it is closed
under the arithmetic operations, including passage to multiplicative inverses of
nonzero elements, and it is therefore a subfield of C. This example ties in with
the notion of minimal polynomial in Chapter V because the members of Q[X]
with θ as a root are all multiples of one nonzero such polynomial that exhibits the
linear dependence. We return to this example occasionally later in this chapter,
particularly in Sections 7–11, and then we treat it in more detail in Chapter IX.
   (11) Algebraic integers in a number field Q[θ]. Algebraic integers were defined
in Section VII.4 as the roots in C of monic polynomials in Z[X], and they were
shown to form a commutative ring with identity. The set of algebraic integers
in Q[θ] is therefore a commutative ring with identity, and it plays somewhat
the same role for Q[θ] that Z plays for Q. We discuss this example further in
Sections 7–11.
   (12) Integral group rings. If G is a group, then we can make the free abelian
° P ZG¢°on
group                    ¢ P of G into a ring by defining multiplication to be
               Pthe elements
     i m i gi     j nj hj =    i, j (m i n j )(gi h j ) when the m i and n j are in Z and the
gi and h j are in G. It is immediate that the result is a ring with identity, and ZG
   2 Use  of the term “division algebra” requires some care. Some mathematicians understand
division algebras to be associative, and others do not. The real algebra O of octonions, as defined in
Problems 52–56 at the end of Chapter VI, is not associative, but it does have division.
374                        VIII. Commutative Rings and Their Modules

is called the integral group ring of G. The group G is embedded as a subgroup
of the group P (ZG)× of units of ZG, each element of g being identified with a
sum ∂(g) =        m i gi in which the only nonzero term is 1g. The ring ZG has
the universal mapping property illustrated in Figure 8.1 and described as follows:
whenever ϕ : G → R is a group homomorphism of G into the group R × of units
of a ring R, then there exists a unique ring homomorphism 8 : ZG → R such
that 8 ∂ = ϕ. The existence of 8 as a homomorphism of additive groups follows
from the universal mapping property of free abelian groups, and then one readily
checks that 8 respects multiplication.3
                                                  ϕ
                                          G −−−→ R
                                          
                                          
                                         ∂y     8

                                        ZG
      FIGURE 8.1. Universal mapping property of the integral group ring of G.
    (13) Quotient rings. If R is a ring and I is a two-sided ideal, then we saw in
Section IV.4 that the additive quotient R/I has a natural multiplication that makes
it into a ring called a quotient ring of R. This in effect was the construction that
obtained the ring Z/mZ from the ring Z.
                    Q of rings. If {Rs | s ∈ S} is a nonempty set of rings, then
    (14) Direct product
a direct product s∈S Rs is a ring whose additive group is any direct product
of the underlying additive groups and whose ring operations are given in entry-
       Q fashion. The resulting ring and the associated ring homomorphisms
by-entry
ps0 : s∈S Rs → Rs0 amount to the product functor for the category of rings;
if each Rs has an identity, the result amounts also to the product functor for the
category of rings with identity.

   We give further examples of rings near the end of this section after we have
defined modules and given some examples.
   Informally a module is a vector space over a ring. But let us be more precise.
If R is a ring, then a left R module4 M is an abelian group with the additional
structure of a “scalar multiplication” R × M → M such that
    (i) r(r 0 m) = (rr 0 )m for r and r 0 in R and m in M,
   3 Universal mapping properties are discussed systematically in Problems 18–22 at the end of
Chapter VI. The subject of such a property, here the pair (ZG, ∂), is always unique up to canonical
isomorphism in a given category, but its existence has to be proved.
   4 Many algebra books write “R-module,” using a hyphen. However, when R is replaced by an

expression, particularly in applications of the theory, the hyphen is often dropped. For an example,
see “module” in Hall’s The Theory of Groups. The present book omits the hyphen in all cases in
order to be consistent.
                          1. Examples of Rings and Modules                     375

   (ii) (r + r 0 )m = rm + r 0 m and r(m + m 0 ) = rm + rm 0 if r and r 0 are in R
        and m and m 0 are in M.
In addition, if R has an identity, we say that M is unital if
   (iii) 1m = m for all m in M.
One may also speak of right R modules. For these the scalar multiplication is
usually written as mr with m in M and r in R, and the expected analogs of (i)
and (ii) are to hold.
   When R is commutative, it is immaterial which side is used for the scalar
multiplication, and one speaks simply of an R module.
   Let R be a ring, and let M and N be two left R modules. A homomorphism
of left R modules, or more briefly an R homomorphism, is an additive group
homomorphism ϕ : M → N such that ϕ(rm) = rϕ(m) for all r in R. Then we
can form a category for fixed R in which the objects are the left R modules and
the morphisms are the R homomorphisms from one left R module to another.
Similarly the right R modules, along with the corresponding kind of R homo-
morphisms, form a category. If R has an identity, then the unital R modules form
a subcategory in each case. These categories are fundamental to the subject of
homological algebra, which we take up in Chapter IV of Advanced Algebra.

  EXAMPLES OF MODULES.
  (1) Vector spaces. If R is a field, the unital R modules are exactly the vector
spaces over R.
   (2) Abelian groups. The unital Z modules are exactly the abelian groups.
Scalar multiplication is given in the expected way: If n is a positive integer, the
product nx is the n-fold sum of x with itself. If n = 0, the product nx is 0. If
n < 0, the product nx is −((−n)x).
   (3) Vector spaces as unital modules for the polynomial ring K[X]. Let V
be a finite-dimensional vector space over the field K, and fix L be in EndK (V ).
Then V becomes a unital K[X] module under the definition A(X)v = A(L)(v)
whenever A(X) is a polynomial in K[X]; here A(L) is the member of EndK (V )
defined as in Section V.3. In Section 6 in this chapter we shall see that some of
the deeper results in the theory of a single linear transformation, as developed in
Chapter V, follow from the theory of unital K[X] modules that will emerge from
the present chapter.
   (4) Modules in the context of algebraic number fields. Let Q[θ] be a subfield
of C as in Example 10 of rings earlier in this section. It is assumed that the Q
vector space Q[θ] is finite-dimensional. Let L be the member of EndQ (Q[θ])
given as left multiplication by θ on Q[θ]. As in the previous example, Q[θ]
becomes a unital Q[X] module. Chapter V defines a minimal polynomial for
376                    VIII. Commutative Rings and Their Modules

L, as well as a characteristic polynomial. These objects play a role in the study
to be carried out in Chapter IX of fields like Q[θ]. If θ is an algebraic integer
as in Example 11 of rings earlier in this section, then we can get more refined
information by replacing Q by Z in the above analysis; this technique plays a role
in the theory to be developed in Sections 7–11.
   (5) Rings and their quotients. If R is a ring, then R is a left R module and also
a right R module. If I is a two-sided ideal in I , then the quotient ring R/I , as
defined in Proposition 4.20, is a left R module and also a right R module. These
modules are automatically unital if R has an identity. Later in this section we
shall consider quotients of R by “one-sided ideals.”
    (6) Spaces of rectangular matrices. If R is a ring, then the space Mmn (R) of
m-by-n matrices with entries in R is an abelian group under addition and becomes
a left R module when multiplication by the scalar r is defined as left multiplication
by r in each entry. Also, if we put S = Mm (R), then Mmn (R)Pis a left S module
under the usual definition of matrix multiplication: (sv)i j = nk=1 sik vk j , where
s is in S and v is in Mmn (R).
   (7) Direct product of R modules. If S is a nonempty set and Q         {Ms }s∈S is
a corresponding system of left R modules, then a direct product s∈S Ms is
obtained as an additive group by forming any direct product of the underlying
additive groups of the Ms ’s and defining scalar multiplication by members of
R to be scalar multiplication
                        Q      in each coordinate. The associated abelian-group
homomorphisms ps0 : s∈S Ms → Ms0 become R homomorphisms under this
definition of scalar multiplication on the direct product. Direct product amounts
to the product functor for the category of left R modules; we omit the easy
verification, which makes use of the corresponding fact about abelian groups. As
in the case of abelian groups, we can speak of an external direct product as the
result of a construction that starts with the product of the sets Ms , and we can
speak of recognizing a direct product as internal when the Ms ’s are contained in
the direct product and the restriction of each ps to Ms is the identity function.
     (8) Direct sum of R modules. If S is a nonempty setL  and {Ms }s∈S is a corre-
sponding system of left R modules, then a direct sum s∈S Ms is obtained as
an additive group by forming any direct sum of the underlying additive groups
of the Ms ’s and defining scalar multiplication by members of R to be scalar
multiplication Lin each coordinate. The associated abelian-group homomorphisms
i s0 : Ms0 → s∈S Ms become R homomorphisms under this definition of scalar
multiplication on the direct sum. Direct sum amounts to the coproduct functor for
the category of left R modules; we omit the easy verification, which makes use
of the corresponding fact about abelian groups. As in the case of abelian groups,
we can speak of an external direct sum as the result of a construction that starts
with a subset of the product of the sets Ms , and we can speak of recognizing a
                           1. Examples of Rings and Modules                       377

direct sum as internal when the Ms ’s are contained in the direct sum and each i s
is the inclusion mapping.
    (9) Free R modules. Let R be a nonzero ring with identity, and let S be a
nonempty set. As in Example 5, let us regard R as a L   unital left R module. Then
the left R module given as the direct sum F(S) =           s∈S R is called a free R
module, or free left R module. We define ∂ : S → F(S) by ∂(s) = i s (1),
where i s is the usual embedding map for the direct sum of R modules. The left
R module F(S) has a universal mapping property similar to the corresponding
property of free abelian groups. This is illustrated in Figure 8.2 and is described
as follows: whenever M is a unital left R module and ϕ : S → M is a function,
then there exists a unique R homomorphism 8 : F(S) → M such that 8 ∂ = ϕ.
The existence of 8 as an R homomorphism follows from the universal mapping
property of direct sums (Example 8) as soon as the property is demonstrated for
S equal to a singleton set. Thus let A be any left R module, and let a ∈ A be
given; then it is evident that r 7→ ra is the unique R homomorphism of the left
R module R into A carrying 1 to a.
                                           ϕ
                                    S   −−−→ M
                                    
                                    
                                   ∂y          8

                                 F(S)

        FIGURE 8.2. Universal mapping property of a free left R module.

   If R is a ring and M is a left R module, then an R submodule N of M is an
additive subgroup of M that is closed under scalar multiplication, i.e., has rm in
N when r is in R and m is in N . In situations in which there is no ambiguity, the
use of “left” in connection with R submodules is not necessary.
   EXAMPLES OF SUBMODULES. If V is a vector space over a field K, then a K
submodule of V is a vector subspace of V . If M is an abelian group, then a Z
submodule of M is a subgroup. In Example 6 of modules, in which S = Mm (R),
then an example of a left S submodule of Mmn (R) is all matrices with 0 in every
entry of a specified subset of the n columns.
   If the ring R has an identity and M is a unital left R module, then the R
submodule of M generated by m ∈ M, i.e., the smallest R submodule containing
m, is Rm, the set of products rm with r in R. In fact, the set of all rm is an abelian
group since (r ± s)m = rm ± sm, it is closed under scalar multiplication since
s(rm) = (sr)m, and it contains m since 1m = m. However, if the left R module
M is not unital, then the R submodule generated by m may not equal Rm, and it
was for that reason that R modules were assumed to be unital in the construction of
free R modules in Example 9 of modules above. More generally the R submodule
378                     VIII. Commutative Rings and Their Modules

of M generated by a finite set {m 1 , . . . , m n } in M is Rm 1 + · · · + Rm n if the left
R module M is unital.
    Example 5 of modules treated R as a left R module. In this setting the left
R submodules are called left ideals in R. That is, a left ideal I is an additive
subgroup of R such that ri is in I whenever r is in R and i is in I . As a special
case of what was said in the previous paragraph, if the ring R has an identity, then
the left R module R is automatically unital, and the left ideal of R generated by
an element a is Ra, the set of all products ra with r in R.
    Similarly a right ideal in R is an additive subgroup I such that ir is in I
whenever r is in R and i is in I . The right ideals are the right R submodules
of the right R module R. If R is commutative, then left ideals, right ideals, and
two-sided ideals are all the same.
    Suppose that ϕ : M → N is an R homomorphism of left R modules. In this
situation we readily verify that the kernel of ϕ, denoted by ker ϕ as usual, is an
R submodule of M, and the image of ϕ, denoted by image ϕ as usual, is an R
submodule of N . The R homomorphism ϕ is one-one if and only if ker ϕ = 0, as
a consequence of properties of homomorphisms of abelian groups. A one-one R
homomorphism of one left R module onto another is called an R isomorphism;
its inverse is automatically an R isomorphism, and “is R isomorphic to” is an
equivalence relation.
    Still with R as a ring, suppose that M is a left R module and N is an R
submodule. Then we can form the quotient M/N of abelian groups. This becomes
a left R module under the definition r(m + N ) = rm + N , as we readily check.
We call M/N a quotient module. The quotient mapping m 7→ m + N of M to
M/N is an R homomorphism onto. A particular example of a quotient module
is R/I , where I is a left ideal in R.
    We can now go over the results on quotients of abelian groups in Section IV.2,
specifically Proposition 4.11 through Theorem 4.14, and check that they extend
immediately to results about left R modules. The statements appear below. The
arguments are all routine, and there is no point in repeating them. In the special
case that R is a field and the R modules are vector spaces, these results specialize
to results proved in Sections II.5 and II.6.

    Proposition 8.1. Let R be a ring, let ϕ : M1 → M2 be an R homomorphism
between left R modules, let N0 = ker ϕ, let N be an R submodule of M1
contained in N0 , and define q : M1 → M1 /N to be the R module quotient
map. Then there exists an R homomorphism ϕ : M1 /N → M2 such that
ϕ = ϕq, i.e, ϕ(m 1 + N ) = ϕ(m 1 ). It has the same image as ϕ, and ker ϕ =
{h 0 N | h 0 ∈ N0 }.
   REMARK. As with groups, one says that ϕ factors through M1 /N or descends
to M1 /N . Figure 8.3 illustrates matters.
                          1. Examples of Rings and Modules                     379

                                           ϕ
                                M1      −−−→ M2
                                 
                                 
                                qy              ϕ


                               M1 /N
 FIGURE 8.3. Factorization of R homomorphisms via a quotient of R modules.
   Corollary 8.2. Let R be a ring, let ϕ : M1 → M2 be an R homomorphism
between left R modules, and suppose that ϕ is onto M2 and has kernel N . Then
ϕ exhibits the left R module M1 /N as canonically R isomorphic to M2 .

   Theorem 8.3 (First Isomorphism Theorem). Let R be a ring, let ϕ : M1 → M2
be an R homomorphism between left R modules, and suppose that ϕ is onto M2
and has kernel K . Then the map N1 7→ ϕ(N1 ) gives a one-one correspondence
between
    (a) the R submodules N1 of M1 containing K and
    (b) the R submodules of M2 .
Under this correspondence the mapping m + N1 7→ ϕ(m) + ϕ(N1 ) is an R
isomorphism of M1 /N1 onto M2 /ϕ(N1 ).
  REMARK. In the special case of the last statement that ϕ : M1 → M2 is
an R module quotient map q : M → M/K and N is an R submodule of
M containing K ±, the last statement of the theorem asserts the R isomorphism
M/N ∼= (M/K   )   (N /K ).

   Theorem 8.4 (Second Isomorphism Theorem). Let R be a ring, let M be a
left R module, and let N1 and N2 be R submodules of M. Then N1 ∩ N2 is an
R submodule of N1 , the set N1 + N2 of sums is an R submodule of M, and the
map n 1 + (N1 ∩ N2 ) 7→ n 1 + N2 is a well-defined canonical R isomorphism

                        N1 /(N1 ∩ N2 ) ∼
                                       = (N1 + N2 )/N2 .

    A quotient of a direct sum of R modules by the direct sum of R submodules
is the direct sum of the quotients, according to the following proposition. The
result generalizes Lemma 4.58, which treats the special case of abelian groups
(unital Z modules).
                                                 L
    Proposition 8.5. Let R be a ring, let M = s∈S Ms be a direct sum of left R
modules,Land for each s in S, let Ns be a left R submodule of Ms . Then the natural
map of s∈S Ms to the direct sum of quotients descends to an R isomorphism
                        M .M                   M
                             Ms        Ns ∼ =      (Ms /Ns ).
                        s∈S       s∈S          s∈S
380                       VIII. Commutative Rings and Their Modules
                   L              L
   PROOF. Let ϕ : s∈S Ms → s∈S (Ms /Ns ) be the R L     homomorphism defined
         }s∈S ) = {m s + Ns }s∈S . The mapping ϕ is onto s∈S (Ms /Ns ), and the
by ϕ({m sL
kernel is s∈S Ns . Then Corollary 8.2 shows that ϕ descends to the required R
isomorphism.                                                                 §

   EXAMPLES OF RINGS, CONTINUED.
   (15) Associative algebras over commutative rings with identity. These directly
generalize Example 6 of rings. Let R be a nonzero commutative ring with identity.
An associative algebra over R, or associative R algebra, is a ring A that is also
a left R module such that multiplication A × A → A is R linear in each variable.
The conditions of R linearity in each variable mean that addition satisfies the
usual distributive laws for a ring and that the following condition is to be satisfied
relating multiplication and scalar multiplication:

              (ra)b = r(ab) = a(rb)             for all r in R and a, b ∈ A.

If A has an identity, the displayed condition says that all scalar multiples of
the identity lie in the center of A, i.e., commute with every element of A.
Examples 2 and 3, treating polynomial rings and matrix rings whose scalars
lie in a commutative ring with identity, furnish examples. Every ring R is an
associative Z algebra when the Z action is defined so as to make the abelian
group underlying the additive structure of R into a Z module. All that needs to be
checked is the displayed formula. For n = 1, we have (1a)b = 1(ab) = a(1b)
since the Z module R is unital. If we also have (na)b = n(ab) = a(nb) for a
positive integer n, then we can add and use the appropriate distributive laws to
obtain ((n + 1)a)b = (n + 1)(ab) = a((n + 1)b). Induction therefore gives
(na)b = n(ab) = a(nb) for all positive integers n, and this equality extends
to all integers n by using additive inverses. The associative R algebras form
a category in which the morphisms from one such algebra to another are the
ring homomorphisms that are also R homomorphisms. The product functor
for this category is the direct product as in Example 14 with an overlay of scalar
multiplication as in Example 7 of modules. The coproduct functor in the category
of commutative associative R algebras with identity is more subtle and involves
a tensor product over R, a notion we postpone introducing until Chapter X.
   (16) Group algebra RG over R. If G is a group and R is a commutative ring
with identity, then we can introduce a °multiplication
                                           P          ¢° P in the      PR module RG
                                                                  ¢ free
on the elements of G by the definition        i ri gi     j sj h j =     i, j (ri s j )(gi h j )
when the ri and s j are in R and the gi and h j are in G. It is immediate that
this multiplication makes the free R module into an associative R algebra with
identity, and RG is called the group algebra of G over R. The special case R = Z
leads to the integral group ring as in Example 12. The group G is embedded as a
                       2. Integral Domains and Fields of Fractions                381

                             ×
                   P (RG) of units of RG, each element of g being identified
subgroup of the group
with a sum ∂(g) = ri gi in which the only nonzero term is 1g. The associative
R algebra RG has a universal mapping property similar to that in Figure 8.1 and
given in Figure 8.4 as follows: whenever ϕ : G → A is a group homomorphism
of G into the group A× of units of an associative R algebra A, then there exists
a unique associative R algebra homomorphism 8 : RG → A such that 8 ∂ = ϕ.
                                             ϕ
                                     G −−−→ A
                                     
                                     
                                    ∂y    8

                                   RG
       FIGURE 8.4. Universal mapping property of the group algebra RG.
   (17) Scalar-valued functions of finite support on a group, with convolution
as multiplication. If G is a group and R is a commutative ring with identity,
denote by C(G, R) the R module of all functions from G into R that are of finite
support in the sense that each function is 0 except on a finite subset of G. This R
module readily becomes an associative R algebra if ring multiplication is taken
to be pointwise multiplication, but the interest here is in a different definition of
multiplication. Instead, multiplication is defined to be convolution with
                               X                         X
            ( f 1 ∗ f 2 )(x) =   f 1 (x y −1 ) f 2 (y) =   f 1 (y) f 2 (y −1 x).
                            y∈G                        y∈G

The sums in question are finite because of the finite support of f 1 and f 2 , and the
sums are equal by a change of variables. This multiplication was introduced in
the special case R = C in Section VII.4, and the argument for associativity given
there in the special case works in general. With convolution as multiplication,
C(G, R) becomes an associative R algebra with identity. Problem 14 at the end
of the chapter asks for a verification that the mapping g 7→ f g with
                                     Ω
                                        1     for x = g,
                           f g (x) =
                                        0     for x 6= g,
extends to an R algebra isomorphism of RG onto C(G, R).


                 2. Integral Domains and Fields of Fractions

For the remainder of the chapter we work with commutative rings only. In several
of the sections, including this one, the commutative ring will be an integral
domain, i.e., a nonzero commutative ring with identity and with no zero divisors.
382                    VIII. Commutative Rings and Their Modules

   In this section we show how an integral domain can be embedded canonically
in a field. This embedding is handy for recognizing certain facts about integral
domains as consequences of facts about fields. For example Proposition 4.28b
established that if R is a nonzero integral domain and if A(X) is a polynomial in
R[X] of degree n > 0, then A(X) has at most n roots. Since the coefficients of
the polynomial can be considered to be members of the larger field that contains
R, this result is an immediate consequence of the corresponding fact about fields
(Corollary 1.14).
   The prototype is the construction of the field Q of rationals from the integral
domain Z of integers as in Section A3 of the appendix, in which one thinks of ab
as a pair (a, b) with b 6= 0 and then identifies pairs by saying that ab = dc if and
only if ad = bc.
   We proceed in the same way in the general case. Thus let R be an integral
domain, form the set
                      e = {(a, b) | a ∈ R, b ∈ R, b 6= 0},
                      F

and impose the equivalence relation (a, b) ∼ (c, d) if ad = bc. The relation
∼ is certainly reflexive and symmetric. To see that it is transitive, suppose that
(a, b) ∼ (c, d) and (c, d) ∼ (e, f ). Then ad = bc and c f = de, and these
together force ad f = bc f = bde. In turn, this implies a f = be since R is an
integral domain and d is assumed 6= 0. Thus ∼ is transitive and is an equivalence
relation. Let F be the set of equivalence classes.
                                  e is (a, b)+(c, d) = (ad+bc, bd), the expression
    The definition of addition in F
we get by naively clearing fractions, and we want to see that addition is consistent
with the equivalence relation. In checking this, we need change only one of the
pairs at a time. Thus suppose that (a 0 , b0 ) ∼ (a, b) and that (c, d) is given. We
know that a 0 b = ab0 , and we want to see that (ad + bc, bd) ∼ (a 0 d + b0 c, b0 d),
i.e., that (ad + bc)b0 d = (a 0 d + b0 c)bd. In other words, we are to check that
adb0 d = a 0 dbd; we see immediately that this equality is valid since ab0 = a 0 b.
Consequently addition is consistent with the equivalence relation and descends
to be defined on the set F of equivalence classes.
    Taking into account the properties satisfied by members of an integral domain,
we check directly that addition is commutative and associative on F, e and it follows
that addition is commutative and associative on F.
    The element (0, 1) is a two-sided identity for addition in F,  e and hence the
class of (0, 1) is a two-sided identity for addition in F. We denote this class
by 0. Let us identify this class. A pair (a, b) is in the class of (0, 1) if and only
if 0 · b = 1 · a, hence if and only if a = 0. In other words, the class of (0, 1)
consists of all (0, b) with b 6= 0.
        e we have (a, b) + (−a, b) = (ab + b(−a), bb) = (0, b2 ) ∼ (0, 1), and
    In F,
therefore the class of (−a, b) is a two-sided inverse to the class of (a, b) under
                       2. Integral Domains and Fields of Fractions                383

addition. Consequently F is an abelian group under addition.
   The definition of multiplication in F  e is (a, b)(c, d) = (ac, bd), and it is
routine to see that this definition is consistent with the equivalence relation.
Therefore multiplication descends to be defined on F. We check by inspection
that multiplication is commutative and associative on F, e and it follows that it is
commutative and associative on F. The element (1, 1) is a two-sided identity for
                   e and the class of (1, 1) is therefore a two-sided identity for
multiplication in F,
multiplication in F. We denote this class by 1.
   If (a, b) is not in the class 0, then a 6= 0, as we saw above. Then ab 6= 0,
and we have (a, b)(b, a) = (ab, ab) ∼ (1, 1) = 1. Hence the class of (b, a) is
a two-sided inverse of the class of (a, b) under multiplication. Consequently the
nonzero elements of F form an abelian group under multiplication.
   For one of the distributive laws, the computation
  (a, b)((c, d) + (e, f )) = (a, b)(c f + de, d f ) = (a(c f + de), bd f )
                           = (ac f + ade, bd f ) ∼ (acb f + bdae, b2 d f )
                           = (ac, bd) + (ae, b f ) = (a, b)(c, d) + (a, b)(e, f )
shows that the classes of (a, b)((c, d) + (e, f )) and of (a, b)(c, d) + (a, b)(e, f )
are equal. The other distributive law follows from this one since F is commutative
under multiplication. Therefore F is a field.
   The field F is called the field of fractions of the integral domain R. The
function η : R → F defined by saying that η(r) is the class of (r, 1) is easily
checked to be a homomorphism of rings sending 1 to 1. It is one-one. Let us
call it the canonical embedding of R into F. The pair (F, η) has the universal
mapping property stated in Proposition 8.6 and illustrated in Figure 8.5.
                                           ϕ
                                    R −−−→ F 0
                                    
                                    
                                   ηy     ϕe


                               F
     FIGURE 8.5. Universal mapping property of the field of fractions of R.

   Proposition 8.6. Let R be an integral domain, let F be its field of fractions,
and let η be the canonical embedding of R into F. Whenever ϕ is a one-one ring
homomorphism of R into a field F 0 carrying 1 to 1, then there exists a unique
ring homomorphism e   ϕ : F → F 0 such that ϕ = e  ϕ η, and eϕ is one-one as a
homomorphism of fields.
   REMARK. We say that e     ϕ is the extension of ϕ from R to F. Once this
proposition has been proved, it is customary to drop η from the notation and
regard R as a subring of its field of fractions.
384                       VIII. Commutative Rings and Their Modules

                                                e we define 8(a, b) = ϕ(a)ϕ(b)−1 .
     PROOF. If (a, b) with b 6= 0 is a pair in F,
This is well defined since b 6= 0 and since ϕ, being one-one, cannot have ϕ(b) = 0.
Let us see that 8 is consistent with the equivalence relation, i.e., that (a, b) ∼
(a 0 , b0 ) implies 8(a, b) = 8(a 0 , b0 ). Since (a, b) ∼ (a 0 , b0 ), we have ab0 =
a 0 b and therefore also ϕ(a)ϕ(b0 ) = ϕ(a 0 )ϕ(b) and 8(a, b) = ϕ(a)ϕ(b)−1 =
ϕ(a 0 )ϕ(b0 )−1 = 8(a 0 , b0 ), as required.
     We can thus define eϕ of the class of (a, b) to be 8(a, b), and e ϕ is well defined
as a function from F to F 0 . If r is in R, then e    ϕ (η(r)) = eϕ (class of (r, 1)) =
8(r, 1) = ϕ(r)ϕ(1)−1 , and this equals ϕ(r) since ϕ is assumed to carry 1 into 1.
Therefore e    ϕ η = ϕ.
     For uniqueness, let the class of (a, b) be given in F. Since b is nonzero,
this class is the same as the class of (a, 1)(b, 1)−1 , which equals η(a)η(b)−1 .
Since (e   ϕ η)(a) = ϕ(a) and (e  ϕ η)(b) = ϕ(b), we must have e  ϕ (class of (a, b)) =
ϕ (η(a))e
e           ϕ (η(b))−1 = ϕ(a)ϕ(b)−1 . Therefore ϕ uniquely determines e      ϕ.       §

   If K is a field, then R = K[X] is an integral domain, and Proposition 8.6 applies
to this R. The field of fractions consists in effect of formal rational expressions
P(X)Q(X)−1 in the indeterminate X, with the expected identifications made.
We write K(X) for this field of fractions. More generally the field of fractions
of the integral domain K[X 1 , . . . , X n ] consists of formal rational expressions in
the indeterminates X 1 , . . . , X n , with the expected identifications made, and is
denoted by K(X 1 , . . . , X n ).


                            3. Prime and Maximal Ideals

In this section, R will denote a commutative ring, not necessarily having an
identity. We shall introduce the notions of “prime ideal” and “maximal ideal,”
and we shall investigate relationships between these two notions.
   A proper ideal I in R is prime if ab ∈ I implies a ∈ I or b ∈ I . The ideal
I = R is not prime, by convention.5 We give three examples of prime ideals; a
fourth example will be given in a proposition immediately afterward.

   EXAMPLES.
   (1) For Z, it was shown in an example just before Proposition 4.21 that each
ideal is of the form mZ for some integer m. We may assume that m ∏ 0. The
prime ideals are 0 and all pZ with p prime. To see this latter fact, consider mZ
with m ∏ 2. If m = ab nontrivially, then neither a nor b is in I , but ab is in I ;
hence I is not prime. Conversely if m is prime, and if ab is in I = mZ, then
    5 This convention is now standard. Books written before about 1960 usually regarded I = R as

a prime ideal. Correspondingly they usually treated the zero ring as an integral domain.
                             3. Prime and Maximal Ideals                         385

ab = mc for some integer c. Since m is prime, Lemma 1.6 shows that m divides
a or m divides b. Hence a is in I or b is in I . Therefore I is prime.
   (2) If K is a field, then each ideal in R = K[X] is of the form A(X)K[X] with
A(X) in K[X], and A(X)K[X] is prime if and only if A(X) is 0 or is a prime
polynomial. In fact, each ideal is of the form A(X)K[X] by Proposition 5.8. If
A(X) is not a constant polynomial, then the argument that A(X)K[X] is prime
if and only if the polynomial A(X) is prime proceeds as in Example 1, using
Lemma 1.16 in place of Lemma 1.6.
   (3) In R = Z[X], the structure of the ideals is complicated, and we shall not
attempt to list all ideals. Let us observe simply that the ideal I = XZ[X] is prime.
In fact, if A(X)B(X) is in XZ[X], then A(X)B(X) = XC(X) for some C(X) in
Z[X]. If the constant terms of A(X) and B(X) are a0 and b0 , this equation says
that a0 b0 = 0. Therefore a0 = 0 or b0 = 0. In the first case, A(X) = X P(X)
for some P(X), and then A(X) is in I ; in the second case, B(X) = X Q(X) for
some Q(X), and then B(X) is in I . We conclude that I is prime.

  Proposition 8.7. An ideal I in the commutative ring R is prime if and only if
R/I is an integral domain.
    PROOF. If a proper ideal I fails to be prime, choose ab in I with a ∈  / I and
b∈ / I . Then a + I and b + I are nonzero in R/I and have product 0 + I . So
R/I is nonzero and has a zero divisor; by definition, R/I fails to be an integral
domain.
    Conversely if R/I (is nonzero and) has a zero divisor, choose a + I and b + I
nonzero with product 0 + I . Then neither a nor b is in I but ab is in I . Since I
is certainly proper, I is not prime.                                            §

   A proper ideal I in the commutative ring R is said to be maximal if R has no
proper ideal J with I $ J . If the commutative ring R has an identity, a simple
way of testing whether an ideal I is proper is to check whether 1 is in I ; in fact,
if 1 is in I , then I ⊇ R I ⊇ R1 = R implies I = R. Maximal ideals exist
in abundance when R is nonzero and has an identity, as a consequence of the
following result.

  Proposition 8.8. In a commutative ring R with identity, any proper ideal is
contained in a maximal ideal.
   PROOF. This follows from Zorn’s Lemma (Section A5 of the appendix).
Specifically let I be the given proper ideal, and form the set S of all proper
ideals that contain I . This set is nonempty, containing I as a member, and we
order it by inclusion upward. If we have a chain in S, then the union of the
members of the chain is an ideal that contains all the ideals in the chain, and it is
386                    VIII. Commutative Rings and Their Modules

proper since it does not contain 1. Therefore the union of the ideals in the chain is
an upper bound for the chain. By Zorn’s Lemma the set S has a maximal element,
and any such maximal element is a maximal ideal containing I .                    §

    Lemma 8.9. If R is a nonzero commutative ring with identity, then R is a field
if and only if the only proper ideal in R is 0.
   PROOF. If R is a field and I is a nonzero ideal in R, let a 6= 0 be in I . Then
1 = aa −1 is in I , and consequently I = R. Conversely if the only ideals in R
are 0 and R, let a 6= 0 be given in R, and form the ideal I = a R. Since 1 is in
R, a is in I . Thus I 6= 0. Then I must be R. So there exists some b in R with
1 = ba, and a is exhibited as having the inverse b.                             §

  Proposition 8.10. If R is a commutative ring with identity, then an ideal I is
maximal if and only if R/I is a field.
   REMARK. One can readily give a direct proof, but it seems instructive to give
a proof reducing the result to Lemma 8.9.
   PROOF. We consider R and R/I as unital R modules, the ideals for each of R
and R/I being the R submodules. The quotient ring homomorphism R → R/I is
an R homomorphism. By the First Isomorphism Theorem for modules (Theorem
8.3), there is a one-one correspondence between the ideals in R containing I and
the ideals in R/I . Then the result follows immediately from Lemma 8.9.       §

   Corollary 8.11. If R is a commutative ring with identity, then every maximal
ideal is prime.
    PROOF. If I is maximal, then R/I is a field by Proposition 8.10. Hence R/I
is an integral domain, and I must be prime by Proposition 8.7.              §

   In the converse direction nonzero prime ideals need not be maximal, as the
following example shows. However, Proposition 8.12 will show that nonzero
prime ideals are necessarily maximal in certain important rings.

   EXAMPLE. In R = Z[X], we have seen that I = XZ[X] is a prime ideal. But
I is not maximal since XZ[X] + 2Z[X] is a proper ideal that strictly contains I .

   Proposition 8.12. In R = Z or R = K[X] with K a field, every nonzero prime
ideal is maximal.
  PROOF. Examples 1 and 2 at the beginning of this section show that every
nonzero prime ideal is of the form I = p R with p prime. If such an I is given
and if J is any ideal strictly containing I , choose a in J with a not in I . Since a
                                 4. Unique Factorization                            387

is not in I = p R, it is not true that p divides a. So p and a are relatively prime,
and there exist elements x and y in R with x p + ya = 1, by Proposition 1.2c or
1.15d. Since p and a are in J , so is 1. Therefore J = R, and I is not strictly
contained in any proper ideal. So I is maximal.                                   §

    EXAMPLE. Algebraic number fields Q[θ]. These were introduced briefly in
Chapter IV and again in Section 1 as the Q linear span of all powers 1, θ, θ 2 , . . . .
Here θ is a nonzero complex number, and we make the assumption that Q[θ] is a
finite-dimensional vector space over Q. Proposition 4.1 showed that Q[θ] is then
indeed a field. Let us see how this conclusion relates to the results of the present
section. In fact, write a nontrivial linear dependence of 1, θ, θ 2 , . . . over Q in
the form c0 + c1 θ + c2 θ 2 + · · · + cn−1 θ n−1 + θ n = 0. Without loss of generality,
suppose that this particular linear dependence has n as small as possible among
all such relations. Then θ is a root of

               P(X) = c0 + c1 X + c2 X 2 + · · · + cn−1 X n−1 + X n .

Consider the substitution homomorphism E : Q[X] → C given by E(A(X)) =
A(θ). This ring homomorphism carries Q[X] onto the ring Q[θ], and the kernel
is some ideal I . Specifically I consists of all polynomials A(X) with A(θ) = 0,
and P(X) is one of these of lowest possible degree. Proposition 5.8 shows that I
consists of all multiples of some polynomial, and that polynomial may be taken
to be P(X) by minimality of the integer n. Proposition 8.1 therefore shows
that Q[θ] ∼ = Q[X]/P(X)Q[X] as a ring. If P(X) were to have a nontrivial
factorization as P(X) = Q 1 (X)Q 2 (X), then P(θ) = 0 would imply Q 1 (θ) = 0
or Q 2 (θ) = 0, and we would obtain a contradiction to the minimality of n.
Therefore P(X) is prime. By Example 2 earlier in the section, I = P(X)Q[X]
is a nonzero prime ideal, and Proposition 8.12 shows that it is maximal. By
Proposition 8.10 the quotient ring Q[θ] = Q[X]/P(X)Q[X] is a field. These
computations with Q[θ] underlie the first part of the theory of fields that we shall
develop in Chapter IX.


                             4. Unique Factorization

We have seen that the positive members of Z and the nonzero members of K[X],
when K is a field, factor into the products of “primes” and that these factorizations
are unique up to order and up to adjusting each of the prime factors in K[X] by
a unit. In this section we shall investigate this idea of unique factorization more
generally. Zero divisors are problematic from the point of view of factorization,
and it will be convenient to exclude them. Therefore we work exclusively with
integral domains.
388                    VIII. Commutative Rings and Their Modules

  The first observation is that unique factorization is not a completely general
notion for integral domains. Let us consider an example in detail.
                       p
   EXAMPLE. R  p = Z[ −5 ]. This is the subring of C whose
                                                         p            p are of
                                                                members
the form a + b −5 withpa and b integers. Since (a + b −5 )(c + d −5 ) =
(ac − 5cd) + (ad + bc) −5, R is closed under multiplication and is indeed a
                         p                            p            p
subring. Define N (a + b −5 ) = a 2 + 5b2 = (a + b −5 )(a + b −5). This
is a nonnegative-integer-valued function on R and is 0 only on the 0 element of
R. Since complex conjugation is an automorphism of C, we check immediately
that
         °       p            p      ¢           p               p
        N (a + b −5 )(c + d −5 ) = N (a + b −5 )N (c + d −5 ).

The group of units of R, i.e., of elements with inverses under multiplication, is
denoted by R × as usual. If r is in R × , then rr −1 = 1, and so N (r)N (r −1 ) =
N (1) = 1. Consequently the units r of R all have N (r) = 1. Setting a 2 +5b2 = 1,
we see that the units are ±1. The product formula for N shows that if we start
factoring a member of R, then factor its factors, and so on, and if we forbid
factorizations into two factors when one is a unit, then the process of factorization
has to stop at some point. So complete factorization makes sense. Now consider
the equality                      p           p
                        6 = (1 + −5 )(1 − −5 ) = 2 · 3.
                                 p                    p
The factors here have N (1 + −5 ) = N (1 − −5 ) = 6, N (2) = 4, and
N (3) = 9. Considering the possible values of a 2 + 5b2 , we seepthat N ( · )pdoes
not take on either of the values 2 and 3 on R. Consequently 1 + −5, 1 − −5,
2, and 3 do not have nontrivial factorizations. On the other hand, considerationp
of the values of N ( · ) shows that 2 and 3 are not products of either of 1 ± −5
with units. We conclude that the displayed factorizations of 6 show that unique
factorization has failed.

   Thus unique factorization is not universal for integral domains. It is time
to be careful about terminology. With Z and K[X], we have referred to the
individual factors in a complete factorization as “primes.” Their defining property
in Chapter I was that they could not be factored further in nontrivial fashion.
Primes in these rings were shown to have the additional property that if a prime
divides a product then it divides one of the factors. It is customary to separate
these two properties for general integral domains. Let us say that a nonzero
element a divides b if b = ac for some c. In this case we say also that a is
a factor of b. In an integral domain R, a nonzero element r that is not a unit
is said to be irreducible if every factorization r = r1r2 in R has the property
that either r1 or r2 is a unit. Nonzero nonunits that are not irreducible are said
                                       4. Unique Factorization                                      389

to be reducible. A nonzero element p that is not a unit is said to be prime6 if
the condition that p divides a product ab always implies that p divides a or p
divides b.
    Prime implies irreducible. In fact, if p is a prime that is reducible, let us write
p = r1r2 with neither r1 nor r2 equal to a unit. Since p is prime, p divides r1 or
r2 , say r1 . Then r1 = pc with c in R, and we obtain p = r1r2 = pcr2 . Since
R is an integral domain, 1 = cr2 , and r2 is exhibited as a unit with inverse c, in
contradiction to the assumption that r2 is not a unit.
                     p irreducible does not imply
    On the other hand,
    p                                              p prime. In fact, we saw       p in
Z[ −5 ] that 1 + −5 is irreducible. But 1 + p −5 divides 2 · 3, and 1 + −5
does not divide either of 2 or 3. Therefore 1 + −5 is not prime.
    We shall see in a moment that the distinction between “irreducible” and “prime”
lies at the heart of the question of unique factorization. Let us make a definition
that helps identify our problem precisely. We say that an integral domain R is a
unique factorization domain if R has the two properties
      (UFD1) every nonzero nonunit of R is a finite product of irreducible ele-
                ments,
      (UFD2) the factorization in (UFD1) is always unique up to order and to
                multiplication of the factors by units.
The problem that arises for us for a given R is to decide whether R is a unique
factorization domain. The following proposition shows the relevance of the
distinction between “irreducible” and “prime.”

  Proposition 8.13. In an integral domain R in which (UFD1) holds, the
condition (UFD2) is equivalent to the condition
   (UFD20 ) every irreducible element is prime.
   REMARKS. In fact, showing that irreducible implies prime was the main step
in Chapter I in proving unique factorization for positive integers and for K[X]
when K is a field. The mechanism for carrying out the proof that irreducible
implies prime for those settings will be abstracted in Theorems 8.15 and 8.17.
   PROOF. Suppose that (UFD2) holds, that p is an irreducible element, and
that p divides ab. We are to show that p divides a orQp divides b.Q We may
assume that ab 6= 0. Write ab = pc, and let a = i pi , b = j p0j , and
     Q
c = k qk be factorizations via (UFD1) into products of irreducible elements.
    6 This definition enlarges the definition of “prime” in Z to include the negatives of the usual prime

numbers. Unique factorization immediately extends to nonzero integers of either sign, but the prime
factors are now determined only up to factors of ±1. In cases where confusion about the sign of an
integer prime might arise, the text will henceforth refer to “primes of Z” or “integer primes” when
both signs are allowed, and to “positive primes” or “prime numbers” when the primes are understood
to be as in Chapter I.
390                      VIII. Commutative Rings and Their Modules
      Q                Q
Then i, j pi p0j = p k qk . By (UFD2) one of the factors on the left side is εp
for some unit ε. Then p either is of the form ε−1 pi and then p divides a, or is of
the form ε−1 p0j and then p divides b. Hence (UFD20 ) holds.
   Conversely suppose that (UFD20 ) holds. Let the nonzero nonunit r have two
factorizations into irreducible elements as r = p1 p2 · · · pm = ε0 q1 q2 · · · qn with
m ≤ n and with ε0 a unit. We prove the uniqueness by induction on m, the case
m = 0 following vacuously since r is not a unit and the case m = 1 following
from the definition of “irreducible.” Inductively from (UFD20 ) we know that pm
divides qk for some k. Since qk is irreducible, qk = εpm for some unit ε. Thus we
can cancel qk and obtain p1 p2 · · · pm−1 = ε0 εq1 q2 · · · qbk · · · qn , the hat indicating
an omitted factor. By induction the factors on the two sides here are the same
except for order and units. Thus the same conclusion is valid when comparing the
two sides of the equality p1 p2 · · · pm = ε0 q1 q2 · · · qn . The induction is complete,
and (UFD2) follows.                                                                        §

   It will be convenient to simplify our notation for ideals. In any commutative
ring R with identity, if a is in R, we let (a) denote the ideal Ra generated by a.
An ideal of this kind with a single generator is called a principal ideal. More
generally, if a1 , . . . , an are members of R, then (a1 , . . . , an ) denotes the ideal
Ra1 + · · · + Ran generated by a1 , . . . , an . For example, in Z[X], (2, X) denotes
the ideal 2Z + XZ of all polynomials whose constant term is even. The following
condition explains a bit the mystery of what it means for an element to be prime.

  Proposition 8.14. A nonzero element p in an integral domain R is prime if
and only if the ideal ( p) in R is prime.
    PROOF. Suppose that the element p is prime. Then the ideal ( p) is not R; in
fact, otherwise 1 would have to be of the form 1 = r p for some r ∈ R, r would be
a multiplicative inverse of p, and p would be a unit. Now suppose that a product
ab is in the ideal ( p). Then ab = pr for some r in R, and p divides ab. Since p
is prime, p divides a or p divides b. Therefore the ideal ( p) is prime.
    Conversely suppose that ( p) is a prime ideal with p 6= 0. Since ( p) 6= R, p
is not a unit. If p divides the product ab, then ab = pc for some c in R. Hence
ab is in ( p). Since ( p) is assumed prime, either a is in ( p) or b is in ( p). In the
first case, p divides a, and in the second case, p divides b. Thus the element p is
prime.                                                                               §

   An integral domain R is called a principal ideal domain if every ideal in R is
principal. At the beginning of Section 3, we saw a reminder that Z is a principal
ideal domain and that so is K[X] whenever K is a field. It turns out that unique
factorization for these cases is a consequence of this fact.
                                4. Unique Factorization                            391

   Theorem 8.15. Every principal ideal domain is a unique factorization domain.
   REMARKS. Let R be the given principal ideal domain. Proposition 8.13 shows
that it is enough to show that (UFD1) and (UFD20 ) hold in R.
   PROOF OF (UFD1). Let a1 be a nonzero nonunit of R. Then the ideal (a1 )
in R is proper and nonzero, and Proposition 8.8 shows that it is contained in a
maximal ideal. Since R is a principal ideal domain, this maximal ideal is of the
form (c1 ) for some c1 , and c1 is a nonzero nonunit. Maximal ideals are prime
by Corollary 8.11, and Proposition 8.14 thus shows that c1 is a prime element,
necessarily irreducible. Therefore the inclusion (a1 ) ⊆ (c1 ) shows that some
irreducible element, namely c1 , divides a1 .
   Write a1 = c1 a2 , and repeat the above argument with a2 . Iterating this
construction, we obtain an = cn an+1 for each n with cn irreducible. Thus
a1 = c1 c2 · · · cn an+1 with c1 , . . . , cn irreducible. Let us see that this process
cannot continue indefinitely. Assuming the contrary, we are led to the strict
inclusions
                             (a1 ) $ (a2 ) $ (a3 ) $ · · · .
          S∞
Put I = n=1 (an ). Then I is an ideal. Since R is a principal ideal domain,
I = (a) for some element a. This element a must be in (ak ) for some k, and then
we have (ak ) = (ak+1 ) = · · · = (a). Since (ak ) = (ak+1 ), ck has to be a unit,
contradiction. Thus ak has no nontrivial factorization, and a1 = c1 · · · ck−1 ak is
the desired factorization. This proves (UFD1).                                       §
   PROOF OF (UFD20 ). If p is an irreducible element, we prove that the ideal ( p)
is maximal. Corollary 8.11 shows that ( p) is prime, and Proposition 8.14 shows
that p is prime. Thus (UFD20 ) will follow.
   The element p, being irreducible, is not a unit. Thus ( p) is proper. Suppose
that I is an ideal with I % ( p). Since R is a principal ideal domain, I = (c)
for some c. Then p = rc for some r in R. Since I 6= ( p), r cannot be a unit.
Therefore the irreducibility of p implies that c is a unit. Then I = (c) = (1) = R,
and we conclude that ( p) is maximal.                                            §

   Let us record what is essentially a corollary of the proof.

  Corollary 8.16. In a principal ideal domain, every nonzero prime ideal is
maximal.
   PROOF. Let ( p) be a nonzero prime ideal. Proposition 8.14 shows that p
is prime, and prime elements are automatically irreducible. The argument for
(UFD20 ) in the proof of Theorem 8.15 then deduces in the context of a principal
ideal domain that ( p) is maximal.                                            §
392                              VIII. Commutative Rings and Their Modules

    Principal ideal domains arise comparatively infrequently, and recognizing
them is not necessarily easy. The technique that was used with Z and K[X]
generalizes slightly, and we take up that generalization now. An integral domain
R is called a Euclidean domain if there exists a function δ : R → {integers ∏ 0}
such that whenever a and b are in R with b 6= 0, there exist q and r in R with
a = bq + r and δ(r) < δ(b). The ring Z of integers is a Euclidean domain if we
take δ(n) = |n|, and the ring K[X] for K a field is a Euclidean domain if we take
δ(P(X)) to be 2deg P if P(X) 6= 0 and to be 0 if P(X) =p0.                 p
    Another example of a Euclidean                   p Z[ −1 ]p= Z + Z 2−1 of
                                    pdomain is the ring
Gaussian integers. It has δ(a + b −1 ) p    = (a + b −1 )(a − b −1 ) = a + b2 ,
a and b being integers. Let us abbreviate −1 as i. To see that δ has the required
property, we first extend δ to Q[i], writing δ(x + yi) = (x + yi)(x − yi) = x 2 + y 2
if x and y are rational. We use the fact that

                            δ(zz 0 ) = δ(z)δ(z 0 )     for z and z 0 in Q[i],

which follows from the computation δ(zz 0 ) = zz 0 · zz 0 = zzz 0 z 0 = δ(z)δ(z 0 ).
For
Ø any    real number
              Ø        u, let [u] be the greatest integer ≤ u. Every real u satisfies
Ø[u + 1 ] − u Ø ≤ 1 . Given a + ib and c + di with c + di 6= 0, we write
      2           2


                  a + bi   (a + bi)(c − di)  ac + bd  bc − ad
                         =      2     2
                                            = 2    2
                                                     + 2      i.
                  c + di       c +d           c +d    c + d2
          h                     i     h            i
              ac+bd         1
Put p =       c2 +d 2
                        +   2    , q = bc−ad
                                         2
                                        c +d 2 + 1
                                                 2 , and r +si = (a +bi)−(c+di)( p +qi).
Then
                                a + bi = (c + di)( p + qi) + (r + si),

and

             °                            ¢             ≥ a + bi            ¥
δ(r + si) = δ (a + bi) − (c + di)( p + qi) = δ(c + di)δ          − ( p + qi) .
                                                          c + di
                                                          ° ac+bd     ¢ ° bc−ad     ¢
The complex number x + yi = a+bi    c+di − ( p + qi) = c2 +d 2 − p + c2 +d 2 − q i
has |x| ≤ 12 and |y| ≤ 12 , and therefore δ(x + yi) = x 2 + y 2 ≤ 14 + 14 = 12 . Hence
δ(r + si) < δ(c + di), as required.
   Some further examples of this kind appear in Problems 13 p         and 25–26 at the
end of the chapter. The matter
                            p      is a little
                                             p delicate. The  ring Z[  −5 ] may seem
superficially similar to Z[ −1 ]. But Z[ −5 ] does not have unique factorization,
andpthe following theorem, in combination with Theorem 8.15, assures us that
Z[ −5 ] cannot be a Euclidean domain.
                                   5. Gauss’s Lemma                                  393

   Theorem 8.17. Every Euclidean domain is a principal ideal domain.
   PROOF. Let I be an ideal in R. We are to show that I is principal. Without
loss of generality, we may assume that I 6= 0. Choose b 6= 0 in I with δ(b) as
small as possible. Certainly I ⊇ (b). If a 6= 0 is in I , write a = bq + r with
δ(r) < δ(b). Then r = a − bq is in I with δ(r) < δ(b). The minimality of b
forces r = 0 and a = bq. Thus I ⊆ (b), and we conclude that I = (b).         §


                                 5. Gauss’s Lemma

In the previous section we saw that every principal ideal domain has unique
factorization. In the present section we shall establish that certain additional
integral domains have unique factorization, namely any integral domain R[X]
for which R is a unique factorization domain. A prototype is Z[X], which will
be seen to have unique factorization even though there exist nonprincipal ideals
like (2, X) in the ring. An important example for applications, particularly in
algebraic geometry, is K[X 1 , . . . , X n ], where K is a field; in this case our result
is to be applied inductively, making use of the isomorphism K[X 1 , . . . , X n ] ∼    =
K[X 1 , . . . , X n−1 ][X n ] given in Corollary 4.31.
    For the conclusion that R[X] has unique factorization if R does, the heart of
the proof is an application of a result known as Gauss’s Lemma, which we shall
prove in this section. Gauss’s Lemma has additional consequences for R[X]
beyond unique factorization, and we give them as well.
    Before coming to Gauss’s Lemma, let us introduce some terminology and
prove one preliminary result. In any integral domain R, we call two nonzero
elements a and b associates if a = bε for some ε in the group R × of units. The
property of being associates is an equivalence relation because R × is a group.
    Still with the nonzero integral domain R, let us define a greatest common
divisor of two nonzero elements a and b to be any element c of R such that c
divides both a and b and such that any divisor of a and b divides c. Any associate
of a greatest common divisor of a and b is another greatest common divisor of
a and b. Conversely if a and b have a greatest common divisor, then any two
greatest common divisors are associates. In fact, if c and c0 are greatest common
divisors, then each of them divides both a and b, and the definition forces each
of them to divide the other. Thus c0 = cε and c = c0 ε0 , and then c0 = c0 ε0 ε and
1 = ε0 ε. Consequently ε is a unit, and c and c0 are associates.
    If R is a unique factorization domain, then any two nonzero elements a and b
have a greatest common divisor. In fact, we decompose a and b into the product
                                                                           Qm
of a unitQby powers of nonassociate irreducible elements as a = ε i=1            piki and
             n
b = ε0 j=1 p0j l j . For each p0j such that p0j is associate to some pi , we replace
p0j by pi in the factorization of b, adjusting ε0 as necessary, and then we reorder
394                        VIII. Commutative Rings and Their Modules

the factors of a and b so that the common pi ’s are the ones for 1 ≤ i ≤ r. Then
     Q
c = ri=1 pimin(ki ,li ) is a greatest common divisor of a and b. We write GCD(a, b)
for a greatest common divisor of a and b; as we saw above, this is well defined
up to a factor of a unit.7
   One should not read too much into the notation. In a principal ideal domain if
a and b are nonzero, then, as we shall see momentarily, GCD(a, b) is defined by
the condition on ideals that

                                    (GCD(a, b)) = (a, b).

This condition implies that there exist elements x and y in R such that

                                    xa + yb = GCD(a, b).

However, in the integral domain Z[X], in which GCD(2, X) = 1, there do not
exist polynomials A(X) and B(X) with A(X)2 + B(X)X = 1.
   To prove that (GCD(a, b)) = (a, b) in a principal ideal domain, write (c)
for the principal ideal (a, b); c satisfies c = xa + yb for some x and y in R.
Since a and b lie in (c), a = rc and b = r 0 c. Hence c divides both a and b.
In the reverse direction if d divides a and b, then ds = a and ds 0 = b. Hence
c = xa + yb = (xs + ys 0 )d, and d divides c. So c is indeed a greatest common
divisor of a and b.
   In a unique factorization domain the definition of greatest common divisor
immediately extends to apply to n nonzero elements, rather than just two. We
readily check up to a unit that
                                            °                           ¢
                 GCD(a1 , . . . , an ) = GCD GCD(a1 , . . . , an−1 ), an .

Moreover, we can allow any of a2 , . . . , an to be 0, and there is no difficulty. In
addition, we have

             GCD(da1 , . . . , dan ) = d GCD(a1 , . . . , an )            up to a unit

if d and a1 are not 0.
    Let R be a unique factorization domain. If A(X) is a nonzero element of R[X],
we say that A(X) is primitive if the GCD of its coefficients is a unit. In this case
no prime of R divides all the coefficients of A(X).

    7 Greatest common divisors can exist for certain integral domains that fail to have unique factor-

ization, but we shall not have occasion to work with any such domains.
                                   5. Gauss’s Lemma                                395

   Theorem 8.18 (Gauss’s Lemma). If R is a unique factorization domain, then
the product of primitive polynomials is primitive.
  PROOF #1. Arguing by contradiction, let A(X) = am X m + · · · + a0 and
B(X) = bn X n + · · · + b0 be primitive polynomials such that every coefficient of
A(X)B(X) is divisible by some prime p. Since A(X) and B(X) are primitive,
we may choose k and l as small as possible such that p does not divide ak and
does not divide bl . The coefficient of X k+l in A(X)B(X) is
                 a0 bk+l + a1 bk+l−1 + · · · + ak bl + · · · + ak+l b0
and is divisible by p. Then all the individual terms, and their sum, are divisible
by p except possibly for ak bl , and we conclude that p divides ak bl . Since p is
prime and p divides ak bl , p must divide ak or bl , contradiction.             §
    PROOF #2. Arguing by contradiction, let A(X) and B(X) be primitive poly-
nomials such that every coefficient of A(X)B(X) is divisible by some prime
p. Proposition 8.14 shows that the ideal ( p) is prime, and Proposition 8.7
shows that R 0 = R/( p) is an integral domain. Let ϕ : R → R 0 [X] be the
composition of the quotient homomorphism R → R 0 and the inclusion of R 0 into
constant polynomials in R 0 [X], and let 8 : R[X] → R 0 [X] be the corresponding
substitution homomorphism of Proposition 4.24 that carries X to X. Since A(X)
and B(X) are primitive, 8(A(X)) and 8(B(X)) are not zero. Their product
8(A(X))8(B(X)) = 8(A(X)B(X)) is 0 since p divides every coefficient of
A(X)B(X), and this conclusion contradicts the assertion of Proposition 4.29 that
R 0 [X] is an integral domain.                                                §

   Let F be the field of fractions of the unique factorization domain R. The
consequences of Theorem 8.18 exploit a simple relationship between R[X] and
F[X], which we state below as Proposition 8.19. Once that proposition is in hand,
we can state the consequences of Theorem 8.18. If A(X) is a nonzero polynomial
in R[X], let c(A) to be the greatest common divisor of the coefficients, i.e.,
     c(A) = GCD(an , . . . , a1 , a0 )    if A(X) = an X n + · · · + a1 X + a0 .
The element c(A) is well defined up to a factor of a unit. In this notation the
definition of “primitive” becomes, A(X) is primitive if and only if c(A) is a unit.
We shall make computations with c(A) as if it were a member of R, in order to
keep the notation simple. To be completely rigorous, one should regard c(A) as
an orbit of the group R × of units in R, using equality to refer to equality of orbits.
   If A(X) is not necessarily primitive, then at least c(A) divides each coefficient
of A(X), and hence c(A)−1 A(X) is in R[X], say with coefficients bn , . . . , b1 , b0 .
Then we have
      c(A) = GCD(an , . . . , a1 , a0 ) = GCD(c(A)bn , . . . , c(A)b1 , c(A)b0 )
                                                   °                ¢
           = c(A)GCD(bn , . . . , b1 , b0 ) = c(A)c c(A)−1 A(X)
396                   VIII. Commutative Rings and Their Modules
                                °           ¢
up to a unit factor, and hence c c(A)−1 A(X) is a unit. We conclude that

         A(X) ∈ R[X]         implies that       c(A)−1 A(X) is primitive.

   Proposition 8.19. Let R be a unique factorization domain, and let F be its
field of fractions. If A(X) is any nonzero polynomial in F[X], then there exist α
in F and A0 (X) in R[X] such that A(X) = α A0 (X) with A0 (X) primitive. The
scalar α and the polynomial A0 (X) are unique up to multiplication by units in R.
  REMARK. We call A0 (X) the associated primitive polynomial to A(X).
According to the proposition, it is unique up to a unit factor in R.
   PROOF. Let A(X) = cn X n + · · · + c1 X + c0 with each ck in F. We can write
                −1
each ck as aQk bk with ak and bk in R and bk 6= 0. We clear fractions.
                                                                Q          That is,
                n
we let β = k=0 bk . Then the k th coefficient of β A(X) is ak l6=k bl and is in
R. Hence β A(X) is in R[X]. The observation just before the proposition shows
that c(β A)−1 β A is primitive. Thus A(X) = α A0 (X) with α = β −1 c(β A) and
A0 (X) = c(β A)−1 β A(X), A0 (X) being primitive. This proves existence.
   If α1 A1 (X) = α2 A2 (X) with α1 and α2 in F and with A1 (X) and A2 (X)
primitive, choose r 6= 0 in R such that rα1 and rα2 are in R. Up to unit factors in
R, we then have rα1 = rα1 c(A1 ) = c(rα1 A1 ) = c(rα2 A2 ) = rα2 c(A2 ) = rα2 .
Hence, up to a unit factor in R, we have α1 = α2 . This proves uniqueness.       §

   Corollary 8.20. Let R be a unique factorization domain, and let F be its field
of fractions.
   (a) Let A(X) and B(X) be nonzero polynomials in R[X], and suppose that
B(X) is primitive. If B(X) divides A(X) in F[X], then it divides A(X) in R[X].
   (b) If A(X) is an irreducible polynomial in R[X] of degree > 0, then A(X) is
irreducible in F[X].
   (c) If A(X) is a monic polynomial in R[X] and if B(X) is a monic factor of
A(X) within F[X], then B(X) is in R[X].
   (d) If A(X), B(X), and C(X) are in R[X] with A(X) primitive and with
A(X) = B(X)C(X), then B(X) and C(X) are primitive.
   PROOF. In (a), write A(X) = B(X)Q(X) in F(X), and let Q(X) = ρ Q 0 (X) be
a decomposition of Q(X) as in Proposition 8.19. Since c(A)°−1 A(X) is primitive,
                                                                         ¢
the corresponding decomposition of A(X) is A(X) = c(A) c(A)−1 A(X) . The
equality A(X) = ρ B(X)Q 0 (X) then reads c(A)(c(A)−1 A(X)) = ρ B(X)Q 0 (X).
Since B(X)Q 0 (X) is primitive according to Theorem 8.18, the uniqueness in
Proposition 8.19 shows that c(A)−1 A(X) = B(X)Q 0 (X) except possibly for a
unit factor in R. Then B(X) divides A(X) with quotient c(A)Q 0 (X), apart from
a unit factor in R. Since c(A)Q 0 (X) is in R[X], (a) is proved.
                                  5. Gauss’s Lemma                               397

    In (b), the condition that deg A(X) > 0 implies that A(X) is not a unit in
F[X]. Arguing by contradiction, suppose that A(X) = B(X)Q(X) in F[X] with
neither of B(X) and Q(X) of degree 0. Let B(X) = β B0 (X) be a decomposition
of B(X) as in Proposition 8.19. Then we have A(X) = B0 (X)(β Q(X)), and (a)
shows that β Q(X) is in R[X], in contradiction to the assumed irreducibility of
A(X) in R[X].
    In (c), write A(X) = B(X)Q(X), and let B(X) = β B0 (X) be a decomposition
of B(X) as in Proposition 8.19. Then we have A(X) = B0 (X)(β Q(X)) with
β Q(X) in F[X]. Conclusion (a) shows that β Q(X) is in R[X]. If b ∈ R is the
leading coefficient of B0 (X) and if q ∈ R is the leading coefficient of β Q(X), then
we have 1 = bq, and consequently b and q are units in R. Since B(X) = β B0 (X)
and B(X) is monic, 1 = βb, and therefore β = b−1 is a unit in R. Hence B(X)
is in R[X].
    In (d), we argue along the same lines as in (a). We may take B(X) =
c(B)(c(B)−1 B(X)) and C(X) = c(C)(c(C)−1 C(X)) as decompositions of
B(X) and £C(X) according to Proposition  §        8.19. Then we have A(X) =
(c(B)c(C)) c(B)−1 B(X)c(C)−1 C(X) . Theorem 8.18 says that the factor in
brackets is primitive, and the uniqueness in Proposition 8.19 shows that 1 =
c(B)c(C), up to unit factors. Therefore c(B) and c(C) are units in R, and B(X)
and C(X) are primitive.                                                            §

   Corollary 8.21. If R is a unique factorization domain, then the ring R[X] is
a unique factorization domain.
   REMARK. As was mentioned at the beginning of the section, Z[X] and
K[X 1 , . . . , X n ], when K is a field, are unique factorization domains as a con-
sequence of this result.
   PROOF. We begin with the proof of (UFD1). Suppose that A(X) is a nonzero
member of R[X]. We may take its decomposition according to Proposition 8.19
to be A(X) = c(A)(c(A)−1 A(X)). Consider divisors of c(A)−1 A(X) in R[X].
These are all primitive, according to (d). Hence those of degree 0 are units
in R. Thus any nontrivial factorization of c(A)−1 A(X) is into two factors of
strictly lower degree, both primitive. In a finite number of steps, this process of
factorization with primitive factors has to stop. We can then factor c(A) within R.
Combining the factorizations of c(A) and c(A)−1 A(X), we obtain a factorization
of A(X).
   For (UFD20 ), let P(X) be irreducible in R[X]. Since the factorization P(X) =
c(P)(c(P)−1 P(X)) has to be trivial, either c(P) is a unit, in which case P(X) is
primitive, or c(P)−1 P(X) is a unit, in which case P(X) has degree 0. In either
case, suppose that P(X) divides a product A(X)B(X).
   In the first case, P(X) is primitive. Since F[X] is a principal ideal domain,
hence a unique factorization domain, either P(X) divides A(X) in F[X] or P(X)
398                     VIII. Commutative Rings and Their Modules

divides B(X) in F[X]. By symmetry we may assume that P(X) divides A(X)
in F[X]. Then (a) shows that P(X) divides A(X) in R[X].
   In the second case, P(X) = P has degree 0 and is prime in R. Put R 0 = R(P)
as in Proof #2 of Theorem 8.18. Then A(X)B(X) maps to zero in the integral
domain R 0 [X], and hence A(X) or B(X) is in P R[X].                        §

   The final application, Eisenstein’s irreducibility criterion, is proved somewhat
in the style of Gauss’s Lemma (Theorem 8.18). We shall give only the analog of
Proof #1 of Gauss’s Lemma, leaving the analog of Proof #2 to Problem 21 at the
end of the chapter.

   Corollary 8.22 (Eisenstein’s irreducibility criterion). Let R be a unique fac-
torization domain, let F be its field of fractions, and let p be a prime in R. If
A(X) = a N X N + · · · + a1 X + a0 is a polynomial of degree ∏ 1 in R[X] such
that p divides a N −1 , . . . , a0 but not a N and such that p2 does not divide a0 , then
A(X) is irreducible in F[X].
  REMARK. The polynomial A(X) will be irreducible in R[X] also unless all its
coefficients are divisible by some nonunit of R.
   PROOF. Without loss of generality, we may replace A(X) by c(A)−1 A(X)
and thereby assume that A(X) is primitive; this adjustment makes use of the
hypothesis that p does not divide a N . Corollary 8.20b shows that it is enough to
prove irreducibility in R[X]. Assuming the contrary, suppose that A(X) factors
in R[X] as A(X) = B(X)C(X) with B(X) = bm X m + · · · + b1 X + b0 , C(X) =
cn X n + · · · + c1 X + c0 , and neither of B(X) and C(X) equal to a unit. Corollary
8.20d shows that B(X) and C(X) are primitive. In particular, B(X) and C(X)
have to be nonconstant polynomials. Define ak = 0 for k > N , bk = 0 for k > m,
and ck = 0 for k > n. Since p divides a0 = b0 c0 and p is prime, p divides either
b0 or c0 . Without loss of generality, suppose that p divides b0 . Since p2 does not
divide a0 , p does not divide c0 .
   We show, by induction on k, that p divides bk for every k < N . The case
k = 0 is the base case of the induction. If p divides b j for j < k, then we have

                    ak = b0 ck + b1 ck−1 + · · · + bk−1 c1 + bk c0 .

Since k < N , the left side is divisible by p. The inductive hypothesis shows
that p divides every term on the right side except possibly the last. Consequently
p divides bk c0 . Since p does not divide c0 , p divides bk . This completes the
induction.
   Since C(X) is nonconstant, the degree of B(X) is < N , and therefore we have
shown that every coefficient of B(X) is divisible by p. Then c(B) is divisible by
p, in contradiction to the fact that B(X) is primitive.                         §
                             6. Finitely Generated Modules                        399

   EXAMPLES.
   (1) Cyclotomic polynomials in Q[X]. Let us see for each prime number p that
the polynomial 8(X) = X p−1 + X p−2 + · · · + X + 1 is irreducible in Q[X].
We have X p − 1 = (X − 1)8(X). Replacing X − 1 by Y gives                 ¢ 1)
                                                                 P p (Y° p+
                                                                               p
                                                                                 −1 =
                                                                             k
Y 8(Y + 1). The left side, by the Binomial Theorem, is k=1 k Y . Hence
                   P p ° p¢ k−1                                 ° ¢
8(Y + 1) =            k=1 k Y    . The binomial coefficient kp is divisible by p
for 1 ≤ k ≤ p − 1 since p is prime, and therefore the polynomial 9(Y ) =
8(Y + 1) satisfies the condition of Corollary 8.22 for the ring Z. Hence 9(Y ) is
irreducible over Q[Y ]. A nontrivial factorization of 8(X) would yield a nontrivial
factorization of 9(Y ), and hence 8(X) is irreducible over Q[X].
   (2) Certain polynomials in K[X, Y ] when K is a field. Since K[X, Y ] ∼          =
K[X][Y ], it follows that K[X, Y ] is a unique factorization domain, and any mem-
ber of K[X, Y ] can be written as A(X, Y ) = an (X)Y n + · · · + a1 (X)Y + a0 (X).
The polynomial X is prime in K[X, Y ], and Corollary 8.22 therefore says that
A(X, Y ) is irreducible in K(X)[Y ] if X does not divide an (X) in K[X], X divides
an−1 (X), . . . , a0 (X) in K[X], and X 2 does not divide a0 (X) in K[X]. The remark
with the corollary points out that A(X, Y ) is irreducible in K[X, Y ] if also there
is no nonconstant polynomial in K[X] that divides every ak (X). For example,
Y 5 + X Y 2 + X Y + X is irreducible in K[X, Y ].


                         6. Finitely Generated Modules

The Fundamental Theorem of Finitely Generated Abelian Groups (Theorem 4.56)
says that every finitely generated abelian group is a direct sum of cyclic groups.
If we think of abelian groups as Z modules, we can ask whether this theorem
has some analog in the context of R modules. The answer is yes—the theorem
readily extends to the case that Z is replaced by an arbitrary principal ideal domain.
The surprising addendum to the answer is that we have already treated a second
special case of the generalized theorem. That case arises when the principal ideal
domain is K[X] for some field K. If V is a finite-dimensional vector space over
K and L : V → V is a K linear map, then V becomes a K[X] module under
the definition Xv = L(v). This module is finitely generated even without the
X present because V is finite-dimensional, and the generalized theorem that we
prove in this section recovers the analysis of L that we carried out in Chapter V.
When K is algebraically closed, we obtain the Jordan canonical form; for general
K, we obtain a different canonical form involving cyclic subspaces that was
worked out in Problems 32–40 at the end of Chapter V.
   The definitions for the generalization of Theorem 4.56 are as follows. Let
R be a principal ideal domain. A subset S of an R module M is called a set
of generators of M if M is the smallest R submodule of M containing all the
400                       VIII. Commutative Rings and Their Modules

membersP of S. If {m s | s ∈ S} is a subset of M, then the set of all finite
sums s∈S rs m s is an R submodule, but it need not contain the elements m s and
therefore need not be the R submodule generated by all the m s . However, if M
                               and all other rs equal to 0 exhibits m s0 as in the R
is unital, then taking rs0 = 1 P
submodule of all finite sums s∈S rs m s . For this reason we shall insist that all
the R submodules in this section be unital.
    We say that the R module M is finitely generated if it has a finite set of
generators. The main theorem gives the structure of unital finitely generated R
modules when R is a principal ideal domain. We need to take a small preliminary
step that eliminates technical complications from the discussion, the same step
that was carried out in Lemma 4.51 and Proposition 4.52 in the case of Z modules,
i.e., abelian groups.

  Lemma 8.23. Let R be a commutative ring with identity, and let ϕ : M → N
be a homomorphism of unital R modules. If ker ϕ and image ϕ are finitely
generated, then M is finitely generated.
    PROOF. Let {x1 , . . . , xm } and {y1 , . . . , yn } be respective finite sets of generators
for ker ϕ and image ϕ. For 1 ≤ j ≤ n, choose x j0 in M with ϕ(x j0 ) = yj . We shall
prove that {x1 , . . . , xm , x10 , . . . , xn0 } is a set of generators for M. Thus let x be in M.
Since ϕ(x) is in image ϕ, there exist r1 , . . . , rn in R with ϕ(x) = r1 y1 +· · ·+rn yn .
The element x 0 = r1 x10 + · · · + rn xn0 of M has ϕ(x 0 ) = r1 y1 + · · · + rn yn = ϕ(x).
Therefore ϕ(x − x 0 ) = 0, and there exist s1 , . . . , sm in R such that x − x 0 =
s1 x1 + · · · + sm xm . Consequently
      x = s1 x1 + · · · + sm xm + x 0 = s1 x1 + · · · + sm xm + r1 x10 + · · · + rn xn0 . §
   Proposition 8.24. If R is a principal ideal domain, then any R submodule
of a finitely generated unital R module is finitely generated. Moreover, any R
submodule of a singly generated unital R module is singly generated.
   REMARK. The proof will show that if M can be generated by n elements, then
so can the unital R submodule.
   PROOF. Let M be unital and finitely generated with a set {m 1 , . . . , m n } of n
generators, and define Mk = Rm 1 + · · · + Rm k for 1 ≤ k ≤ n. Then Mn = M
since M is unital. We shall prove by induction on k that every R submodule of
Mk is finitely generated. The case k = n then gives the proposition. For k = 1,
suppose that S is an R submodule of M1 = Rm 1 . Since S is an R submodule
and every member of S lies in Rm 1 , the subset I of all r in R with rm 1 in S is
an ideal with I m 1 = S. Since every ideal in R is singly generated, we can write
I = (r0 ). Then S = I m 1 = Rr0 m 1 , and the single element r0 m 1 generates S.
   Assume inductively that every R submodule of Mk is known to be finitely
generated, and let Nk+1 be an R submodule of Mk+1 . Let q : Mk+1 → Mk+1 /Mk
                             6. Finitely Generated Modules                        401
                                                                  Ø
be the quotient R homomorphism, and let ϕ be the restriction q Ø Nk+1 , mapping
Nk+1 into Mk+1 /Mk . Then ker ϕ = Nk+1 ∩ Mk is an R submodule of Mk and is
finitely generated by the inductive hypothesis. Also, image ϕ is an R submodule
of Mk+1 /Mk , which is singly generated with generator equal to the coset of
m k+1 . Since an R submodule of a singly generated unital R module was shown
in the previous paragraph to be singly generated, image ϕ is finitely generated.
Applying Lemma 8.23 to ϕ, we see that Nk+1 is finitely generated. This completes
the induction and the proof.                                                  §

    According to the definition in Example 9 of modules in Section 1, a free R
module is a direct sum, finite or infinite, of copies of the R module R. A free R
module is said to have finite rank if some direct sum is a finite direct sum. A
unital R module M is said to be cyclic if it is singly generated, i.e., if M = Rm 0
for some m 0 in M. In this case, we have an R isomorphism M ∼       = R/I , where I
is the ideal {r ∈ R | rm 0 = 0}.
    Before coming to the statement of the theorem and the proof, let us discuss
the heart of the matter, which is related to row reduction of matrices. We regard
the space M1n (R) of all 1-row matrices with n entries in R as a free R module.
Suppose that R is a principal ideal domain, and suppose that we have a particular
2-by-n matrix with entries in R and with the property that the two rows have
nonzero elements a and b, respectively, in the first column. We can regard
the set of R linear combinations of the two rows of our particular matrix as
an R submodule of the free R module M1n (R). Let c = GCD(a, b). This
member of R is defined only up to multiplication by a unit, but we make a
definite choice of it. The idea is that we can do a kind of invertible row-reduction
step that simultaneously replaces the two rows of our 2-by-n matrix by a first row
whose first entry is c and a second row whose first entry is 0; in the process the
corresponding R submodule of M1n (R) will be unchanged. In fact, we saw in the
previous section that the hypothesis on R implies that there exist members x and
y of R with xa + yb = c. Since c divides a and b, we can   ° rewrite  this equality as
                                                               x     y ¢
x(ac−1 ) + y(bc−1 ) = 1. Then the 2-by-2 matrix M = −bc−1 ac−1 with entries
in R has the property that
                     µ                 ∂µ        ∂ µ          ∂
                         x          y      a ∗          c ∗
                                                   =             .
                       −bc−1 ac−1          b ∗          0 ∗
                                                 °c ∗¢
This equation shows explicitly that the rows of 0 ∗ lie in the R linear span of the
         °a ∗¢
rows of b ∗ . The key fact about M is that its determinant x(ac−1 ) + y(bc−1 )
is 1 and≥that M ¥is therefore invertible with entries in R: the inverse is just
              −1                                                   °a ∗¢
M −1 = ac−1 −y . This invertibility shows that the rows of b ∗ lie in the R
            bc °x
                 c ∗¢
linear span of 0 ∗ . Consequently the R linear span of the rows of our given
2-by-n matrix is preserved under left multiplication by M.
402                           VIII. Commutative Rings and Their Modules

   In effect we can do the same kind of row reduction of matrices over R as we
did with matrices over Z in the proof of Theorem 4.56. The only difference is
that this time we do not see constructively how to find the x and y that relate a,
b, and c. Thus we would lack some information if we actually wanted to follow
through and calculate a particular example. We were able to make calculations
to imitate the proof of Theorem 4.56 because we were able to use the Euclidean
algorithm to arrive at what x and y are. In the present context we would be able
to make explicit calculations if R were a Euclidean domain.

    Theorem 8.25 (Fundamental Theorem of Finitely Generated Modules). If R
is a principal ideal domain, then
     (a) the number of R summands in a free R module of finite rank is independent
         of the direct-sum decomposition,
     (b) any R submodule of a free R module of finite rank n is a free R module
         of rank ≤ n,
     (c) any finitely generated unital R module is the finite direct sum of cyclic
         modules.

   REMARK. Because of (a), it is meaningful to speak of the rank of a free
R module of finite rank; it is the number of R summands. By convention
the 0 module is a free R module of rank 0. Then the statement of (b) makes
sense. Statement (c) will be amplified in Corollary 8.29 below. Some people
use the name “Fundamental Theorem of Finitely Generated Modules” to refer to
Corollary 8.29 rather than to Theorem 8.25.
    PROOF. Let F be a free R module of the form Rx1 ⊕ · · · ⊕ Rxn , and
suppose that y1 , . . . , ym are elements of F such that no nontrivial combination
                                               Pn of Proposition 2.2. Define an
r1 y1 + · · · + rm ym is 0. We argue as in the proof
m-by-n matrix C with entries in R by yi = j=1 Ci j x j for 1 ≤ i ≤ m. If Q is
the field of fractions of R, then we can regard C as a matrix with entries in Q. As
such, the matrix has rank ≤ n. If m > n, then the rows are linearly   Pm dependent,
and we can find members q1 , . . . , qm of Q, not all 0, such that i=1      qi Ci j = 0
for 1 ≤ j P  ≤ n. Clearing fractions, we obtain members r1 , . . . , rm of R, not all 0,
               m
such that i=1     ri Ci j = 0 for 1 ≤ j ≤ n. Then
         m
         P              m
                        P    ≥P
                              n         ¥ Pn ≥P
                                              m         ¥      n
                                                               P
              ri yi =     ri    Ci j x j =      ri C i j x j =   0x j = 0,
        i=1             i=1       j=1            j=1   i=1                j=1


in contradiction to the assumed independence property of y1 , . . . , ym . Therefore
we must have m ≤ n.
   If we apply this conclusion to a set x1 , . . . , xn that exhibits F as free and to
another set, possibly infinite, that does the same thing, we find that the second
                                 6. Finitely Generated Modules                               403

set has ≤ n members. Reversing the roles of the two sets, we find that they both
have n members. This proves (a).
    For (b) and (c), we shall reduce the result to a lemma saying that a certain kind
of result can be achieved by row and column reduction of matrices with entries in
R. Let F be a free R module of rank n, defined by a subset x1 , . . . , xn of F, and let
M be an R submodule of F. Proposition 8.24 shows that M is finitely generated.
We let y1 , . . . , ym be generators, not necessarily withP          any independence property.
Define an m-by-n matrix C with entries in R by yi = nj=1 Ci j x j . We can recover
F as the set of R linear combinations of x1 , . . . , xn , and we can recover M as the
set of R linear combinations of y1 , . . . , ym .
    If B is an n-by-n matrix with entries in R and with determinant in the group
  ×
R of units, then        PCorollary       5.5 shows that B −1 exists and has entries in R. If
                          n
                 0
we define xi = j=1 Bi j x j , then any R linear combination of x10 , . . . , xn0 is an
                                                                              Pn     −1     0
R
P   linear    combination      of
                               P   x 1 , . . . , x n . Also,  the computation  i=1 (B )ki x i =
            −1
   i, j (B )ki Bi j x j =         j δk j x j = x k shows that any R linear combination of
x1 , . . . , xn is an R linear combination of x10 , . . . , xn0 . Thus we can recover the
same F and M if we replace C by C B. Arguing in the same way with y1 , . . . , ym
and y10 , . . . , ym0 , we see that we can recover the same F and M if we replace C B
by AC B, where A is an m-by-m matrix with entries in R and with determinant
in R × .
    Lemma 8.26 below will say that we can find A and B such that the nonzero
entries of D = AC B are exactly the diagonal ones Dkk for 1 ≤ k ≤ l, where l is
a certain integer with 0 ≤ l ≤ min(m, n).
    That is, the resulting equations restricting y10 , . . . , ym0 in terms of x10 , . . . , xn0
will be of the form
                                    Ω
                             0           Dkk xk0        for 1 ≤ k ≤ l,
                            yk =                                                              (∗)
                                        0               for l + 1 ≤ k ≤ m.

   Now let us turn to (b) and (c). For (b), the claim is that the elements yk0 with
1 ≤ k ≤ l exhibit M as a free R module. We know that y10 , . . . , ym0 generate M
and hence that y10 , . . . , yl0 generate M. For the independence, suppose we can find
                                                    P
members r1 , . . . , rl not all 0 in R such that lk=1 rk yk0 = 0. Then substitution
      Pl
gives k=1 rk Dkk xk0 = 0, and the independence of x10 , . . . , xl0 forces rk Dkk = 0
for 1 ≤ k ≤ l. Since R is an integral domain, rk = 0 for such k. Thus indeed the
elements yk0 with 1 ≤ k ≤ l exhibit M as a free R module. Since l ≤ min(m, n),
the rank of M is at most the rank of F.
   For (c), let S be a finitely generated unital R module, say with n generators.
By the universal mapping property of free R modules (Example 9 in Section 1),
there exists a free R module F of rank n with S as quotient. Let x1 , . . . , xn be
generators of F that exhibit F as free, and let M be the kernel of the quotient R
404                  VIII. Commutative Rings and Their Modules

homomorphism M → S, so that S ∼           = F/M. Then (b) shows that M is a free
R module of rank m ≤ n. Let y1 , . . . , ym be generators of M thatPexhibit M
as free, and define an m-by-n matrix C with entries in R by yi = nj=1 Ci j x j
for 1 ≤ i ≤ m. The result is that we are reduced to the situation we have just
considered, and we can obtain equations of the form (∗) relating their respective
generators, namely y10 , . . . , ym0 for M and x10 , . . . , xn0 for F.
   For 1 ≤ k ≤ n, define Fk = Rxk0 and
                       Ω
                            Ryk0 = R Dkk xk0        for 1 ≤ k ≤ l,
                Mk =
                           0                        for l + 1 ≤ k ≤ n,

so that M ∼
          = M1 ⊕· · ·⊕ Mn . Then Fk /Mk is R isomorphic to the cyclic R module
R/(Dkk ) if 1 ≤ k ≤ l, while Fk /Mk = Fk is isomorphic to the cyclic R module
R if l + 1 ≤ k ≤ n. Applying Proposition 8.5, we obtain

      = (F1 ⊕ · · · ⊕ Fn )/(M1 ⊕ · · · ⊕ Mn ) ∼
  F/M ∼                                       = (F1 /M1 ) ⊕ · · · ⊕ (Fn /Mn ).

Thus F/M is exhibited as a direct sum of cyclic R modules.                       §

   To complete the proof of Theorem 8.25, we are left with proving the following
lemma, which is where row and column reduction take place.

   Lemma 8.26. Let R be a principal ideal domain. If C is an m-by-n matrix
with entries in R, then there exist an m-by-m matrix A with entries in R and
with determinant in R × and an n-by-n matrix B with entries in R and with
determinant in R × such that for some l with 0 ≤ l ≤ min(m, n), the nonzero
entries of D = AC B are exactly the diagonal entries D11 , D22 , . . . , Dll .
   PROOF. The matrices A and B will be constructed as products of matrices of
determinant ±1, and then det A and det B equal ±1 by Proposition 5.1a. The
matrix A will correspond to row operations on C, and B will correspond to
column operations. Each factor will be the identity except in some 2-by-2 block.
Among the row and column operations of interest   ≥ are¥ the interchange of two
rows or two columns, in which the 2-by-2 block is 01 10 . Another row operation
of interest replaces two rows having respective j th entries a and b by R linear
combinations of them in which a and b are replaced by° c = GCD(a,       b) and 0.
        −1               −1                                   x    y ¢
If x(ac ) + y(bc ) = 1, then the 2-by-2 block is −bc−1 ac−1 . A similar
operation is possible with columns.
   The reduction involves an induction that successively constructs the entries
D11 , D22 , . . . , Dll , stopping when the part of C involving rows and columns
numbered ∏ l + 1 has been replaced by 0. We start by interchanging rows and
columns to move a nonzero entry into position (1, 1). By a succession of row
                               6. Finitely Generated Modules                           405

operations as in the previous paragraph, we can reduce the entry in position (1, 1)
to the greatest common divisor of the entries of C in the first column, while
reducing the remaining entries of the first column to 0. Next we do the same
thing with column operations, reducing the entry in position (1, 1) to the greatest
common divisor of the members of the first row, while reducing the remaining
entries of the first row to 0. Then we go back and repeat the process with row
operations and with column operations as many times as necessary until all the
entries of the first row and column other than the one in position (1, 1) are 0. We
need to check that this process indeed terminates at some point. If the entries that
appear in position (1, 1) as the iterations proceed are c1 , c2 , c3 , . . . , then we have
(c1 ) ⊆ (c2 ) ⊆ (c3 ) ⊆ · · · . The union of these ideals is an ideal, necessarily a
principal ideal of the form (c), and c occurs in one of the ideals in the union; the
chain of ideals must be°constant after   that stage. Once the corner entry becomes
                             x    y ¢
constant, the matrices −bc−1 ac−1 for the row operations can be chosen to be
              ≥          ¥
                   1   0
of the form −ba −1 1 , and the result is that the row operations do not change
the entries of the first row. Similar remarks apply to the matrices for the column
operations. The upshot is that we can reduce C in this way so that all entries of
the first row and column are 0 except the one in position (1, 1). This handles
the inductive step, and we can proceed until at some l th stage we have only the 0
matrix to process.                                                                       §

   This completes the proof of Theorem 8.25. In Theorem 4.56, in which we
considered the special case of abelian groups, we obtained a better conclusion
than in Theorem 8.25c: we showed that the direct sum of cyclic groups could
be written as the direct sum of copies of Z and of cyclic groups of prime-power
order, and that in this case the decomposition was unique up to the order of the
summands. We shall now obtain a corresponding better conclusion in the setting
of Theorem 8.25.
   The existence of the decomposition into cyclic modules of a special kind uses a
very general form of the Chinese Remainder Theorem, whose classical statement
appears as Corollary 1.9. The generalization below makes use of the following
operations of addition and multiplication of ideals in a commutative ring with
identity: if I and J are ideals, then I + J denotes the set of sums x + y with
x ∈ I and y ∈ J , and I J denotes the set of all finite sums of products x y with
x ∈ I and y ∈ J ; the sets I + J and I J are ideals.

    Theorem 8.27 (Chinese Remainder Theorem). Let R be a commutative ring
with identity, and let I1 , . . . , In be ideals in R such that Ii + I j = R whenever
i 6= j.
    (a) If elements x1 , . . . , xn of R are given, then there exists x in R such that
x ≡ x j mod I j , i.e., x − x j is in I j , for all j. The element x is unique if
406                     VIII. Commutative Rings and Their Modules

I1 ∩ · · · ∩ In = 0.   Q
   (b) The map ϕ : R → nj=1 R/I j given by ϕ(r) = (. . . , r + I j , . . . ) is an onto
                                 T
ring homomorphism, its kernel is nj=1 I j , and the homomorphism descends to a
ring isomorphism
                        ±T n
                      R          ∼ R/I1 × · · · × R/In .
                              Ij =
                               j=1
                          Tn
   (c) The intersection     j=1 I j   and the product I1 · · · In coincide.
   PROOF. For existence in (a) when n = 1, we take x = x1 . For existence
when n = 2, the assumption I1 + I2 = R implies that there exist a1 ∈ I1 and
a2 ∈ I2 with a1 + a2 = 1. Given x1 and x2 , we put x = x1 a2 + x2 a1 , and then
x ≡ x1 a2 ≡ x1 mod I1 and x ≡ x2 a1 ≡ x2 mod I2 .
   For general n, the assumption I1 + I j = R for j ∏ 2 implies that there
      a j ∈ I1 and b j ∈ I j with a j + b j = 1. If we expand out the product
exist Q
1 = nj=2 (a j + b j ), then all terms but one on the right side involve some a j
                                                                        T
and are therefore in I1 . That one term is b2 b2 · · · bn , and it is in nj=2 I j . Thus
      T
I1 + nj=2 I j = R. The case n = 2, which was proved above, yields an element
y1 in R such that
                                                              T
                y1 ≡ 1 mod I1       and      y1 ≡ 0 mod j6=1 I j .
Repeating this process for index i and using the assumption Ii + I j = R for j 6= i,
we obtain an element yi in R such that
                                                         T
                yi ≡ 1 mod Ii       and       yi ≡ 0 mod j6=i I j .
If we put x = x1 y1 + · · · + xn yn , then we have x ≡ xi yi mod Ii ≡ xi mod Ii for
each i, and the proof of existence is complete.
    For uniqueness in (a), if we have two elements x and x 0 satisfying the con-
gruences, then their difference x − x 0 lies in I j for every j, hence is 0 under the
assumption that I1 ∩ · · · ∩ In = 0.
    In (b), the map ϕ is certainly a ring homomorphism. The existence result in (a)
shows that ϕ is onto, and the proof of the uniqueness result identifies the kernel.
The isomorphism follows.
    For (c), consider the special case that I and J are ideals with I + J = R.
Certainly I J ⊆ I ∩ J . For the reverse inclusion, choose x ∈ I and y ∈ J with
x + y = 1; this is possible since I + J = R. If z is in I ∩ J , then z = zx + zy
with zx in J I and zy in I J . Thus z is exhibited as in I J .
    Consequently I1 I2 = I1 ∩ I2 . SupposeT inductively that I1 · · · Ik = I1 ∩· · ·∩ Ik .
We saw in the proof of (a) that Ik+1 + j6=k+1 I j = R, and thus we certainly have
         T
Ik+1 + kj=1 I j = R. The special case in the previous paragraph, in combination
                                                                          ° Tk       ¢
with the inductive hypothesis, shows that Ik+1 I1 · · · Ik = Ik+1 ·           j=1 I j =
Tk+1
   j=1 I j . This completes the induction and the proof.                               §
                              6. Finitely Generated Modules                             407

   Corollary 8.28. Let R be a principal ideal domain, and let a = εp1k1 · · · pnkn
be a factorization of a nonzero nonunit element a into the product of a unit and
powers of nonassociate primes. Then there is a ring isomorphism
                                     k
                              = R/( p11 ) × · · · × R/( pnkn ).
                        R/(a) ∼

                        k                                                           k
   PROOF. Let I j = ( p j j ) in Theorem 8.27. For i 6= j, we have GCD( piki , p j j ) =
                                                                                k
1. Since R is a principal ideal domain, there exist a and b in R with apiki +bp j j = 1,
                                k
and consequently ( piki ) + ( p j j ) = R. The theorem applies, and the corollary
follows.                                                                       §

   Corollary 8.29. If R is a principal ideal domain, then any finitelyLgenerated
                                                                         s
unital R module M is the direct sum of a nonunique free R submodule i=1     R of
a well-defined finite rank s ∏ 0 and the R submodule T of all members m of M
such that rm = 0 for some r 6= 0 in R. In turn, the R submodule T is isomorphic
to a direct sum
                                     Mn
                                              k
                                T ∼
                                  =      R/( p j ),j
                                        j=1

                                                  k
where the p j are primes in R and the ideals ( p j j ) are not necessarily distinct. The
number of summands ( pk ) for each class of associate primes p and each positive
integer k is uniquely determined by M.
    REMARK. As mentioned with Theorem 8.25, some people use the name
“Fundamental Theorem of Finitely Generated Modules” to refer to Corollary
8.29 rather than to Theorem 8.25.
                                                    L
    PROOF. Theorem 8.25c gives M = F ⊕ nj=1 Ra j , where F is a free R
submodule of some finite rank s and the a j ’s are nonzero members of M that are
each annihilated by some nonzeroL      member of R. The set T of all m with rm = 0
for some r 6= 0 in R is exactly nj=1 Ra j . Then F is R isomorphic to M/T ,
hence is isomorphic to the same free R module independently of what direct-sum
decomposition of M is used. By Theorem 8.25a, s is well defined.
    The cyclic R module Ra j is isomorphic to R/(b j ), where (b j ) is the ideal of
all elements r in R with ra j = 0. The ideal (b j ) is nonzero by assumption and
is not all of R since the element r = 1 has 1a j = a j 6= 0. Applying Corollary
                                                          Ln           k
8.28 for each j and adding the results, we obtain T ∼   = i=1 R/( pi i ) for suitable
primes pi and powers ki . The isomorphism in Corollary 8.28 is given as a ring
isomorphism, and we are reinterpreting it as an R isomorphism. The primes
pi that arise for fixed (b j ) are distinct, but there may be repetitions in the pairs
( pi , ki ) as j varies. This proves existence of the decomposition.
408                     VIII. Commutative Rings and Their Modules

    If p is a prime in R, then the elements m of T such that pk m = 0 for some k
                                                        L           k
are the ones corresponding to the sum of the terms in nj=1 R/( p j j ) in which p j
is an associate of p. Thus, to complete the proof, it is enough to show that the R
isomorphism class of the R module

                              N = R/( pl1 ) ⊕ · · · ⊕ R/( plm )

with p fixed and with 0 < l1 ≤ · · · ≤ lm completely determines the integers
l1 , . . . , lm .
     For any unital R module L, we can form the sequence of R submodules
p j L. The element p carries p j L into p j+1 L, and thus each p j L/ p j+1 L is an
R module on which p acts as 0. Consequently each p j L/ p j+1 L is an R/( p)
module. Corollary 8.16 and Proposition 8.10 together show that R/( p) is a field,
and therefore we can regard each p j L/ p j+1 L as an R/( p) vector space.
     We shall show that the dimensions dim R/( p) ( p j N / p j+1 N ) of these vector
spaces determine the integers l1 , . . . , lm . We start from

                       p j N = p j R/( pl1 ) ⊕ · · · ⊕ p j R/( plm ).

The term p j R/( plk ) is 0 if j ∏ lk . Thus
                               M                   M
                     pjN =         p j R/( plk ) =   p j R/ plk R.
                                j<lk                  j<lk

Similarly                      M                          M
                 p j+1 N =            p j+1 R/( plk ) =          p j+1 R/ plk R.
                               j<lk                       j<lk

Proposition 8.5 and Theorem 8.3 give us the R isomorphisms
                      M°               ¢±° j+1      ¢ M j
    p j N / p j+1 N ∼
                    =     p j R/ plk R    p R/ plk R ∼
                                                     =     p R/ p j+1 R,
                       j<lk                                              j<lk

and these must descend to R/( p) isomorphisms. Consequently

        dim R/( p) ( p j N / p j+1 N ) = #{k | lk > j} dim R/( p) ( p j R/ p j+1 R).

The coset p j + p j+1 R of p j R/ p j+1 R has the property that multiplication by arbi-
trary elements of R yields all of p j R/ p j+1 R. Therefore dim R/( p) ( p j R/ p j+1 R)
= 1, and we obtain

                      dim R/( p) ( p j N / p j+1 N ) = #{k | lk > j}.

Thus the R module N determines the integers on the right side, and these deter-
mine the number of lk ’s equal to each positive integer j. This proves uniqueness.
                                                                                §
                             6. Finitely Generated Modules                       409

   Let us apply Theorem 8.25 and Corollary 8.29 to the principal ideal domain
R = K[X], where K is a field. The particular unital module of interest is a
finite-dimensional vector space V over K, and the scalar multiplication by K[X]
is given by A(X)v = A(L)(v) for each polynomial A(X), where L is a fixed
linear map L : V → V . Let us see that the results of this section recover the
structure theory of L as developed in Chapter V.
   Since V is finite-dimensional over K, V is certainly finitely generated over
R = K[X]. Theorem 8.25 gives

              V ∼
                = R/(A1 (X)) ⊕ · · · ⊕ R/(An (X)) ⊕ R ⊕ · · · ⊕ R

as R modules and in particular as vector spaces over K. Each summand R is
infinite-dimensional as a vector space, and consequently no summand R can be
present. Corollary 8.29 refines the decomposition to the form

                      = R/(P1 (X)k1 ) ⊕ · · · ⊕ R/(Pm (X)km )
                    V ∼

as R modules, the polynomials Pj (X) being prime but not necessarily distinct.
Since the R isomorphism is in particular an isomorphism of K vector spaces,
each R/(Pj (X)k j ) corresponds to a vector subspace Vj , and V = V1 ⊕ · · · ⊕ Vm .
Since the R isomorphism respects the action by X, we have L(Vj ) ⊆ Vj for each
 j. Thus the direct sum decompositions of Theorem 8.25 and Corollary 8.29 are
yielding a decomposition of V into a direct sum of vector subspaces invariant
under L. Since the j th summand is of the form R/(Pj (X)k j ), L acts on Vj in a
particular way, which we have to analyze.
    Let us carry out this analysis in the case that K is algebraically closed (as for
example when K = C), seeing that each Vj yields a Jordan block of the Jordan
canonical form (Theorem 5.20a) of L. For the case of general K, the analysis
can be seen to lead to the corresponding more general results that were obtained
in Problems 32–40 at the end of Chapter V.
    Since K is algebraically closed, any polynomial in K[X] of degree ∏ 1 has a
root in K and therefore has a first-degree factor X − c. Consequently all primes
in K[X] are of the form X − c, up to a scalar factor, with c in K. To understand
the action of L on Vj , we are to investigate K[X]/((X − c)k ).
    Suppose that A(X) is in K[X] and is of degree n ∏ 1. Expanding the
monomials of A(X) by the Binomial Theorem as
                                            Pj    ° j ¢ j−i
                 X j = ((X − c) + c) j =       i=0 i c      (X   − c)i ,

we see that A(X) has an expansion as

                  A(X) = a0 + a1 (X − c) + · · · + an (X − c)n
410                     VIII. Commutative Rings and Their Modules

for suitable coefficients a0 , . . . , an in K. Let the invariant subspace that we are
studying be Vj0 ⊆ V . Since Vj0 is isomorphic as an R module to K[X]/((X −c)k ),
(X − c)k acts on Vj0 as 0. So does every higher power of X − c, and hence

         A(X)        acts as     a0 + a1 (X − c) + · · · + ak−1 (X − c)k−1 .
   The polynomials on the right, as their coefficients vary, represent distinct
cosets of K[X]/((X − c)k ): in fact, if two were to be in the same coset, we could
subtract and see that (X − c)k could not divide the difference unless it were 0.
The distinct cosets match in one-one K linear fashion with the members of Vj0 ,
and thus dim Vj0 = k. Let us write down this match. Let v0 be the member of
Vj0 that is to correspond to the coset 1 of K[X]/(X − c)k . On Vj0 , K[X] is acting
with Xv = L(v). We define recursively vectors v1 , . . . , vk−1 of Vj0 by

  v1 = (L − cI )v0 = (X −c)v0            √→ (X −c) · 1 = X −c,
  v2 = (L − cI )v1 = (X −c)v1            √→ (X −c) · (X −c) = (X −c)2 ,
          ..
           .
vk−1 = (L − cI )vk−2 = (X −c)vk−2 √→ (X −c) · (X −c)k−2 = (X −c)k−1 ,
        (L − cI )vk−1 = (X −c)vk−1 √→ (X −c) · (X −c)k−1 = (X −c)k ≡ 0.
We conclude from this correspondence that the vectors v0 , v1 , . . . , vk−1 form a
basis of Vj0 and that the matrix of L − cI in the ordered basis vk−1 , . . . , v1 , v0 is
                                 0 1 0 0 ··· 0 0 
                                     01 0     ··· 0 0
                                                     
                                     0 1     ··· 0 0 
                                        . . . . .. .. 
                                           . . . .     .
                                                      
                                             0 1 0
                                                  01
                                                   0

Hence the matrix of L in the same ordered basis is
                               c 1 0 0 ··· 0 0 
                                     c 1 0    ··· 0 0
                                                     
                                      c 1    ··· 0 0 
                                        . . . . .. ..  ,
                                           . ..     .
                                                      
                                             c   1 0
                                                  c 1
                                                    c

i.e., is a Jordan block. Thus Theorem 8.25 and Corollary 8.29 indeed establish
the existence of Jordan canonical form (Theorem 5.20a) when K is algebraically
closed. It is easy to check that Corollary 8.29 establishes also the uniqueness
statement in Theorem 5.20a.
           7. Orientation for Algebraic Number Theory and Algebraic Geometry   411

 7. Orientation for Algebraic Number Theory and Algebraic Geometry

The remainder of the chapter introduces material on commutative rings with iden-
tity that is foundational for both algebraic number theory and algebraic geometry.
Historically algebraic number theory grew out of Diophantine equations, particu-
larly from two problems—from Fermat’s Last Theorem and from representation
of integers by binary quadratic forms. Algebraic geometry grew out of studying
the geometry of solutions of equations and out of studying Riemann surfaces.
Algebraic geometry and algebraic number theory are treated in more detail in
Advanced Algebra.
    These two subjects can be studied on their own, but they also have a great
deal in common. The discovery that the plane could be coordinatized and that
geometry could be approached through algebra was one of the great advances of
all time for mathematics. Since then, fundamental connections between algebraic
number theory and algebraic geometry have been discovered at a deeper level,
and the distinction between the two subjects is more and more just a question of
one’s point of view. The emphasis in the remainder of this chapter will be on
one aspect of this relationship, the theory that emerged from trying to salvage
something in the way of unique factorization.
    By way of illustration, let us examine an analogy between what happens with
a certain ring of “algebraic integers” and what happens with a certain “algebraic
curve.” The ring of algebraic
                         p               p in question was introduced already in
                                   integers
Section 4. It is R = Z[ −5] = Z + Z −5. The units are ±1. Our investigation
of unique factorization was aided by the function
                      p                p            p
              N (a + b −5 ) = (a + b −5 )(a − b −5 ) = a 2 + 5b2 ,

which has the property that
        °       p           p  ¢          p            p
       N (a + b −5 )(c + d −5 ) = N (a + b −5 )N (c + d −5 ).

With this function we could determine candidates for factors
                                                           p     of particular
                                                                       p       ele-
ments. In connection with the equality 2 · 3 = (1 + −5 )(1 − −5 ), we
saw that the two factors on the left side and the two factors on the right side are
all irreducible. Moreover, neither factor on the left is the product of a unit and
a factor on the right. Therefore R is not a unique factorization domain.
                                                                       p      As a
consequence it cannot be a principal ideal domain. In fact, (2, 1 + −5 ) is an
example of an ideal that is not principal. We shall return shortly to examine this
ring further.
   Now we introduce the algebraic curve. Consider y 2 = (x − 1)x(x + 1) as
an equation in two variables x and y. To fix the ideas, we think of a solution as
a pair (x, y) of complex numbers. Although the variables in this discussion are
412                        VIII. Commutative Rings and Their Modules

complex, it is convenient to be able to draw pictures of the solutions, and one does
this by showing only the solutions (x, y) with x and y in R. Figure 8.6 indicates
the set of solutions in R2 for this particular curve. We can study these solutions
for a while, looking for those pairs (x, y) with x and y rationals or integers, but
a different level of understanding comes from studying functions on the locus of
complex solutions. The functions of interest are polynomial functions in the pair
(x, y), and we identify two of them if they agree on the locus. Thus we introduce
the ring
                      R 0 = C[x, y]/(y 2 − (x − 1)x(x + 1)).

There is a bit of a question whether this is indeed the space of restrictions, but
that question is settled affirmatively by the “Nullstellensatz” in Section VII.1 of
Advanced Algebra and a verification that the principal ideal (y 2 −(x −1)x(x +1))
is prime.8 The ring R 0 is called the “affine coordinate ring” of the curve, and the
curve itself is an example of an “affine algebraic curve.”




            FIGURE 8.6. Real points of the curve y 2 = (x − 1)x(x + 1).

   We can recover the locus of the curve from the ring R 0 as follows. If (x0 , y0 ) is a
point of the curve, then it is meaningful to evaluate members of R 0 at (x0 , y0 ), and
we let I(x0 ,y0 ) be the ideal of all members of R 0 vanishing at (x0 , y0 ). Evaluation
at (x0 , y0 ) exhibits the ring R 0 /I(x0 ,y0 ) as isomorphic to C, which is a field. Thus
I(x0 ,y0 ) is a maximal ideal and is in particular prime. It turns out for this example
that all nonzero prime ideals are of this form.9 We return to make use of this
geometric interpretation of prime ideals in a moment.

    8 The polynomial y 2 − (x − 1)x(x + 1) is prime since (x − 1)x(x + 1) is not a square, or since

Eisenstein’s criterion applies. The principal ideal (y 2 − (x − 1)x(x + 1)) is therefore prime by
Proposition 8.14. What the Nullstellensatz says when the underlying field is algebraically closed is
that the only polynomials vanishing on the zero locus of a prime ideal are the members of the ideal.
    9 In Section 9, Example 3 of integral closures in combination with Proposition 8.45 shows that

every nonzero prime ideal of R 0 is maximal. (In algebraic geometry one finds that this property of
prime ideals is a reflection of the 1-dimensional nature of the curve.) The Nullstellensatz says that
the maximal ideals are all of the form I(x0 ,y0 ) .
              7. Orientation for Algebraic Number Theory and Algebraic Geometry                 413

   Now let us consider factorization in R 0 . Every element of R 0 can be written
uniquely as A(x) + B(x)y, wherep A(x) and B(x) are polynomials. The analog
in R 0 of the quantity N (a + b −5 ) in the ring R is the quantity
                °              ¢
              N A(x) + B(x)y = (A(x) + B(x)y)(A(x) − B(x)y)
                                       = A(x)2 − B(x)2 y 2
                                       = A(x)2 − B(x)2 (x 3 − x).
Easy computation shows that
  °                           ¢   °            ¢ °            ¢
N (A(x) + B(x)y)(C(x) + D(x)y) = N A(x) + B(x)y N C(x) + D(x)y ,
and hence N ( · ) gives us a device to use to check whether elements of R 0 are
irreducible. We find in the equation
                                                      p               p
   (x + y)(x − y) = x 2 − (x 3 − x) = −x(x − 12 (1 + 5))(x − 12 (1 − 5))
that the two elements on the left side and the three elements on the right side are
irreducible. Therefore unique factorization fails in R 0 .
   Although unique factorization fails for the elements of R 0 , there is a notion
of factorization for ideals in R 0 that behaves well algebraically and has a nice
geometric interpretation. Recall that the nonzero prime ideals correspond to the
points of the locus y 2 = (x − 1)x(x + 1) via passage to the zero locus, the ideal
corresponding to (x0 , y0 ) being called I(x0 ,y0 ) . For any two ideals I and J , we
can form the product ideal I J whose elements are the sums of products of a
member of I and a member of J . Then I(xk 0 ,y0 ) may be interpreted as the ideal of
all members of R 0 vanishing at (x0 , y0 ) to order k or higher, and I(xk11 ,y1 ) · · · I(xknn ,yn )
becomes the ideal of all members of R 0 vanishing at each (x j , yj ) to order at
least k j . We shall see in Section 11 that every nonzero proper ideal I in R 0
factors in this way. The points (x j , yj ) and the integers k j have a geometric
interpretation in terms of I and are therefore uniquely determined: the (x j , yj )’s
form the locus of common zeros of the members of I , and the integer k j is the
greatest integer such that the vanishing at (x j , yj ) is always at least to order k j . In
a sense, factorization of elements was the wrong thing to consider; the right thing
to consider is factorization of ideals, which is unique because of the associated
geometric interpretation.        p
    Returning to the ring R = Z[ −5 ], we can ask whether factorization of ideals
is a useful notion in R. Again I J is to be the setpof all sums of products            pof an
element in I and an element in J . For I = (2,  p  1 +     −5 ) and  J =
                                                                       p  (2,   1  −     −5 ),
we get all sums of expressions (2a + b(1 + −5 ))(2c + d(1 − −5 )) in which
a, b, c, d are in Z, hence all sums of expressions
                                                       p
                 2(2ac + 3bd) + 2(bc + ad) + 2 −5(bc − ad).
414                     VIII. Commutative Rings and Their Modules

All such elements are divisible by 2. Two examples come by taking a = c = 1
and b = d = 0 and by taking a = c = 0 and b = d = 1; these give 4 and 6.
Subtracting, we see that 2 is a sum of products. Thus I J = (2). The element 2
is irreducible and not prime, and we know from Proposition 8.14 that the ideal
(2) therefore cannot be prime. What we find is that the ideal (2) factors even
though the element 2 does not factor. It turns out that R has unique factorization
of ideals, just the way R 0 does.
    The prime ideals of the ring R have a certain amount of structure in terms of
the primes or prime ideals of Z. To understand what to expect, let us digress    p for
a moment to discuss what happens with the ring R 00 = Z[i] = Z + Z −1 of
Gaussian integers. This too was introduced in Section 4, and it is a Euclidean do-
main, hence a principal ideal domain. It has unique factorization. Its appropriate
N ( · ) function is N (a + ib) = a 2 + b2 . Problems 27–31 at the end of the chapter
ask one to verify that the primes of R 00 , up to multiplication by one of the units
±1 and ±i, are members of R 00 of any of the three kinds
           p = 4n + 3 that is prime in Z and has n ∏ 0,
           p = a ± ib with a 2 + b2 prime in Z of the form 4n + 1 with n ∏ 0,
           p = 1 ± i (these are associates).
These three kinds may be distinguished by what happens to the function N ( · ).
In the first case N ( p) = p2 is the square of a prime of Z and is the square of
a prime of R 00 , in the second case N ( p) is a prime of Z that is the product of
two distinct primes of R 00 , and in the third case N ( p) is a prime of Z that is the
square of a prime of R 00 , apart from a unit factor. The nonzero prime ideals of R 00
are the principal ideals generated by the prime elements of R 00 , and they fall into
three types as well. Each nonzero prime ideal P has a prime p of Z attached to
it, namely the one with ( p) = Z ∩ P, and the type of the ideal corresponds to the
nature of the factorization of the ideal p R 00 of R 00 . Specifically in the first case
p R 00 is a prime ideal in R 00 , in the second case p R 00 is the product of two distinct
prime ideals in R 00 , and in the third case p R 00 is the square of a prime ideal in R 00 .
    The structure of the prime ideals in R is of the same nature as with R 00 .
Each nonzero prime ideal P has a prime p of Z attached to it, again given
by ( p) = Z ∩ P, and the three kinds correspond to the factorization of the ideal
p R of R. Let us be content to give examples of the three possible behaviors:
              11R     is prime in R,
               2R     is the product of two distinct prime ideals in R,
                                                        p
                5R    is the square of the prime ideal ( −5 ) in R.
We have already seen the decomposition of 2R, and the decomposition of 5R is
easy to check. With 11R, the idea is to show that 11 is a prime element in R.
Thus let 11 divide a product in R. Then N (11) = 112 divides the product of
             7. Orientation for Algebraic Number Theory and Algebraic Geometry         415

the N ( · )’s, 11 divides the product of p     the N ( · )’s, and 11 must divide one of the
N ( · )’s. Say that 11 divides N (a + b −5 ), i.e., that a 2 + 5b2 ≡ 0 mod 11. If
11 divides one of a or b, thenpthis congruence shows that 11 divides the other of
them; then 11 divides a + b −5, as we wanted to show. The other possibility
is that 11 divides neither a nor b. Then (ab−1 )2 ≡ −5 mod 11 says that −5 is a
square modulo 11, and we readily check that it is not. The conclusion is that 11
is indeed prime in R.
    This structure for the prime ideals of R has anpanalog with the curve and its
ring R 0 . The analogs for the curve case of Z and −5 for the number-theoretic
case are C[x] and y. The primes of C[x] are nonzero scalars times polynomials
x − c with c complex, and the relevant question for R 0 is how the ideal (x − c)R 0
decomposes into prime ideals. We can think about this problem algebraically or
geometrically. Algebraically, the ideal of all polynomials vanishing at (x0 , y0 ) is
I(x0 ,y0 ) = (x − x0 , y − y0 ), the set of all (x − x0 )A(x)− y0 B(x)+ y B(x) with A(x)
and B(x) in C[x]. The intersection with C[x] consists of all (x − x0 )A(x) and is
therefore the principal ideal (x − x0 ). We want to factor the ideal (x − x0 )R 0 .
    If we pause for a moment and think about the problem geometrically, the answer
is fairly clear. Ideals correspond to zero loci with multiplicities. The question
is the factorization of the ideal of all polynomials vanishing when x = x0 . For
most values of the complex number x0 , there are two choices of the complex y
such that (x0 , y) is on the locus since y is given by a quadratic equation, namely
y 2 = (x0 − 1)x0 (x0 + 1). Thus for most values of x0 , (x − x0 )R 0 is the product
of two distinct prime ideals. The geometry thus suggests that

                  (x − x0 )R 0 = (x − x0 , y − y0 )(x − x0 , y + y0 ),

where y02 = (x0 − 1)x0 (x0 + 1) and it is assumed that y0 6= 0. We can verify this
algebraically: The members of the product ideal are the polynomials
°                               ¢°                               ¢
 (x − x0 )A(x) + (y − y0 )B(x) (x − x0 )C(x) + (y + y0 )D(x)
                                    °                                            ¢
  = (x − x0 )2 A(x)C(x) + (x − x0 ) A(x)(y + y0 )D(x)) + C(x)(y − y0 )B(x)
    + (y 2 − y02 )B(x)D(x).
                                  °                       ¢
The last term on the right side is (x 3 − x) − (x03 − x0 ) B(x)D(x) and is divisible
by x − x0 . Therefore every member of the product ideal lies in the principal
ideal (x − x0 ). On the other hand, the product ideal contains (x − x0 )(x − x0 )
            2     2       3     3                             2           2
      ° (y − y0 2) = (x − 2x0¢) − (x − x0 ) = (x − x0 )(x + x x0 + x0 ). Since
and also
GCD (x −x0 ), (x +x x0 +x0 ) = 1, the product ideal contains x −x0 . Therefore
the product ideal equals (x − x0 ).
   The exceptional values of x0 are −1, 0, +1, where the locus has y0 = 0.
The geometry of the factorization is not so clear in this case, but the algebraic
416                     VIII. Commutative Rings and Their Modules

computation remains valid. Thus we have (x − x0 )R 0 = (x − x0 , y)2 if x0 equals
−1, 0, or +1. The conclusion is that the nonzero prime ideals of R 0 are of two
types, with (x − x0 )R 0 equal to
      the product of two distinct prime ideals in R 0 if x0 is not in {−1, 0, +1},
      the square of a prime ideal in R 0 if x0 is in {−1, 0, +1}.
The third type, with (x − x0 )R 0 prime in R 0 , does not arise. Toward the end of
Chapter IX we shall see how we could have anticipated the absence of the third
type.

    That is enough of a comparison for now. Certain structural results useful in
both algebraic number theory and algebraic geometry are needed even before
we get started at factoring ideals, and those are some of the topics for the
remainder of this chapter. In Section 11 we conclude by establishing unique
factorization of ideals for a class of examples that includes the examples p       above.
In the examples above, the rings we consideredp        were Z[X]/(X 2 + 5) = Z[ −5 ]
and C[x, y]/(y 2 − (x − 1)x(x + 1)) ∼        = C[x][ (x − 1)x(x + 1) ]. In each case
the notation [ · ] refers to forming the ring generated by the coefficients and the
expression or expressions in brackets.
    First we establish a result saying that ideals in the rings of interest are not
too wild. For example, in algebraic geometry, one wants to consider the set of
restrictions of the members of K[X 1 , . . . , X n ], K being a field, to the locus of
common zeros of a set of polynomials. The general tool will tell us that any ideal
in K[X 1 , . . . , X n ] is finitely generated; thus a description of what polynomials
vanish on the locus under study is not completely out of the question. The tool is
the Hilbert Basis Theorem and is the main result of Section 8.
    Second we need a way of understanding, in a more generalp       setting, the relation-
ship that we used   p   in the  above  examples  between  Z and  Z[  −5 ], and between
C[x] and C[x][ (x − 1)x(x + 1) ]. The tool is the notion of integral closure and
is the subject of Section 9.
    Third we need a way of isolating the behavior of prime ideals, of eliminating
the influence of algebraic or geometric factors that have nothing to do with the
prime ideal under study. The tool is the notion of localization and is the subject
of Section 10.
    In Section 11 we make use of these three tools to establish unique factorization
of ideals for a class of integral domains known as “Dedekind domains.” It is easy
to see that principal ideal domains are Dedekind domains, and we shall show
that many other integral domains, including the examples above, are Dedekind
domains. A refined theorem producing Dedekind domains will be obtained
toward the end of Chapter IX once we have introduced the notion of a “separable”
extension of fields.
                   8. Noetherian Rings and the Hilbert Basis Theorem         417

           8. Noetherian Rings and the Hilbert Basis Theorem

In this section, R will be a commutative ring with identity, and all R modules
will be assumed unital. We begin by introducing three equivalent conditions on
a unital R module.

  Proposition 8.30. If R is a commutative ring with identity and M is a unital
R module, then the following conditions on R submodules of M are equivalent:
   (a) (ascending chain condition) every strictly ascending chain of R sub-
       modules M1 $ M2 $ · · · terminates in finitely many steps,
   (b) (maximum condition) every nonempty collection of R submodules has
       a maximal element under inclusion,
   (c) (finite basis condition) every R submodule is finitely generated.

    PROOF. To see that (a) implies (b), let C be a nonempty collection of R
submodules of M. Take M1 in C. If M1 is not maximal, choose M2 in C properly
containing M1 . If M2 is not maximal, choose M3 in C properly containing M2 .
Continue in this way. By (a), this process must terminate, and then we have found
a maximal R submodule in C.
    To see that (b) implies (c), let N be an R submodule of M, and let C be
the collection of all finitely generated R submodules of N . This collection is
nonempty since 0 is in it. By (b), C has a maximal element, say N 0 . If x is in
N but x is not in N 0 , then N 0 + Rx is a finitely generated R submodule of N
that properly contains N 0 and therefore gives a contradiction. We conclude that
N 0 = N , and therefore N is finitely generated.
S∞  To see that (c) implies (a), let M1 $ M2 $ · · · be given, and put N =
   n=1 Mn . By (c), N is finitely generated. Since the Mn are increasing with n,
we can find some Mn 0 containing all the generators. Then the sequence stops no
later than at Mn 0 .                                                           §

   Let us apply Proposition 8.30 with M taken to be the unital R module R. As
always, the R submodules of R are the ideals of R.

  Corollary 8.31. If R is a commutative ring with identity, then the following
conditions on R are equivalent:
   (a) ascending chain condition for ideals: every strictly ascending chain of
       ideals in R is finite,
   (b) maximum condition for ideals of R: every nonempty collection of ideals
       in R has a maximal element under inclusion,
   (c) finite basis condition for ideals: every ideal in R is finitely generated.
418                    VIII. Commutative Rings and Their Modules

   The corollary follows immediately from Proposition 8.30. A commutative
ring with identity satisfying the equivalent conditions of Corollary 8.31 is said to
be a Noetherian commutative ring.

   EXAMPLES.
   (1) Principal ideal domains, such as Z and K[X] when K is a field. The finite
basis condition for ideals is satisfied since every ideal is singly generated. The fact
that (c) implies (a) has already been proved manually for principal ideal domains
twice in this chapter—once in the proof of (UFD1) for a principal ideal domain
in Theorem 8.15 and once in the proof of Lemma 8.26.
   (2) Any homomorphic image R 0 of a Noetherian commutative ring R, provided
1 maps to 1. In fact, if I 0 ⊆ R 0 is an ideal, its inverse image I is an ideal in R;
the image of a finite set of generators of I is a finite set of generators of I 0 .
   (3) K[X 1 , . . . , X n ] when K is a field. This commutative ring is Noetherian by
application of the Hilbert Basis Theorem (Theorem 8.32 below) and induction on
n. This ring is also a unique factorization domain, as we saw in Section 5.
   (4) Z[X]. This commutative ring is Noetherian, also by the Hilbert        p Basis
Theorem below. Example 2 shows therefore that the quotient Z[ −5 ] =
Z[X]/(X 2 + 5) is Noetherian. This ring is an integral domain, and we have
seen that it is not a unique factorization domain.

  Theorem 8.32 (Hilbert Basis Theorem). If R is a nonzero Noetherian com-
mutative ring, then so is R[X].
    PROOF. If I is an ideal in R[X] and if k ∏ 0 is an integer, let L k (I ) be the
union of {0} and the set of all nonzero elements of R that appear as the coefficient
of X k in some element of degree k in I . First let us see that {L k (I )}k∏0 is an
increasing sequence of ideals in R. In fact, if A(X) and B(X) are polynomials of
degree k in I with leading terms ak X k and bk X k , then A(X) + B(X) has degree
k if bk 6= −ak , and hence ak + bk is in L k (I ) in every case. Similarly if r is in R
and rak 6= 0, then r A(X) has degree k, and hence rak is in L k (I ) in every case.
Consequently L k (I ) is an ideal in R. Since I is closed under multiplication by
X, L k (I ) ⊆ L k+1 (I ) for all k ∏ 0.
    Next let us prove that if J is any ideal in R[X] such that I ⊆ J and L k (I ) =
L k (J ) for all k ∏ 0, then I = J . Let B(X) be in J with deg B(X) = k. Arguing
by contradiction, we may suppose that B(X) is not in I and that k is the smallest
possible degree of a polynomial in J but not in I . Since L k (I ) = L k (J ), we
can find A(X) in I whose leading term is the same as the leading term of B(X).
Since B(X) is not in I , B(X) − A(X) is not in I . Since I ⊆ J , B(X) − A(X) is
in J . Since deg(B(X) − A(X)) ≤ k − 1, we have arrived at a contradiction to
the defining property of k. We conclude that I = J .
                     8. Noetherian Rings and the Hilbert Basis Theorem               419

    Now let {I j } j∏0 be an ascending chain of ideals in R[X], and form L i (I j ) for
each i. When i or j is fixed, these ideals are increasing as a function of the other
index, j or i. By the maximum condition in R, L i (I j ) ⊆ L p (Iq ) for some p
and q and all i and j. For i ∏ p and j ∏ q, we have L i (I j ) ⊇ L p (Iq ) and
thus L i (I j ) = L p (Iq ). The case j = q gives L p (Iq ) = L i (Iq ), and therefore
L i (I j ) = L i (Iq ) for i ∏ p and j ∏ q. For any fixed i, the ascending chain
condition on ideals gives L i (I j ) = L i (In(i) ) for j ∏ n(i), and the above argument
shows that we may take n(i) = q if i ∏ p. Hence n(i) may be taken to be bounded
in i, say by n 0 , and L i (I j ) = L i (In 0 ) for all i ∏ 0 and j ∏ n 0 . By the result
of the previous paragraph, I j = In 0 for j ∏ n 0 , and hence the ascending chain
condition has been verified for ideals in R[X].                                        §

    Proposition 8.33. In a Noetherian integral domain R, every nonzero nonunit
is a product of irreducible elements.
  REMARK. The proof below gives an alternative argument for (UFD1) in
Theorem 8.15, an argument that does not so explicitly use the full force of Zorn’s
Lemma.
   PROOF. Let a1 be a nonzero nonunit of R. If a1 is not irreducible, then a1
has a factorization a1 = a2 b2 in which neither a2 nor b2 is a unit. If a2 is not
irreducible, then a2 has a factorization a2 = a3 b3 in which neither a3 nor b3 is
a unit. We continue in this way as long as it is possible to do so. Let us see
that this process cannot continue indefinitely. Assume the contrary. The equality
a1 = a2 b2 with b2 not a unit says that the inclusion of ideals (a1 ) ⊆ (a1 , a2 ) is
proper. Arguing in this way with a2 , a3 , and so on, we obtain

                       (a1 ) $ (a1 , a2 ) $ (a1 , a2 , a3 ) $ · · · ,

in contradiction to the ascending chain condition for ideals. Because of this
contradiction we conclude that for some n, an does not have any decomposition
an = an+1 bn+1 with bn+1 a nonunit. Hence an is irreducible. The upshot is that
our original element a1 has an irreducible factor, say c1 .
   Write a1 = c1 d2 . If d2 is not a unit, repeat the process with it, obtaining
d2 = c2 d3 with c2 irreducible. If d3 is not a unit, we can again repeat this process.
This process cannot continue indefinitely because otherwise we would have a
strictly increasing sequence of ideals

                        (c1 ) $ (c1 , c2 ) $ (c1 , c2 , c3 ) $ · · · ,

in contradiction to the ascending chain condition for ideals. Thus for some n, we
have a1 = c1 c2 . . . cn dn+1 with c1 , . . . , cn irreducible and with dn+1 equal to a
unit. Grouping cn and dn+1 as a single irreducible factor, we obtain the desired
factorization of the given element a1 .                                              §
420                      VIII. Commutative Rings and Their Modules

   Proposition 8.34. If R is a Noetherian commutative ring, then any R submod-
ule of a finitely generated unital R module is finitely generated.

   REMARK. The proof follows the lines of the argument for Proposition 8.24.

   PROOF. Let M be a unital finitely generated R module with a set {m 1 , . . . , m n }
of n generators, and define Mk = Rm 1 +· · ·+ Rm k for 1 ≤ k ≤ n. Then Mn = M
since M is unital. We shall prove by induction on k that every R submodule of
Mk is finitely generated. The case k = n then gives the proposition. For k = 1,
suppose that S is an R submodule of M1 = Rm 1 . Let I be the subset of all r
in R with rm 1 in S. Since S is an R submodule, I is an ideal in R, necessarily
finitely generated since R is Noetherian. Let I = (r1 , . . . , rl ). Then S = I m 1 =
Rr1 m 1 + Rr2 m 1 + · · · + Rrl m 1 , and the elements r1 m 1 , r2 m 1 , . . . , rl m 1 form a
finite set of generators of S.
   Assume inductively that every R submodule of Mk is known to be finitely
generated, and let Nk+1 be an R submodule of Mk+1 . Let q : Mk+1Ø → Mk+1 /Mk
be the quotient R homomorphism, and let ϕ be the restriction q Ø Nk+1 , mapping
Nk+1 into Mk+1 /Mk . Then ker ϕ = Nk+1 ∩ Mk is an R submodule of Mk and is
finitely generated by the inductive hypothesis. Also, image ϕ is an R submodule
of Mk+1 /Mk , which is singly generated with generator equal to the coset of
m k+1 . Since an R submodule of a singly generated unital R module was shown
in the previous paragraph to be finitely generated, image ϕ is finitely generated.
Applying Lemma 8.23 to ϕ, we see that Nk+1 is finitely generated. This completes
the induction and the proof.                                                                §

                                  9. Integral Closure

In this section, we let R be an integral domain, F be its field of fractions, and
K be a any field containing F. Sometimes we shall assume also that dim F K is
finite. The main cases of interest are as follows.

   EXAMPLES OF GREATEST INTEREST.
   (1) R = Z, F = Q, and dim F K < ∞. In Chapter IX we shall see in this case
from the “Theorem of the Primitive Element” that K is necessarily of the form
Q[θ] as already described in Section 1 and in Chapter IV. This is the setting we
used in Section 7 as orientation for certain problems in algebraic number theory.
    (2) R = K[X] for a field K, F = K(X) is the field of fractions of R, and
K is a field containing F with dim F K < ∞. In the special case K = C, this
is the setting we used in Section 7 as orientation for treating curves in algebraic
geometry.
                                     9. Integral Closure                         421

  Proposition 8.35. Let R be an integral domain, F be its field of fractions, and
K be any field containing F. Then the following conditions on an element x of
K are equivalent:
   (a) x is a root of a monic polynomial in R[X],
   (b) the subring R[x] of K generated by R and x is a finitely generated R
       module,
   (c) there exists a finitely generated nonzero unital R module M ⊆ K such
       that x M ⊆ M.
    REMARK. When the equivalent conditions of the proposition are satisfied,
we say that x is integral over R or x is integrally dependent on R. In this
terminology, in Section VII.5 and in Section 1 of the present chapter, we defined an
algebraic integer to be any member of C that is integral over Z. The equivalence
of (a) and (c) in this setting allowed us to prove that the set of algebraic integers
is a subring of C.
   PROOF. If (a) holds, we can write x n + an−1 x n−1 + · · · + a1 x + a0 = 0
for suitable coefficients in R. Solving for x n and substituting, we see that the
subring R[x], which equals R + Rx + Rx 2 + · · · , is actually given by R[x] =
R + Rx + · · · + Rx n−1 . Therefore R[x] is a finitely generated R module, and
(b) holds.
   If (b) holds, then we can take M = R[x] to see that (c) holds.
   If (c) holds, let m 1 , . . . , m k be generators of M as an R module. Then we can
find members ai j of R for which
                          xm 1 = a11 m 1 + · · · + a1k m k ,
                               ..
                                .
                          xm k = ak1 m 1 + · · · + akk m k .
This set of equations, regarded as a single matrix equation over K , becomes
                    x−a −a · · · −a   m 1   
                          11  12        1k                0
                    −a21 x−a22 · · · −a2k   m.2   0 
                                 ..        .       = .. .
                                     .          .         .
                       −ak1   −ak2   · · · x−akk     mk        0

The k-by-k matrix on the left is therefore not invertible, and its determinant, which
is a member of the field K , must be 0. Expanding the determinant and replacing
x by an indeterminate X, we obtain a monic polynomial of degree k in R[X] for
which x is a root. Thus (a) holds.                                                 §

   If R, F, and K are as above, the integral closure of R in K is the set of all
members of K that are integral over R. In Corollary 8.38 we shall prove that the
integral closure of R in K is a subring of K .
422                       VIII. Commutative Rings and Their Modules

    EXAMPLES OF INTEGRAL CLOSURES.
    (1) The integral closure of Z in Q is Z itself. This fact amounts to the statement
that a rational root of a monic polynomial with integer coefficients is an integer;
this was proved10 in the course of Lemma 7.30. Recall the argument: If x = p/q
is a rational number in lowest terms that satisfies x n +an−1 x n−1 +· · ·+a1 a +a0 =
0, then we clear fractions and obtain pn +an−1 pn−1 q +· · ·+a1 pq n−1 +a0 q n = 0.
Examining divisibility by q, we see that q divides pn . Hence any prime factor of
q divides p and shows that p/q cannot be in lowest terms. Therefore q has no
prime factors, and p/q is an integer.
                                                           p
    (2) Let us determine the integral closure of Z in Q( m ), where m is a square-
free integer other thanp 0 or 1. The result is going to be that the integral closure
consists of all a + b m with
                     Ω
                        both in Z                         if m ≡/ 1 mod 4,
            a and b                                 1
                        both in Z or both in Z + 2        if m ≡ 1 mod 4.

In other words, the integral closure is
                     Ω p
                       Z[ m ]                            / 1 mod 4,
                                                    if m ≡
                                 p                                                             (∗)
                       Z[ 12 (1 + m )]              if m ≡ 1 mod 4.

In fact, consider the polynomial

                           P(X) = X 2 − 2a X + (a 2 − mb2 ),
                              p
whose roots are exactly a ± bp m. If a and b are in Z, then P(X) has coefficients
in Z, and hence both of a ± b m are in the integral closure. If m ≡ 1 mod 4 and
a and b are both in Z + 12 , write a = c/2 and b = d/2 with c and d in 2Z + 1.
Since a 2 − mb2 = 14 (c2 − md 2 ), we have

              c2 − md 2 ≡ c2 − d 2 mod 4 ≡ 1 − 1 mod 4 ≡ 0 mod 4,

and therefore 14 (c2 − md 2 ) = a 2 − mb2 is in Z. Consequently the polynomial
                      p
P(X) exhibits a + b m as in the integral closure. p
   For the reverse inclusion, suppose that z = a + b m is in the integral closure
and is not in Z. Then z is a root of some monic polynomial A(X) in Z[X].
In addition, z is a root of P(X) above, and P(X) is a monic prime polyno-
mial in Q[X] because it has no rational first-degree factor. Writing A(X) =
B(X)P(X) + R(X) in Q[X] with R(X) = 0 or deg R(X) < deg P(X) = 2 and
   10 It is not assumed that the reader has looked at Chapter VII. A result that implies Lemma 7.30

will be obtained below as Corollary 8.38, which makes no use of material from Chapter VII.
                                  9. Integral Closure                            423

substituting z for X, we see that R(z) = 0, and we conclude that R(X) = 0.
Thus P(X) divides A(X). By Corollary 8.20c, P(X) is in Z[X]. Hence 2a and
a 2 − mb2 are in Z. One case is that a is in Z, and then mb2 is in Z; since m is
square free, there are no candidates for primes dividing the denominator of b, and
so b is in Z. The other case is that a is in Z + 12 , and then mb2 is in Z + 14 . So
m(2b)2 is in 4Z + 1. Since m is square free, there are no candidates for primes
dividing the denominator of 2b, and 2b is an integer. Since m(2b)2 is in 4Z + 1,
m ≡ 1 mod 4 and 2b ≡ 1 mod 2 are forced. This completes the proof that the
integral closure is given by (∗).
    (3) Under the assumption that the characteristic of the field K is  p not 2, let
us determine the integral closure T of R = K[x] in K = K(x)[ P(x) ] =
K(x)[y]/(y 2 − P(x)), where P(X) is a square-free polynomial in K[x]. Par-
enthetically we need to check that K is a field. Since K(x) is a field, K(x)[y]
is a principal ideal domain, and the question is whether (y 2 − P(x)) is a prime
( = maximal) ideal. We have only to observe that y 2 − P(x) is irreducible because
P(x) is not a square, and then it follows that K is a field. Thus the situation for
this example fits the setting of Proposition 8.35 with R = K[x], F = K(x), and
K = F(y)/(y 2 − P). We are p     going to show that the integral closure T of R in
K consists of all A(x) + B(x) P(x) with A(x) and B(x) both in R = K[x]. It
follows that the integral closure will be
                               p                         p
                    T = K[x][ P(x) ] = K[x] + K[x] P(x).                          (∗)
To see this, first let A(x) and B(x) be in K[x], and consider the monic polynomial

                        Q(y) = y 2 − 2Ay + (A2 − P B 2 )                  (∗∗)
                                                  p
in K[x][y]. Its roots in K are exactly A(x) ± B(x) P(x), and thus wep see that
both of A(x) ± B(X)P(x) are in T . Conversely let z = A(x) + B(x) P(x) be
in T but not R. Here A(x) and B(x) are in K(x). Then z is a root in K of some
monic polynomial M(y) whose coefficients are in K[x]. In addition, z is a root
of the member Q(y) of K(x)[y] defined in (∗∗). The division algorithm gives
M(y) = N (y)Q(y) + W (y) in K(x)[y] with W = 0 or deg W < deg Q = 2.
Substituting z ∈ T for y, we obtain
               0 = M(z) = N (z)Q(z) + W (z) = N (z)0 + W (z).
Thus W (z) = 0. If deg W = 1, then z is in F, and the same argument as in
Example 1 shows that z is in R; since we are assuming that z is not in R, we
conclude that W = 0. Therefore Q(y) divides M(y). By Corollary 8.20c, M(y)
is in K[x][y]. Hence 2A and A2 − P B 2 are in K[x]. Since the characteristic of
K is not 2, A is in K[x]. Then P B 2 is in K[x], and B must be in K[x] since P is
square free. Thus T is given as in (∗).
424                     VIII. Commutative Rings and Their Modules

   From these examples we can extract a rough description of the situation that
will interest us. We start with a ring R such as Z or K[x], along with its field
of fractions F. We assume that the integral closure of R in F is R itself, as is
the case with Z in Q and as we shall see is the case with K[x] in K(x). Let K
be a field containing F with dim F K < ∞. We are interested in an analog T of
integral elements relative to K , and what works as T is the integral closure of R
in K .

   Lemma 8.36. If A, B, and C are integral domains with A ⊆ B ⊆ C such that
C is a finitely generated B module and B is a finitely generated A module, then
C is a finitely generated A module.
   PROOF. Let C be generated over B by c1 , . . . , cr , and let B be generated over A
by b1 , . . . , bs . Then C is generated over A by the sr elements b j ci for 1 ≤ i ≤ r
and 1 ≤ j ≤ s.                                                                       §

   Proposition 8.37. Let R be an integral domain, F be its field of fractions, and
K be any field containing F. If x1 , . . . , xr are members of K integral over R,
then the subring R[x1 , . . . , xr ] of K generated by R and x1 , . . . , xr is a finitely
generated R module.
   REMARKS. The ring R[x1 , . . . , xr ] is certainly finitely generated over R as a
ring. The proposition asserts more—that it is finitely generated as an R module.
This means that all products of powers of the x j ’s are in the R linear span of
finitely many of them.
    PROOF. We induct on r. Since x1 is assumed integral over R, the case r = 1
follows from Proposition 8.35b. For the inductive step, suppose that R[x1 , . . . , xs ]
is a finitely generated R module. Since xs+1 is integral over R, it is certainly
integral over R[x1 , . . . , xs ]. Thus Proposition 8.35b shows that R[x1 , . . . , xs+1 ]
is a finitely generated R[x1 , . . . , xs ] module. Taking A = R, B = R[x1 , . . . , xs ],
and C = R[x1 , . . . , xs+1 ] in Lemma 8.36, we see that R[x1 , . . . , xs+1 ] is a finitely
generated R module.                                                                       §

   Corollary 8.38. Let R be an integral domain, F be its field of fractions, and
K be any field containing F. Then the integral closure of R in K is a subring
of K .
   REMARK. A special case of this corollary appears in somewhat different
language as Lemma 7.30.
   PROOF. Let x and y be integral over R. Then R[x, y] is a finitely gener-
ated R module by Proposition 8.37. We have (x ± y)R[x, y] ⊆ R[x, y] and
(x y)R[x, y] ⊆ R[x, y]. Taking M = R[x, y] in Proposition 8.35c and using the
                                   9. Integral Closure                              425

implication that (c) implies (a) in that proposition, we see that x ± y and x y are
integral over R.                                                                 §

   Corollary 8.39. Let A, B, and C be integral domains with A ⊆ B ⊆ C. If
every member of B is integral over A and if every member of C is integral over
B, then every member of C is integral over A.
   PROOF. Let K be the field of fractions of C, and regard C as a subring of
K . If x is in C, then x is a root of some monic polynomial with coefficients in
B, say x n + bn−1 x n−1 + · · · + b0 = 0. By Proposition 8.37 the subring D =
A[bn−1 , . . . , b0 ] of C is a finitely generated A module. Since x is integral over D,
D[x] is a finitely generated D module, by a second application of Proposition 8.37.
Lemma 8.36 shows that D[x] is a finitely generated A module. By Proposition
8.35, x is integral over A.                                                           §

    We say that the integral domain R is integrally closed if R equals its integral
closure in its field of fractions. Example 1 above in essence observed that the
ring Z of integers is integrally closed. Example
                                               p 2 above showed, for the case
m = −3, p that  the integral closure
                                 p   of Z in Q[ −3 ] is something other than the
ring Z[ −3 ]; consequently Z[ −3 ] cannot be integrally
                                                      p     closed.pA more direct
argument is to observe that the element x = 12 (−1 + −3 ) of Q[ −3 ] satisfies
                                  p
x 2 + x + 1 = 0 but is not in Z[ −3 ].

   Corollary 8.40. Let R be an integral domain, F be its field of fractions, and
K be any field containing F. Then the integral closure T of R in K is integrally
closed.
   PROOF. Corollary 8.38 shows that T is a subring of K . Let C be the integral
closure of T in K . We apply Corollary 8.39 to the integral domains R ⊆ T ⊆ C.
The corollary says that every member of C is integral over R, and hence C ⊆ T .
That is, C = T . Let η : T → L be the one-one homomorphism of T into its
field of fractions, and let ϕ : T → K be the inclusion. By Proposition 8.6, there
exists a unique ring homomorphism e    ϕ : L → K such that ϕ = e   ϕ η. Identifying
L with eϕ (L) ⊆ K , we can treat L as a subfield of K containing T . Since the only
elements of K integral over T have been shown to be the members of T , the only
elements of the subfield L integral over T are the members of T . Therefore T is
integrally closed.                                                               §

   Proposition 8.41. If R is a unique factorization domain, then R is integrally
closed.
   PROOF. Suppose that y −1 x is a member of the field of fractions F of R, with
                   6 0, and suppose that y −1 x satisfies the equation
x and y in R and y =
             (y −1 x)n + an−1 (y −1 x)n−1 + · · · + a1 (y −1 x) + a0 = 0
426                     VIII. Commutative Rings and Their Modules

with coefficients in R. Clearing fractions and moving x n over to one side by
itself, we have
                 x n = −y(an−1 x n−1 + · · · + a1 x y n−2 + a0 y n−1 ).
If a prime p in R divides y, then it divides x n and must divide x. If R is a unique
factorization domain, this says that we cannot arrange for GCD(x, y) to equal 1
unless no prime divides y. In this case, y is a unit in R. Consequently y −1 x is
in R.                                                                             §

    Since Z is a unique factorization domain, Proposition 8.41 gives a new proof
that Z is integrally closed. We see also that K[x] is integrally closed when K is
a field.                           p
    We saw above that the ring Z[ −3 ] is not integrally closed; consequently it
                                 domain. Another
cannot be a unique factorization p           p way of drawing this conclusion
is to verify in the equality (1 + −3 )(1 − −3 ) = 2 · 2 that the two elements
on the left are irreducible and are not associates of the irreducible element 2 on
the right.
    A more significant example, taking advantage of the contrapositive of Propo-
sition 8.41, is that any polynomial ring K[X 1 , . . . , X n ] over a field K is integrally
closed. In fact, we know from Section 5 that K[X 1 , . . . , X n ] has unique factor-
ization.

   Proposition 8.42. Let R be an integral domain, F be its field of fractions, and
K be any field containing F. If dim F K < ∞, then any x in K has the property
that there is some c 6= 0 in R such that cx is integral over R.
    REMARKS. Consequently K may be regarded as the field of fractions of the
integral closure T of R in K . In fact, let {xi } be a basis of K over F, and choose
ci 6= 0 in R for each i such that yi = ci xi is integral over R. Then {yi } is a basis
for K over F consisting of members of T , and it follows that every member of
K is the quotient of a member of T by a member of R. Proposition 8.6 supplies
a one-one ring homomorphism of the field of fractions for T into K , and the
description just given for the elements of K shows that this homomorphism is
onto K . Therefore K may be regarded as the field of fractions of T .
    PROOF. Since dim F K < ∞, the elements 1, x, x 2 , . . . of K are linearly
dependent over F. Therefore an x n + · · · + a1 x + a0 = 0 for a suitable n and
for suitable members of F with an 6= 0. Clearing fractions, we may assume that
an , . . . , a1 , a0 are in R and that an 6= 0. Multiplying the equation by ann−1 , we
obtain
           (an x)n + an−1 (an x)n−1 + · · · + a1 ann−2 (an x) + a0 ann−1 = 0.
Thus we can take c = an .                                                               §
                                  9. Integral Closure                            427

   In the base rings Z and K[x] of our examples, every nonzero prime ideal is
                                    p ideal domains. In Section 7 we
maximal because the rings are principal                              pmentioned
that every nonzero prime ideal in Z[ −5 ] is maximal even though Z[ −5 ] is not
a principal ideal domain. The remainder of this section, particularly Proposition
8.45, shows that the feature that every nonzero prime ideal is maximal is always
preserved in our passage from R to T .

   Proposition 8.43. Let R be an integral domain, F be its field of fractions, K
be any field containing F, and T be the integral closure of R in K . If Q is a
nonzero prime ideal of T , then P = R ∩ Q is a nonzero prime ideal of R.
   REMARKS. Corollary 8.38 shows that T is a ring. A construction for prime
ideals that goes in the reverse direction, from R to T , appears below as Proposition
8.53.
   PROOF. Let Q be a nonzero prime ideal of T , and put P = R ∩ Q. The ideal
P is proper since 1 is not in Q and cannot be in P. It is prime since x y ∈ P
implies that x y is in Q, x or y is in Q, and x or y is in R ∩ Q = P. To see that
P is nonzero, take t 6= 0 in Q. Since t is integral over R, t satisfies some monic
polynomial equation t n + an−1 t n−1 + · · · + a1 t + a0 = 0 with coefficients in R.
Without loss of generality, a0 6= 0 since otherwise we could divide the equation
by a positive power of t. Then a0 = t (−t n−1 − an−1 t n−2 − · · · − a1 ) exhibits a0
as in Q as well as in R. Thus P is nonzero.                                        §

   Lemma 8.44. Let R and T be integral domains with R ⊆ T and with every
element of T integral over R. If T 0 is an integral domain and ϕ : T → T 0 is a
homomorphism of rings onto T 0 , then every member of T 0 is integral over ϕ(R).
   PROOF. If t is in T , then t satisfies some monic polynomial equation of the
form t n +an−1 t n−1 +· · ·+a1 t +a0 = 0 with coefficients in R. Applying ϕ to this
equation, we see that ϕ(t) satisfies a monic polynomial equation with coefficients
in ϕ(R).                                                                         §

  Proposition 8.45. Let R be an integral domain, F be its field of fractions,
K be any field containing F, and T be the integral closure of R in K . If every
nonzero prime ideal of R is maximal, then every nonzero prime ideal of T is
maximal.
   REMARK. As with Proposition 8.43, Corollary 8.38 shows that T is a ring.
   PROOF. Let Q be a nonzero prime ideal in T , and let P = R ∩ Q.
Since P is a nonzero prime ideal of R by Proposition 8.43, the hypotheses say that
P is maximal in R. We shall apply Lemma 8.44 to the quotient homomorphism
T → T /Q. The lemma says that every element of the integral domain T /Q is
428                         VIII. Commutative Rings and Their Modules

integral over the subring (R + Q)/Q. Composing the inclusion homomorphism
R → T with the homomorphism T → T /Q yields a ring homomorphism
R → T /Q that carries P into the 0 coset. Since P = R ∩ Q, this ring
homomorphism descends to a one-one ring homomorphism R/P → T /Q. The
Second Isomorphism Theorem (for abelian groups) identifies the image of R/P
with (R + Q)/Q. Since P is maximal as an ideal in R, R/P is a field. The
ring isomorphism R/P ∼     = (R + Q)/Q thus shows that every element of T /Q is
integral over a field.
   Let us write k for this field isomorphic to R/P, and let k 0 be the field of fractions
of T /Q. We can now argue as in the proof of Proposition 4.1. If x 6= 0 is in T /Q,
then x satisfies a monic polynomial equation x m +cm−1 x m−1 +· · ·+c1 x +c0 = 0
with coefficients in k, and we may assume that c0 6= 0. Then the equality
x −1 = −c0−1 (c1 + · · · + am−1 x m−2 + x m−1 ) shows that the member x −1 of k 0 is
in fact in T /Q. Therefore T /Q is a field, and the ideal Q is maximal in T . §



                             10. Localization and Local Rings

In this section, R denotes a commutative ring with identity. The objective is to
enlarge or at least adjust R so as to make further elements of R become invertible
under multiplication. The prototype is the construction of the field of fractions
for an integral domain. A subset S of R is called a multiplicative system if 1
is in S and if the product of any two members of S is in S. The multiplicative
system will be used as a set of new allowable denominators, and the new ring will
be denoted11 by S −1 R.
    The construction proceeds along the same lines as in Section 2, except that
some care is needed to take into account the possibility of zero divisors in R and
even in S. We begin with an intermediate set

                                   e = {(r, s) | r ∈ R, s ∈ S}
                                   R

and impose the relation (r, s) ∼ (r 0 , s 0 ) if t (rs 0 − sr 0 ) = 0 for some t ∈ S. To
check transitivity, suppose that (r, s) ∼ (r 0 , s 0 ) and (r 0 , s 0 ) ∼ (r 00 , s 00 ). Then we
have t (rs 0 − sr 0 ) = 0 and t 0 (r 0 s 00 − s 0r 00 ) = 0 for some t and t 0 in S, and hence
                                              °                ¢    °                         ¢
          s 0 tt 0 (rs 00 − sr 00 ) = s 00 t 0 t (rs 0 − sr 0 ) + st t 0 (r 0 s 00 − s 0r 00 ) = 0.

Since s 0 tt 0 is in S, (r, s) ∼ (r 00 , s 00 ). Thus ∼ is an equivalence relation.

   11 Some   authors write R S instead of S −1 R.
                                   10. Localization and Local Rings                                       429

   The set of equivalence classes is denoted by S −1 R and is called the localiza-
tion12 of R with respect to S. Addition and multiplication are defined in R                      e by
(r, s) + (r 0 , s 0 ) = (rs 0 + sr 0 , ss 0 ) and (r, s)(r 0 , s 0 ) = (rr 0 , ss 0 ). Simple variants
of the arguments in Section 2 show that these operations descend to operations
on S −1 R. For example, with addition let (r, s), (r 0 , s 0 ), and (r 00 , s 00 ) be in R     e with
  0 0            00 00                  0 0 00       0 00                            0
(r , s ) ∼ (r , s ), i.e., with t (r s − s r ) = 0 for some t ∈ S. Then the
equivalence

     (r, s) + (r 0 , s 0 ) = (rs 0 + sr 0 , ss 0 ) ∼ (rs 00 + sr 00 , ss 00 ) = (r, s) + (r 00 , s 00 )

holds because
               °                                          ¢
            t 0 (rs 0 + sr 0 )ss 00 − (rs 00 + sr 00 )ss 0 = s 2 t 0 (r 0 s 00 − s 0r 00 ) = 0.

Similarly multiplication is well defined.
   The result is that S −1 R is a commutative ring with identity and that the mapping
r 7→ r ∗ , where r ∗ is the class of (r, 1), is a ring homomorphism of R into S −1 R
carrying 1 to 1. Let us observe the following simple properties of S −1 R:
     (i) S −1 R = 0 if and only if 0 is in S, since S −1 R = 0 if and only if
         (1, 1) ∼ (0, 1), if and only if t (1 · 1 − 1 · 0) = 0 for some t ∈ S.
    (ii) r 7→ r ∗ is one-one if and only if S contains no zero divisors, since r ∗ = 0
         if and only if (r, 1) ∼ (0, 1), if and only if tr = 0 for some t ∈ S.
   (iii) s ∗ is a unit in S −1 R for each s ∈ S, since the class of (1, s) is a multi-
         plicative inverse for s ∗ .
   (iv) every member of S −1 R is of the form (s ∗ )−1r ∗ for some r ∈ R and s ∈ S,
         since (r, s) = (r, 1)(1, s) is the class of r ∗ (s ∗ )−1 .
    (v) S −1 R is an integral domain if R is an integral domain and 0 is not in S.
In working with localizations, we shall normally drop the superscript ∗ on the
image r ∗ in S −1 R of an element r of R.
   Localizations arise in algebraic number theory and in algebraic geometry. In
applications to algebraic number theory, the ring R typically is an integral domain,
and therefore the map r 7→ r ∗ is one-one. In applications to algebraic geometry,
S may have zero divisors.

   EXAMPLES OF LOCALIZATIONS.
   (1) R is arbitrary, and S = {1}. Then S −1 R = R.
    12 Some authors use a term like “ring of fractions” or “ring of quotients” in connection with

localization in the general case or in some special cases. We shall not use these terms. In any event,
“ring of quotients” is emphatically not to be confused with “quotient ring” as in Chapter IV, which
is the coset space of a ring modulo an ideal.
430                       VIII. Commutative Rings and Their Modules

   (2) R is arbitrary, and S = {nonzero elements that are not zero divisors in R}.
Then every nonzero element of S −1 R is a zero divisor or is a unit. In this example
when S consists of all members of R other than 0, then R is an integral domain
and S −1 R is the field of fractions of R.
   (3) R is arbitrary, P is a prime ideal in R, and S is the set-theoretic complement
of P. The identity is in S since P is proper. The prime nature of P is used in
checking that S is a multiplicative system: if s and t are in S, then neither is in
P, by definition, and their product st cannot be in P since P is prime; thus the
product st is in S. With these definitions,

                              S −1 R    is often denoted by R P

and is called the localization of R at the prime P. In practice this is the most
important example of a localization,13 directly generalizing the construction of
the field of fractions of an integral domain as the localization at the prime ideal 0.
Here are some special cases, K being a field in the cases in which it occurs:
        (a) When R = Z and P = ( p) for a prime number p, the set S consists
of nonzero integers not divisible by p, and R P is the subset of all members of Q
whose denominators are not divisible by p.
        (b) When R = K[X] and P = (X −c), the set S consists of all polynomials
that are nonvanishing at c, and R P is the set of formal rational expressions in X
that are finite at c.
        (c) When R = K[X, Y ] and P = (X − c, Y − d), the set S consists of
all polynomials in X and Y that are nonvanishing at (c, d), and R P is the set of
formal rational expressions in X and Y that are finite at (c, d).
        (d) When R = K[X, Y ] and P = (X), the set S consists of all polynomials
in X and Y that are not divisible by X, and R P is the set of formal rational
expressions in X and Y that are meaningful as rational expressions in Y when X
is set equal to 0. For example, 1/(X + Y ) is in R P , but 1/ X is not.
    (4) R is arbitrary, {Pα } is a nonempty collection of prime ideals, and S is the
set of all elements of R that lie in none of the ideals Pα . Then S −1 R may be
regarded as the localization of R at the set of all primes Pα .
    (5) R is arbitrary, u is an element of R, and S = {1, u, u 2 , . . . }. For example,
if R = Z/( p2 ), where p is a prime, and if u = p, then 0 is in S, and observation
(i) shows that S −1 R = 0.
    (6) R is a Noetherian integral domain, E is an arbitrary set of nonzero elements
of R, and S is the set of all finite products of members of E, including the element
   13 Beware  of confusing R P with R/P. The ring R P is obtained by suitably enlarging R, at least
in the case that R is an integral domain, whereas the ring R/P is obtained by suitably factoring
something out from R.
                              10. Localization and Local Rings                           431

1 as the empty product. Let us see that the same S −1 R results when E is replaced
by a certain set E 0 of units and irreducible elements of R, namely the union
of R × and the set of all irreducible elements x in R such that x −1 is in S −1 R.
Define T to be the set of all finite products of members of E 0 . We show that
S −1 R = T −1 R. If e is in E 0 , then either e is a unit in R, in which case e−1
lies in R and therefore also S −1 R, or e is irreducible in R with e−1 in S −1 R.
Passing to finite products of members of E 0 , we see that T −1 ⊆ S −1 R. Hence
T −1 R ⊆ S −1 R. Now let s be in S, and use Proposition 8.33 to write s as a
product of irreducible elements s = s1 · · · sn . Then s j−1 = s −1 (s1 · · · sbj · · · sn ),
with sbj indicating a missing factor. By construction, each s j is in E 0 . Therefore
each s j is in T , and s is in T . Consequently S ⊆ T , and S −1 R ⊆ T −1 R.

   The localization of R at S is characterized up to canonical isomorphism by the
same kind of universal mapping property that characterizes the field of fractions
of an integral domain. To formulate a proposition, let us write η for the homo-
morphism r 7→ r ∗ of R into S −1 R. Then the pair (S −1 R, η) has the universal
mapping property stated in Proposition 8.46 and illustrated in Figure 8.7.
                                                ϕ
                                      R      −−−→ T
                                      
                                      
                                     ηy             ϕe


                                    S −1 R
     FIGURE 8.7. Universal mapping property of the localization of R at S.

   Proposition 8.46. Let R be a commutative ring with identity, let S be a
multiplicative system in R, let S −1 R be the localization of R at S, and let η be the
canonical homomorphism of R into S −1 R. Whenever ϕ is a ring homomorphism
of R into a commutative ring T with identity such that ϕ(1) = 1 and such that
ϕ(s) is a unit in T for each s ∈ S, then there exists a unique ring homomorphism
ϕ : S −1 R → T such that ϕ = e
e                               ϕ η.
   PROOF. If (r, s) with s ∈ S is a pair in R,       e we define 8(r, s) = ϕ(r)ϕ(s)−1 .
This is well defined since ϕ(s) is assumed to be a unit in T . Let us see that
8 is consistent with the equivalence relation, i.e., that (r, s) ∼ (r 0 , s 0 ) implies
8(r, s) = 8(r 0 , s 0 ). Since (r, s) ∼ (r 0 , s 0 ), we have u(rs 0 − r 0 s) = 0 for some
u ∈ S, and therefore also ϕ(u)(ϕ(r)ϕ(s 0 ) − ϕ(r 0 )ϕ(s)) = 0. Since ϕ(u) is a
unit, ϕ(r)ϕ(s 0 ) = ϕ(r 0 )ϕ(s). Hence 8(r, s) = ϕ(r)ϕ(s)−1 = ϕ(r 0 )ϕ(s 0 )−1 =
8(r 0 , s 0 ), as required.
   We can thus define e     ϕ of the class of (r, s) to be 8(r, s), and e  ϕ is well defined
as a function from S −1 R to T . It is a routine matter to check that e          ϕ is a ring
homomorphism. If r is in R, then e         ϕ (η(r)) = e   ϕ (class of (r, 1)) = 8(r, 1) =
432                     VIII. Commutative Rings and Their Modules

ϕ(r)ϕ(1)−1 , and this equals ϕ(r) since ϕ is assumed to carry 1 into 1. Therefore
ϕ η = ϕ.
e
   For uniqueness, observation (iv) shows that the most general element of S −1 R
is of the form η(r)η(s)−1 with r ∈ R and s ∈ S. Since (e         ϕ η)(r) = ϕ(r)
and (eϕ η)(s) = ϕ(s), we must have e    ϕ (η(r)η(s)−1 ) = eϕ (η(r))eϕ (η(s))−1 =
          −1
ϕ(r)ϕ(s) . Therefore ϕ uniquely determines e    ϕ.                             §

   We shall examine the relationship between ideals in R and ideals in the local-
ization S −1 R. If I is an ideal in R, then S −1 I = {s −1 i | s ∈ S, i ∈ I } is easily
checked to be an ideal in S −1 R and is called the extension of I to S −1 R. If J
is an ideal in S −1 R, then R ∩ J , i.e., the inverse image of J under the canonical
homomorphism η : R → S −1 R, is an ideal in R and is called the contraction
of J .

   Proposition 8.47. Let R be a commutative ring with identity, and let S −1 R
be a localization. If J is an ideal in S −1 R, then S −1 (R ∩ J ) = J . Consequently
the mapping I 7→ S −1 I is a one-one mapping of the set of all ideals I in R of
the form I = R ∩ J onto the set of all ideals in S −1 R, and this mapping respects
intersections and inclusions.
   REMARKS. As in the definition of contraction, R ∩ J means η−1 (J ), where
η : R → S −1 R is the canonical homomorphism. The map I 7→ S −1 I that carries
arbitrary ideals of R to ideals of S −1 R need not be one-one; the localization could
for example be the field of fractions of an integral domain and have only trivial
ideals. The proposition says that the map I 7→ S −1 I is one-one, however, when
restricted to ideals of the form I = R ∩ J .
   PROOF. From the facts that R ∩ J ⊆ J and J is an ideal in S −1 R, we obtain
 −1
S (R ∩ J ) ⊆ S −1 J ⊆ J . For the reverse inclusion let x be in J , and write
x = s −1r with r in R and s in S. Then sx = r is in R ∩ J , and therefore x is in
S −1 (R ∩ J ).
   For the conclusion about the mapping I 7→ S −1 I , the mapping is one-one
because S −1 (R ∩ J1 ) = S −1 (R ∩ J2 ) implies J1 = J2 by what we have just
shown; hence R ∩ J1 = R ∩ J2 . The mapping is onto because if J is given,
then J = S −1 (R ∩ J ) by what has already been shown. To see that the mapping
respects the intersection of ideals, let ideals R ∩ Jα be given for α in some
nonempty set. Then
             °T            ¢           T        T      T
      S −1    α   (R ∩ Jα ) = S −1 (R ∩ α Jα ) = α Jα = α S −1 (R ∩ Jα ).

Finally the fact that the mapping respects the intersection of two ideals implies
that it respects inclusions.                                                   §
                                  10. Localization and Local Rings                     433

   Corollary 8.48. Let R be a commutative ring with identity, and let S −1 R be
a localization.
   (a) If R is Noetherian, then S −1 R is Noetherian.
   (b) If every nonzero prime ideal in R is maximal, then the same thing is true
in S −1 R.
   (c) If R is an integral domain that is integrally closed and if S −1 R is not zero,
then S −1 R is integrally closed.
   (d) If I is an ideal in R, then the ideal S −1 I of S −1 R is proper if and only if
I ∩ S = ∅.
   PROOF. For (a), let {Jα } be a nonempty collection of ideals in S −1 R. Con-
traction of ideals is one-one by the first conclusion of Proposition 8.47, and it
respects inclusions because it is given by the inverse image of a function. Since R
is Noetherian, Corollary 8.31b produces a maximal element R ∩ J from among
the ideals R ∩ Jα of R. The first and second conclusions of Proposition 8.47
together show that J = S −1 (R ∩ J ) ⊇ S −1 (R ∩ Jα ) = Jα for all α. Hence J is
maximal among the Jα .
   For (b), let J1 be a nonzero prime ideal in S −1 R. Arguing by contradiction,
suppose that J2 is an ideal in S −1 R with J1 $ J2 $ S −1 R. Then R ∩ J1 ⊆
R ∩ J2 ⊆ R. If either of these inclusions were an equality, then use of the second
conclusion of Proposition 8.47 would give a corresponding equality for J1 , J2 , R,
and there is no such equality. Hence R ∩ J1 $ R ∩ J2 $ R.
   If J1 is prime in S −1 R, then R ∩ J1 is prime in R: In fact, if a and b are
members of R such that ab is in R ∩ J1 , then ab is in J1 , and either a or b must be
in J1 since J1 is prime. Since a and b are both in R, one of a and b is in R ∩ J1 .
Thus R ∩ J1 is prime.14
   By assumption for (b), R ∩ J1 is then maximal in R, and this conclusion
contradicts the fact that R ∩ J1 $ R ∩ J2 $ R. The assumption that J2 exists has
thus led us to a contradiction. Consequently there can be no such J2 , and J1 is a
maximal ideal in S −1 R.
   For (c), let F be the field of fractions of R, so that R ⊆ S −1 R ⊆ F. The field
of fractions of S −1 R is the field F as a consequence of Proposition 8.6. If x is a
member of F that is integral over S −1 R and if x satisfies x n +bn−1 x n−1 +· · ·+b0 =
0 with coefficients in S −1 R, then we can find a common element s of S and rewrite
this equation as

                         x n + (s −1 an−1 )x n−1 + · · · + (s −1 a0 ) = 0

with an−1 , . . . , a0 in R. Multiplying by s n , we obtain

                (sx)n + an−1 (sx)n−1 + · · · + a1 s n−2 (sx) + a0 s n−1 = 0.
   14 Problem   9 at the end of the chapter puts this argument in a broader context.
434                       VIII. Commutative Rings and Their Modules

Therefore sx is integral over R. Since R is integrally closed, sx is in R. Write
r = sx. Then x = s −1r with r in R and s in S. Hence x is exhibited as in S −1 R,
and we conclude that S −1 R is integrally closed.
   For (d), suppose that I ∩ S is nonempty. If s is in I ∩ S, then 1 = s −1 s is
in S −1 I and the ideal S −1 I equals S −1 R. Conversely if S −1 I = S −1 R, then 1
is in S −1 I = {s −1 i | s ∈ S, i ∈ I }, and hence 1 = s −1 i for some s and i;
consequently I ∩ S contains the element i = s.                                   §

   A local ring is a commutative ring with identity having a unique maximal
ideal. An equivalent definition is given in Proposition 8.49 below, and then it
follows that the localization S −1 R of Example 2 earlier in this section is a local
ring. Corollary 8.50 below will produce a more useful example: localization with
respect to a prime ideal, as in Example 3 earlier, always yields a local ring.15

    Proposition 8.49. A nonzero commutative ring R with identity is a local ring
if and only if the nonunits of R form an ideal.
  REMARK. The zero ring is not local, having no proper ideals, and its set of
nonunits is empty, hence is not an ideal.
   PROOF. If the nonunits of R form an ideal, then that ideal is a unique maximal
ideal since a proper ideal cannot contain a unit; hence R is local. Conversely
suppose that R is local and that M is the unique maximal ideal. If x is any
nonunit, then the principal ideal (x) is a proper ideal since 1 is not of the form xr.
By Proposition 8.8, (x) is contained in some maximal ideal, and we must have
(x) ⊆ M since M is the unique maximal ideal. Then x is in M, and we conclude
that every nonunit is contained in M.                                               §

   Corollary 8.50. Let R be an integral domain, let P be a prime ideal of R, let
S be the set-theoretic complement of P, and let R P = S −1 R be the localization
of R at P. Then R P is a local ring, its unique maximal ideal is M = S −1 P, and
P can be recovered from M as P = R ∩ M. If Q is any prime ideal of R that is
not contained in P, then S −1 Q = S −1 R.
    PROOF. The subset S −1 P of S −1 R is an ideal by Proposition 8.47, and Corol-
lary 8.48d shows that it is proper. Every member of S −1 R that is not in S −1 P
is of the form s 0−1 s with s and s 0 in S and hence is a unit. Since no unit lies in
any proper ideal, S −1 R has M = S −1 P as its unique maximal ideal, and S −1 R
is local by Proposition 8.49.
    15 For Example 3 with R = K[X] and P = (X − c), the sense in which the ring R is “local”
                                                                                      P
has a geometric interpretation: the only spot in K where we can regard members of R P as K-valued
functions is “near” the point c, with “near” depending on the element of R P . See the discussion
after the proof of Corollary 8.50 below.
                                  10. Localization and Local Rings                                  435

   The contraction R ∩ M consists of all elements in R of the form s −1 p with s
in S and p in P. Let us see that the contraction equals P. Certainly R ∩ M ⊇ P.
For the reverse inclusion the equation s −1 p = r says that p = rs. If r is not in
P, then the facts that s is not in P and P is prime imply that p = rs is not in P,
contradiction. Thus r is in P, and we conclude that P can be recovered from M
as P = R ∩ M.
   If Q is any prime ideal of R that is not contained in P, then S −1 Q = S −1 R. In
fact, any element q of Q that is not in P is in S; therefore 1 is in the ideal S −1 Q,
and S −1 Q = S −1 R.                                                                §

   The construction of R P in the corollary reduces to the construction of the
field of fractions of R if P = 0. Other interesting and typical cases occur for
suitable nonzero P’s when R = K[X, Y ], K being a field. One such prime ideal
is P = (X − c, Y − d); then, as was mentioned in connection with Example 3
above, the localization of R at P consists of the rational expressions f (X, Y )
that are well defined at (c, d). The maximal ideal in this case consists of all such
rational expressions that are 0 at (c, d). Another example of a nonzero prime
ideal in R = K[X, Y ] is P = (X); then the localization of R at P consists of
the rational expressions f (X, Y ) whose denominators are not divisible by X,
and the maximal ideal consists of all such rational expressions f (X, Y ) whose
numerators are divisible by X if f is written in lowest terms.
   A number-theoretic analog of the localizations of the previous paragraph is the
localization of R = Z at ( p), where p is a prime number. The discussion with
Example 3 above mentioned that the localization consists of all members of Q
with no factor of p in the denominator. In this case the maximal ideal consists
of those rationals q whose numerators are divisible by p if q is written in lowest
terms.

   We conclude this section with introductory remarks about a product operation
on ideals. Let R be a nonzero commutative ring with identity. If I and J are ideals
in R, then once again I J denotes16 the set of all sums of products of a member of
I by a member of J . Certainly I J is closed under addition and negatives, and the
fact that r(I J ) = (r I )J ⊆ I J for r ∈ R shows that I J is an ideal. Localization
with respect to a prime ideal is a handy tool for extracting information about
products of ideals. We illustrate with Propositions 8.52 and 8.53 below. The first
of these will play an important role in Section 11.

    16 Sometimes, such as in the equality S −1 S −1 = S −1 , the product notation is meant to refer only

to the set of all products, not to all sums of products. With ideals we are to allow sums of products.
The applicable convention will normally be clear from the context, but we shall be explicit when
there might be a possibility of confusion.
436                     VIII. Commutative Rings and Their Modules

    Lemma 8.51 (Nakayama’s Lemma). Let R be a commutative ring with
identity, let I be an ideal of R contained in all maximal ideals, and let M be
a finitely generated unital R module. If I M = M, then M = 0.
  REMARK. Here I M means the set of sums of products of a member of I by a
member of M. The lemma applies to no ideals if R = 0.
    PROOF. We induct on the number of generators of M. If M is singly generated,
say by a generator m, then the hypothesis I M = M implies that rm = m for
some r in I . Thus (1 − r)m = 0. If 1 − r is a unit, then we can multiply by its
inverse and obtain m = 0; we conclude that M = 0. If 1 − r is not a unit, then
it lies in some maximal ideal P, by application of Proposition 8.8 to the proper
principal ideal (1 − r). Since r lies in P by hypothesis, 1 lies in P, and we have
a contradiction to the fact that P is proper.
    Suppose that the lemma holds for n − 1 or fewer generators,    P         and let M be
generated by m 1 , . . . , m n . Since I M = M, we have nj=1 r j m j = m 1 for
                                                   P
suitable r1 , . . . , rn in I . Then (1 − r1 )m 1 = nj=2 r j m j . If 1 − r1 is a unit, then
we can multiply by its inverse and see that the generator m 1 is unnecessary; we
conclude that M = 0 by induction. If 1 − r1 is not a unit, then it lies in some
maximal ideal P. Since r1 lies in P by hypothesis, 1 lies in P, and we have a
contradiction.                                                                            §

   Proposition 8.52. Let R be a Noetherian commutative ring, and let I and P
be ideals in R with P prime. If I P = I , then I = 0.
   PROOF. Let us localize with respect to the prime ideal P. If we write S for the
set-theoretic complement of P in R, then R P = S −1 R is a local ring by Corollary
8.50, and its unique maximal ideal is S −1 P. Since (S −1 I )(S −1 R) = S −1 I R =
S −1 I , S −1 I is an ideal in R P . Also, (S −1 I )(S −1 P) = S −1 I P = S −1 I , and
S −1 I has to be proper. In Nakayama’s Lemma (Lemma 8.51), let us take M to
be the S −1 R module S −1 I . Since S −1 P is the only maximal ideal in S −1 R, M is
contained in all maximal ideals of S −1 R. Since R is Noetherian, Corollary 8.48a
shows S −1 R to be Noetherian, and the ideal S −1 I is a finitely generated S −1 R
module by Corollary 8.31c. The lemma applies since (S −1 P)(S −1 I ) = S −1 I ,
and the conclusion is that S −1 I = 0. Then the subset I of S −1 I must be 0. §

  Proposition 8.53. Let R be an integral domain, F be its field of fractions,
K be any field containing F, and T be the integral closure of R in K . If P is a
maximal ideal in R, then P T 6= T , and there exists a maximal ideal Q of T with
P = R ∩ Q.
  REMARKS. This result inverts the construction of Proposition 8.43, of course
not necessarily uniquely. The examples in Section 7 illustrate what can happen
                                  11. Dedekind Domains                                437

in simple cases. More detailed analysis of what can happen in general requires
some field theory and is postponed to Chapter IX, specifically when we discuss
“splitting of prime ideals in extensions.”
   PROOF. If P T 6= T , then Proposition 8.8 supplies a maximal ideal Q of T
with P T ⊆ Q. Since 1 is not in Q, we then have P ⊆ R ∩ Q $ R. Consequently
the maximality of P implies that P = R ∩ Q.
   Arguing by contradiction, we now assume that P T = T . Localizing, let S
be the set-theoretic complement of P in R, so that S −1 P is the unique maximal
ideal of S −1 R by Corollary 8.50. From P T = T , we can write
                                1 = a1 t1 + · · · + an tn                             (∗)
with each ai in P and each ti in T . If we define T0 to be the subring R[t1 , · · · , tn ]
of T , then T0 is a finitely generated R module by Proposition 8.37, and S −1 T0
is therefore a finitely generated S −1 R module. Equation (∗) shows that 1 lies
in P T0 . Multiplying by an arbitrary element of T0 , we see that P T0 = T0 .
Since S −1 S −1 = S −1 , we obtain (S −1 P)(S −1 T0 ) = S −1 T0 . Nakayama’s Lemma
(Lemma 8.51) allows us to conclude that S −1 T0 = 0. Since 1 lies in T0 , we have
arrived at a contradiction.                                                            §


                              11. Dedekind Domains

   A Dedekind domain is an integral domain with the following three properties:
     (i) it is Noetherian,
    (ii) it is integrally closed,
   (iii) every nonzero prime ideal is maximal.
Every principal ideal domain R is a Dedekind domain. In fact, (i) every ideal
in R is singly generated, (ii) R is integrally closed by Proposition 8.41, and (iii)
every nonzero prime ideal in R is maximal by Corollary 8.16.
   We shall be interested in Dedekind domains that are obtained by enlarging a
principal ideal domain suitably. The general theorem in this direction is that if
R is a Dedekind domain with field of fractions F and if K is a field containing
F with dim F K finite, then the integral closure of R in K is a Dedekind domain.
Let us state something less sweeping.

    Theorem 8.54. If R is a Dedekind domain with field of fractions F and if K
is a field containing F with dim F K finite, then the integral closure T of R in K
is a Dedekind domain if any of the following three conditions holds:
     (a) T is Noetherian,
     (b) T is finitely generated as an R module,
     (c) the field extension F ⊆ K is “separable.”
438                     VIII. Commutative Rings and Their Modules

   REMARKS. The term “separable” will be defined in Chapter IX, and the fact that
(c) implies (b) will be proved at that time. It will be proved also that characteristic 0
implies separable. For now, we shall be content with showing that (b) implies (a)
and that (a) implies that T is a Dedekind domain.
   PROOF. We are given that R satisfies conditions (i), (ii), (iii) above, and we are
to verify the conditions for T . Condition (ii) holds for T by Corollary 8.40, and
Proposition 8.45 shows that (iii) holds. If (a) holds, then T satisfies the defining
conditions of a Dedekind domain.
   Let us see that (b) implies (a). If (b) holds, then Proposition 8.34 shows that
every R submodule of T is finitely generated. Since T ⊇ R, every T submodule
of T is finitely generated. That is, every ideal of T is finitely generated, and T is
Noetherian. Thus (a) holds, and the proof is complete.                             §

        p 2 of integral closures in Section 9 showed that the integral closure of
   Example
Z inpQ( m ) is doublypgenerated as a Z module, a set of generators being either
{1, m } or {1, 12 (1 + m )}, depending on the value of m. Example 3 showed,
under the assumption thatpK has characteristic different from 2, that the integral
closure of K[x] in K(x)[
                      p       P(x) ] is doubly generated as a K[x] module, a set of
generators being {1, P(x) }. Since Z and K[x] are principal ideal domains and
hence Dedekind domains, these examples give concrete cases in which hypothesis
(b) in Theorem 8.54 is satisfied. Consequently in each case the theorem asserts
that a certain explicit integral closure is a Dedekind domain.

   Theorem 8.55 (unique factorization of ideals). If R is a Dedekind domain,
                                                                            Q       k
then each nonzero proper ideal I in R decomposes as a finite product nj=1 Pj j ,
where the Pj ’s are distinct nonzero prime ideals and the k j ’s are positive integers.
Moreover,
    (a) the decomposition into positive powers of distinct nonzero prime ideals
        is unique up to the order of the factors,
    (b) the power P k of a nonzero prime ideal P appearing in the decomposition
        of I is characterized as the unique nonnegative integer such that P k
        contains I and P k+1 does not contain I (with k = 0 interpreted as saying
        that P is not one of the Pj ),
    (c) whenever I, J1 , J2 are nonzero ideals with I J1 = I J2 , then J1 = J2 ,
    (d) whenever I and J1 are two nonzero proper ideals with I ⊆ J1 , then there
        exists a nonzero ideal J2 with I = J1 J2 .

   Let us say that a nonzero ideal J1 divides a nonzero ideal I if I = J1 J2 for
some ideal J2 . We say also that J1 is a factor of I . Conclusion (d), once it
is established, is an important principle for working with ideals in a Dedekind
domain: to contain is to divide.
                                  11. Dedekind Domains                                439

   Thinking along these lines leads us to expect that prime ideals play some
special role with respect to containment. Such a role is captured by the following
lemma.

  Lemma 8.56. In an integral domain, if P is a prime ideal such that
P ⊇ I1 · · · In for the product of the ideals I1 , . . . , In , then P ⊇ I j for some j.
  PROOF. By induction it is enough to handle n = 2. Thus suppose P ⊇ I1 I2 .
We are to show that P ⊇ I1 or P ⊇ I2 . Arguing by contradiction, suppose
on the contrary that x ∈ I1 and y ∈ I2 are elements with x ∈    / P and y ∈/ P.
Then x y cannot be in P since P is prime, but x y is in I1 I2 ⊆ P, and we have a
contradiction.                                                                §

  Lemma 8.57. Let R be a Dedekind domain, and let I be a nonzero ideal of
R. Then there exists a finite product P1 · · · Pk of nonzero prime ideals, possibly
empty and not necessarily having distinct factors, such that P1 · · · Pk ⊆ I .
    PROOF. We argue by contradiction. Among all nonzero ideals for which there
is no such finite product, choose one, say J , that is maximal under inclusion.
This choice is possible since R is Noetherian. The ideal J cannot be prime since
otherwise J ⊆ J would be the containment asserted by the lemma. Thus we can
choose elements a1 and a2 in R with a1 a2 ∈ J , a1 ∈            / J , and a2 ∈/ J . Define
ideals I1 and I2 by I1 = J + Ra1 and I2 = J + Ra2 . These strictly contain
J , and their product manifestly has I1 I2 ⊆ J . By maximality of J , we can find
products P1 · · · Pk and Q 1 · · · Q l of nonzero prime ideals with P1 · · · Pk ⊆ I1 and
Q 1 · · · Q l ⊆ I2 . Then P1 · · · Pk Q 1 · · · Q l ⊆ I1 I2 ⊆ J , contradiction.        §

   Lemma 8.58. Let R be a Dedekind domain, regard R as embedded in its field
of fractions F, let P be a nonzero prime ideal in R, and define
                                   ©               ™
                             P −1 = x ∈ F | x P ⊆ R .

Then the set P P −1 of sums of products equals R.
   PROOF. By definition of P −1 , P ⊆ P P −1 ⊆ R. Since P is an ideal and
P P −1 is closed under addition and negatives, P P −1 is an ideal. Property (iii) of
Dedekind domains shows that P is a maximal ideal in R, and therefore P P −1 = P
or P P −1 = R. We are to rule out the first alternative.
   Thus suppose that P P −1 = P. Since R is Noetherian by (i), P is a finitely
generated R submodule of F. The equality P P −1 = P implies that each member
x of P −1 has x P ⊆ P, and Proposition 8.35c implies that each such x is integral
over R. Since R is integrally closed by (ii), x is in R. Thus P −1 ⊆ R, and the
definition of P −1 shows that P −1 = R.
440                    VIII. Commutative Rings and Their Modules

   Fix a nonzero element a of P. Applying Lemma 8.57, find a product of
nonzero prime ideals such that P1 · · · Pk ⊆ (a) ⊆ P. Without loss of generality,
we may assume that k is as small as possible among all such inclusions. Since
P is prime and P1 · · · Pk ⊆ P, Lemma 8.56 shows that P contains some Pj , say
P1 . By (iii), P1 is maximal, and therefore P = P1 . Form the product P2 · · · Pk ,
taking this product to be R if k = 1. Then P2 · · · Pk is not a subset of (a), by
minimality of k, and there exists a member b of P2 · · · Pk that is not in (a). On
the other hand, P P2 · · · Pk ⊆ (a) shows that Pb ⊆ (a), hence that a −1 b P ⊆ R.
Thus a −1 b is in P −1 , which we are assuming is R. In other words, a −1 b is in R,
and b is in a R = (a), contradiction.                                             §

   PROOF OF THEOREM 8.55. Arguing by contradiction, we may assume because
R is Noetherian that I is maximal among the nonzero proper ideals that do not
decompose as products of prime ideals. Then certainly I is not prime. Application
of Proposition 8.8 produces a maximal ideal P containing I , and P is prime
by Corollary 8.11. Multiplying I ⊆ P by P −1 as in Lemma 8.58, we obtain
I ⊆ P −1 I ⊆ P −1 P = R, the equality holding by Lemma 8.58. Hence P −1 I
is an ideal. An equality I = P −1 I would imply that P I = P P −1 I = I by
Lemma 8.58, and then Proposition 8.52 would yield I = 0, a contradiction
to the hypothesis that I is nonzero. An equality P −1 I = R would imply
I = P P −1 I = P R = P by Lemma 8.58, in contradiction to the fact that
I is not prime. We conclude that I $ P −1 I $ R. The maximal choice
of I shows that P −1 I decomposes as a product P −1 I = P1 · · · Pr of prime
ideals, not necessarily distinct. One more application of Lemma 8.58 yields
I = P P −1 I = P P1 · · · Pr , and we have a contradiction. We conclude that every
nonzero proper ideal decomposes as a product of prime ideals. Grouping equal
factors, we can write the decomposition as in the statement of the theorem.
   Next let us establish uniqueness as in (a). Suppose that we have two equal
decompositions P1 · · · Pr = Q 1 · · · Q s as the product of prime ideals, and suppose
that r ≤ s. We show by induction on r that r = s and that the factors on the
two sides match, apart from their order. The base case of the induction is r = 0,
and then it is evident that s = 0. Assume the uniqueness for r − 1. Since P1 is
prime and P1 ⊇ Q 1 · · · Q s , P1 ⊇ Q j for some j by Lemma 8.56. By (iii) for
Dedekind domains, Q j is a maximal ideal, and therefore P1 = Q j . Multiplying
the equality P1 · · · Pr = Q 1 · · · Q s by P1−1 and applying Lemma 8.58 to each
side, we obtain P2 · · · Pr = Q 1 · · · Q j−1 Q j+1 · · · Q s . The inductive hypothesis
implies that r − 1 = s − 1 and the factors on the two sides match, apart from
their order. Then we can conclude about the equality P1 · · · Pr = Q 1 · · · Q s that
r = s and that the factors on the two sides match, apart from their order. This
proves (a).
   Let us establish the formula in (b) for k j . Suppose that P is a prime ideal.
                                11. Dedekind Domains                              441

By (a), we can write I = P n J for a certain integer n ∏ 0 in such a way that P
does not appear in the unique decomposition of J . Certainly P k ⊇ I for k ≤ n
because P k ⊇ P k P n−k = P n ⊇ P n J = I . Suppose P n+1 ⊇ I . Multiplying
P n+1 ⊇ I = P n J by n factors of P −1 and using Lemma 8.58 repeatedly, we
obtain P ⊇ P −n I = J . Since P is prime, Lemma 8.56 shows that P must
contain one of the factors when J is decomposed as the product of prime ideals,
and we have a contradiction to the maximality of this factor unless this factor is
P itself. In this case, P appears in the decomposition of J , and again we have a
contradiction.
   For (c), if I J1 = I J2 , substitute the unique decompositions as products of
prime ideals for I , J1 , and J2 , and use (a) to cancel the factors from I on each
side, obtaining J1 = J2 .
   For (d), suppose that I and J1 are two nonzero proper ideals with I ⊆ J1 . If
Piki is the largest power of a prime ideal Pi appearing in the decomposition of J1 ,
then Piki ⊇ J1 ⊇ I , and (b) shows that Piki appears in the decomposition of I . In
other words, if li is the largest power of Pi appearing in the decomposition of I ,
                         Q
then li ∏ ki . Let J2 = i Pili −ki . Then we obtain I = J1 J2 , and (d) is proved. §

   Corollary 8.59. Let R be a Dedekind domain, and let P be a nonzero prime
ideal in R. Then there exists an element π in P such that π is not in P 2 , and any
such element has the property that π k is not in P k+1 for any k ∏ 1.
   PROOF. Proposition 8.52 shows that P 2 is a proper subset of P, and therefore
we can find an element π in P that is not in P 2 . Since the principal ideal (π) has
(π) ⊆ P and (π) $ P 2 , the factorization of (π) involves P but not P 2 . Thus we
can use Theorem 8.55 to write (π) = P Q 1 · · · Q n for prime ideals Q 1 , . . . , Q n
different from P. Then (π k ) = (π)k = P k Q k1 · · · Q kn , and (b) of the theorem
says that P k+1 does not contain (π k ).                                            §

   Corollary 8.60. Let R be a Dedekind domain, and let P be a nonzero prime
ideal in R. For any integer e ∏ 1, the natural action of R on powers of P
makes P e−1 /P e into a vector space over the field R/P, and this vector space is
1-dimensional.
  REMARKS. This technical-sounding corollary will be used crucially late in
Chapter IX of this volume and again in Chapter V of Advanced Algebra.
   PROOF. Since R(P e−1 ) ⊆ P e−1 and P(P e−1 ) ⊆ P e , we obtain

                          (R/P)(P e−1 /P e ) ⊆ P e−1 /P e .

Thus P e−1 /P e is a unital R/P module, i.e., a vector space over the field R/P.
We show that it has dimension 1. Corollary 8.59 shows that there exists a member
442                     VIII. Commutative Rings and Their Modules

π of P not in P 2 , and it shows that π k is not in P k+1 for any k. This element
π has the property that (π) = P Q 1 · · · Q r for nonzero prime ideals Q 1 , . . . , Q r
distinct from P, and thus
                Rπ e−1 = (π e−1 ) = (π)e−1 = P e−1 Q e−1
                                                     1   · · · Q re−1 .
Hence
                    Rπ e−1 + P e = P e−1 (Q e−1
                                            1   · · · Q re−1 + P).
The ideal in parentheses on the right side strictly contains P since the failure
of P to divide Q e−1
                  1   · · · Q re−1 means that P does not contain Q e−1
                                                                   1   · · · Q re−1 (by
Theorem 8.55d). Since P is maximal, the ideal in parentheses is R, and we see
that R(π e−1 + P e ) = P e−1 /P e . Therefore (R/P)(π e−1 + P e ) = P e−1 /P e . This
formula says that P e−1 /P e consists of all scalar multiples of a certain element,
and it follows that P e−1 /P e is 1-dimensional.                                     §

  Lemma 8.61. If P and Q are distinct maximal ideals in an integral domain R
and if k and l are positive integers, then P k + Q l = R.
   PROOF. We know that P k + Q l is an ideal. Arguing by contradiction, assume
that it is proper. Then we can find a maximal ideal M with M ⊇ P k + Q l . This
M satisfies M ⊇ P k and M ⊇ Q l . By Lemma 8.56, M ⊇ P and M ⊇ Q. Since
P and Q are distinct and maximal, we obtain P = M = Q, contradiction.        §

   Corollary 8.62. If R is a Dedekind domain with only finitely many prime
ideals, then R is a principal ideal domain.
   REMARKS. Corollary 8.48 may be used to produce examples to which Corol-
lary 8.62 is applicable. All we have to do is to take one of our standard Dedekind
domains R and localize with respect to a nonzero prime ideal P. The corollary
says that the result R P is a Dedekind domain, and it has a unique maximal ideal,
hence a unique nonzero prime ideal. The conclusion is that R P is a principal
ideal domain.
    PROOF. Let P1 , . . . , Pn be the distinct nonzero prime ideals. Theorem 8.55
shows that any nonzero ideal I in R factors uniquely as I = P1k1 · · · Pnkn with
each k j ∏ 0. For 1 ≤ i ≤ n, Corollary 8.59 produces πi in Pi such that πi is not
in Pi2 , and it shows that πim is not in Pim+1 .
                                  k
    Lemma 8.61 gives Piki + Pj j = R if i 6= j. Applying the Chinese Remainder
Theorem (Theorem 8.27a), we can find an element a in R with a ≡ πiki mod Piki +1
for 1 ≤ i ≤ n. Using Theorem 8.55 again, let (a) = P1l1 · · · Pnln be the unique
factorization of the principal ideal (a). The defining property of a shows that a
is in Piki but not Piki +1 for each i. Thus (a) is contained in Piki but not in Piki +1 .
By Theorem 8.55b, li = ki for each i. Hence the ideal I = P1k1 · · · Pnkn = (a) is
exhibited as principal.                                                               §
                                       12. Problems                                   443

                                                        Q       k
   Corollary 8.63. If R is a Dedekind domain and if I = nj=1 Pj j is the unique
factorization of a nonzero proper ideal I as the product of positive powers of
                                              Q       k
distinct prime ideals Pj , then the map r 7→ nj=1 Pj j defined on R by r 7→
             k
(. . . , r + Pj j , . . . ) descends to a ring isomorphism
                                     k
                                = R/P1 1 × · · · × R/Pnkn .
                            R/I ∼
                                                   k
   PROOF. Lemma 8.61 shows that Piki + Pj j = R if i 6= j. Then the result
follows immediately from the Chinese Remainder Theorem (Theorem 8.27). §

                                     12. Problems
1.   This problem examines ring homomorphisms of the field of real numbers into
     itself that carry 1 into 1. Let ϕ be such a homomorphism.
     (a) Prove that ϕ is the identity on Q.
     (b) Prove that ϕ maps squares into squares.
     (c) Prove that ϕ respects the ordering of R, i.e., that a ≤ b implies ϕ(a) ≤ ϕ(b).
     (d) Prove that ϕ is the identity on R.
2.   An element r in a commutative ring with identity is called nilpotent if r n = 0
     for some integer n. Prove that if r is nilpotent, then 1 + r is a unit.
3.   If R is a field, prove that the embedding of R in its field of fractions exhibits R
     as isomorphic to its field of fractions.
4.   Prove that X is prime in R[X] if R is an integral domain.
5.   Suppose that R is an integral domain that is not a field.
     (a) Prove that there is a nonzero prime ideal in R[X] that is not maximal.
     (b) Prove that there is an ideal in R[X] that is not principal.
6.   This problem makes use of real-analysis facts concerning closed bounded inter-
     vals of the real line. Let R be the ring of all continuous functions from [0, 1] into
     R, with pointwise multiplication as the ring multiplication.
     (a) Prove for each x0 in [0, 1] that the set Ix0 of members of R that vanish at x0
          is a maximal ideal of R.
     (b) Prove that any maximal ideal I of R that is not some Ix0 contains finitely
          many members f 1 , . . . , f n of R that have no common zero on [0, 1].
     (c) By considering f 12 + · · · + f n2 in (b), prove that every maximal ideal of R is
          of the form Ix0 for some x0 in [0, 1].
7.   Let R be the ring of all bounded continuous functions from R into R, with
     pointwise multiplication as the ring multiplication. Say that a member f of R
     vanishes at infinity if for each ≤ > 0, there is some N such that | f (x)| < ≤
     whenever |x| ∏ N . Answer the following:
444                      VIII. Commutative Rings and Their Modules

      (a) Show that the subset I∞ of all members of R that vanish at infinity is an
          ideal but not a maximal ideal.
      (b) Why must R have at least one maximal ideal I that contains I∞ ?
      (c) Why can there be no x0 in R such that the maximal ideal I of (b) consists
          of all members of R that vanish at x0 ?
                                     p
8.    Let I be a nonzero ideal in Z[ −5 ].
      (a) Prove that I contains some positive integer.
      (b) Prove that I , as an abelian group under addition, is free abelian of rank 2.
      (c) If n denotes the least positive integer in I , prove that I has a Z basis of the
                          p                                     p           p
          form {n, a + b −5 } for a suitable member a + b −5 of Z[ −5 ].
9.    Let ϕ : R → R 0 be a homomorphism of commutative rings with identity such
      that ϕ(1) = 1. Prove that if P 0 is a prime ideal in R 0 , then P = ϕ −1 (P 0 ) is a
      prime ideal in R.
10. Determine the maximal ideals of each of the following rings:
    (a) R × R,
    (b) R[X]/(X 2 ),
    (c) R[X]/(X 2 − 3X + 2),
    (d) R[X]/(X 2 + X + 1).
11. (a) Prove or disprove: If I is a nonzero prime ideal in Q[X], then Q[X]/I is a
        unique factorization domain.
    (b) Prove or disprove: If I is a nonzero prime ideal in Z[X], then Z[X]/I is a
        unique factorization domain.
12. (Partial fractions) Let R be a principal ideal domain, and let F be its field of
    fractions.
    (a) Let n be a nonzero member of R with a factorization n = cd such that
         GCD(c, d) = 1. Prove for each m in R that the member mn −1 of F has a
         decomposition as mn −1 = ac−1 + bd −1 with a and b in R.
    (b) Let n be a nonzero member of R with a factorization n = p1k1 · · · prkr , the
         elements p j being nonassociate primes in R. Prove for each m in R that the
         member mn −1 of F has a decomposition as mn −1 = q1 p1−k1 + · · · + qr pr−kr
         with all q j in R.
13. (a) By adapting the proof that the ring of Gaussian integers forms a Euclidean
                                               p
        domain, prove that the function
                                   p    δ(a + b −2) = a 2 + 2b2 satisfies δ(rr 0 ) =
        δ(r)δ(r 0 ) and exhibits Z[ −2] as a Euclidean domain.
                                         p
    (b) It was shown in Section 9 that Z[ −3 ] is not a unique factorization domain,
        hence cannot be a Euclidean domain. What goes wrong with   p continuing the
        adaptation in the previous problem so that it applies to Z[ −3 ]?
                                       12. Problems                                  445

14. Let G be a group, and let R be a commutative ring with identity. Examples 16
    and 17 in Section 1 defined the group algebra RG and the R algebra C(G, R)
    of functions from G into R, convolution being the multiplication in C(G, R).
    Prove that the mapping g 7→ f g described with Example 17 extends to an R
    algebra isomorphism of RG onto C(R, G).
15. Let I be an ideal in Z[X], and suppose that the lowest degree of a nonzero
    polynomial in I is n and that I contains some monic polynomial of degree n.
    Prove that I is a principal ideal.
16. For each integer n > 0, exhibit an ideal In in Z[X] that cannot be written with
    fewer than n generators.
17. Let ϕ be the substitution homomorphism ϕ : K[x, y] → K[t] defined by x 7→ t 2 ,
    y 7→ t 3 , and ϕ(c) = c for c ∈ K.
    (a) Prove that ker ϕ is the principal ideal (y 2 − x 3 ).
    (b) What is image ϕ?
18. Let R = Z[i].
    (a) Show that each unital R module M may be regarded as an abelian group
        with an abelian-group homomorphism ϕ : M → M for which ϕ 2 is the
        mapping m 7→ −m.
    (b) Show conversely that if M is an abelian group and there exists an abelian-
        group homomorphism ϕ : M → M for which ϕ 2 is the mapping m 7→ −m,
        then M may be regarded as a unital R module.
19. Let R be a unique factorization domain, and let F be its field of fractions. Let
    A(X) and B(X) be nonzero polynomials in F[X], let A0 (X) and B0 (X) be their
    associated primitive polynomials, and suppose that B(X) divides A(X) in F[X].
    Prove that B0 (X) divides A0 (X) in R[X].
20. Prove that an integral domain with finitely many elements is a field.
21. Two proofs of Theorem 8.18 were given, one using direct multiplication of
    polynomials and the other using polynomials with coefficients taken modulo
    ( p), and it was stated that proofs in both these styles could be given for Corollary
    8.22. A proof in the first style was supplied in the text. Supply a proof in the
    second style.
22. Let K be a field. ≥          ¥
    (a) Prove that det WY
                             X
                             Z
                                  , when considered as a polynomial in K[W, X, Y, Z ],
        is irreducible.
    (b) Let X i j be indeterminates for i and j from 1 to n. Doing an induction, prove
        that the polynomial det[X i j ] is irreducible in K[X 11 , X 12 , . . . , X nn ].
23. Prove that two members of Z[X] are relatively prime in Q[X] if and only if the
    ideal they generate in Z[X] contains a nonzero integer.
446                    VIII. Commutative Rings and Their Modules

24. Let V be the Z[i] module with two generators u 1 , u 2 related by the conditions
     (1 + i)u 1 + (2 − i)u 2 = 0 and 3u 1 + 5iu 2 = 0. Express V as the direct sum of
     cyclic Z[i] modules.
                                           £       p     §
Problems 25–26 concern the ring R = Z 12 (1 + −m ) , where m is a square-free
                                             p
integer > 1 with m ≡ 3 mod 4. Let F = Q[ −m ] be the field of fractions of R.
                    p
25. For z = x + y −m in F, define δ(z) = x 2 + my 2 .
     (a) Show that δ(zw) = δ(z)δ(w).
     (b) Show that if for each z in F there is some r in R with δ(z − r) < 1, then δ
          exhibits R as a Euclidean domain.
26. Prove that the condition of part (b) of the previous problem is satisfied for m = 3,
                                     £        p       §
    7, and 11, and conclude that Z 12 (1 + −m ) is a Euclidean domain for these
    values of m.

Problems 27–31 classify the primes in the ring Z[i] of Gaussian integers. This ring
is a Euclidean domain and therefore is a unique factorization domain. Members of
this ring will be written as a + bi, and it is understood that a and b are in Z. Put
N (a + bi) = (a + bi)(a − bi) = a 2 + b2 .
27. Let a + bi be prime in Z[i]. Prove that
    (a) a − bi is prime.
    (b) N (a + bi) is a power of some positive prime p in Z.
    (c) N (a + bi) equals p or p2 when p is as in (b).
    (d) N (a + bi) = p2 in (c) forces a + bi = p, apart from a unit factor.
28. Prove that no prime a + bi in Z[i] has N (a + bi) = p with p of the form 4n + 3.
    Conclude that every positive prime in Z of the form 4n + 3 is a prime in Z[i].
29. Prove that the only primes a + bi of Z[i] for which N (a + bi) equals 2 or 22 are
    1 + i and its associates, for which N (a + bi) = 2.
30. Prove that if p is a positive prime in Z of the form 4n + 1, then −1 is a square
    in the finite field F p .
31. Let p be a positive prime in Z of the form 4n + 1.
    (a) Prove that there exist ring homomorphisms ϕ1 of Z[X] onto F p [X]/(X 2 +1)
        and ϕ2 of Z[X] onto Z[i]/( p).
    (b) Prove that ker ϕ1 and ker ϕ2 are both equal to the ideal ( p, X 2 + 1) in Z[X],
        and deduce a ring isomorphism Z[i]/( p) ∼     = F p [X]/(X 2 + 1).
    (c) Taking into account the results of Problems 27 and 30, show that p is not
        prime in Z[i] and is therefore of the form p = N (a + bi) = a 2 + b2 for
        some prime a + bi in Z[i].
    (d) Prove a uniqueness result for the decomposition p = a 2 + b2 , that if also
        p = a 02 + b02 , then a 0 + b0 i is an associate either of a + bi or of a − bi.
                                       12. Problems                                      447

Problems 32–35 establish a theory of elementary divisors. This theory provides
a different uniqueness result, beyond the one in Corollary 8.28, to accompany the
Fundamental Theorem of Finitely Generated Modules over a Principal Ideal Domain.
When specialized to K[X] for a field K, the theory yields the rational canonical form
of a member of Mn (K). Let R be a nonzero principal ideal domain. If C and D are
members of Mmn (R), let us say that C and D are equivalent if there exist A in
Mm (R) and B in Mn (R) with det A in R × , det B in R × , and D = AC B. Fix m
and n, and put k = min(m, n). If C is a member of Mmn (R), its diagonal entries
are the entries C11 , C22 , . . . , Ckk . The matrix C will be called diagonal if its only
nonzero entries are diagonal entries. Problems 26–31 of Chapter V are relevant for
Problem 34.
32. (a) Suppose that C is a diagonal matrix in Mmn (R) with C11 6= 0. Show that
        C is equivalent to a matrix C 0 described as follows: all entries of C 0 are the
        same as those of C except possibly for the entries C21   0 , . . . , C 0 in the first
                                                                              k1
                                       0
        column, and these satisfy C j1 = C j j .
    (b) By applying the algorithm of Lemma 8.26 to the matrix C 0 in (a), prove that
        any nonzero diagonal matrix C in Mmn (R) is equivalent to a diagonal matrix
                         00 divides all the diagonal entries of C 00 .
        C 00 such that C11
    (c) By iterating the construction in (a) and (b), prove that any diagonal matrix
        C in Mmn (R) is equivalent to a diagonal matrix D having the following
        properties: The nonzero diagonal entries of D are the entries D j j with
        1 ≤ j ≤ l for some integer l with 0 ≤ l ≤ k. For each j with 1 ≤ j < l,
        D j j divides D j+1, j+1 .
33. (a) Establish the following uniqueness theorem: Let D and E be diagonal
        matrices in Mmn (R) whose diagonal entries satisfy the divisibility property
        in (c) of the previous problem. Prove that if D and E are equivalent, then they
        have the same number of nonzero entries, and their corresponding diagonal
        entries are associates.
    (b) Combine Corollary 8.29, Problem 32, and Problem 33a to establish the
        following elementary-divisors version of the Fundamental Theorem of
        Finitely Generated Modules: If R is a principal ideal domain, then any
                      Ls unital R module M is the direct sum of a nonunique free R
        finitely generated
        submodule i=1       R of a well-defined finite rank s ∏ 0 and the R submodule
        T of all members m of M such that rm = 0 for some r 6= 0 in R. In turn,
                                                                  Ll
        the R submodule T is isomorphic to a direct sum T ∼    = j=1 R/(d j ), where
        the d j are nonzero nonunits in R such that d j divides d j+1 for 1 ≤ j < l.
        The number of l of summands and the ideals (d j ) are uniquely determined
        by M.
34. (a) (Rational decomposition) Let K be a field, and let L : V → V be a K
        linear mapping from a finite-dimensional K vector space V to itself. By
        applying Theorem 8.25 and the results of the previous problems to V as a
448                        VIII. Commutative Rings and Their Modules

          K[X] module with Xv = L(v), prove the following: V can be written as
          the direct sum of cyclic subspaces V1 , . . . , Vr under L in such a way that
          the minimal polynomial of L on Vj divides the minimal polynomial of L on
          Vj+1 for 1 ≤ j < r; moreover, the integer r and the minimal polynomials
          are uniquely determined by L, and any two linear mappings with the same
          r and matching minimal polynomials are similar over K.
      (b) (Rational canonical form) Interpret the result of (a) as saying something
          about similarity over K of any matrix in Mnn (K) to a certain block diagonal
          matrix with blocks of the form in Problem 28 for Chapter V and with minimal
          polynomials having a suitable divisibility property.
35. Let K and L be fields with K ⊆ L, and suppose that two members of Mn (K) are
    conjugate via GL(n, L). Prove that they are conjugate via GL(n, K).

Problems 36–39 concern symmetric polynomials in n indeterminates over a field. Let
F be a field, and let R = F[X 1 , . . . , X n ]. If σ ∈ Sn is a permutation, then there
is a corresponding substitution homomorphism of rings σ ∗ : R → R fixing F and
carrying each X j into X σ ( j) . A symmetric polynomial A in R is a member of R
for which σ ∗ A = A for every permutation σ . The symmetric polynomials form a
subring of R containing the constants. The main result about symmetric polynomials
is that every symmetric polynomial is a polynomial in the “elementary symmetric
polynomials”; these will be defined below.
36. Prove that the ring homomorphisms σ ∗ satisfy (σ τ )∗ = σ ∗ τ ∗ . Deduce that each
    σ ∗ : R → R is an isomorphism.
37. Prove that the homogeneous-polynomial expansion of any symmetric polynomial
    is into symmetric polynomials.
                                                                               ∼
38. For each permutation σ , let σ ∗∗ be the substitution homomorphism of R[X] =
      F[X 1 , . . . , X n , X] acting as σ ∗ on R and carrying X to itself.
      (a) Prove that (σ τ )∗∗ = σ ∗∗ τ ∗∗ and that each σ ∗∗ is a ring isomorphism of
          R[X].
      (b) Prove that each coefficient in R[X] of any polynomial fixed by all σ ∗∗ is a
          symmetric polynomial in R.
      (c) The polynomial (X − X 1 )(X − X 2 ) · · · (X − X n ) is fixed by all σ ∗∗ , and its
          coefficients are called the elementary symmetric polynomials. Show that
          they are
                  P                P                       P
           E1 =       X i , E2 =          X i X j , E3 =        X i X j X j , . . . , En = X 1 X 2 · · · X n .
                  i                i< j                i< j<k


39. Order the monomials of total degree m by saying that the monomial a X 1k1 · · · X nkn
                   P
    with a 6= 0 and k j = m is greater than the monomial a 0 X 1l1 · · · X nln with a 0 6= 0
        P
    and l j = m if the first j for which k j 6= l j has k j > l j .
                                        12. Problems                                  449

    (a) If A(X 1 , . . . , X n ) is a nonzero symmetric polynomial homogeneous of de-
        gree m and if a X 1k1 · · · X nkn is its nonzero monomial that is highest in the
        above order, why must it be true that k1 ∏ k2 ∏ · · · ∏ kn ?
    (b) Verify that the largest monomial in E 1c1 · · · E ncn in the ordering is

                                 X 1c1 +c2 +···+cn X 2c2 +···+cn · · · X ncn .

    (c) Show that if A(X 1 , . . . , X n ) is a nonzero symmetric polynomial homoge-
        neous of degree m, then there exist a symmetric polynomial M = E 1c1 · · · E ncn
        homogeneous of degree m and a scalar r such that the largest monomials in
        A and r M are equal.
    (d) With notation as in (c), show that A − r M equals 0 or else the largest
        monomial of A is greater than the largest monomial of A − rm.
    (e) Deduce that every symmetric polynomial is a polynomial in the elementary
        symmetric polynomials.

Problems 40–43 concern the Pfaffian of a (2n)-by-(2n) alternating matrix X = [xi j ]
with entries in a field K. Here “alternating” means that xi j = −x ji for all i and j
and xii = 0 for all i. The Pfaffian is the polynomial in the entries of X with integer
coefficients given by


                                    X                      n
                                                           Y
                    Pfaff(X) =                  (sgn τ )         xτ (2k−1),τ (2k) ,
                                 certain τ ’s              k=1
                                   in S2n



where the sum is taken over those permutations τ such that τ (2k − 1) < τ (2k) for
1 ≤ k ≤ n and such that τ (1) < τ (3) < · · · < τ (2n−1). The Pfaffian was introduced
in Problems 23–28 at the end of Chapter VI. It was shown in those problems that
the Pfaffian satisfies det X = (Pfaff(X))2 . The present problems will make use of
that result but of no other results from Chapter VI. They will also make use of facts
concerning continuous functions and connected open subsets of Euclidean space.
40. Prove by induction on m that the open subset of Cm on which a nonzero poly-
    nomial function P(z 1 , . . . , z m ) is nonzero is pathwise connected and therefore
    connected.
41. For this problem let K = C.
    (a) For any two matrices A and X in M2n (C) with X alternating, prove that
         Pfaff(At X A) = ±(det A)Pfaff(X) with the sign depending on A and X.
    (b) Fix X, and allow A to vary. Using Problem 40, prove that the sign is always
         positive in (a). That is, prove that Pfaff(At X A) = (det A)Pfaff(X).
450                      VIII. Commutative Rings and Their Modules

42. For this problem let K be any field. By regarding the expressions Pfaff(At X A)
    and (det A)Pfaff(X) as polynomials with coefficients in Z in the indeterminates
    Ai j for all i and j and the indeterminates X i j for i < j, and using the prin-
    ciple of permanence of identities in Section V.2, prove that Pfaff(At X A) =
    (det A)Pfaff(X) whenever A and X are in M2n (K) and X is alternating.
43. Section VI.5 defines a particular alternating matrix J for which Pfaff(J ) = 1.
    A symplectic matrix g over K is one for which g t J g = J . Prove that every
    symplectic matrix has determinant 1.
Problems 44–47 concern Dedekind domains. Let R be such a domain. It is to be
proved that each nonzero ideal I is doubly generated in the sense that I = Ra + Rb
for suitable members a and b of R.
44. Let R1 , . . . , Rn be nonzero commutative rings with identity, not necessarily
    integral domains. Prove that if every ideal of each R j is principal, then every
    ideal in R1 × · · · × Rn is principal.
45. Let P be a nonzero prime ideal, and let k be a positive integer.
    (a) Prove that the only nonzero proper ideals in R/P k are P/P k , P 2 /P k , . . . ,
        P k−1 /P k .
    (b) Using the element π in the statement of Corollary 8.59, prove that each of
        the ideals in (a) is principal.
46. Combining Corollary 8.63 with Problems 44 and 45, conclude that the quotient
    of R by any nonzero proper ideal has only principal ideals.
47. Let I be a nonzero proper ideal in R. By letting a be any nonzero element of I
    and by applying (c) in the previous problem to the ideal I /(a) of R/(a), prove
    that I = Ra + Rb for a suitable b in I .
Problems 48–53 introduce and classify “fractional ideals” in Dedekind domains. Let
R be a Dedekind domain, regarded as a subring of its field of fractions F. A fractional
ideal in F is a finitely generated R submodule of F.
48. Prove that the fractional ideals in F that lie in R are exactly the ordinary ideals
    of R.
49. Prove for any fractional ideal M that there exists a nonzero member a of F such
    that a M lies in R and hence is an ordinary ideal. Conclude that the product of
    two fractional ideals is a fractional ideal.
50. Prove that if I is a nonzero ideal of R and if I −1 is defined by

                                  I −1 = {x ∈ F | x R ⊆ I },

      then I −1 is a fractional ideal in F. Conclude that if P is a prime ideal in R, then
      P −1 as defined in Lemma 8.58 is a fractional ideal in F.
                                     12. Problems                                  451

51. Prove, by arguing with an ideal that is maximal among those for which the
    statement is false, that to any nonzero ideal I in R corresponds some fractional
    ideal M of F such that I M = R.
52. Prove in the notation of the previous two problems that M = I −1 .
53. Deduce that every nonzero fractional ideal is of the form I J −1 , where I and J
    are nonzero ideals. Conclude that
                                                              Qn
    (a) the nonzero fractional ideals are exactly all products i=1  Piki , where the Pi
         are distinct nonzero prime ideals and the ki are arbitrary nonzero integers,
         positive or negative,
    (b) the nonzero fractional ideals form a group.
                                       CHAPTER IX

                             Fields and Galois Theory



Abstract. This chapter develops some general theory for field extensions and then goes on to
study Galois groups and their uses. More than half the chapter illustrates by example the power
and usefulness of the theory of Galois groups. Prerequisite material from Chapter VIII consists
of Sections 1–6 for Sections 1–13 of the present chapter, and it consists of all of Chapter VIII for
Sections 14–17 of the present chapter.
    Sections 1–2 introduce field extensions. These are inclusions of a base field in a larger field.
The fundamental construction is of a simple extension, algebraic or transcendental, and the next
construction is of a splitting field. An algebraic simple extension is made by adjoining a root of an
irreducible polynomial over the base field, and a splitting field is made by adjoining all the roots of
such a polynomial. For both constructions, there are existence and uniqueness theorems.
    Section 3 classifies finite fields. For each integer q that is a power of some prime number, there
exists one and only one finite field of order q, up to isomorphism. One finite field is an extension of
another, apart from isomorphisms, if and only if the order of the first field is a power of the order of
the second field.
    Section 4 concerns algebraic closure. Any field has an algebraic extension in which each
nonconstant polynomial over the extension field has a root. Such a field exists and is unique up
to isomorphism.
    Section 5 applies the theory of Sections 1–2 to the problem of constructibility with straightedge
and compass. First the problem is translated into the language of field theory. Then it is shown that
three desired constructions from antiquity are impossible: “doubling a cube,” trisecting an arbitrary
constructible angle, and “squaring a circle.” The full proof of the impossibility of squaring a circle
uses the fact that π is transcendental over the rationals, and the proof of this property of π is deferred
to Section 14. Section 5 concludes with a statement of the theorem of Gauss identifying integers n
such that a regular n-gon is constructible and with some preliminary steps toward its proof.
    Sections 6–8 introduce Galois groups and develop their theory. The theory applies to a field
extension with three properties—that it is finite-dimensional, separable, and normal. Such an
extension is called a “finite Galois extension.” The Fundamental Theorem of Galois Theory says in
this case that the intermediate extensions are in one-one correspondence with subgroups of the Galois
group, and it gives formulas relating the corresponding intermediate fields and Galois subgroups.
    Sections 9–11 give three standard initial applications of Galois groups. The first is to proving the
theorem of Gauss about constructibility of regular n-gons, the second is to deriving the Fundamental
Theorem of Algebra from the Intermediate Value Theorem, and the third is to proving the necessity
of the condition of Abel and Galois for solvability of polynomial equations by radicals—that the
Galois group of the splitting field of the polynomial have a composition series with abelian quotients.
    Sections 12–13 begin to derive quantitative information, rather than qualitative information, from
Galois groups. Section 12 shows how an appropriate Galois group points to the specific steps in
the construction of a regular n-gon when the construction is possible. Section 13 introduces a tool

                                                  452
                                         1. Algebraic Elements                                       453

known as Lagrange resolvents, a precursor of modern harmonic analysis. Lagrange resolvents are
used first to show that Galois extensions in characteristic 0 with cyclic Galois group of prime order p
are simple extensions obtained by adjoining a pth root, provided all the pth roots of 1 lie in the base
field. Lagrange resolvents and this theorem about cyclic Galois groups combine to yield a derivation
of Cardan’s formula for solving general cubic equations.
    Section 14 begins the part of the chapter that depends on results in the later sections of Chap-
ter VIII. Section 14 itself contains a proof that π is transcendental; the proof is a nice illustration of
the interplay of algebra and elementary real analysis.
    Section 15 introduces the field polynomial of an element in a finite-dimensional extension field.
The determinant and trace of this polynomial are called the norm and trace of the element. The
section gives various formulas for the norm and trace, including formulas involving Galois groups.
With these formulas in hand, the section concludes by completing the proof of Theorem 8.54 about
extending Dedekind domains, part of the proof having been deferred from Section VIII.11.
    Section 16 discusses how prime ideals split when one passes, for example, from the integers to
the algebraic integers in a number field. The topic here was broached in the motivating examples
for algebraic number theory and algebraic geometry as introduced in Section VIII.7, and it was the
main topic of concern in that section. The present results put matters into a wider context.
    Section 17 gives two tools that sometimes help in identifying Galois groups, particularly of
splitting fields of monic polynomials with integer coefficients. One tool uses the discriminant of the
polynomial. The other uses reduction of the coefficients modulo various primes.




                                     1. Algebraic Elements

If K and k are fields such that k is a subfield of K, we say that K is a field
extension of k. When it is necessary to refer to this situation in some piece of
notation, we often write K/k to indicate the field extension. In this section we
shall study field extensions in a general way, and in the next section we shall
discuss constructions and uniqueness results involving them.
   If K and K0 are two fields and if ϕ is a ring homomorphism of K into K0 with
ϕ(1) = 1, then ϕ is automatically one-one since K has no nontrivial ideals. We
refer to ϕ as a field map or field mapping.1 If K and K0 are both field extensions
of a field k and if the restriction of a field map ϕ to k is the identity, then ϕ is
called a k field map or a field map fixing k. The terminology “k field map” is
consistent with the view that K and K0 are two R algebras for R = k in the sense
of Examples 6 and 15 in Section VIII.1, and that the isomorphism in question is
just an R algebra isomorphism.
   If a field map ϕ : K → K0 is onto K0 , then ϕ is a field isomorphism; it is a
k field isomorphism if K and K0 are extensions of k and ϕ is the identity on k.
When K = K0 and ϕ is onto K0 , ϕ is called an automorphism of K; if also ϕ is
the identity on a subfield k, then ϕ is called a k automorphism of K.

   1 This   is the notion of morphism in the category of fields.
454                              IX. Fields and Galois Theory

  Throughout this section we let K/k be a field extension. If x1 , . . . , xn are
members of K, we let

           k[x1 , . . . , xn ] = subring of K generated by 1 and x1 , . . . , xn ,
           k(x1 , . . . , xn ) = subfield of K generated by 1 and x1 , . . . , xn .

The latter, in more detail, means the set of all quotients ab−1 with a and b in
k[x1 , . . . , xn ] and with b 6= 0. It is referred to as the field obtained by adjoining
x1 , . . . , xn to k. Because of this description of the elements of k(x1 , . . . , xn ), the
field k(x1 , . . . , xn ) can be regarded as the field of fractions F of k[x1 , . . . , xn ]. In
fact, we argue as follows: let η : k[x1 , . . . , xn ] → F be the natural ring homo-
morphism a 7→ class of (a, 1) of k[x1 , . . . , xn ] into its field of fractions; then the
universal mapping property of F stated in Proposition 8.6 gives a factorization of
the inclusion ∂ : k[x1 , . . . , xn ] → k(x1 , . . . , xn ) as ∂ =e
                                                                  ∂η, and the field mapping
∂ has to be onto k(x1 , . . . , xn ) since the class of (a, b) maps to the member ab−1
e
of k(x1 , . . . , xn ).
    As in Chapter IV and elsewhere, we let k[X] be the ring of polynomials in
the indeterminate X with coefficients in k. For each x in K, we have a unique
substitution homomorphism ϕx : k[X] → k[x] carrying k to itself and carrying
X to x. We say that x is algebraic over k if ϕx is not one-one, i.e., if x is a root
of some nonzero polynomial in k[X], and that x is transcendental over k if ϕx
is one-one.

    EXAMPLES.
                                                                                  p
    (1) If k = R, if K = C, and if x is the usual element i =                      −1, then
ϕi (X 2 + 1) = 0, and i is algebraic over R.
    (2) If k = Q, if K = C, and if θ is a complex number with the property that
θ n + cn−1 θ n−1 + · · · + c1 θ + c0 = 0 for some n and for some coefficients in Q,
then θ is algebraic over Q. This situation was the subject of Proposition 4.1, of
Example 2 in Section IV.4, and of Example 10 in Section VIII.1.
   (3) Let k = Q and K = C. For π equal to the usual trigonometric
                                                              P          constant,
given as the least positive real such that eiπ = −1 when e z = ∞
                                                               n=0  z n
                                                                        /n!, it will
be proved in Section 14 that there is no polynomial F(X) in Q[X] with F(π) = 0,
and π is consequently transcendental over Q.
    (4) If k = Z/2Z and K is the 4-element field constructed in Example 3 of
fields in Section IV.4, then any element of K is algebraic over k.
                                       p
    (5) If k = C(X) and if K = C(X)[ (X − 1)X (X + 1) ] as with the ring R 0
in
p Section VIII.7 and as in Example 3 of integral closures in Section VIII.9, then
   (X − 1)X (X + 1) is algebraic over C(X).
                                1. Algebraic Elements                          455

  Suppose that x in K is algebraic over k. Then

                      ker ϕx = {F(X) ∈ k[X] | F(x) = 0}

is an ideal in k[X] that is necessarily nonzero and principal. A generator is
determined up to a constant factor as any nonzero polynomial in the ideal that has
lowest possible degree, and we might as well take this polynomial to be monic.
Thus ker ϕx is of the form (F0 (X)) for some unique monic polynomial F0 (X), and
this polynomial F0 (X) is called the minimal polynomial of x over k. Review of
the example at the end of Section VIII.3 may help motivate the first five results
below.

   Proposition 9.1 If x ∈ K is algebraic over k, then the minimal polynomial of
x over k is prime as a polynomial in K[X].
   PROOF. Suppose that F(X) factors nontrivially as F(X) = G(X)H (X). Since
F(x) = 0, either G(x) = 0 or H (x) = 0, and then we have a contradiction to
the fact that F has minimal degree among all polynomials vanishing at x. §

   Theorem 9.2. If x ∈ K is algebraic over k, then the field k(x) coincides with
the ring k[x]. Moreover, if the minimal polynomial of x over k has degree n,
then each element of k(x) has a unique expansion as

          cn−1 x n−1 + cn−2 x n−2 + · · · + c1 x + c0   with all ci ∈ k.

   PROOF. Since the substitution ring homomorphism ϕx carries k[X] onto k[x],
we have an isomorphism of rings k[x] ∼   = k[X]/ ker ϕx = k[X]/(F0 (X)), where
F0 (X) is the minimal polynomial of x over k. Since F0 is prime, (F0 (X)) is a
nonzero prime ideal and hence is maximal. Thus k[x] is a field. Consequently
k(x) = k[x].
   Any element in k[x], hence in k(x), is a polynomial in x. Since F0 (x) = 0,
we can solve F0 (x) = 0 for its leading term, say x n , obtaining x n = G(x), where
G(X) = 0 or deg G(X) ≤ n − 1. Thus the expansions in the statement of the
theorem yield all the members of k[x]. If an element has two such expansions,
we subtract them and obtain a nonzero polynomial H (X) of degree at most n − 1
with H (x) = 0, in contradiction to the minimality of the degree of F0 (X).      §

   Corollary 9.3. If x ∈ K is algebraic over k, then the field k(x), regarded as
a vector space over k, is of dimension n, where n is the degree of the minimal
polynomial of x over k. The elements 1, x, x 2 , . . . , x n−1 form a basis of k(x)
over k.
  PROOF. This is just a restatement of the second conclusion of Theorem 9.2. §
456                           IX. Fields and Galois Theory

   We say that the field extension K/k is an algebraic extension if every element
of K is algebraic over k.

   Proposition 9.4. If the vector-space dimension of K over k is some finite n,
then K is an algebraic extension of k, and each element x of K has some nonzero
polynomial F(X) in k[X] of degree at most n for which F(x) = 0.
   PROOF. This is immediate since the elements 1, x, x 2 , . . . , x n of K have to be
linearly dependent over k.                                                          §

   When K/k is a field extension, we write [K : k] for the vector-space dimension
dimk K, and we call this the degree of K over k. If [K : k] is finite, we say that
K is a finite extension of k, or finite algebraic extension of k, the condition
“algebraic” being automatic by Proposition 9.4.

   Corollary 9.5. If x is in K, then x is algebraic over k if and only if k(x) is a
finite algebraic extension of k. In this case the minimal polynomial of x over k
has degree [k(x) : k].
   PROOF. If x is algebraic over k, then [k(x) : k] is finite and is the degree of the
minimal polynomial of x over k, by Corollary 9.3. Proposition 9.4 shows in this
case that k(x) is a finite algebraic extension. If x is transcendental over k, then the
substitution homomorphism ϕx is one-one, and dimk k(x) ∏ dimk k[X] = +∞.
                                                                                     §

    Theorem 9.6. Let k, K, and L be fields with k ⊆ K ⊆ L, and suppose that
[K : k] = n and [L : K] = m, finite or infinite. Let {ω1 , ω2 , . . . } be a vector-
space basis of K over k, and let {ξ1 , ξ2 , . . . } be a vector-space basis of L/K. Then
the mn products ωi ξ j form a basis of L over k.
                                                          P
    PROOF OF SPANNING. If ξ is in L, write ξ = j a j ξ j with each a j in K and
with only finitely many a j ’s not 0. Then expand each a j in terms of the ωi ’s, and
substitute.                                                                           §
                                                    P
    PROOF OF LINEAR INDEPENDENCE. Let i, j ci j ωi ξ j = 0 with the ci j ’s in k.
                                                                       P
Since the members ξ j of L are linearly independent over K, i ci j ωi = 0 for
each j. Since the members ωi of K are linearly independent over k, ci j = 0 for
all i and j.                                                                          §

   Corollary 9.7. If k, K, and L are fields with k ⊆ K ⊆ L, then

                             [L : k] = [L : K] [K : k] .

   PROOF. This is immediate by counting basis elements in Theorem 9.6.               §
                             2. Construction of Field Extensions                          457

   Theorem 9.8. If K/k is a field extension and if x1 , . . . , xn are members of K
that are algebraic over k, then k(x1 , . . . , xn ) is a finite algebraic extension of k.
   REMARK. If a finite algebraic extension of k turns out to be of the form k(x)
for some x, we say that the extension is a simple algebraic extension.
   PROOF. Since xi is algebraic over k, it is algebraic over k(x1 , . . . , xi−1 ). Hence
[k(x1 , . . . , xi ) : k(x1 , . . . , xi−1 )] is finite. Applying Corollary 9.7 repeatedly, we
see that k(x1 , . . . , xn ) is a finite extension of k. Proposition 9.4 shows that it is a
finite algebraic extension.                                                                  §
                       p p
   EXAMPLE. The sum 2+ 3 2 is algebraic over Q, as a consequence of Theorem
9.8. This fact suggests Corollary 9.9 below.

   Corollary 9.9 If K/k is a field extension, then the elements of K that are
algebraic over k form a field.
   PROOF. If x and y in K are algebraic over k, then k(x, y) is a finite algebraic
extension of k, according to Theorem 9.8. This extension contains x ± y and x y,
and it contains x −1 if x 6= 0. The corollary therefore follows from Proposition
9.4.                                                                            §

   For the special case of Corollary 9.9 in which K = C and k = Q, this subfield
of C is called the field of algebraic numbers, and any finite algebraic extension of
Q within C is called a number field, or an algebraic number field. The seeming
discrepancy between this definition and the definition given in remarks with
Proposition 4.1 (that in essence a “number field” is any simple algebraic extension
of Q) will be resolved by the Theorem of the Primitive Element (Theorem 9.34
below).


                        2. Construction of Field Extensions

In this section, k denotes any field. Our interest will be in constructing extension
fields for k and in addressing the question of uniqueness under additional hy-
potheses. We begin with a kind of converse to Proposition 9.1 that generalizesp the
method described in Section A4 of the appendix for constructing C = R( −1 )
from R and the polynomial X 2 + 1 .

   Theorem 9.10 (existence theorem for simple algebraic extensions). If F(X) is
a monic prime polynomial in k[X], then there exists a simple algebraic extension
K = k(x) of k such that x is a root of F(X). Moreover, F(X) is the minimal
polynomial of x over k.
458                         IX. Fields and Galois Theory

   PROOF. Define K = k[X]/(F(X)) as a ring. Since F(X) is prime, (F(X)) is
a nonzero prime ideal, hence maximal. Therefore K is a field, an extension field
of k. Define x to be the coset X + (F(X)). Then F(x) = F(X) + (F(X)) =
0 + (F(X)), and x is therefore algebraic over k. It is immediate that K = k[x],
and Theorem 9.2 shows that K = k(x). If G(x) = 0 for some G(X) in k[X],
then G(X) is in (F(X)). We conclude that F(X) has minimal degree among all
polynomials with x as a root, and F(X) is therefore the minimal polynomial. §

    Theorem 9.11 (uniqueness theorem for simple algebraic extensions). If F(X)
is a monic prime polynomial in k[X] and if K = k(x) and K0 = k(y) are two
simple algebraic extensions such that x and y are roots of F(X), then there exists
a field isomorphism ϕ of K onto K0 fixing k and carrying x to y.
   EXAMPLE
     p       . The monicppolynomial F(X) = X 3 − 2 is prime in Q[X], and
x = 2 and y = e2πi/3 3 2 are roots of it within C. The fields Q(x) and Q(y)
      3


are subfields of C and are distinct because Q(x) is contained in R and Q(y) is
not. Nevertheless, these fields are Q isomorphic, according to the theorem.
    PROOF. In view of the proof of Theorem 9.10, there is no loss of generality
in assuming that K = k[X]/(F(X)). Since y is algebraic over k, we can
form the substitution homomorphism ϕ y : k[X] → k(y). This is a k alge-
bra homomorphism. Its kernel is the ideal (F(X)) since F(X) is the minimal
polynomial of y, and ϕ y therefore descends to a one-one k algebra homomorphism
ϕ y : k(x) → k(y). Since dim k(x) and dim k(y) both match the degree of F(X),
ϕ y is onto k(y) and is therefore the required k isomorphism.                 §

   We say that a nonconstant polynomial F(X) in k[X] splits in a given extension
field if F(X) factors completely into degree-one factors over that extension field.
A splitting field over k for a nonconstant polynomial F(X) in k[X] is an extension
field L of k such that F(X) splits in L and such that L is generated by k and the
roots of F(X) in L.
                                      p
   E XAMPLES. Let k = Q. Then Q( −1 ) is a splitting field for X 2 + 1, because
  p                      p                          p                       p
± −1 are both in Q( −1 ) and they generate  p     Q( −1 ) over Q. But Q( 3 2) is
not a splitting field for X 3 − 2 because Q( 3 2) does not contain the two nonreal
roots of X 3 − 2.

  Theorem 9.12 (existence of splitting field). If F(X) is a nonconstant polyno-
mial in k[X], then there exists a splitting field of F(X) over k.
   PROOF. We begin by constructing a certain extension field K of k in which
F(X) factors completely into degree-one factors in K[X]. We do so by induction
on n = deg F(X). For n = 1, there is nothing to prove. For general n, let G(X)
                           2. Construction of Field Extensions                    459

be a prime factor of F(X), and apply Theorem 9.10 to obtain a simple algebraic
extension k1 = k(x1 ) over k such that G(x1 ) = 0. Then F(x1 ) = 0, and the
Factor Theorem (Corollary 1.13) gives F(X) = (X − x1 )H (X) for some H (X)
in k1 (X) of degree n − 1. Since deg H (X) = n − 1 < deg F(X), the inductive
hypothesis produces an extension K of k1 such that H (X) is a constant multiple
of (X − x2 ) · · · (X − xn ) with all xi in K. Then F(X) factors into degree-one
factors in K[X], and the induction is complete.
   Within the constructed field K, let L be the subfield L = k(x1 , . . . , xn ). Then
F(X) still factors completely into degree-one factors in L(X), and L is generated
by k and the xi . Hence L is a splitting field.                                     §

   EXAMPLES OF SPLITTING FIELDS.
                                                                                 p
   (1) k = Q and F(X) = Xp    3
                                −2.
                                  °  The proof
                                           p    of Theorem
                                                      p    ¢ 9.12 takes k1 = Q( 3 2)
and writes X 3 − 2 = (X − 3 2) X 2 + 3 2 X + ( 3 2)2 . Then the proof adjoins
                                           p          p                       p
one root θ (hence both roots) of X 2 + 3 2 X + ( 3 2)2 , setting K = Q( 3 2, θ).
With this choice of K, the splitting field turns out to be L = K. In fact, to see that
L is not a proper subfield of K, we observe that 6 = [K p  : k] = [K : L] [L : Q] by
Corollary 9.7 and that the proper containment L % Q( 3 2) implies [L : Q] > 3.
Since [L : Q] is a divisor of 6 greater than 3, [L : Q] = 6. Thus [K : L] = 1,
and K = L.
    (2) k = Q and F(X) = X 3 − X − 13 . Application of Corollary 8.20c to
the polynomial G(X) = −3X 2 F(1/ X) = X 3 + 3X 2 − 3 shows that G(X)
has no degree-one factor and hence is irreducible over Q. Then it follows that
F(X) is irreducible over Q. The proof of Theorem 9.12 takes k1 = Q(r), where
r 3 − r − 13 = 0. Then division gives

                 X3 − X −    1
                             3   = (X − r)(X 2 + r X + (r 2 − 1)).

The discriminant b2 − 4ac of the quadratic factor is

                                                          r2
                     r 2 − 4(r 2 − 1) = 4 − 3r 2 =               ,
                                                       (1 + 2r)2

the right-hand equality following from direct computation. This discriminant is
a square in k1 = Q(r), and hence X 2 + r X + (r 2 − 1) factors into degree-one
factors in Q(r) without passing to an extension field. Therefore L = Q(r) with
[L : Q] = 3.

  Theorem 9.13 (uniqueness of splitting field). If F(X) is a nonconstant poly-
nomial in k[X], then any two splitting fields of F(X) over k are k isomorphic.
460                           IX. Fields and Galois Theory

   The idea of the proof is simple enough, but carrying out the idea runs into a
technical complication. The idea is to proceed by induction, using the uniqueness
result for simple algebraic extensions (Theorem 9.11) repeatedly until all the roots
have been addressed. The difficulty is that after one step the coefficients of the
two quotient polynomials end up in two distinct but k isomorphic fields. Thus
at the second step Theorem 9.11 does not apply directly. What is needed is the
reformulated version given below as Theorem 9.110 , which lends itself to this kind
of induction. In addition, as soon as the induction involves at least three steps, the
above statement of Theorem 9.13 does not lend itself to a direct inductive proof.
For this reason we shall instead prove a reformulated version Theorem 9.130 of
Theorem 9.13 that is ostensibly more general than Theorem 9.13.
   Recall from Proposition 4.24 that a general substitution homomorphism that
starts from a polynomial ring can have two ingredients. One is the substitution
of some element, such as x, for the indeterminate X, and the other is a homo-
morphism that is made to act on the coefficients. If the homomorphism is σ ,
let us write F σ (X) to indicate the polynomial obtained by applying σ to each
coefficient of F(X).

   Theorem 9.110 . Let k and k0 be fields, and let σ : k → k0 be a field
isomorphism. Suppose that F(X) is a monic prime polynomial in k[X] and that
K = k(x) and K0 = k0 (x 0 ) are simple algebraic extensions such that F(x) = 0
and FØσ (x 0 ) = 0. Then there exists a field isomorphism ϕ : k(x) → k0 (x 0 ) such
that ϕ Øk = σ and ϕ(x) = x 0 .
    PROOF. The argument is essentially unchanged from the proof of Theorem
9.11. We start from the substitution homomorphism G(X) 7→ G σ (x 0 ) that
replaces X by x 0 and that operates by σ on the coefficients. This descends to
a field map of k[x] into k0 [x 0 ], and the homomorphism must be onto k0 [x 0 ] by a
count of dimensions.                                                              §

   Theorem 9.130 . Let k and k0 be fields, and let σ : k → k0 be a field
isomorphism. If F(X) is a nonconstant polynomial in k[X] and if L and L0
                                                      Ø F (X) over k , then there
are respective splitting fields for F(X) over k and for  σ          0

exists a field isomorphism ϕ : L → L such that ϕ k = σ and such that ϕ sends
                                        0             Ø
the set of roots of F(X) to the set of roots of F σ (X).
   PROOF. We proceed by induction on n = deg F(X), the case n = 1 being
evident. Assume the result for degree n − 1. Let G(X) be a prime factor of F(X)
over k. Then G σ (X) is a prime factor of F σ (X) over k0 . The polynomials G(X)
and G σ (X) have roots in L and L0 , respectively. Fix one such root for each, say x1
and x10 . By Theorem 9.110 , there exists a field isomorphism σ1 : k(x1 ) → k0 (x10 )
extending σ and satisfying σ1 (x1 ) = x10 . Write F(X) = (X − x1 )H (X) with
coefficients in k(x1 ), by the Factor Theorem (Corollary 1.13). Applying σ1 to
                                         3. Finite Fields                                  461

the coefficients, we obtain F σ (X) = (X − x10 )H σ1 (X) with coefficients in k0 (x10 ).
Then L and L0 are splitting fields for H (X) and H σ1 (X) over k(x1 ) and k0 (x10 ),
respectively. By induction we can extend σ1 to an isomorphism ϕ : L → L0 , and
the theorem readily follows.                                                         §


                                       3. Finite Fields

In this section we shall use the results on splitting fields in Section 2 to classify
finite fields up to isomorphism. So far, the examples of finite fields that we have
encountered are the prime fields F p = Z/ pZ with p elements, p being any prime
number, and the field of 4 elements in Example 3 of fields in Section IV.4. Every
finite field has to contain a subfield isomorphic to one of the prime fields F p , and
Proposition 4.33 observed as a consequence that any finite field necessarily has
pn elements for some prime number p and some integer n > 0.

   Theorem 9.14. For each pn with p a prime number and with n a positive
integer, there exists up to isomorphism one and only one field with pn elements.
                                         n
Such a field is a splitting field for X p − X over the prime field F p .

   If q = pn , it is customary to denote by Fq a field of order q. The theorem
says that Fq exists and is unique up to isomorphism. Some authors refer to finite
fields as Galois fields.
   Some preparation is needed before we can come to the proof of the theorem.
We need to carry over the simplest aspects of differential calculus to polynomials
with coefficients in an arbitrary field k. First we give an informal definition of
the derivative of a polynomial;
                       P          then we give a more precise definition. For any
polynomial F(X) = nj=0 c j X j in k[X], we informally define the derivative to
be the polynomial

                                 n
                                 P                    n−1
                                                      P
                     F 0 (X) =         jc j X j−1 =         ( j + 1)c j+1 X j .
                                 j=1                  j=0


The more precise definition uses the definition of members of k[X] as infinite
sequences of members of k whose terms are 0 from some point on. In this notation
if F = (c0 , c1 , . . . , cn , 0, . . . ) with c j in the j th position for j ≤ n and with 0 in
the j th position for j > n, then F 0 = (c1 , 2c2 , . . . , ncn , 0, . . . ) with ( j + 1)c j+1
in the j th position for j ≤ n − 1 and with 0 in the j th position for j > n − 1. In
any event, the mapping F 7→ F 0 is k linear from k[X] to itself. The operation is
called differentiation.
462                            IX. Fields and Galois Theory

  Proposition 9.15. Differentiation on k[X] satisfies the product rule: F = G H
implies F 0 = G 0 H + G H 0 .
    PROOF.    Because of the k linearity, it is enough to prove the result for monomi-
als. Thus     let G(X) = X m and H (X) = X n , so that F(X) = X m+n . Then
F 0 (X) =     (m + n)X m+n−1 , G 0 (X)H (X) = m X m+n−1 , and G(X)H 0 (X) =
n X m+n−1 .   Hence we indeed have F 0 (X) = G 0 (X)H (X) + G(X)H 0 (X).            §

   Corollary 9.16. If n is a positive integer, if r is in k, and if F(X) = (X − r)n
in k[X], then F 0 (X) = n(X − r)n−1 .
   PROOF. This is immediate by induction from Proposition 9.15 since the deriv-
ative of X − r is 1.                                                         §

   Corollary 9.17. Let r be in k, and let F(X) be in k[X]. If (X − r)2 divides
F(X), then F(r) = F 0 (r) = 0. Conversely if F(r) = F 0 (r) = 0, then (X − r)2
divides F(X).
    PROOF. Write F(X) = (X − r)2 G(X). If we substitute r for X, we see that
F(r) = 0. If instead we differentiate, using Proposition 9.15 and Corollary 9.16,
then we obtain F 0 (X) = 2(X − r)G(X) + (X − r)2 G 0 (X). Substituting r for X,
we obtain F 0 (r) = 0 + 0 = 0.
    For the converse, let F(r) = F 0 (r) = 0. Proposition 4.28a shows that F(X) =
(X − r)G(X). Differentiating this identity by means of Proposition 9.15 gives
F 0 (X) = G(X)+(X −r)G 0 (X). Substitutingr for X yields 0 = F 0 (r) = G(r)+0
and shows that G(r) = 0. By Proposition 4.28, G(X) = (X − r)H (X). Hence
F(X) = (X − r)2 H (X).                                                          §

   Lemma 9.18. If k is a field of characteristic p 6= 0, then the map ϕ : k → k
given by ϕ(x) = x p is a field mapping.
   REMARK. The map x 7→ x p is often called the Frobenius map. If k is a finite
field, then it must carry k onto k since one-one implies onto for functions from a
finite set to itself; in this case the map is an automorphism of k.
   PROOF. The computation ϕ(uv) = (uv) p = u p v p = ϕ(u)ϕ(v) shows that ϕ
respects products. If u and v are in k, then
                                         p−1
                                         P°    p¢ p− j j
      ϕ(u + v) = (u + v) p = ϕ(u) +            j u    v    + ϕ(v) = ϕ(u) + ϕ(v),
                                         j=1

                                                        ° ¢
the last equality holding since the binomial coefficient pj has a p in the numerator
for 1 ≤ j ≤ p − 1. Thus ϕ is a ring homomorphism. Since ϕ(1) = 1, ϕ is a field
mapping.                                                                          §
                                     3. Finite Fields                                 463

    PROOF OF UNIQUENESS IN THEOREM 9.14. Let k be a finite field, say of
characteristic p, and let P be the prime field of order p within k. We know that P
is isomorphic to F p = Z/ pZ. Since k is a finite-dimensional vector space over P,
we know also that k has order q = pn for some integer n > 0. The multiplicative
group k× of k thus has order q − 1, and every x 6= 0 in k therefore satisfies
x q−1 = 1. Taking x = 0 into account, we see that every member of k satisfies
x q = x. Forming the polynomial X q − X in P[X], we see that every member of
k is a root of this polynomial. Iterated application q times of the Factor Theorem
(Corollary 1.13) shows that X q − X factors into degree-one factors in k. Since
every member of k is a root of X q − X, k is a splitting field of X q − X over P.
Then the uniqueness of the prime field up to isomorphism, in combination with
the uniqueness of the splitting field of X q − X given in Theorem 9.130 , shows
that k is uniquely determined up to isomorphism.                                 §
   PROOF OF EXISTENCE IN THEOREM 9.14. Let q = pn be given, and define k to
be a splitting field of X q − X over F p = Z/ pZ. The field k exists by Theorem
9.12, and it has characteristic p. Since X q − X is monic of degree q, the definition
of splitting field says that we can write

         X q − X = (X − u 1 )(X − u 2 ) · · · (X − u q )       with all u j ∈ k.

Because of Lemma 9.18, the map ϕ(u) = u q , which is the n th power of the
map u 7→ u p , is a field mapping of k into itself. The members of k fixed by
ϕ form a subfield of k, and these elements of k are exactly the members of the
set S = {u 1 , . . . , u q }. Therefore S is a subfield of k, necessarily containing
F p = Z/ pZ. Since X q − X splits in S and since the roots of X q − X generate
S, S is a splitting field of X q − X over F p . In other words, S = k. To complete
the proof, it is enough to show that the elements u 1 , . . . , u q are distinct, and then
k will be a field of q elements. The question is therefore whether some root of
X q − X has multiplicity at least 2, i.e., whether (X −r)2 divides X q − X for some
r in k. Corollary 9.17 gives a necessary condition for this divisibility, saying that
the derivative of X q − X must have r as a root. However, the derivative of X q − X
is q X q−1 − 1 = −1, and the constant polynomial −1 has no roots. We conclude
that k has q elements.                                                                  §

   Corollary 9.19. If q and r are integers with 2 ≤ q ≤ r, then the finite field
Fq is isomorphic to a subfield of the finite field Fr if and only if r = q n for some
integer n ∏ 1.
   PROOF. If Fq is isomorphic to a subfield of Fr , then we may consider Fr as a
vector space over Fq , say of dimension n. In this case, Fr has q n elements.
                                                                      n
   Conversely let r = q n , and regard Fr as a splitting field of X q − X over the
prime field F p , by Theorem 9.14. Let S be the subset of Fr of all roots of X q − X.
464                             IX. Fields and Galois Theory

                                q n −1
Putting a = q − 1 and k =       q−1      = q n−1 + q n−2 + · · · + 1, we have

                  X ka − 1 = (X a − 1)(X (k−1)a + X (k−2)a + · · · + 1).
                                                                 n              n
Multiplying by X, we see that X q − X is a factor of X q − X. Since X q − X
splits in Fr and has distinct roots, the same is true of X q − X. Therefore |S| = q.
   Let q = pm . The m th power of the homomorphism of Lemma 9.18 on k = Fr
is x 7→ x q , and the subset of Fr fixed by this homomorphism is a subfield. Thus
S is a subfield, and it has q elements.                                           §



                                 4. Algebraic Closure

Algebraically closed fields—those for which every nonconstant polynomial with
coefficients in the field has a root in the field—were introduced in Section V.1, and
it was mentioned at that time that every field is a subfield of some algebraically
closed field. We shall prove that existence theorem in this section in a form
lending itself to a uniqueness result.
   Throughout this section let k be a field. We begin by giving further descriptions
of algebraically closed fields that take the theory of Sections 1–2 into account.

   Proposition 9.20. The following conditions on the field k are equivalent:
      (a)   k has no nontrivial algebraic extensions,
      (b)   every irreducible polynomial in k[X] has degree 1,
      (c)   every polynomial in k[X] of positive degree has at least one root in k,
      (d)   every polynomial in k[X] of positive degree factors over k into polyno-
            mials of degree 1.

    PROOF. If (a) holds, then (b) holds since any irreducible polynomial of degree
greater than 1 would give a nontrivial simple algebraic extension (Theorem 9.10).
If (b) holds and a polynomial of positive degree is given, apply (b) to an irreducible
factor to see that the given polynomial has a root; thus (c) holds. Condition (c)
implies condition (d) by induction and the Factor Theorem. If (d) holds and if
K is an algebraic extension of k, let x be in K, and let F(X) be the minimal
polynomial of x over k. Then F(X) is irreducible over k, and (d) says that F(X)
has degree 1. Hence x is in k, and we conclude that K = k.                          §

   A field satisfying the equivalent conditions of Proposition 9.20 is said to be
algebraically closed.
                                 4. Algebraic Closure                            465

   EXAMPLES OF ALGEBRAICALLY CLOSED FIELDS.
   (1) The Fundamental Theorem of Algebra (Theorem 1.18) says that C is
algebraically closed. This theorem was not proved in Chapter I, but a proof
will be given in this chapter in Section 10.
   (2) Let K be the subset of all members of C that are algebraic over Q. By
Corollary 9.9, K is a subfield of C. Example 1 shows that every polynomial in
Q[X] splits in K, and Lemma 9.21 below then allows us to conclude that K is
algebraically closed.
    (3) Fix a prime number p, and start with k0 = F p as the prime field Z/ pZ.
Enumerate the members of F p [X], letting Fn (X) be the n th such polynomial. We
construct kn by induction on n so that kn is a splitting field for Fn (X) over kn−1
when n ∏ 1. Then k0 ⊆ k1 ⊆ k2 ⊆ · · · is an increasing sequence of fields
containing F p . Let K be the union. Any two elements of K lie in a single kn , and
it follows that K is closed under the field operations. Any three elements lie in a
single kn , and it follows that any of the defining properties of a field is valid in
K because it is valid in kn . Therefore K is a field. This field is an extension of
F p , and every polynomial in F p [X] splits in K. As in Example 2, Lemma 9.21
below shows that K is algebraically closed.

   Lemma 9.21. If K/k is an algebraic extension of fields and if every non-
constant polynomial in k[X] splits into degree-one factors in K, then K is
algebraically closed.
   PROOF. Let K0 be an algebraic extension of K, and let x be in K0 . Let G(X)
be the minimal polynomial of x over K, and write G(X) as

            G(X) = X n + cn−1 X n−1 + · · · + c0          with all ci ∈ K.

Then x is algebraic over k(cn−1 , . . . , c0 ), which is a finite extension of k by
Theorem 9.8. By Corollary 9.7, x lies in a finite extension of k. Thus Proposition
9.4 shows that x is algebraic over k. Let F(X) be the minimal polynomial of x
over k. By assumption this splits over K, say as

              F(X) = (X − x1 ) · · · (X − xm )          with all xi ∈ K.

Evaluating at x and using the fact that F(x) = 0, we see that x = x j for some j.
Therefore x is in K, and K is algebraically closed.                            §

   An extension field K/k is an algebraic closure of k if K is algebraic over k
and if K is algebraically closed. Example 2 of algebraically closed fields above
gives an algebraic closure of Q, and Example 3 gives an algebraic closure of F p .
466                           IX. Fields and Galois Theory

  Theorem 9.22 (Steinitz). Every field k has an algebraic closure, and this is
unique up to k isomorphism.
   REMARKS. The proof of existence is modeled on the argument for Example 3
of algebraic closures. However, we are not free in general to use a simple union
of a sequence of fields and have to work harder. Because there is no evident set
of possibilities within which we are forming extension fields, Zorn’s Lemma is
inconvenient to use and tends to result in an unintuitive construction. Instead,
we use Zermelo’s Well-Ordering Theorem, whose use more closely parallels the
inductive construction in Example 3.
    PROOF OF EXISTENCE. With k as the given field, let S be the set of nonconstant
polynomials s(X) in k[X], and introduce a well ordering into S by means of
Zermelo’s Well-Ordering Theorem (Section A5 of the appendix). Let us write ≺
for “strictly precedes in the ordering” and - for “equals or strictly precedes.” For
each s ∈ S, let s̄ be the successor of s, i.e., the first element among all elements t
with s ≺ t. We write s0 for the first element of S. Without loss of generality, we
may assume that S has a last element s∞ . The idea is to construct simultaneously
two kinds of things:
      (i) an algebraic extension field ks /k for each s ∈ S such that ks0 = k and
          such that ks̄ is a splitting field for s(X) over ks whenever s ≺ s∞ ,
     (ii) a field mapping ϕut : kt → ku for each ordered pair of elements t and u
          in S having t - u, such that ϕtt = 1 for all t and such that t - u - v
          implies ϕvt = ϕvu ϕut .
                                                                         S
These extension fields and mappings are to be such that ks = t≺s ϕst (kt )
whenever s is not a successor and is not s0 . If such a system of extension fields
and field homomorphisms exists, then Lemma 9.21 applies to a splitting field
over ks∞ of the nonconstant polynomial s∞ (X) and shows that this splitting field
is algebraically closed; since this splitting field is an algebraic extension of k, it
is an algebraic closure of k.
    A partial such system through t0 means a system consisting of fields ks with
s - t0 and field homomorphisms ϕut with t - u - t0 such that the above
conditions hold as far as they are applicable. A partial system exists through
the first member s0 of S because we can take ks0 = k and ϕs0 s0 = 1. Arguing
by contradiction, we suppose that such a system of extension fields and field
homomorphisms fails to exist through some member of S. Let t0 be the first
member of S such that there is no partial system through t0 .
    Suppose that t0 is the successor of some element t1 in S. We know that a partial
system exists through t1 . If we let kt0 be a splitting field for t1 (X) over kt1 , and
if we define                        Ω
                                      ϕt0 t1 ϕt1 t  for t - t1 ,
                            ϕt0 t =
                                      1             for t = t0 ,
                                    4. Algebraic Closure                                467

then the enlarged system is a partial system through t0 , contradiction. Thus t0
cannot be the successor of some element of S.
    When t0 is not a successor, at least kt is defined for t ≺ t0 and ϕut is defined
for t - u ≺ t0 . We want to form a union, but we have to keep the field operations
aligned properly in the process. Define a “t-allowable tuple” to be a function
u 7→ xu defined for t - u ≺ t0 such that xu is in ku and ϕvu (xu ) = xv whenever
t - u - v ≺ t0 . If x is in kt , then an example of a t-allowable tuple is given by
u 7→ ϕut (x) for t - u ≺ t0 .
    If t ≺ t0 and t 0 ≺ t0 , then we can apply field operations to the t-allowable tuple
u 7→ xu and to the t 0 -allowable tuple u 7→ yu , obtaining max(t, t 0 )-allowable
tuples u 7→ xu + yu , u 7→ −xu , u 7→ xu yu , and xu 7→ xu−1 as long as xt 6= 0.
These operations are meaningful since each ϕvu is a field mapping.
    If t ≺ t0 and t 0 ≺ t0 , we say that the t-allowable tuple u 7→ xu is equivalent to
the t 0 -allowable tuple u 7→ yu if xu = yu for max(t, t 0 ) - u ≺ t0 . The result is
an equivalence relation, and the equivalence relation respects the field operations
in the previous paragraph. We define kt0 to be the set of equivalence classes of
allowable tuples with the inherited field operations. The 0 element is the class of
the s0 -allowable tuple u 7→ 0, and the multiplicative identity is the class of the
s0 -allowable tuple u 7→ 1. It is a routine matter to check that kt0 is a field.
    If t ≺ t0 is given, we define the function ϕt0 t : kt → kt0 as follows: if x is
in kt , we form the t-allowable tuple u 7→ ϕut (x) and take its equivalence class,
which is a member of kt0 , as ϕt0 t (x). Then ϕt0 t is evidently a field mapping. It
is evident also that ϕt0 v ϕvu = ϕt0 u when u - v ≺ t0 . Defining ϕt0 t0 to be the
identity, we have a complete system of field mappings ϕvu for kt0 .
    The final step is to check that kt0 is the union of the images of the ϕt0 t for t ≺ t0 .
Thus choose a representative of an equivalence class in kt0 . Let the representative
be a t-allowable tuple u 7→ xu for t - u ≺ t0 . The element xt is in kt , and the
condition xu = ϕut (xt ) is just the condition that the class of u 7→ xu be the image
of xt under ϕt0 t . Hence every member of kt0 is in the image of some ϕt0 t with
t ≺ t0 , and we have a contradiction to the hypothesis that a partial system through
t0 does not exist. This completes the proof of existence.                                §

   For the uniqueness in Theorem 9.22, we again need a serious application of
the Axiom of Choice, but here Zorn’s Lemma can be applied fairly routinely.
The proof will show a little more than is needed, and in fact the uniqueness in
Theorem 9.22 will be derived as a consequence of Theorem 9.23 below.

   Theorem 9.23. Let K0 be an algebraically closed field, and let K be an algebraic
extension of a field k. If ϕ is a field mapping of k into K0 , then ϕ can be extended
to a field mapping of K into K0 .
468                              IX. Fields and Galois Theory

   PROOF OF UNIQUENESS IN THEOREM 9.22 USING THEOREM 9.23. Let K and
K0 be algebraic closures of k, and let ϕ : k → K0 be the inclusion  Ø mapping.
Theorem 9.23 supplies a field mapping 8 : K → K0 such that 8Øk = ϕ, i.e.,
such that 8 fixes k. Since K is an algebraic closure of k, so is 8(K). Then K0 is
an algebraic extension of the algebraically closed field 8(K), and we must have
8(K) = K0 . Thus 8 is a k isomorphism of K onto K0 .
    PROOF OF THEOREM 9.23. Let S be the set of all triples (L, L0 , √) such
that L is a field with k Ø⊆ L ⊆ K and √ is a field mapping of L onto the
subfield L0 of K0 with √ Øk = ϕ. The set S is nonempty since (k, ϕ(k), ϕ) is
a member of it. Defining (L1 , L01 , √1 ) ⊆ (L2 , L02 , √2 ) to mean that L1 ⊆ L2 ,
that L01 ⊆ L02 , and that √1 as a set of ordered pairs is a subset of √2 as a set
                                                                            ¢ α , Lα , √α )} is
                                                                                   0
of ordered pairs, we partially order S by° S     inclusionSupward.   S If {(L
                                                   α Lα ,     α L°αS
                                                                  0
a nonempty chain     ° Sin S, ¢formSthe triple                      , α √α , and put √ =¢
S                                                                           S 0 S
     √
   α α  .  Then   √        L
                         α α     =     L
                                      α α
                                         0
                                           , and consequently        α Lα ,   α Lα ,    α √α
is an upper bound in S for the chain. By Zorn’s Lemma, S has a maximal element
(L0 , L00 , √0 ). We shall prove that L0 = K, and the proof will be complete.
    Fix x in K, and let F(X) be the minimal polynomial of x over L0 . The
minimal polynomial of √0 (x) over L00 is then F √0 (X). Since K0 is algebraically
closed, F √0 (X) has a root x 0 in K0 . By Theorem 9.110 , √0 : L0 → L0 can be
extended to an isomorphism 90 : L0 (x) → L00 (x 0 ) such that √0 (x) = x 0 . Then
(L0 (x), L00 (x 0 ), 90 ) is in S and contains (L0 , L00 , √0 ). This containment, if strict,
would contradict the fact that (L0 , L00 , √0 ) is a maximal element of S. Thus
equality must hold: L0 (x) = L0 . Therefore x is in L0 , and we conclude that
L0 = K.                                                                                      §

   The use of algebraic closures allows us to simplify understanding of splitting
fields. If we are working with a field k and is k is a fixed algebraic closure of k,
then the existence and uniqueness of the splitting field of a polynomial F(X) in
k[X] becomes evident; no isomorphisms are involved. Namely let α1 , . . . , αn be
the roots of F(X) in k. Then the subfield of k generated by k and α1 , . . . , αn is
the splitting field of F(X), and it is manifestly unique. Henceforth when we refer
to the splitting field of a polynomial over a field k, it is with an understanding of
working within a fixed algebraic closure in this way.


         5. Geometric Constructions by Straightedge and Compass

Classical Euclidean geometry attached a certain emphasis to constructions in the
Euclidean plane that could be made by straightedge and compass. These are
often referred to casually as constructions by “ruler and compass,” but one is not
                 5. Geometric Constructions by Straightedge and Compass          469

allowed to use the markings on a ruler. Thus “straightedge and compass” is a
more accurate description.
    In these constructions the starting configuration may be regarded as a line with
two points marked on the line. Allowable constructions are the following: to form
the line through a given point different from finitely many other lines through that
point, to form the line through two distinct points, to form a circle with a given
center and a radius different from that of finitely many other circles through the
point, and to form a circle with a given center and radius. Intersections of a line
or a circle with previous lines and circles establish new points for continuing the
construction.
    For example a line perpendicular to a given line at a given point can be
constructed by drawing any circle centered at the point, using the two intersection
points as centers of new circles, drawing those circles so as to have radius larger
than the first circle, and forming the line between their two points of intersection.
An angle at the point P of intersection between two intersecting lines A and B
may be bisected by drawing any circle centered at P, selecting one of the points
of intersection on each line so that P and the two new points Q and R describe
the angle, drawing circles with that same radius centered at Q and R, and forming
the line between the points of intersection of the two circles. And so on.
    Three notable problems remained unsolved in antiquity:
      (i) how to double a cube, i.e., how to construct the side of a cube of double
          the volume of a given cube,
     (ii) how to trisect any constructible angle, i.e., how to divide the angle into
          three equal parts by means of constructed lines,
    (iii) how to square a circle, i.e., how to construct the side of a square whose
          area equals that of a given disk.
In this section we shall use the elementary field theory of Sections 1–2 to show that
doubling a cube and trisecting a 60-degree angle are impossible with straightedge
and compass. As to (iii), we shall reduce a proof of the impossibility of squaring
the circle to a proof that π is transcendental over Q. This latter proof we give in
Section 14.
    The first step is to translate the problem of geometric constructibility into a
statement in algebra. Since we are given two points on a line, we can introduce
Cartesian coordinates for the Euclidean plane, taking one of the points to be (0, 0)
and the other point to be (1, 0). Points in the Euclidean plane are now determined
by their Cartesian coordinates, which determine all distances. Distances in turn
can be laid off on the x-axis from (0, 0). Thus the question becomes, what points
on the x-axis can be constructed?
    Let C be the set of constructible x coordinates. We are given that 0 and 1 are
in C. Closure of C under addition and subtraction is evident; the straightedge is
not even necessary for this step. Figure 9.1 indicates why the positive elements
470                          IX. Fields and Galois Theory



                                        c

                                a


                                    b           d

          FIGURE 9.1. Closure of positive constructible x coordinates
                      under multiplication and division.

of C are closed under multiplication and division. In more detail we take two
intersecting lines and mark three known positive members of C as the distances
a, b, c in the figure. Then we form the line through the two points marking a
and b, and we form a line parallel to that line through the point marked off by
the distance c. The intersection of this parallel line with the other original line
defines a distance d. Then a/b = c/d, and so d = bc/a. By taking a = 1, we
see that we can multiply any two members b and c in C, obtaining a result in C.
By instead taking c = 1, we see that we can divide. The conclusion is that C is a
field.




                            c

                                a
                                            b
          FIGURE 9.2. Closure of positive constructible x coordinates
                             under square roots.

   Figure 9.2 indicates why the positive elements of C are closed under taking
square roots. In more detail let a and b be positive members of C with a < b. By
forming a circle whose diameter is a segment of length b and by forming a line
perpendicular to that line at the point marked by a, we determine
                                                               p        the pictured
right triangle with a side c satisfying a/c = c/b. Then c = ab. By taking one
of a and b to be 1, we see that the square root of the other of a and b is in C. This
completes the proof of the direct part of the following theorem.

  Theorem 9.24. The set C of x coordinates that can be constructed from x = 1
and x = 0 by straightedge and compass forms a subfield of R such that the square
                  5. Geometric Constructions by Straightedge and Compass          471

root of any positive element of the field lies in the field. Conversely the members
of C are those real numbers lying in some subfield Fn of R of the form
                    p                 p                           p
           F1 = Q( a0 ), F2 = F1 ( a1 ), . . . , Fn = Fn−1 ( an−1 )
with each a j in Fj and with a0 , . . . , an−1 all ∏ 0.
   PROOF OF CONVERSE. Suppose we have a subfield F = Fn of R of the
kind described in the statement of the theorem. The possibilities for obtaining
a new constructible point from F by an additional construction arise from three
situations: the intersection of two lines, each passing through two points of F;
the intersection of a line and a circle, each determined by data from F; and the
intersection of two circles, each determined by data from F.
   In the case of two intersecting lines, each line is of the form ax + by = c for
suitable coefficients a, b, c in F, and the intersection is a point (x, y) in F × F.
So intersections of lines do not force us to enlarge F.
   For a line and a circle, we assume that the line is given by ax + by = c with
a, b, c in F, that the circle has radius in F and center in F × F, and that the lines
and the circle actually intersect. The circle is then given by (x −h)2 +(y−k)2 = r 2
with h, k, r in F. Substitution of the equation of the line into the equation of the
circle gives us a quadratic equation either for x, and x then determines y, or for
y, and y then determines x. The quadratic equation has real roots,     p and thus its
discriminant is ∏ 0. The result is that x and y are in a field F( l ) for some
l ∏ 0 in F.
   For two circles, without loss of generality, we may take their equations to be
               x 2 + y2 = r 2      and       (x − h)2 + (y − h)2 = s 2
with r, h, k, s in F. Subtracting gives 2xh + 2yk = h 2 + k 2 − s 2 + r 2 . With this
equation and with x 2 + y 2 = r 2 , we again have a line and circle that are being
intersected. Thus the same remarks apply as in the previous paragraph.
   The conclusion is that any new single construction
                                               p       of points of intersection by
straightedge and compass leads from F to F( l ) for some l ∏ 0 in F. Thus
every member of the set C is as described in the theorem.                          §

   To apply the theorem to prove the impossibility of the three never-accomplished
constructions that were described earlier in the section, we observe that [Fi : Fi−1 ]
in the theorem equals 1 or 2 for each i. Consequently every member of the
constructible set C lies in a finite algebraic extension of Q of degree 2k for some
                                                                                  p k.
                                                                                  3
   For the problem of doublingpa cube, the question amounts to constructingp        2.
We argue by contradiction. If 3 2 lies in Fn as in the theorem, then Q( 3 2 ) ⊆ Fn .
With k as the integer ≤ n such that [Fn : Q] = 2k , Corollary 9.7 gives
                                       p         p                      p
        2k = [Fn : Q] = [Fn : Q( 2 )] [Q( 2 ) : Q] = 3[Fn : Q( 2 )].
                                       3         3                      3
472                               IX. Fields and Galois Theory

Thus 3 must divide a power of 2, and we have arrived at a contradiction. We
conclude that it is not possible to double a cube with straightedge and compass.
   For the problem of trisecting any constructible angle, let us show that a 60◦
angle cannot be trisected. A 60◦ angle is itself constructible, being the angle
between two sides in an equilateral triangle. Trisecting a 60◦ angle amounts to
constructing cos 20◦ ; sin 20◦ is then (1 − cos2 20◦ )1/2 . To proceed, we derive an
equation satisfied by cos 20◦ , starting from
                                                                                  p
               (cos 20◦ + i sin 20◦ )3 = cos 60◦ + i sin 60◦ =           1
                                                                         2   +   i 3
                                                                                  2 .
We expand the left side and extract the real part of both sides to obtain
                            cos3 20◦ − 3 cos 20◦ sin2 20◦ = 12 .
Substituting sin2 20◦ = 1 − cos2 20◦ and simplifying, we see that r = cos 20◦
satisfies
                             4r 3 − 3r − 12 = 0.
Arguing with Corollary 8.20 as in Example 2 of splitting fields in Section 2, we
readily check that 4X 3 − 3X − 12 is irreducible over Q. Hence [Q(cos 20◦ ) : Q]
= 3, and we are led to the same contradiction as for the problem of doubling
the cube. Therefore it is not possible to trisect a 60◦ angle with straightedge and
compass.
   For the problem of squaring a circle, let A be the area of the circle, and let
                                                      2            2
             p If the square has side x, then x = A = πr
r be the radius.                                                 p , with r given.
Thus x = r π, and the essence of the matter is to construct π. However, π
is known to be transcendental by a theorem of p    F. Lindemann (1882); we give a
proof in Section 14. Since π is transcendental, π is transcendental.
   A fourth notable problem, which leads to further insights, concerns the con-
struction of a regular polygon of outer radius 1 with n sides. This construction
is easy with straightedge and compass when n is a power of 2 or is 3 times a
power of 2, and Euclid showed that a construction is possible for n = 5. But a
construction cannot be managed with straightedge and compass for n = 9, for
example, because a central angle in this case is 40◦ and the constructibility of
cos 40◦ would imply the constructibility of cos 20◦ . Thus the question is, for what
values of n can a regular n-gon be constructed with straightedge and compass?
   The remarkable answer was given by Gauss. By a Fermat number is meant
                            N
any integer of the form 22 + 1. A Fermat prime is a Fermat number that is
prime. The Fermat numbers for N = 0, 1, 2, 3, 4 are 3, 5, 17, 257, 65537, and
each is a Fermat prime. No larger Fermat primes are known.2 The answer given
   2 Many   Fermat numbers for N ∏ 5 are known not to be prime, sometimes by the discovery of
                                                                         N
an explicit factor and sometimes by a verification that 3 to the power 22 −1 is not congruent to −1
             N                                                                               5
           2
modulo (2 + 1). (Cf. Lemma 9.46.) For example Euler discovered that 641 divides 22 + 1.
                                         2 N +1
Computer calculations have shown that 2         is not prime if 5 ≤ N ≤ 32.
                    5. Geometric Constructions by Straightedge and Compass                       473

by Gauss, which we shall prove in stages in Sections 6–9, is as follows.

   Theorem 9.25 (Gauss).3 A regular n-gon is constructible with straightedge
and compass if and only if n is the product of distinct Fermat primes and a power
of 2.

    We can show the relevance of Fermat primes right now, and we can give an
indication that if n is a prime number, then a regular n-gon can be constructed if
and only if n is a Fermat prime. But a full proof even of this statement will make
use of Galois groups, which we take up in the next three sections.
    For the necessity let n be prime, and suppose that a regular n-gon is con-
structible. Returning from degrees to radians, we observe that each central angle
is 2π/n. Thus the constructibility implies the constructibility of cos 2π/n, and it
follows that e2πi/n = cos 2π/n + i sin 2π/n is in the field C + iC of constructible
points in the complex plane. We have the factorization

                  X n − 1 = (X − 1)(X n−1 + X n−2 + · · · + X + 1).

and e2πi/n is a root of the second factor. The first example of Eisenstein’s criterion
(Corollary 8.22) in Section VIII.5 shows that the second factor is irreducible.
According to the results of Section 1, Q(e2πi/n ) is a simple algebraic extension
of Q of degree n − 1.
   Applying Theorem 9.24, we see that n − 1 must be a power of two. Let us
write n − 1 = 2m . Suppose m = a2 N with a odd. If a > 1, then the equality
          N           N
n = 2a2 + 1 = (22 )a + 1a exhibits n as the sum of two a th powers, necessarily
                N
divisible by 22 +1. Since n is assumed prime, we conclude that a = 1. Therefore
        N
n = 22 + 1, and n is a Fermat prime.
   We do not quite succeed in proving the converse at this point. If n is the Fermat
           N
prime 22 + 1, then the above argument shows that the degree of Q(e2πi/n ) over
        N
Q is 22 . However, we cannot yet conclude that Q(e2πi/n ) can be built from Q
by successively adjoining 2 N square roots, and thus the converse part of Theorem
9.24 is not immediately applicable. Once we have the theory of Galois groups in
hand, we shall see that the existence of these intermediate extensions involving
square roots is ensured, and then the constructibility follows.



   3 Gauss announced both the necessity and the sufficiency in this theorem in his Disquisitiones
Arithmeticae in 1801, but he included a proof of only the sufficiency (partly in his articles 336 and
365). A proof of the necessity appeared in a paper of Pierre-Laurent Wantzel in 1837.
474                           IX. Fields and Galois Theory

                            6. Separable Extensions

The Galois group Gal(K/k) of a field extension K/k is defined to be the set

                      Gal(K/k) = {k automorphisms of K}

with composition as group operation. An instance of this group was introduced in
the context of Example 9 of Section IV.1; in this example the field k was the field
Q of rationals and the field K was a number field Q[θ], where θ is algebraic over
Q. In studying Gal(K/k) in this chapter, we ordinarily assume that dimk K < ∞,
but there will be instances where we do not want to make such an assumption.
   Beginning in this section, we take up a study of Galois groups in general.
We shall be interested in relationships between fields L with k ⊆ L ⊆ K and
subgroups of Gal(K/k). If H is a subgroup of Gal(K/k), then
                            ©                               ™
                    K H = x ∈ K | ϕ(x) = x for all ϕ ∈ H

is a field called the fixed field of H ; it provides an example of an intermediate
field L and gives a hint of the relationships we shall investigate. We begin with
some examples; in each case the base field k is the field Q of rationals.

   EXAMPLES OF GALOIS GROUPS.
               p                                                 Ø
   (1a) K = Q( −1 ). If ϕ is in Gal(K/Q), then we must have ϕ ØQ = 1, and
  p                                          p           p
p −1 ) must be p
ϕ(                a root of X 2 + 1. Thus ϕ( −1 ) = ± −1. Since Q and
   p generate Q( p−1 ), there are at most two such ϕ’s. Onpthe other hand,
  −1                                                                     p
Q( −1 ) and Q(− −1 ) are simple extensions of Q such that −1 and − −1
have the same minimal
                p               p Theorem
                       polynomial.        p 9.11 therefore produces a Q auto-
morphism of Q( −1 ) with ϕ( −1 ) = − −1, namely complex conjugation.
We conclude that Gal(K/Q) has order 2, hence that Gal(K/Q) ∼ = C2 .
                p
   (1b) K = Q( 2 ). The same argument applies as in Example 1a, and the
conclusion                    ∼
               pGal(K/Q) = C2 . The nontrivial element of the Galois group
        p is that
carries 2 to − 2 and is different from complex conjugation.
              p                                  Ø             p
   (2) K = Q( 3 2 ). If ϕ is in Gal(K/Q), then ϕ Ø = 1, and ϕ( 3 2 ) has to be
                                                        Q
a root of X 3 − 2.
                p  But Kpis a subfield of R,pand there is onlyp  one root of X 3 − 2
in R. Hence ϕ( 2 ) = 2. Since Q and 2 generate Q( 2 ) as a field, we see
                 3        3                  3                 3


that ϕ = 1. We conclude that Gal(K/Q) has order 1, i.e., is the trivial group.
   (3) K = Q(r), where r is a root of X 3 − X − 13 . Any ϕ in Gal(K/Q) fixes Q
and sends r to a root of X 3 − X − 13 . In Example 2 of splitting fields in Section 2,
we saw that all three complex roots of X 3 − X − 13 lie in K. Arguing as in
Example 1a, we see that Gal(K/Q) has order 3, hence that Gal(K/Q) ∼         = C3 .
                                      6. Separable Extensions                                     475

    (4) K = Q(e2πi/17 ). According to Section 5, this is the field we need to
consider in addressing the constructibility of a regular 17-gon. We saw in that
section that [K : Q] = 16 and that the minimal polynomial of e2πi/17 over Q
is X 16 + X 15 + · · · + X + 1. The other roots of the minimal polynomial in
C are e2πil/17 for 2 ≤ l ≤ 16, and these all lie in K. Theorem 9.11 therefore
gives us a Q automorphism ϕl of K sending e2πi/17 into e2πil/17 for each l with
1 ≤ l ≤ 16. Since Q and e2πi/17 generate K, a Q automorphism of K is
completely determined by its effect on e2πi/17 . Thus the order of Gal(K/Q)
is 16. Let us determine the group structure. Since ϕl sends e2πi/17 into e2πil/17 , it
sends e2πir/17 = (e2πi/17 )r into (e2πil/17 )r = e2πilr/17 . If we drop the exponential
from the notation, we can think of ϕl as defined on the integers modulo 17, the
formula being ϕl (r) = rl mod 17. From this viewpoint ϕl is an automorphism
of the additive group of F17 . Lemma 4.45 shows that the group of additive
automorphisms of F17 is isomorphic to F×       17 , and it follows from Corollary 4.27
that Gal(K/Q) ∼  = C16 . For our application of constructibility of a regular 17-
gon, we would like to know whether the elements of K are constructible. Taking
Theorem 9.24 into account, we therefore seek an intermediate field L of which
K is a quadratic extension. Since we know that Gal(K/Q) is cyclic, we can let
H ⊆ Gal(K/Q) ∼     = C16 be the 2-element subgroup, and it is natural to try the
fixed field L = K H . To understand this fixed field, we need to understand the
isomorphism F×     ∼
                17 = C 16 better. Modulo 17, we have

                  32 = 9,       34 = −22 ,       38 = 24 = −1,          316 = 1.

Consequently 3 is a generator of the cyclic group F×                  8
                                                   17 . Then H = {3 , 1} = {±1},
and L = {x ∈ K | ϕ−1 (x) = ϕ+1 (x) = x}. Since ϕ−1 (e       2πir/17
                                                                    ) = e−2πir/17 =
e2πir/17 with the overbar indicating complex conjugation, we see that

                                L = K H = {x ∈ K | x = x̄}.

It is not hard to check that indeed [K : L] = 2. Next we need a subfield L0 of
                                          0
L with [L : L0 ] = 2. We try L0 = K H with H 0 equal to the 4-element cyclic
subgroup of Gal(K/Q). Here we have a harder time checking whether L is indeed
a quadratic extension of L0 , but we shall see in Section 8 that it is.4 We continue
in this way, and ultimately we end up with the chain of subfields that exhibits the
members of K as constructible.

                                                     p examples as a general
   We seek to formulate the kind of argument in the above
theorem. We have to rule out the bad behavior of Q( 3 2 ), where one root of the
    4 Actually, Section 8 will point out how Corollary 9.36 in Section 7 already handles this step. In

fact, Corollary 9.37 handles this step with no supplementary argument.
476                           IX. Fields and Galois Theory

minimal polynomial lies in the field but others do not, and we shall do this by
assuming that the extension field is a “normal” extension, in a sense to be defined
in Section 7. In addition, our style of argument shows that we might run into
trouble if our irreducible polynomials over k can have repeated roots in K. We
shall rule out this bad behavior by insisting that the extension be “separable,” a
condition that we introduce now. The extension will automatically be separable
if K has characteristic 0.
   For the remainder of this section, fix the base field k. An irreducible polynomial
F(X) in k[X] is called separable if it splits into distinct degree-one factors in its
splitting field, i.e., if

          f (X) = an (X − x1 ) · · · (X − xn )       with xi 6= x j for i 6= j.

Once this splitting into distinct degree-one factors occurs in the splitting field, it
occurs in any larger field as well.

   Lemma 9.26. A polynomial F(X) in k[X] has no repeated roots in its splitting
field K if and only if GCD(F, F 0 ) = 1, where F 0 (X) is the derivative of F(X).
    PROOF. The polynomial F(X) has repeated roots in K if and only if F(X) is
divisible by (X − r)2 for some r ∈ K, if and only if some r ∈ K has F(r) =
F 0 (r) = 0 (by Corollary 9.17), if and only if some r ∈ K has (X − r) dividing
F(X) and also F 0 (X) (by the Factor Theorem), if and only if some r ∈ K has
(X − r) dividing GCD(F, F 0 ) when the GCD is computed in K, if and only
if GCD(F, F 0 ) 6= 1 when the GCD is computed in K (by unique factorization
in K[X]). However, the Euclidean algorithm calculates GCD(F, F 0 ) without
reference to the field, and the GCD is therefore the same when computed in K as
it is when computed in k. The lemma follows.                                 §

  Proposition 9.27. An irreducible polynomial F(X) in k[X] is separable if
and only if F 0 (X) 6= 0. In particular, every irreducible (necessarily nonconstant)
polynomial is separable if k has characteristic 0.
   PROOF. Since the polynomial F(X) is irreducible and GCD(F, F 0 ) divides
F(X), GCD(F, F 0 ) equals 1 or F(X) in all cases. If F 0 (X) = 0, then GCD(F, F 0 )
= F(X), and Lemma 9.26 implies that F(X) is not separable. Conversely
if F 0 (X) 6= 0, then the facts that GCD(F, F 0 ) divides F 0 (X) and that deg F 0 <
deg F together imply that GCD(F, F 0 ) cannot equal F(X). So GCD(F, F 0 ) = 1,
and Lemma 9.26 implies that F(X) is separable.                                     §

   Fix an algebraic extension K of k. We say that an element x of K is separable
over k if the minimal polynomial of x over k is separable. We say that K is a
separable extension of k if every x in K is separable over k.
                                6. Separable Extensions                           477

   EXAMPLES OF SEPARABLE EXTENSIONS AND EXTENSIONS NOT SEPARABLE.
   (1) In characteristic 0, every algebraic extension K of k is separable, by
Proposition 9.27.
   (2) Every algebraic extension K of a finite field k is separable. In fact, if x is
in K, then [k(x) : k] is finite. Hence k(x) is a finite field. Then we may assume
that K is a finite field, say of order q = pn with p prime. Since the multiplicative
group K× has order q − 1, every nonzero element of K is a root of X q−1 − 1, and
every element of K is therefore a root of X q − X. The minimal polynomial F(X)
of x over k must then divide X q − X. However, we know that X q − X splits over
K and has no repeated roots. Thus F(X) splits over K and has no repeated roots.
Then F(X) is separable over k, and x is separable over k.
   (3) Let k = F p (x) be a transcendental extension of the finite field F p . Because
this extension is transcendental, X p − x is irreducible over k. Let K be the
simple algebraic extension k[X]/(X p − x), which we can write more simply as
k(x 1/ p ). The minimal polynomial of x 1/ p over k is X p − x, and its derivative is
p X p−1 = 0 since the derivative of the constant x is 0. By Proposition 9.27, x 1/ p
is not separable over k.

   The way that separability enters considerations with Galois groups is through
the following theorem, explicitly or implicitly. One of the corollaries of the
theorem is that if K/k is an algebraic extension, then the set of elements in K
separable over k is a subfield of K.

   Theorem 9.28. Let k ⊆ L ⊆ K be an inclusion of fields such that K is a
simple algebraic extension of L of the form K = L(α), let K be an algebraic
closure of K, and let M(X) be the minimal polynomial of α over L. Then the
number of field mappings of K into K fixing k is the product of the number of
distinct roots of M(X) in K by the number of field mappings of L into K fixing k.
   REMARKS. An algebraic closure K of K exists by Theorem 9.22. Because K
is known to exist, the present theorem reduces to Theorem 9.11 when L = k.
                                                                           Ø
   PROOF. AnyØ field mapping ϕ : K → K is uniquely determined by ϕ ØL and
ϕ(α). If σ = ϕ ØL , then the equality M(α) = 0 implies that M σ (ϕ(α)) = 0, and
thus ϕ(α) has to be a root of M σ (X). The number of distinct roots of M σ (X)
in K equals the number of distinct roots of M(X) in K; hence the number of
possibilities for ϕ(α) is at most the number of distinct roots of M(X) in K.
Consequently the number of such ϕ’s fixing k is bounded above by the product
of the number of distinct roots of M(X) in K times the number of field mappings
σ of L into K fixing k.
   For an inequality in the reverse direction, let σ : L → K be any field mapping
of L into K fixing k, put L0 = σ (L), let x be any root of M σ (X), and form the
478                           IX. Fields and Galois Theory

subfield L0 (x) of K. Theorem      0
                            Ø 9.11 shows that there exists a field isomorphism
ϕ : L(α) → L (x) with ϕ ØL = σ and ϕ(α) = x, and we can regard ϕ as a
                 0

field mapping of K into K fixing k, extending σ , and having ϕ(α) = x. Thus
the number of field mappings ϕ : K → k fixing k is bounded below by the
product of the number of distinct roots of M(X) in K times the number of field
homomorphisms σ of L into K fixing k.                                        §

   Corollary 9.29. Let K = k(α1 , . . . , αn ) be a finite algebraic extension of
the field k, and let K be an algebraic closure of K. Then the number of field
mappings of K into K fixing k is ≤ [K : k]. Moreover, the following conditions
are equivalent:
      (a) the number of field mappings of K into K fixing k equals [K : k],
      (b) each α j is separable over k(α1 , . . . , α j−1 ) for 1 ≤ j ≤ n,
      (c) each α j is separable over k for 1 ≤ j ≤ n.

   PROOF. The minimal polynomial of α j over k(α1 , . . . , α j−1 ) divides the min-
imal polynomial of α j over k. If the second of these polynomials has distinct
roots in its splitting field, so does the first. Thus (c) implies (b).
   For 1 ≤ j ≤ n, let the minimal polynomial of α j over k(α1 , . . . , α j−1 ) be
M j (X), let d j be the degree of M j (X), and let s j be the number of distinct roots
of M j (X) in K. Then s j ≤ d j with equality for a particular j if andQonly if α j
is separable over k(α1 , . . . , α j−1 ), by definition. Also, [K : k] = nj=1 d j by
                                                                              Q
Corollary 9.7, and the number of field mappings of K into K fixing k is nj=1 s j
by iterated application of Theorem 9.28. From these facts, the first conclusion of
the corollary is immediate, and so is the equivalence of (a) and (b).
   Condition (a) is independent of the order of enumeration of α1 , . . . , αn . Since
we can always take any particular α j to be first, we see that (a) implies (c). §

   Corollary 9.30. Let K = k(α1 , . . . , αn ) be a finite algebraic extension of the
field k. If each α j for 1 ≤ j ≤ n is separable over k, then K/k is a separable
extension.
   PROOF. Let β be in K, We apply the equivalence of (a) and (c) in Corollary
9.29 once to the set of generators {α1 , . . . , αn } and once to the set of generators
{β, α1 , . . . , αn }, and the result is immediate.                                  §

   Corollary 9.31. If K/k is an algebraic field extension, then the subset L of
elements of K that are separable over k is a subfield of K.
   PROOF. If α and β are given in L, we apply Corollary 9.30 to the extension
k(α, β) of k to see that L contains the subfield generated by k and the elements
α and β.                                                                      §
                                6. Separable Extensions                           479

  Proposition 9.32. If K/k is a separable algebraic extension and if L is a field
with k ⊆ L ⊆ K, then K is separable over L, and L is separable over k.
   PROOF. The separability assertion about L/k says the same thing about el-
ements of L that separability of K/k says about those same elements, and it is
therefore immediate that L/k is separable.
   Next let us consider K/L. If x is in K, let F(X) be its minimal polynomial
over k, and let G(X) be its minimal polynomial over L. Since F(X) is in L[X]
and F(x) = G(x) = 0, G(X) divides F(X). Since K/k is separable, F(X)
splits into distinct degree-one factors in its splitting field F. The field F contains
the splitting field of G(X), and thus the degree-one factors of G(X) in F[X] are a
subset of the degree-one factors of F(X) in F[X]. There are no repeated factors
for F(X), and there can be no repeated factors for G(X). Thus x is separable
over L, and K/L is a separable extension.                                           §

    In studying Galois groups, we shall be chiefly interested in the following
situation in Corollary 9.29: K is an algebraic field extension K = k(α1 , . . . , αn )
of k for which every field mapping of K into an algebraic closure that fixes k
actually carries K into itself. We seek conditions under which this situation arises,
and then we mine the consequences. As we did in the study begun in Theorem
9.28, we begin with the case of a simple algebraic extension.
    Let K = k(∞ ) be a simple algebraic extension of k, and let F(X) be the
minimal polynomial of ∞ over k. Any member ϕ of the Galois group Gal(K/k)
carries ∞ to another root ∞ 0 of F(X), and ϕ is uniquely determined by ∞ 0 since
k and ∞ generate the field K. An element ϕ of Gal(K/k) carrying ∞ to ∞ 0 can
exist only if ∞ 0 is in K. If ∞ 0 is in K, then k(∞ ) ⊇ k(∞ 0 ), and the equal finite
dimensionality of k(∞ ) and k(∞ 0 ) forces k(∞ ) = k(∞ 0 ). In other words, if ∞ 0 is
in K, then the unique k isomorphism k(∞ ) → k(∞ 0 ) of Theorems 9.10 and 9.11
carrying ∞ to ∞ 0 is a member of Gal(K/k). Making a count of what happens to
all the elements ∞ 0 , we see that we have proved the following.

  Proposition 9.33. Let K = k(∞ ) be a simple algebraic extension of k, and let
F(X) be the minimal polynomial of ∞ . Then

                               | Gal(K/k)| ≤ [K : k]

with equality if and only if F(X) is a separable polynomial and K is the splitting
field of F(X) over k.
                             p
   EXAMPLE. For K = Q( 3 2 ) with minimal polynomial F(X), we know that
F(X) does not split in K; the nonreal roots of F(X) do not lie in K. Proposition
9.33 gives us | Gal(K/Q)| < [K : Q] = 3, and a glance at the argument preceding
Proposition 9.33 shows that | Gal(K/Q)| has to be 1.
480                            IX. Fields and Galois Theory

    It is possible to investigate the case of several generators directly, but it is more
illuminating to reduce it to the case of a single generator as in Proposition 9.33.
The tool for doing so is the following important theorem.

   Theorem 9.34 (Theorem of the Primitive Element). Let K/k be a separable
algebraic extension with [K : k] < ∞. Then there exists an element ∞ in K such
that K = k(∞ ).

    PROOF. We may assume that k is infinite because Corollary 4.27 shows that
the multiplicative group of a finite field is cyclic. With k infinite, we can write
K = k(x1 , . . . , xn ), and we proceed by induction on n, the case n = 1 being
trivial. For general n, let L = k(x1 , . . . , xn−1 ), so that K = L(xn ). By the
inductive hypothesis, L is of the form L = k(α) for some α in K, and thus
K = k(α, xn ). Changing notation, we see that it is enough to prove that whenever
K is a separable algebraic extension of the form K = k(α, β), then K is of the
form K = k(∞ ) for some ∞ . We shall show this for ∞ of the form ∞ = β + cα
for some c in k.
    Let F(X) and G(X) be the minimal polynomials of α and β over k, and let
K0 be an extension in which F(X)G(X) splits, i.e., in which F(X) and G(X)
both split. Let α1 = α, α2 , . . . , αm and β1 = β, β2 , . . . , βn be the roots of
F(X) and G(X) in K0 , In each case the roots are necessarily distinct by definition
of separability of α and β. Define L = k(∞ ) with ∞ = β + cα, where c is a
member of k yet to be specified. For suitable c, we shall show that α is in L.
Then β = ∞ − cα must be in L, and we obtain K ⊆ L. Since ∞ is in K, the
reverse inclusion is built into the construction, and thus we will have K = L.
    We shall compute the minimal polynomial of α over L. We know that α is a
root of F(X), and we put H (X) = G(∞ − cX). Then H (X) is in L[X] ⊆ K0 [X],
and G(β) = 0 implies H (α) = 0. Therefore X − α divides both F(X) and H (X)
in the ring K0 [X]. Let us determine GCD(F, H ) in K0 [X]. The separability of
α says that X − α divides F(X) only once. Since F(X) splits in K0 [x], any
other prime divisor of GCD(F, H ) in K0 [X] has to be of the form X − αi with
i 6= 1. The definition of H (X) gives H (αi ) = G(∞ − cαi ). If G(∞ − cαi ) = 0,
then ∞ − cαi = β j for some j, with the consequence that β + cα − cαi = β j
and c = (β j − β)(α − αi )−1 . Since k is an infinite field, we can choose c in
K different from all the finitely many quotients (β j − β)(α − αi )−1 . For such a
choice of c, GCD(F, H ) = X − α in K0 [X]. Then GCD(F, H ) = X − α, up to
a scalar factor, in L[X] since F(X) and H (X) are in L[X] and since the GCD
can be computed without reference to the field containing both elements. The
ratio of the constant term to the coefficient of X has to be in L independently of
the scalar factor multiplying X − α, and therefore α is in L. This completes the
proof.                                                                           §
                                       7. Normal Extensions                                       481

                                   7. Normal Extensions

In using Galois groups to help inpunderstanding field extensions, an example to
keep in mind is the extension Q( 3 2 )/Q. In this case the Galois group is trivial
and therefore gives us no information about the extension. Thus it makes sense
to regard the failure of equality to hold in an inequality | Gal(K/k)| ≤ [K : k] as
an undesirable situation.5
   Proposition 9.33 suggests that the failure of equality to hold in the inequality
| Gal(K/k)| ≤ [K : k] has something to do with two phenomena. One is the
possible failure of some polynomials over k to be separable, and the other is the
failure of polynomials over k to split fully in K once they have at least one root
in K. Having examined separability in Section 6, we turn to this question of full
splitting of polynomials.
   Accordingly, we make a definition, choosing among several equivalent condi-
tions the one that is usually the easiest to check in practice. A finite6 algebraic
extension K of a field k is said to be normal over k if K is the splitting field of some
F(X) in k[X]. The following proposition gives some equivalent formulations of
this condition.

  Proposition 9.34A. Let K be a finite algebraic extension of a field k, and regard
K as contained in a fixed algebraic closure K. Then the following conditions on
K are equivalent.
   (a) K is the splitting field of some F(X) in k[X], i.e., K is normal over k,
   (b) every irreducible polynomial M(X) in k[X] with a root in K splits in K,
       i.e., K contains the splitting field for each such M(X),
   (c) every k isomorphism of K into K carries K into itself.

   REMARK. Although (a) is often the easiest of the conditions to check, (b) is
often the easiest to disprove. It is therefore quite handy to know the equivalence.
    PROOF. Suppose that (a) holds. Let F(X) be as in (a), and let its roots be
∞1 , . . . , ∞n . Let M(X) be an irreducible polynomial in k[X] with a root α in K,
and let L be the splitting field of M(X) over K. Let β be any root of M(X) in
L. Since M(X) is irreducible over k, Theorem 9.11 produces a k isomorphism
σ of k(α) onto k(β) with σ (α) = β. The isomorphism σ leaves F(X) fixed,
since the coefficients of F(X) are in k. Now the splitting field of F(X) over k(α)
    5 We obtained this inequality in Proposition 9.33 only when K has a single generator over k, but

we take this case as indicative of what to expect more generally.
    6 Many books do not restrict the definition to finite extensions. The additional generality of

infinite algebraic extensions will not be of benefit for our current purposes, and thus we restrict to
finite extensions for now. But in Section VII.6 of Advanced Algebra, we shall enlarge the definition
of “normal” to allow infinite algebraic extensions.
482                           IX. Fields and Galois Theory

is K, since the roots of F(X) are in K and generate K over k(α). Similarly the
splitting field of F(X) over k(β) is K(β). Application                          0
                                                           Ø of Theorem 9.13 yields
a field isomorphism ϕ of K onto K(β) such that ϕ Øk(α) = σ and such that ϕ
carries the roots of F(X) to the roots of F(X). We can express α as a rational
expression in ∞1 , . . . , ∞n with coefficients in k, and then β = ϕ(α) is the same
rational expression in ϕ(∞1 ), . . . , ϕ(∞n ), which themselves are members of K.
Therefore β is in K, and the conclusion is that M(X) splits in K.
   Suppose that (b) holds. Let ϕ be a k isomorphism of K into K, and let α be
any element of K. The minimal polynomial M(X) of α over k is irreducible and
has α as a root in K. By (b), M(X) splits in K. The element ϕ(α) has to be a
root of M(X) since ϕ fixes the coefficients of M(X), and all the roots of M(X)
are assumed to lie in K. Therefore ϕ(α) lies in K, and (b) implies (c).
   Suppose that (c) holds. Since K is a finite algebraic extension of k, we can
                                                                 Qn of K. Let Pj (X) be
write K = k(α1 , . . . , αn ) for suitable elements α1 , . . . , α
the minimal polynomial of α j over k, and put F(X) = nj=1 Pj (X). Since the
roots α1 , . . . , αn generate K over k, it is enough to show that every root of F(X)
lies in K , i.e., each root of each Pj (X) lies in K. Let β be a root of Pj (X) in K.
We know from Theorem 9.11 that there is a k isomorphism ϕ of k(α j ) onto k(β)
with ϕ(α j ) = β. Theorem 9.23 shows that ϕ extends to a field mapping of K into
K, and (c) shows that the extended ϕ sends K into itself. Therefore β = ϕ(α j )
lies in K, and all the roots of F(X) in K lie in K. Thus (c) implies (a).            §

   Now we can put together the properties of normal and separable extensions.
It will be convenient to be able to refer in this context to the equivalence of (a)
and (b) that was proved in Proposition 9.34A, and thus we repeat the statement
of that equivalence here.

   Proposition 9.35. Let K be a finite separable algebraic extension of a field k,
so that | Gal(K/k)| ≤ [K : k]. Then the following are equivalent.
    (a) K is the splitting field of some F(X) in k[X], i.e., K is normal over k,
    (b) every irreducible polynomial M(X) in k[X] with a root in K splits in K,
         i.e., K contains the splitting field for each such M(X),
    (c) | Gal(K/k)| = [K : k],
    (d) k = KG for G = Gal(K/k).
   REMARKS. The equivalence of (a) and (b) is part of Proposition 9.34A, and
the fact that they are equivalent with (c) follows from Proposition 9.33 and the
Theorem of the Primitive Element (Theorem 9.34). We prove that the equivalent
(a), (b), and (c) imply (d), and that (d) implies (b).
   PROOF. Suppose that the equivalent (a), (b), and (c) hold for K/k. We prove
(d). Write G = Gal(K/k), and let k0 = KG . Since every member of Gal(K/k)
                                7. Normal Extensions                            483

fixes k0 , Gal(K/k) ⊆ Gal(K/k0 ). Meanwhile, (a) for K/k implies (a) for K/k0 ,
and K is separable over k0 by Proposition 9.32. Since (a) implies (c), (c) holds
for both k0 and k, and we have

               [K : k] = | Gal(K/k)| ≤ | Gal(K/k0 )| = [K : k0 ].

Since k0 ⊇ k, the inequality of dimensions implies that k0 = k. Thus (d) holds.
   Suppose (d) holds. We prove (b). Let M(X) be an irreducible polynomial
in k[X] having a root r in K. The polynomial M(X) is necessarily the minimal
polynomial of r over k. Define
                                     Q
                            J (X) =     (X − ϕ(r)).                          (∗)
                                      ϕ∈G

If ϕ0 is in G, then F ϕ0 is given by replacing each ϕ(x) by ϕ0 ϕ(r), and the product
is unchanged. Therefore J (X) = J ϕ0 (X), and J (X) is in KG [X]. From the
assumption in (d), KG = k. Therefore J (X) is in k[X]. Since J (r) = 0 and
since M(X) is the minimal polynomial of r over k, M(X) divides J (X). Over
K, J (X) splits because of its definition in (∗). By unique factorization in K[X],
M(X) must split too. Thus M(X) splits in K[X], and (b) holds.                     §

   Corollary 9.36. If K is a finite normal separable extension of k and if L is a
field with k ⊆ L ⊆ K, then K is a finite normal separable extension of L, and the
subgroup H = Gal(K/L) of Gal(K/k) has

                          |H | · [L : k] = | Gal(K/k)| .

  PROOF. The field K is a separable extension of the intermediate field L by
Proposition 9.32, and it is a normal extension by Proposition 9.35a. Therefore
Proposition 9.35c gives | Gal(K/L)| = [K : L], and we have

|H |·[L : k] = | Gal(K/L)|·[L : k] = [K : L]·[L : k] = [K : k] = | Gal(K/k)|,

the last two equalities holding by Corollary 9.7 and Proposition 9.35c.          §

   Corollary 9.37. Let K/k be a separable algebraic extension, and suppose that
H is a finite subgroup of Gal(K/k). Then K/K H is a finite normal separable
extension, H is the subgroup Gal(K/K H ) of Gal(K/k), and [K : K H ] = |H |.
   PROOF. Proposition 9.32 shows that K is separable over K H . For an arbitrary
element x of K, form the polynomial in K[X] given by
                                   Q
                          F(X) =       (X − ϕ(x)).
                                      ϕ∈H
484                          IX. Fields and Galois Theory

If ϕ0 is in H , then F ϕ0 is given by replacing each ϕ(x) by ϕ0 ϕ(x), and the product
is unchanged. Therefore F(X) = F ϕ0 (X), and F(X) is in K H [X]. Thus F(X)
is a polynomial in K H [X] that has x as a root and splits in K. The minimal
polynomial M(X) of x over K H must divide F(X), and it too has x as a root.
By unique factorization in K[X], M(X) must split in K. Thus K/K H will be a
normal extension if it is shown that [K : K H ] < ∞.
    The element x has [K H (x) : K H ] = deg M(X) ≤ deg F(X) = |H |, and
the claim is that [K : K H ] ≤ |H |. Assuming the contrary, we would at
some point have an inequality [K H (x1 , . . . , xn ) : K H ] > |H | because every
element of K is algebraic over k. By the Theorem of the Primitive Element
(Theorem 9.34), K H (x1 , . . . , xn ) = K H (z) for some element z, and therefore
[K H (x1 , . . . , xn ) : K H ] = [K H (z) : K H ] ≤ |H |, contradiction. We conclude
that [K : K H ] ≤ |H |. From the previous paragraph, K/K H is a finite separable
normal extension.
    The definition of K H shows that H ⊆ Gal(K/K H ), and Proposition 9.35c
gives | Gal(K/K H )| = [K : K H ]. Putting these facts together with the inequality
[K : K H ] ≤ |H | from the previous paragraph, we have

                    |H | ≤ | Gal(K/K H )| = [K : K H ] ≤ |H |

with equality on the left only if H = Gal(K/K H ). Equality must hold throughout
the displayed line since the ends are equal, and therefore H = Gal(K/K H ). §


                 8. Fundamental Theorem of Galois Theory

We are now in a position to obtain the main result in Galois theory.

   Theorem 9.38 (Fundamental Theorem of Galois Theory). If K is a finite
normal separable extension of k, then there is a one-one inclusion-reversing
correspondence between the subgroups H of Gal(K/k) and the subfields L of K
that contain k, corresponding elements H and L being given by

                     L = KH         and       H = Gal(K/L).

   The effect of the theorem is to take an extremely difficult problem, namely
finding intermediate fields, and reduce it to a problem that is merely difficult,
namely finding the Galois group. For example the finiteness of Gal(K/k) implies
that there are only finitely many subgroups of Gal(K/k), and the theorem therefore
implies that there are only finitely many intermediate fields; this finiteness of the
number of intermediate fields is not so obvious without the theorem.
                           8. Fundamental Theorem of Galois Theory                           485

   As a reminder of the availability of Theorem 9.38, Proposition 9.35, and
Corollary 9.36, it is customary to refer to a finite normal separable extension
as a finite Galois extension.
   Before coming to the proof of the theorem, let us examine what the theorem
says for the examples in Section 6. In each case the field k is the field Q of
rationals. The extensions are separable because the characteristic is 0.

   EXAMPLES.
                  p
   (1a) K = Q( −1 ). This is the splitting field7 for X 2 + 1. Proposition
9.33 gives | Gal(K/Q)| = [K : Q] = 2. Thus Gal(K/Q) ∼           = C2 . There are no
nontrivial subgroups, and there are consequently no intermediate fields. We knew
this already since there cannot be any intermediate Q vector spaces between Q
and K. Thus the theorem tells us nothing new.
                p
   (1b) K = Q( 2 ). Similar remarks apply.
                 p
   (2) K = Q( 3 2 ). This extension is not normal, as a consequence of (b)
in Proposition 9.34A. (Namely X 3 − 2 has a root in K but does             p in K.)
                                                                  p not split
Theorem 9.38 does not apply to K. If we adjoin r to K with r 2 +( 3 2 )r +( 3 2 )2 =
0, we obtain the splitting field K0 for X 3 − 2 over Q. Then K0 is a normal
extension of Q, and the theorem applies. Since each element of Gal(K0 /Q)
permutes the three roots of X 3 − 2 and is determined by its effect on these roots,
Gal(K0 /Q) is isomorphic to a subgroup of the symmetric group S3 . The Galois
group Gal(K0 /Q) has order [K0 : Q] = 6 and hence is isomorphic to the whole
symmetric group S3 . The group S3 has three subgroups of order 2 and one
subgroup of order 3. Therefore K0 has three intermediate fields of degree 3 and
one of degree 2. The intermediate fields of degree 3 are the three fields generated
by Q and one of the three roots of X 3 − 2. The intermediate field of degree 2
corresponds to the alternating subgroup of order 3 and is the subfield generated
by Q and the cube roots of 1. It is the splitting field for X 2 + X + 1 over Q.
    (3) K = Q(r), where r is a root of X 3 − X − 13 . We know from Section 2
that X 3 − X − 13 is irreducible over Q and splits in K, and K by definition is
therefore normal. Proposition 9.33 tells us that Gal(K/Q) has order 3 and hence
is isomorphic to C3 . There are no nontrivial subgroups, and Theorem 9.38 tells
us that there are no intermediate fields. We could have seen in more elementary
fashion that there are no intermediate fields by using Corollary 9.7, since the
corollary tells us that the degree of an intermediate field would have to divide 3.
    (4) K = Q(e2π1/17 ). We have seen that [K : Q] = 16 and that Gal(K/Q) ∼      =
  × ∼
F17 = C16 . Let c be a generator of the cyclic Galois group. Let H2 = {1, c8 },
  7 It is customary to regard the algebraic closure of Q as a subfield of C, and thus there is no

ambiguity in referring to the splitting field.
486                              IX. Fields and Galois Theory

H4 = {1, c4 , c8 , c12 }, and H8 = {1, c2 , c4 , c6 , c8 , c10 , c12 , c14 }. Then put

                     L2 = K H2 ,        L4 = K H4 ,         L8 = K H8 .

The inclusions among our subgroups are

                         {1} ⊆ H2 ⊆ H4 ⊆ H8 ⊆ Gal(K/Q),

and the theorem says that the correspondence with intermediate fields reverses
inclusions. Then we have

                               K ⊇ L2 ⊇ L4 ⊇ L8 ⊇ Q.

Applying Corollary 9.36, we see that each of these subfields is a quadratic ex-
tension of the next-smaller one. Theorem 9.24 says that the members of K are
therefore constructible with straightedge and compass. Consequently a regular
17-gon is constructible with straightedge and compass. The constructibility or
nonconstructibility of regular n-gons for general n will be settled in similar fashion
in the next section. In Section 12 we return to the question of using Galois theory
to guide us through the actual steps of the construction when it is possible.

   PROOF OF THEOREM 9.38. The function L 7→ Gal(K/L) has domain the
set of all intermediate fields and range the set of all subgroups of Gal(K/k),
since an element in Gal(K/L) is necessarily in Gal(K/k). Each such exten-
sion K/L is separable by Proposition 9.32 and is normal by Proposition 9.34A.
Thus Proposition 9.35d applies to each K/L and shows that L = KGal(K/L) .
Consequently the function L 7→ Gal(K/L) is one-one. If H is a subgroup of
Gal(K/k), then Corollary 9.37 shows that L = K H is an intermediate field for
which H = Gal(K/L), and therefore the function L 7→ Gal(K/L) is onto.
   It is immediate from the definition of Galois group that L1 ⊆ L2 implies
Gal(K/L1 ) ⊇ Gal(K/L2 ), and it is immediate from the formula L = KGal(K/L)
that Gal(K/L1 ) ⊇ Gal(K/L2 ) implies L1 ⊆ L2 . This completes the proof. §

   Corollary 9.39. If K is a finite Galois extension of k and if L is a subfield of
K that contains k, then L is a normal extension of k if and only if Gal(K/L) is
a normal subgroup of Gal(K/k). In this case, the map Gal(K/k) → Gal(L/k)
given by restriction from K to L is a group homomorphism that descends to a
group isomorphism
                                 ±
                         Gal(K/k) Gal(K/L) ∼
                                           = Gal(L/k).
                            8. Fundamental Theorem of Galois Theory               487

   PROOF. Let L correspond to H = Gal(K/L) in Theorem 9.38, so that L = K H .
If ϕ is in Gal(K/k), then
                      −1
             Kϕ H ϕ        = {k ∈ K | ϕhϕ −1 (k) = k for all h ∈ H }
                           = {ϕ(k 0 ) ∈ K | ϕh(k 0 ) = ϕ(k 0 ) for all h ∈ H }
                           = {ϕ(k 0 ) ∈ K | h(k 0 ) = k 0 for all h ∈ H }
                           = ϕ(K H ) = ϕ(L).

Since the correspondence of Theorem 9.38 is one-one onto, ϕ H ϕ −1 = H if and
only if ϕ(L) = L. Therefore H is a normal subgroup of Gal(K/k) if and only if
ϕ(L) = L for all ϕ ∈ Gal(K/k).
   Now suppose that H is a normal subgroup of Gal(K/k). We have just seen that
ϕ(L) =Ø L for all ϕ ∈ Gal(K/k). Then each ϕ defines by restriction a member
ϕ = ϕ ØL of Gal(L/k), and ϕ 7→ ϕ is certainly a group homomorphism. The
kernel of ϕ 7→ ϕ is the subgroup of Gal(K/k) given by
                          ©              Ø Ø      ™
                           ϕ ∈ Gal(K/k) Ø ϕ ØL = 1 ,

              ± Gal(K/L). Thus ϕ 7→ ϕ descends to a one-one homomorphism
and this is just
of Gal(K/k) Gal(K/L) into Gal(L/k), and we have

                       | Gal(K/k)|/| Gal(K/L)| ≤ | Gal(L/k)|.

We make use of Corollary 9.7 relating degrees of extensions. Applying Proposi-
tion 9.35c to K/k and K/L, as well as Proposition 9.33 to L/k, we obtain
                                       ±
                      [L : k] = [K : k] [K : L]
                              = | Gal(K/k)|/| Gal(K/L)|
                              ≤ | Gal(L/k)| ≤ [L : k],

with equality at the first ≤ sign only if ϕ 7→ ϕ is onto Gal(L/k) and with equality
at the second ≤ sign only if L is the splitting field over k of the minimal polynomial
of a certain element ∞ of L. Equality must hold in both cases because the end
members of the display are equal, and we conclude that ϕ 7→ ϕ is onto and that
L/k is a normal extension.
    We are left with proving that if L/k is a normal extension, then H is a normal
subgroup of Gal(K/k). Thus let L/k be normal. In view of the conclusion
of the first paragraph of the proof, it is enough to prove that ϕ(L) = L for all
ϕ ∈ Gal(K/k). By definition of normal extension, L is the splitting field of some
polynomial F(X) in k[X]. We may assume that F(X) is monic. Let us write

               F(X) = (X − x1 ) · · · (X − xn )            with all x j in L.
488                           IX. Fields and Galois Theory

Applying a given member ϕ of Gal(K/k) to the coefficients, we obtain

                      F(X) = (X − ϕ(x1 )) · · · (X − ϕ(xn )),

and here the ϕ(x j )’s are known only to be in K. By unique factorization in K[X],
ϕ(xi ) = x j (i) for some j = j (i). Therefore ϕ(xi ) is in L for all i. Since L is the
splitting field of F(X) over k, L = k(x1 , . . . , xn ). Thus ϕ maps L into L.       §

   The examples of Galois groups given in Section 6 all involved fields that are
finite extensions of the rationals Q. As we shall see in Section 17, it is important for
the understanding of Galois groups of finite extensions of Q to be able to identify
Galois groups of finite extensions of finite fields. This matter is addressed in the
following proposition.

    Proposition 9.40. Let K be a finite extension of the finite field Fq , where
q = pa and p is prime, and suppose that [K : Fq ] = n. Then K is a Galois
extension of Fq , the Galois group Gal(K/Fq ) is cyclic of order n, and a generator
                                                          a
is the a th -power Frobenius automorphism x 7→ x q = x p .
                                                                       n
    PROOF. Theorem 9.14 shows that K is a splitting field for X q − X over F p .
                                     n
Hence it is a splitting field for X q − X over Fq , and K/Fq is a normal extension.
                      n
The polynomial X q − X has no multiple roots, and it follows that K/Fq is a
separable extension.
    Define ϕ by ϕ(x) = x q . Lemma 9.18 shows that ϕ is an automorphism of K.
Since every member of Fq× has order dividing q − 1, every nonzero element of Fq
is fixed by ϕ. The map ϕ certainly carries 0 to 0, and thus ϕ is in Gal(K/Fq ). By
a similar argument, ϕ n fixes every element of K, and hence ϕ n = 1. Corollary
4.27 shows that K× is cyclic, hence that there exists an element y in K× such
that y l 6= 1 for 1 ≤ l < q n − 1. This y has y l 6= y for 2 ≤ l ≤ q n − 1. Then
              k
ϕ k (y) = y q cannot be 1 for 1 ≤ k ≤ n − 1, and ϕ must have order exactly n.
This shows that ϕ generates a cyclic subgroup of order n in Gal(K/Fq ). Since
n is an upper bound for the order of Gal(K/Fq ) by Proposition 9.33, this cyclic
subgroup exhausts the Galois group.                                              §
   EXAMPLE. Suppose that we are given a polynomial with coefficients in F p
and we want to find the Galois group of a splitting field. Since there are efficient
computer programs for factoring the polynomial into irreducible polynomials,
let us take that factorization as done. The Galois group will be cyclic of some
order with generator the Frobenius automorphism x 7→ x p . For an irreducible
polynomial of degree n, a splitting field has degree n, and the smallest power of
x 7→ x p that gives the identity is the n th power. The conclusion is that the Galois
group is cyclic of order equal to the least common multiple of the degrees of the
irreducible constituents, a generator being the Frobenius automorphism.
                   9. Application to Constructibility of Regular Polygons          489

           9. Application to Constructibility of Regular Polygons

In this section we use Galois theory to give a proof of Theorem 9.25 concerning
the constructibility of regular n-gons. Let us recall the statement.

   THEOREM 9.25 (Gauss). A regular n-gon is constructible with straightedge
and compass if and only if n is the product of distinct Fermat primes and a power
of 2.
                                                                               N
    PROOF OF SUFFICIENCY. First suppose that n is a Fermat prime n = 22 + 1.
                                                                          N
Let K = Q(e2πi/n ). We saw in Section 5 that the degree [K : Q] is 22 , hence is
a power of 2. Furthermore we know that K is a separable extension of Q, being
of characteristic 0, and it is normal, being the splitting field for X n − 1 over Q.
                                                                              N
In Section 6 we saw that the Galois group Gal(K/Q) is cyclic of order 22 . Let
c be a generator of this group. For each integer k with 0 ≤ k ≤ 2 N , let H2k be
                                                                                2 N −k
the unique cyclic subgroup of Gal(K/Q) of order 2k . For this subgroup, c2
is a generator. Put L2k = K H2k . Then we have inclusions

       {1} ⊆ H2 ⊆ H22 ⊆ · · · H2k ⊆ · · · ⊆ H22 N −1 ⊆ H22 N = Gal(K/Q),

the index being 2 at each stage. Theorem 9.38 says that the correspondence
with intermediate fields reverses inclusions and that the degree of each consec-
utive extension of subfields matches the index of the corresponding consecutive
subgroups. The intermediate fields are therefore of the form

             K ⊇ L2 ⊇ L22 ⊇ · · · L2k ⊇ · · · ⊇ L22 N −1 ⊇ L22 N = Q,

and the degree in each case is 2. In view of the formula for the roots of a
quadratic polynomial, each extension is obtained by adjoining some square root.
By Theorem 9.24 the members of K are constructible with straightedge and
compass. In particular, e2πi/n is constructible, and a regular n-gon is constructible.
   Next suppose that e2πi/r and e2πi/s are both constructible and that GCD(r, s) =
1. Choose integers a and b with ar + bs = 1, so that as + br = rs1 . Then the
equality (e2πi/s )a (e2πi/r )b = e2πi/(rs) shows that e2πi/(rs) is constructible. This
proves the sufficiency for any product of distinct Fermat primes. Bisection of an
angle is always possible with straightedge and compass, as was observed in the
third paragraph of Section 5, and the proof of the sufficiency in Theorem 9.25 is
therefore complete.                                                                 §

    REMARKS. The above proof shows that the construction is possible, but it gives
little clue how to carry out the construction. We shall address this matter further
in Section 12.
490                         IX. Fields and Galois Theory

    We turn our attention to the necessity—that n has to be the product of distinct
Fermat primes and a power of 2 if a regular n-gon is constructible. For the moment
let n ∏ 1 be any integer. Let us consider the distinct n th roots of 1 in C, which
are ek2πi/n for 0 ≤ k < n. The order of each of these elements divides n, and the
order is exactly n if and only if GCD(k, n) = 1. In this case we say that ek2πi/n
is a primitive n th root of 1. Define the cyclotomic polynomial 8n (X) by
                                     Y
                       8n (X) =                 (X − ek2πi/n ).
                                  GCD(k,n)=1,
                                    0≤k<n


Each such polynomial is monic by inspection. The splitting field Q(e2πi/n ) in C
is called a cyclotomic field. Since the complex roots of X n − 1 are exactly the
numbers ek2πi/n , we have
                                         Y
                              Xn − 1 =          8d (X),
                                         d|n


the product being taken over the positive divisors d of n.

   Lemma 9.41. Each cyclotomic polynomial 8n (X) lies in Z[X], and the degree
of 8n (X) is ϕ(n), where ϕ is the Euler ϕ function defined just before Corollary
1.10.

   PROOF. We know that 8n (X) is in C[X], and we begin by showing by induction
on n that 8n (X) is in Q[X]. For n = 1, we have 81 [X] = X − 1, and the
assertion isQtrue. If it is true for all d with 1 ≤ d < n, then the formula
X n − 1 = d|n 8d (X) and induction show that X n − 1 = 8n (X)F(X) for some
F(X) in Q[X]. By the division algorithm, X n − 1 = F(X)Q(X) + R(X) for
polynomials Q(X) and R(X)°     in Q[X] with
                                          ¢ R(X) = 0 or deg R(X) < deg F(X).
Subtraction gives F(X) 8n (X) − Q(X) = −R(X) in C[X]. If R(X) is not
                      ¢ F(X) gives a contradiction. Therefore R(X) = 0 and
0, then° deg R(X) < deg
F(X) 8n (X) − Q(X) = 0. Since C[X] is an integral domain, 8n (X) = Q(X).
Thus 8n (X) is in Q[X], and the induction is complete.
                    Qis in Z[X], we again induct, the case nn= 1 being clear. The
   To see that 8n (X)
           n
formula X − 1 = d|n 8d (X) and induction show that X − 1 = 8n (X)F(X)
for some F(X) in Z[X]. Since 8n (X) is known to be in Q[X], Corollary 8.20c
shows that 8n (X) is in Z[X], and the induction is complete.                   §

   Lemma 9.42. Each cyclotomic polynomial 8n (X) is irreducible as a member
of Q[X].
                   9. Application to Constructibility of Regular Polygons          491

   PROOF. Let ≥ be a primitive n th root of 1, let p be a prime number not dividing
n, let F(X) be the minimal polynomial of ≥ over Q, and let G(X) be the minimal
polynomial of ≥ p . The main step is to show that F(X) = G(X).
   To carry out this step, we observe that F(≥ ) = G(≥ p ) = 0 and that F(X)
and G(X) must divide 8n (X). Arguing by contradiction, suppose that F(X) 6=
G(X). Then GCD(F, G) = 1 since F(X) and G(X) are irreducible over Q, and
therefore F(X)G(X) divides 8n (X). Hence we can write
                            X n − 1 = F(X)G(X)H (X),
and H (X) is a monic member of Z[X] by Lemma 9.41 and Corollary 8.20c.
Since ≥ is a root of G(X p ), we must have G(X p ) = F(X)M(X) for some
monic polynomial M(X) in Z[X]. We apply the substitution homomorphism to
Z[X] → F p [X] that carries X to X and reduces the coefficients modulo p; the
mapping on the coefficients will be denoted by a bar. Then we have
  X n − 1̄ = F(X)G(X)H (X)              and           G(X) p = G(X p ) = F(X)M(X),
the equality G(X) p = G(X p ) following from Lemma 9.18. If Q(X) is a prime
factor of F(X), then Q(X) divides G(X) p and therefore must divide G(X). So
Q(X)2 divides X n − 1̄. Therefore X n − 1̄ has multiple roots in its splitting field,
in contradiction to Corollary 9.17 and the fact that the derivative of X n − 1̄ is
nonzero at each nonzero member of F p (since GCD( p, n) = 1 by assumption).
We conclude that F(X) = G(X).
   Now suppose that r is a positive integer with GCD(r, n) = 1. Then we can
write r = p1 · · · pl with each p j not dividing n, and we see inductively that ≥ r has
F(X) as minimal polynomial. Thus F(X) has at least ϕ(n) roots. Since F(X)
divides 8n (X), we must have F(X) = 8n (X). Therefore 8n (X) is irreducible
over Q.                                                                              §

   PROOF OF NECESSITY IN THEOREM 9.25. Theorem 9.24 shows that the degree
[Q(e2πi/n ) : Q] must be a power of 2 if a regular n-gon is constructible. Since
e2πi/n is a root of 8n (X) and since Lemma 9.42 shows 8n (X) to be irreducible
over Q, 8n (X) is the minimal polynomial of e2πi/n over Q. By Lemma 9.41 the
degree in question is given by [Q(e2πi/n ) : Q] = ϕ(n), where ϕ is the Euler ϕ
function. Corollary 1.10 shows that if n = p1k1 · · · prkr is a prime factorization of
n into distinct prime powers with each k j > 0, then
                                        r
                                        Q      k −1
                             ϕ(n) =          pj j     ( p j − 1).
                                       j=1

For constructibility this must be a power of 2. Then each p j dividing n must be 1
more than a power of 2, i.e., must be 2 or a Fermat prime, and the only p j allowed
to have p2j dividing n is p j = 2.                                               §
492                               IX. Fields and Galois Theory

      10. Application to Proving the Fundamental Theorem of Algebra

In this section we use Galois theory to give a proof of the Fundamental Theorem
of Algebra. Let us recall the statement.

  THEOREM 1.18 (Fundamental Theorem of Algebra). Any polynomial in C[X]
with degree ∏ 1 has at least one root.

   We begin with a lemma that handles three easy special cases.

   Lemma 9.43. There are no finite extensions of R of odd degree greater than 1,
the only extension of R of degree 2 up to R isomorphism is C, and there are no
finite extensions of C of degree 2.
   PROOF. If K is a finite extension of R of odd degree and if x is in K, then
[R(x) : R] is odd, and consequently the minimal polynomial F(X) of x over
R is irreducible of odd degree. By Proposition 1.20, which is derived from the
Intermediate Value Theorem of Section A3 of the appendix, F(X) has at least
one root in R. Therefore F(X) has degree 1, and x is in R.
   If F(X) is an irreducible polynomial in R[X] of degree 2, then F(X) splits in
C by the quadratic formula, and hence the only extension of R of degree 2 is C,
up to R isomorphism, by the uniqueness of splitting fields (Theorem 9.13).
   Let G(X) = X 2 + bX + c be a polynomial in C[X] of degree 2. Then G(X)
has a root x in C given by the quadratic formula since every member of C has
a square root8 in C, and G(X) cannot be irreducible. Since any finite extension
of C of degree 2 would have to be of the form C(x), with x equal to a root of an
irreducible quadratic polynomial over C, there can be no such extension.      §

    PROOF OF THEOREM 1.18. First let us show that every irreducible member
F(X) of R[X] splits over C. Let K be a splitting field for F(X). Say that
[K : R] = 2m N with N odd. Then K is a Galois extension of R, and | Gal(K/R)|
= 2m N . By the Sylow Theorems (particularly Theorem 4.59a), let H be a Sylow
2-subgroup of Gal(K/R). This H has |H | = 2m . The field L = K H that
corresponds to H under Theorem 9.38 has [L : R] = N with N odd, and the
first conclusion of Lemma 9.43 shows that N = 1. Thus | Gal(K/R)| = 2m .
Corollary 4.40 shows that Gal(K/R) has nested subgroups of all orders 2m−k
with 0 ≤ k ≤ m, and Theorem 9.38 says that the corresponding fixed fields are
nested and have respective degrees 2k with 0 ≤ k ≤ m. The extension field of
R for k = 1 is necessarily C by Lemma 9.43, and Lemma 9.43 shows that there
   8 To see that every member of C has a square root in C, let c + di be given with c and d real and
                                                            p                           p
with d 6= 0. Let a and b be real numbers with a 2 = 12 (c + c2 + d 2 ), b2 = 12 (−c + c2 + d 2 ),
                                    2
and sgn(ab) = sgn d. Then (a + bi) = c + di.
        11. Application to Unsolvability of Polynomial Equations with Nonsolvable Group          493

are no quadratic extensions of C. Therefore m = 0 or m = 1, and the possible
splitting fields for F(X) are R and C in the two cases.
   To complete the proof, suppose that K is a finite algebraic extension of C of
degree n. Then K is a finite algebraic extension of R of degree 2n. The Theorem
of the Primitive Element allows us to write K = R(x) for some x ∈ K, and
the minimal polynomial of x over R necessarily has degree 2n. The previous
paragraph shows that this polynomial splits in C. Thus x is in C, and K = C.
This completes the proof.                                                     §


                  11. Application to Unsolvability of Polynomial
                    Equations with Nonsolvable Galois Group

The quadratic formula for finding the roots of a quadratic polynomial has in
principle been known since the time of the Babylonians about 400 B.C.9 The
corresponding problem of finding roots of cubics was unsolved until the sixteenth
century, and Cardan’s formula was discovered at that time. The original formula
assumes real coefficients and was in two parts, a first case corresponding to
what we now view as one real root and two complex roots, the second case
corresponding to what we view as three real roots.10 There is a similar formula,
but more complicated, for solving quartics. Further centuries passed with no
progress on finding a corresponding formula for the roots of a polynomial of
degree 5 or higher. The introduction of Galois theory in the early nineteenth
century made it possible to prove a surprising negative statement about all degrees
beyond 4.
   Suppose that we are given a polynomial equation with coefficients in the field
Q or a more general field k of characteristic 0. In this section we use Galois
theory to address the question whether the roots of the equation in a splitting field
can be expressed in terms of k and the adjunction of finitely many n th roots to the
field, for various values of n. For the moment let us say in this case that the roots
are “expressible in terms of the members of k and radicals.” We shall make this
notion more precise shortly.
   Recall from Section IV.8 that with a finite group G, we can find a strictly
decreasing sequence of subgroups starting with G and ending with {1} such
   9 The Babylonians did not actually have equations but had an algorithmic method that amounted

to completing the square.
    10 Cardan’s name was Girolamo Cardano. The solution in the first case of the cubic seems to

have been discovered by Scipione dal Ferro and later by Nicolo Tartaglia. Dal Ferro died in 1526
and passed the secret method to his student Antonio Fior. In 1535 Fior engaged in a public contest
with Tartaglia at solving cubics, and he lost. Cardano wheedled the solution method in the first case
from Tartaglia, published it in 1539, and discovered and published the solution in the second case.
Cardano’s student Lodovico Ferrari discovered how to solve quartics, and Cardano published that
solution as well. See “St. Andrews” in the Selected References for more information.
494                               IX. Fields and Galois Theory

that each subgroup is normal in the next larger one and each quotient group is
simple. Such a series was defined to be a composition series for G. The Jordan–
Hölder Theorem (Corollary 4.50) says that the respective consecutive quotients
are isomorphic for any two composition series, apart from the order in which they
appear. We define the finite group G to be solvable if each of the consecutive
quotients is cyclic of prime order, rather than nonabelian. It is enough that the
group have a normal series for which each of the consecutive quotients is abelian.
   Examples of solvable and nonsolvable groups are obtainable from the calcula-
tions in Section IV.8: abelian groups and groups of prime-power order are always
solvable, the symmetric group S4 and each of its subgroups are solvable, and the
symmetric group S5 is not solvable since a composition series is S5 ⊇ A5 ⊇ {1}
and the group A5 is simple (Theorem 4.47).
   Modulo a precise definition for a field k of the words “expressible in terms of
the members of k and radicals,” the answer to our main question is as follows.
   Theorem 9.44 (Abel, Galois).11 Let k be a field of characteristic 0, let F(X)
be in k[X], and let K be a splitting field of F(X) over k. Then the roots of F(X)
are expressible in terms of the members of k and radicals if and only if the group
Gal(K/k) is solvable.
   EXAMPLE. With k = Q, let F(X) be the polynomial F(X) = X 5 − 5X + 1 in
Q[X]. We shall show that
     (i) F(X) is irreducible over Q,
    (ii) F(X) has three roots in R and one pair of conjugate complex roots in C,
   (iii) the splitting field K over Q of any polynomial of degree 5 for which (i)
         and (ii) hold has Galois group with Gal(K/Q) ∼   = S5 .
We know that from Theorem 4.47 that S5 is not solvable, and Theorem 9.44
therefore allows us to conclude that the roots of X 5 − 5X + 1 are not expressible
in terms of the members of Q and radicals.
   To prove (i), we apply Eisenstein’s criterion (Corollary 8.22) to the polynomial
F(X − 1) = X 5 − 5X 4 + 10X 3 − 10X 2 + 5 and to the prime p = 5, and the
irreducibility is immediate.
   To prove (ii), we observe that F(−2) < 0, F(0) > 0, F(1) < 0, F(2) > 0.
Applying the Intermediate Value Theorem (Section A3 of the appendix), we see
that there are at least three roots in R. Since F 0 (X) = 5(X 4 − 1) has exactly the
two roots ±1 in R, F(X) has at most three roots in R by an application of the
Mean Value Theorem.
   To prove (iii), label the roots 1, 2, 3, 4, 5 with 1 and 2 denoting the nonreal
roots. Each member of the Galois group permutes the roots and is determined
   11 Abel proved that there is no general solution via radicals that gives the roots of polynomials
of degree 5. Galois found the present theorem, which shows how to decide the question for each
individual polynomial of degree 5.
       11. Application to Unsolvability of Polynomial Equations with Nonsolvable Group   495

by its effect on the roots. Thus Gal(K/Q) may be regarded as a subgroup of S5 .
Since F(X) is irreducible over Q, 5 divides [K : Q] and 5 divides | Gal(K/Q)|.
By the Sylow Theorems, Gal(K/Q) contains an element of order 5, hence a 5-
cycle. Some power of this 5-cycle carries root 1 to root 2. So we may assume
that the 5-cycle is (1 2 3 4 5). Also, Gal(K/Q) contains complex conjugation,
which acts as (1 2). Then Gal(K/Q) contains

                    (1 2 3 4 5)(1 2)(1 2 3 4 5)−1 = (2 3),
                    (1 2 3 4 5)(2 3)(1 2 3 4 5)−1 = (3 4),
                    (1 2 3 4 5)(3 4)(1 2 3 4 5)−1 = (4 5).

Since the set {(1 2), (2 3), (3 4), (4 5)} of transpositions is easily shown from
Corollary 1.22 to generate S5 , Gal(K/Q) = S5 .

    Let K0 be a finite extension of the given field k. A root tower for K0 over k is
a finite sequence of extensions

                      k = K00 ⊆ K01 ⊆ · · · ⊆ Kl−1
                                               0
                                                   ⊆ Kl0 = K0

such that for each i with 0 ≤ i ≤ l − 1, there is a prime number n i > 1 and there
is an element ri in Ki+1
                       0
                            with ai = rini in Ki0 and ri not in Ki0 . Then it follows that
ri is not in Ki for any k with 0 < k < n i .
  k            0
                                                                               np
    (If we write ai = rini , then we might think of writing Ki+1   0
                                                                       = Ki0 ( i a i ), but
this formulation ispless precise at the moment since it does not specify precisely
                   n
which choice of i a i is to be used.)
    With “root tower” now well defined, we can make a precise definition and
thereby complete the precise formulation of Theorem 9.44. Let k be the given
field of characteristic 0, let F(X) be in k[X], and let K be a splitting field of F(X)
over k. We say that the roots of F(X) are expressible in terms of members of
k and radicals if there exists some finite extension K0 of K having a root tower
over k.
    The statement of Theorem 9.44 is now completely precise, and the remainder
of the section will be devoted to the proof of one direction of the theorem: if the
roots are expressible in terms of members of k and radicals, then the Galois group
is solvable. The proof of the converse direction of the theorem is postponed to
Section 13. We begin with a lemma.

   Lemma 9.45. Let k be a field of any characteristic, and let p be a prime
number. If a is a member of k such that X p − a has no root in k, then X p − a is
irreducible in k.
496                                 IX. Fields and Galois Theory

   PROOF. First suppose that p is different from the characteristic. Let L be a
splitting field for X p − a. The derivative of X p − a, evaluated at any root of
X p − a in L, is nonzero, and Corollary 9.17 shows that X p − a splits as the
product of p distinct linear factors in L. The quotient of any two roots of X p − a
is a pth root of 1. Fixing one of these two roots of X p − a and letting the other
vary, we obtain p distinct pth roots of 1. Thus L contains all p of the pth roots
of 1. Proposition 4.26 shows that the group of pth roots of 1 is cyclic. Let ≥ be a
generator. If a 1/ p denotes one of the roots of X p − a in L, then the set of all the
roots is given by {a 1/ p ≥ k | 0 ≤ k ≤ p − 1}.
   Now suppose that X p − a has a nontrivial factorization X p − a = F(X)G(X)
in k[X]. Possibly by adjusting the leading coefficients of F(X) and G(X), we
may assume that F(X) and G(X) are both monic. Unique factorization in L[X]
then implies that there is a nonempty subset S of {k | 0 ≤ k ≤ p − 1} with a
nonempty complement S c such that
                 Q                                          Q
       F(X) =         (X − ≥ k a 1/ p )  and     G(X) =         (X − ≥ k a 1/ p ).
                  k∈S                                              k∈S c

If S has m elements, then the constant term of F(X) is (−a 1/ p )m ω, where ω
is some pth root of 1. Thus x = (a 1/ p )m ω is in k. Since GCD(m, p) = 1,
we can choose integers c and d with cm + dp = 1. Since x is in k, so is
x c a d = (a 1/ p )mc+dp ωc = a 1/ p ωc . But a 1/ p ωc is a root of X p −a, in contradiction
to the hypothesis that no root of X p − a lies in k. Hence X p − a is irreducible.
    If p equals the characteristic of k, then Lemma 9.18 gives the factorization
X p −a = (X −a 1/ p ) p , where a 1/ p is one root of X p −a in K. Then we can argue
as above except that ≥ and ω are to be replaced by 1 throughout. This completes
the proof of the lemma.                                                                    §

   PROOF OF NECESSITY IN THEOREM 9.44 THAT Gal(K/k) BE SOLVABLE. We
are to prove that if some finite extension K0 of K has a root tower over k, then
Gal(K/k) is solvable.
   Step 1. We enlarge each field in the given root tower to obtain a root tower

                        k ⊆ K000 ⊆ K001 ⊆ · · · ⊆ Kl−1
                                                   00
                                                       ⊆ Kl00 = K00

of a finite extension K00 of K0 in such a way that K000 is the normal extension of k
obtained by adjoining all n th roots of 1 for a suitably large n and such that each
Ki+1
  00
       is the normal extension of Ki00 for 0 ≤ i ≤ l − 1 obtained by adjoining all n ith
roots of the member ai of Ki0 . Using Theorem 9.22, choose an algebraic closure
K0 of K0 . Let n be the product of the integers n 0 , n 1 , . . . , nl−1 . Let ≥1 , . . . , ≥n−1
be the n th roots of 1 in K0 other than 1 itself, define subfields of K0 by

                        Ki00 = Ki0 (≥1 , . . . , ≥n−1 )   for 0 ≤ i ≤ l,
          11. Application to Unsolvability of Polynomial Equations with Nonsolvable Group       497

and put K00 = Kl0 . The field K000 is a splitting field for X n −1 over k and is therefore
a normal extension. The field Ki+1    00
                                          is given by Ki+1    00
                                                                    = Ki00 (ri ), where ri is a root
in Ki+1 of the polynomial X − ai in Ki [X]. Here n i is prime. Lemma 9.45
        00                         ni               00

shows that either ri is in Ki00 [X] or X ni − ai is irreducible in Ki00 [X]. In the first
case, Ki+1 00
                 = Ki00 , and we have a normal extension. In the second case, Ki+1            00
                                                                                                  is
a splitting field for X − ai over Ki because it is generated by Ki and one root
                            ni             00                                       00

of X ni − ai and because all n ith roots of 1 already lie in K000 ; thus again we have a
normal extension.
     Step 2. The Galois group of K000 over k is abelian. In fact, Proposition 4.26
shows that the group of n th roots of 1 in K000 is cyclic. Let ≥ be a generator,            Ø and
                 k n−1
let U = {≥ }k=0 . The map of Gal(K0 /k) into Aut U given by ϕ 7→ ϕ ØU is a
                                               00

one-one homomorphism, and Aut U is isomorphic to (Z/nZ)× . Since (Z/nZ)×
is abelian, it follows that Gal(K000 /k) is abelian.
     Step 3. The Galois group of Ki+1       00
                                                  over Ki00 is trivial or is cyclic of order
n i . In fact, the Galois group is trivial if Ki+1     00
                                                             = Ki00 . The contrary case is that
[Ki+1 : Ki ] = n i , and then Gal(Ki+1 /Ki ) has order n i , which is prime. Every
     00       00                         00        00
                                                          00
group of order n i is cyclic, and hence Gal(Ki+1             /Ki00 ) is cyclic.
     Step 4. We extend the root tower to a larger field L ⊇ K00 that is a normal
extension of k. The resulting root tower of L will be written as

                k ⊆ L0 = K000 ⊆ L1 = K001 ⊆ · · ·
                   ⊆ Lk−1 = Kl−1
                             00
                                 ⊆ Ll = K00 ⊆ Ll+1 ⊆ · · · ⊆ Lt = L.

As it is, we cannot say that K00 is the splitting field over k for the product of the
minimal polynomials used in Step 1, because the elements ai are not assumed to
lie in k. To adjust the tower to correct this problem, write K00 as

                       K00 = k(r0 , r1 , . . . , rl−1 , ≥ ) = k(x0 , . . . , xl ),

with ≥ as in Step 2. Here r0 , . . . , rl−1 are the given elements that define the
original root tower, and we define xl = ≥ and x j = r j for 0 ≤ j < l. Since K00 is
a finite extension of k, each x j has a minimal polynomial G j (X) over k. Define
             Q
G(X) = lj=0 G j (X), and let L be the splitting field of G(X) in the algebraic
closure K0 . The field L is a normal extension of k. The roots of G(X) are the
members of L that are roots of some G j (X). Each x j is a root of its own G j (X).
If x j0 is another root of G j (X), then there is a k isomorphism of k(x j ) onto k(x j0 ),
and we know by the uniqueness of splitting fields (Theorem 9.130 )12 that this
   12 The   theorem is to be applied to σ : k(x j ) → k(x j0 ) with F(X) = F σ (X) = G(X) and with
L0 = L.
498                                   IX. Fields and Galois Theory

extends to a k isomorphism of L onto L. Hence to each root θ of G(X) in L
corresponds some x j and some ϕ ∈ Gal(K/k) with ϕ(x j ) = θ. Thus
                      °                                     ¢
                 L = k {ϕ(x j ) | 0 ≤ j ≤ l and ϕ ∈ Gal(L/k} .

For any ϕ in Gal(L/k) and any j ≤ l − 1, the element ϕ(x j ) of L satisfies
                                                         n
                                  (ϕ(x j ))n j = ϕ(x j j ) = ϕ(a j ),

and the element on the right is in ϕ(K j00 ). Any element ϕ(≥ ) is an n th root of 1
and hence is already in K000 ; such elements are redundant for ϕ 6= 1. Enumerate
Gal(L/k) as ϕ1 , . . . , ϕs with ϕ1 = 1. The tower for K00 is to be continued with
the fields obtained by adjoining one at a time the elements

      ϕ2 (r0 ), . . . , ϕ2 (rl−1 ), ϕ3 (r0 ), . . . , ϕ3 (rl−1 ), . . . , ϕs (r0 ), . . . , ϕs (rl−1 ).

The final field is L, and then we have an enlarged tower as asserted.
   Step 5. Gal(L/k) is a solvable group. In fact, first we prove by induction
downward on i that Gal(L/Li ) is solvable, the case i = t being the case of
the trivial group. Let i < t be given. We have arranged that Li+1 is a normal
extension of Li . Since L is normal over all the smaller
                                                     ± fields by Step 4, Corollary
9.39 therefore gives Gal(Li+1 /Li ) ∼  = Gal(L/Li ) Gal(L/Li+1 ). The group on
the left side is cyclic by Step 3 or the analogous proof with some r j replaced by
a suitable ϕ(r j ), and thus a normal series with abelian quotients for Gal(L/Li+1 )
may be extended by including the term Gal(L/Li ), and the result is still a normal
series with abelian quotients. Thus Gal(L/Li ) is solvable. This completes the
induction and shows that Gal(L/L0 ) is±solvable. To complete the proof we use the
isomorphism Gal(L0 /k) ∼    = Gal(L/k) Gal(L/L0 ) given by Corollary 9.39. The
group on the left side is abelian by Step 2, and thus a normal series with abelian
quotients for Gal(L/L0 ) may be extended by including the term Gal(L/k), and the
result is still a normal series with abelian quotients. Thus Gal(L/k) is solvable.
   Step 6. Gal(K/k) is a solvable group. We have L ⊇ K ⊇ k with L/k normal by
Step 4 and with K/k normal since K is a splitting field of F(X) over±k. Applying
Corollary 9.39, we obtain an isomorphism Gal(K/k) ∼        = Gal(L/k) Gal(L/K).
Then Step 6 will follow from Step 5 if it is shown that any homomorphic im-
age of a solvable group is solvable. Thus let G be a solvable group, and let
ϕ : G → H be an onto homomorphism. Write G = G 1 ⊇ · · · ⊇ G m = {1}
with abelian quotients, and define Hi = ϕ(G i ). Passage to the quotient gives
us a homomorphism ϕi carrying G i onto Hi /Hi+1 . Since ϕ(G i+1 ) ⊆ Hi+1 ,
ϕ induces a homomorphism ϕ i of G i /G i+1 onto Hi /Hi+1 . As the image of
an abelian group under a homomorphism, Hi /Hi+1 is abelian. Therefore H is
solvable. This completes the proof.                                              §
                            12. Construction of Regular Polygons                         499

                      12. Construction of Regular Polygons

Theorem 9.25 proved the constructibility of regular n-gons when n is the product
of a power of 2 and distinct Fermat primes, but it gave little clue how to carry
out the construction. In this section we supply enough further detail so that one
can actually carry out the construction. It is enough to handle the case that n is a
                      N
Fermat prime, n = 22 + 1, and we shall suppose that n is a prime of this form.
   Let ≥ = e2πi/n . The field of interest is Q(≥ ), with [Q(≥ ) : Q] = n − 1. The
usual basis of Q(≥ ) over Q is {1, ≥, ≥ 2 , . . . , ≥ n−2 }, but we shall use the basis

                                     {≥, ≥ 2 , ≥ 3 , . . . , ≥ n−1 }

instead, in order to identify the Galois group Gal(Q(≥ )/Q) more readily with F×           n,
where Fn = Z/nZ is the field of n elements. In more detail we associate the addi-
tive group of Fn with the additive group of exponents of the members of the cyclic
group {1, ≥, ≥ 2 , ≥ 3 , . . . , ≥ n−1 }, and members of the Galois group correspond to the
various multiplications of these exponents by F×         n = {1, 2, . . . , n − 1}. The group
Fn is known to be cyclic of order n − 1, and thus the isomorphic Galois group
  ×

is cyclic. If a generator σ of the Galois group is to correspond to multiplication
by a generator g of F×                      s
                            n , then σ (≥ ) = ≥
                                                 gs
                                                    for all s. With the prime n of the form
  2N
2 + 1, let us note for the sake of completeness why we can always take g = 3.

  Lemma 9.46. The number 3 is a generator of F×
                                              n when n is prime of the form
 2N
2 + 1 with N > 0.
   REMARKS. We verified this assertion for n = 17 in Section 6, and in principle
one could verify the lemma in any particular case in the same way. Here is a
general argument using the law of quadratic reciprocity, whose full statement and
proof will be given in Chapter I of Advanced Algebra. For a prime number n
that is congruent to 1 modulo 4, quadratic reciprocity implies that 3 is a square
modulo n if and only if n is a square modulo 3. Since
                N             N −1               N −2                  1    1
             22 − 1 = (22            + 1)(22            + 1) · · · (22 + 1)(22 − 1)
       1                               N
and 22 − 1 = 3, 3 divides 22 − 1. Thus n is congruent to 2 modulo 3, n is
not a square modulo 3, and 3 is not a square modulo n. The nonsquares modulo
       N
n = 22 + 1 are exactly the generators of Fn× , and therefore 3 is a generator.

   Taking Lemma 9.46 into account, we suppose for the remainder of this section
that the generator σ of the Galois group corresponds to multiplication of exponents
of ≥ by 3. Then σ (≥ ) = ≥ 3 and σ (≥ s ) = ≥ 3s . These formulas and Q linearity tell
us explicitly how σ operates on all of Q(≥ ).
500                                 IX. Fields and Galois Theory

    The fixed fields that arise within Q(≥ ) correspond to subgroups of the group
                                             N
Gal(Q(≥ )/Q) ∼   = {σ j | 0 ≤ j < 22 }, and there is one for each power of 2 from
          N                                                                    N
20 to 22 . Fix attention on the subgroup Hl of order l, and write 22 = kl, with
k and l being powers of 2. A generator of this subgroup is σ k , and the subgroup
is Hl = {1, σ k , σ 2k , . . . , σ (l−1)k }. Let Kl be the fixed field of this subgroup, or
equivalently of its generator σ k ; this has dimension k over Q.
    We shall determine a basis of Kl over Q. Since σ (≥ s ) = ≥ 3s , we have σ k (≥ s ) =
   k
≥ 3 s . For 0 ≤ r ≤ k − 1, the k elements
                                r        r+k           r+2k                r+k(l−1)
                       ηr = ≥ 3 + ≥ 3          + ≥3           + ··· + ≥3
are linearly independent over Q because they involve disjoint sets of basis vectors
of Q(≥ ) as r varies. The computation
                               ° r       r+k   r+2k          r+k(l−1) ¢
                σ k (ηr ) = σ k ≥ 3 + ≥ 3 + ≥ 3 + · · · + ≥ 3
                               r+k         r+2k           r+3k                r+kl
                         = ≥3         + ≥3        + ≥3           + ··· + ≥3
                               r         r+k           r+2k             r+k(l−1)
                         = ≥3 + ≥3             + ≥3           + ··· + ≥3
                         = ηr
shows that each of these vectors is in Kl . Hence {η0 , . . . , ηk−1 } is a basis of
Kl over Q. The elements of this basis are called the periods of l terms of the
cyclotomic field.
                                                      N
   The extreme cases for the periods are (k, l) = (22 , 1), for which 0 ≤ r ≤
  N                 r                     N
22 − 1 with ηr = ≥ 3 , and (k, l) = (1, 22 ), for which r = 0 with
                                                 N
            0      1       2                   22 −1
   η0 = ≥ 3 + ≥ 3 + ≥ 3 + · · · + ≥ 3                  = ≥ + ≥ 2 + ≥ 3 + · · · + ≥ n−1 = −1.
   Two facts enter into determining how to write ≥ in terms of rationals and square
roots. The first is that at stage k for k ∏ 2, the sum of certain pairs of ηr ’s is
an η for stage k − 1. The second is that the product of two ηr ’s at stage k is an
integer combination of η’s from the same stage and that the sum formulas express
this combination in terms of η’s from earlier stages. The result is that at the k th
stage we obtain expressions for the sum and product of two ηr ’s in terms of η’s
from earlier stages. Therefore the two ηr ’s at stage k are the roots of a quadratic
equation whose coefficients involve η’s from earlier stages. Consequently we
can compute the ηr ’s explicitly by induction on k. To proceed further, we need
to know the formula for the product of two ηr ’s, which is due to Gauss.
   To multiply two ηr ’s, we need to multiply various powers of ≥ , and the expo-
nents get added in the process. This addition is not readily compatible with terms
        r       s
like ≥ 3 and ≥ 3 , and for that reason Gauss introduced new notation. Define
                               k      2k             k(l−1)   P t3kv
              η(t) = ≥ t + ≥ t3 + ≥ t3 + · · · + ≥ t3       =    ≥
                                                                            v mod l
                            12. Construction of Regular Polygons                                501

for 0 ≤ t ≤ n − 1. Then η(0) = l, and for 0 < t ≤ n − 1, η(t) is the ηr in which
≥ t occurs. Gauss’s product formula is given by
                       P ° P s3ku +t3kv ¢
           η(s) η(t) =            ≥
                       u mod l       v mod l
                        P ° P                            ku +t3k(u+w)     ¢
                   =                           ≥ s3                           with v 7→ u + w
                       u mod l       w mod l
                         P ° P                              kw )3ku   ¢
                   =                           ≥ (s+t3
                       w mod l       u mod l
                         P           (s+t3kw )
                   =             η               .
                       w mod l

In words, this says that to multiply two η’s, we add the η’s for the exponents
obtained by multiplying the first term of η(s) by all the terms of η(t) .
   At this point it is more illuminating to work some examples than to try for a
general result.
                                                     N
   EXAMPLE 1. n = 5, N = 1, 22 = 4. The relevant pairs (k, l) to study in
sequence are (k, l) = (1, 4), (2, 2), (4, 1), and the case (k, l) = (1, 4) is trivial
                               P         s
since the only subscripted η is 3s=0 ≥ 3 = −1.




                                                                              °   ¢
 FIGURE 9.3. Construction of a regular pentagon. The circle with center 12 , 14
                                    °       ¢
   and radius 14 meets the line from 12 , 14 to the origin at a point at distance
                           cos(2π/5) from the origin.

  For k = 2, i.e., for the case that there are 2 periods of 2 terms each, we go
back to the definition of the η’s and find that
                                         0+2·0              0+2·1
                           η0 = ≥ 3              + ≥3               = ≥ 1 + ≥ 4,
                                         1+2·0              1+2·1
                           η1 = ≥ 3              + ≥3               = ≥ 3 + ≥ 2.
502                              IX. Fields and Galois Theory

We form those sums of pairs of η’s that yield an η from the previous step. Here
there is only one pair, and the sum is given by
                                        η0 + η1 = −1.
Next we form the elements η(t) , remembering that for t > 0, η(t) is the ηr in
which ≥ t occurs. Then
         η(0) = 2,       η(1) = η0 ,     η(2) = η1 ,     η(3) = η1 ,     η(4) = η0 .
We apply Gauss’s product formula to compute the product of the two η’s whose
sum we have identified. The formula gives
                 η0 η1 = η(1) η(2) = η(4) + η(3) = η0 + η1 = −1,
the second equality following since the rule for the indices is to extract a power
of ≥ appearing in η(1) and add that index to all the powers of ≥ appearing in η(2) .
Since η0 and η1 have sum −1 and product −1, they are the roots of the quadratic
equation                                                  p
                  x 2 + x − 1 = 0,       namely 12 (−1 ± 5 ).
Deciding which root is η0 and which is η1 involves looking at signs. The two
roots of the quadratic equation are of opposite sign because the constant term of
the quadratic equation is negative. Since η0 = ≥ + ≥ −1 = e2πi/5 + e−2πi/5 =
2 cos(2π/5) is positive, we obtain
                             p                                p
              η0 = 12 (−1 + 5 )        and      η1 = 12 (−1 − 5 ).
   The computation can in principle stop here, since knowing cos(2π/5) gives
us sin(2π/5) and therefore e2πi/5 . See Figure 9.3. But it is instructive to carry
out the algorithm anyway. We are thus to treat k = 4. The periods of 1 term are
                       ξ0 = ≥,    ξ1 = ≥ 3 ,    ξ2 = ≥ 4 ,      ξ3 = ≥ 2 .
The corresponding objects with superscripts are
          ξ (0) = 1,     ξ (1) = ξ0 ,    ξ (2) = ξ3 ,    ξ (3) = ξ1 ,    ξ (4) = ξ2 .
The relevant sums of pairs are
                                        ξ0 + ξ2 = η0 ,
                                        ξ1 + ξ3 = ξ1 .
We again use Gauss’s product formula, and this time we obtain
                          ξ0 ξ2 = ξ (1) ξ (4) = ξ (5) = ξ (0) = 1.
Hence ξ0 and ξ2 are the roots of the quadratic equation
                                                 p     q    ° −1+p5
                                             −1+ 5
                                               2   ± i  4 −     2   )2
         y 2 − η0 y + 1 = 0,      namely                               .
                                                        2
The root y involving the plus sign is e2πi/5 .
                                 12. Construction of Regular Polygons                                 503

                                                     N
   EXAMPLE 2.13 n = 17, N = 2, 22 = 16. The relevant pairs (k, l) have
kl = 16, and the case (k, l) = (1, 16) is trivial since the only subscripted η is
P15 3s
   s=0 ≥  = −1.
   For k = 2, the 2 periods have 8 terms each, and
              0+2·0      0+2·1       0+2·2       0+2·3      0+2·4       0+2·5      0+2·6      0+2·7
   η0 = ≥ 3           + ≥3       + ≥3        + ≥3        + ≥3       + ≥3        + ≥3       + ≥3
      = ≥ 1 + ≥ 9 + ≥ 13 + ≥ 15 + ≥ 16 + ≥ 8 + ≥ 4 + ≥ 2 ,
              1+2·0      1+2·1       1+2·2       1+2·3      1+2·4       1+2·5      1+2·6      1+2·7
   η1 = ≥ 3           + ≥3       + ≥3        + ≥3        + ≥3       + ≥3        + ≥3       + ≥3
      = ≥ 3 + ≥ 10 + ≥ 5 + ≥ 11 + ≥ 14 + ≥ 7 + ≥ 12 + ≥ 6 .
We form those sums of pairs of η’s that yield an η from the previous step. Here
there is only one pair, and the sum is given by
                                             η0 + η1 = −1.
Next we form the elements η(t) , remembering that for t > 0, η(t) is the ηr in
which ≥ t occurs. Then η(0) = 2,
           η(1) = η(9) = η(13) = η(15) = η(16) = η(8) = η(4) = η(2) = η0 ,
           η(3) = η(10) = η(5) = η(11) = η(14) = η(7) = η(12) = η(6) = η1 .

To compute η0 η1 by means of Gauss’s product formula, we use η0 = η(1) and
η1 = η(3) . Then
   η0 η1 = η(1) η(3) = η(4) + η(11) + η(6) + η(12) + η(15) + η(8) + η(13) + η(7) ,
the indices on the right side being the indices for η1 plus one. Resubstituting in
terms of η0 and η1 , we obtain
                                    η0 η1 = 4η0 + 4η1 = −4.
Therefore η0 and η1 are the roots of the quadratic equation
                                                         p
                 x 2 + x − 4 = 0,       namely 12 (−1 ± 17 ).
Deciding which root is η0 and which is η1 involves looking at signs. The two
roots of the quadratic equation are of opposite sign. Since
      η0 = (≥ 1 + ≥ −1 ) + (≥ 2 + ≥ −2 ) + (≥ 4 + ≥ −4 ) + (≥ 8 + ≥ −8 )
            °                                                            ¢
         = 2 cos(2π/17) + cos(4π/17) + cos(8π/17) + cos(16π/17)
            °                     ¢
         > 2 12 + 12 + 0 + (−1) = 0,
  13 The   discussion of this example closely follows that in Van der Waerden, Vol. I, Section 54.
504                                 IX. Fields and Galois Theory

η0 is the positive root, and we have
                             p                                                    p
             η0 = 12 (−1 + 17 )                     and           η1 = 12 (−1 −    17 ).
  For k = 4, the 4 periods have 4 terms each, and
                    0+4·0       0+4·1       0+4·2         0+4·3
           ξ0 = ≥ 3         + ≥3        + ≥3        + ≥3          = ≥ 1 + ≥ 13 + ≥ 16 + ≥ 4 ,
                    1+4·0       1+4·1       1+4·2         1+4·3
           ξ1 = ≥ 3         + ≥3        + ≥3        + ≥3          = ≥ 3 + ≥ 5 + ≥ 14 + ≥ 12 ,
                    2+4·0       2+4·1       2+4·2         2+4·3
           ξ2 = ≥ 3         + ≥3        + ≥3        + ≥3          = ≥ 9 + ≥ 15 + ≥ 8 + ≥ 2 ,
                    3+4·0       3+4·1       3+4·2         3+4·3
           ξ3 = ≥ 3         + ≥3        + ≥3        + ≥3          = ≥ 10 + ≥ 11 + ≥ 7 + ≥ 6 .
The sums of pairs of these that yield η’s are
                                           ξ0 + ξ2 = η0
                                           ξ1 + ξ3 = η1 .
We can read off superscripted ξ ’s from the exponents on the right sides of the
formulas for ξ0 , . . . , ξ3 , and the results are

                               ξ (1) = ξ (13) = ξ (16) = ξ (4) = ξ0 ,
                               ξ (3) = ξ (5) = ξ (14) = ξ (12) = ξ1 ,
                               ξ (9) = ξ (15) = ξ (8) = ξ (2) = ξ2 ,
                              ξ (10) = ξ (11) = ξ (7) = ξ (6) = ξ3 .
Then the relevant products are

      ξ0 ξ2 = ξ (1) ξ (9) = ξ (10) + ξ (16) + ξ (9) + ξ (3) = ξ3 + ξ0 + ξ2 + ξ1 = −1,
      ξ1 ξ3 = ξ (3) ξ (6) = ξ (13) + ξ (14) + ξ (10) + ξ (9) = ξ0 + ξ1 + ξ3 + ξ2 = −1.
Thus ξ0 and ξ2 are the roots of the quadratic equation
                                         y 2 − η0 y − 1 = 0,
while ξ1 and ξ3 are the roots of the quadratic equation
                                         y 2 − η1 y − 1 = 0.

                               ° these equations each have
Since ξ0 ξ2 and ξ1 ξ3 are negative,                     ¢ roots of opposite
sign.
 °    We   observe  that ξ0 = 2  cos(2π/17)
                                 ¢          + cos(8π/17) > 0 and that ξ3 =
2 cos(14π/17) + cos(12π/17) < 0, and we conclude that the signs are
                                        ξ0 > 0 and ξ2 < 0,
                                        ξ1 > 0 and ξ3 < 0.
                          12. Construction of Regular Polygons                   505




                                                                                °
FIGURE 9.4. Construction of a regular 17-gon. The small circle has center 12 , 18 )
  and radius 18 . Two circles are drawn tangent to it with center (0, 0); their radii
 are η0 /4 and |η1 |/4. Their x intercepts and height 12 determine the dashed box.
   The diameter of the large solid semicircle is ξp0 /2, and its heavy part is ∏0 /2.
  The separate semicircle at the left constructsp ξ1 /4 from ξ1 /2, and the chord
          in the large semicircle is at distance ξ1 /4 from the diameter.


   For k = 8, the 8 periods have 2 terms each, and the two with sum ξ0 are
                                 0+8·0      0+8·1
                        ∏0 = ≥ 3         + ≥3       = ≥ 1 + ≥ 16 ,
                                 4+8·0      4+8·1
                        ∏4 = ≥ 3         + ≥3       = ≥ 13 + ≥ 4 .
Their sum and their product are given by
                      ∏0 + ∏4 = ξ0 ,
                         ∏0 ∏4 = ≥ 14 + ≥ 5 + ≥ 12 + ≥ 3 = ξ1 .
Thus ∏0 and ∏4 are the roots of the quadratic equation
                                 z 2 − ξ0 z + ξ1 = 0.
Since ∏0 = 2 cos(2π/17) > 2 cos(8π/17) = ∏4 , ∏0 is the larger of the two roots
of the equation.
   In summary, we have successively defined
                     °      p ¢                 °         p ¢
              η0 = 12 − 1 + 17       and η1 = 12 − 1 − 17 ,
                   °    q        ¢              °      q         ¢
            ξ0 = 12 η0 + η02 + 4     and ξ2 = 12 η0 − η02 + 4 ,
                   °    q        ¢              °      q         ¢
            ξ1 = 12 η1 + η12 + 4     and ξ3 = 12 η1 − η12 + 4 ,
                                 °     q         ¢
                          ∏0 = 12 ξ0 + ξ02 − 4ξ1 .
506                                  IX. Fields and Galois Theory

Since ∏0 = 2 cos(2π/17), these formulas explicitly point to how to construct a
regular 17-gon. See Figure 9.4.


                          13. Solution of Certain Polynomial
                         Equations with Solvable Galois Group

In this section we investigate what specific information can be deduced about a
finite Galois extension in characteristic 0 when the Galois group is solvable.
The tool is a precursor of modern harmonic analysis14 known as “Lagrange
resolvents.” The argument of the previous section could be regarded as an instance
of applying the theory of Lagrange resolvents, but Lagrange resolvents give only
the simpler formulas of the previous section, not the Gauss product formula.

    Proposition 9.47. Let K be a finite normal extension of a field k of charac-
teristic 0, suppose that Gal(K/k) is cyclic of order n with σ as a generator, and
suppose that X n − 1 splits in k. Fix a generator σ of Gal(K/k) and a primitive
n th root ω of 1 in k. For 0 ≤ r < n, define k linear maps Er : K → K by
                                 X
                     Er x = n −1      ω−kr σ k x     for x ∈ K.
                                        k mod n

Then
   (a) Er E s equals E s if r = s and equals 0 if r 6≡ s mod n, so that the Er ’s are
       commuting
       P            projection operators whose images are linearly independent,
   (b)           E
          r mod n r = I , so that the direct sum of the images of the E r ’s is
       all of K,
   (c) σ (x) = ωr x for all r and for all x in image Er ,
   (d) image E 0 = k.
    REMARKS. The integers k and r depend only on their values modulo n, and the
summation indices “k mod n” and “r mod n” are to be interpreted accordingly.
The operators Er are known classically as Lagrange resolvents, apart from
the constant n −1 . The proposition says that the k linear map σ has a basis of
eigenvectors, that the eigenvalues are a subset of the powers ωr , and that each Er
is the projection operator on the eigenspace for the eigenvalue ωr along the sum
of the remaining eigenspaces.
   14 Lagrange    resolvents give a certain specific Fourier decomposition relative to a cyclic group.
Similar formulas apply whenever a cyclic group acts linearly on a vector space over k and the relevant
roots of 1 lie in k. For the corresponding decomposition of a vector space over C when a finite group
G acts linearly, see Problems 47–52 at the end of Chapter VII. The decomposition in those problems
can be seen to work for any field k of characteristic 0 for which the values of all irreducible characters
of G lie in k. The values of the characters are sums of certain roots of 1, and thus it is enough that
k contain a certain finite set of roots of 1.
          13. Solution of Certain Polynomial Equations with Solvable Galois Group   507

   PROOF. For x in K, we compute
                             P −kr k ° P −ls l ¢
             Er E s x = n −2    ω σ      ω σ x
                                 k mod n               l mod n
                                   P         P
                        = n −2                 ω−kr σ k ω−ms+ks σ m−k x
                               k mod n m mod n
                                 P ° P k(s−r) ¢ −ms m
                        = n −2                 ω         ω σ x.
                                 m mod n    k mod n

The expression in parentheses on the right side is the sum of a finite geometric
series. If s ≡ r mod n, then every term in the sum is 1, and the sum is n. If
                                  n(s−r)
s 6≡ r mod n, then the sum is 1−ω
                               1−ωs−r = 0. Thus (a) follows.
    Next we calculate
       P            P −1 P −kr k                 P −1 ° P −kr ¢ k
            Er x =       n          ω σ x=            n          ω      σ x.
    r mod n        r mod n       k mod n                   k mod n     r mod n

As in the previous paragraph, the sum in parentheses is n if k = 0 and it is 0 if
k 6≡ 0 mod n. Therefore only the k = 0 term on the right side contributes, and
the right side simplifies to x. This proves (b).
   The computation
                                      P −kr k+1
                   σ (Er x) = n −1        ω σ x
                                         k mod n
                                           P
                              =n    −1
                                                   ω(−l+1)r σ l x
                                      l mod n
                                    r −1
                                           P
                              =ω n                    ω−lr σ l x = ωr Er x
                                            l mod n

shows that σ (y) = ωr y for every y of the form Er x, and these y’s are the members
of the image of Er . This proves (c).
   Combining (b) and (c), we see that σ (x) = x if and only if x is in image E 0 .
Since Gal(K/k) is cyclic, the members of K fixed by σ are the members fixed
by the Galois group, and these are the members of k by Proposition 9.35d. This
proves (d).                                                                      §

    Corollary 9.48. Let K be a finite normal extension of a field k of characteris-
tic 0, suppose that Gal(K/k) is cyclic of prime order p, and suppose that X p − 1
splits in k. Then there exist a in k and x in K such that x p = a and K = k(x).
    REMARKS. In other words, a finite normal extension field in characteristic 0
with Galois group cyclic of prime order p is necessarily obtained by adjoining a
pth root of some element of the base field, provided that the base field contains
all the pth roots of 1. Once the extension field contains one pth root of an element
of the base field, it has to contain all pth roots, since the base field by assumption
contains a full complement of pth roots of 1.
508                            IX. Fields and Galois Theory

   PROOF. We apply Proposition 9.47 with n = p. Since [K : k] = p > 1, (d)
shows that E 0 is not the identity. By (b), some Er with r 6= 0 is not the 0 operator.
Let x be a nonzero element in image Er . Since the generator σ of the Galois group
is a field automorphism, σ (x p ) = σ (x) p = (ωr x) p = ωr p x p = x p . Since x p is
fixed by the Galois group, x p lies in k. Then the element a = x p has the property
that x p = a and K ⊇ k(x) % k. Since [K : k] is prime, Corollary 9.7 shows that
there are no intermediate fields between K and K. Therefore K = k(x).               §

   We shall apply Corollary 9.48 to prove the converse statement in Theorem
9.44—that solvability of the Galois group for a polynomial equation in charac-
teristic 0 implies that the solutions of the equation are expressible in terms of
radicals and the base field. We begin with a lemma that handles a special case.

   Lemma 9.49. Let k be a fieldQof characteristic 0, let n > 0 be an integer,
                                    n
and let K be a splitting field for r=1 (X r − 1) over k. Then K/k is a Galois
extension, the Galois group of Gal(K/k) is abelian, and K has a root tower over k.
    PROOF. Being a splitting field in characteristic 0, K is a finite Galois extension
of k. For 1 ≤ r ≤ n, let ωr be a primitive r th root of 1 in K. The primitive
r th roots of 1 are parametrized by the group (Z/rZ)× once some ωr is specified,
the parametrization being k 7→ ωrk . If σ is in Gal(K/k), then σ (ωr ) = ωrk for
some such k. This correspondence respects multiplication in (Z/rZ)× since if
σ (ωr ) = ωrk and σ 0 (ωr ) = ωrl , then σ 0 (σ (ωr )) = σ 0 (ωrk ) = σ 0 (ωr )k = ωrkl .
Thus for each r, we have a homomorphism of Gal(K/k) into the abelian group
(Z/rZ)× . Putting these homomorphisms together as r varies and using the fact
that the ωr ’s generate KQover k, we obtain a one-one homomorphism of Gal(K/k)
                           n
into the abelian group r=1     (Z/rZ)× . Consequently Gal(K/k) is isomorphic to
a subgroup of an abelian group and is abelian.
    It follows from Corollary 9.39 that every extension of intermediate fields is
Galois and has abelian Galois group. For 1 ≤ r ≤ n, we introduce the interme-
diate field Kr = k(ω1 , ω2 , . . . , ωr ). Here K1 = k(1) = k. For 1 <Pr < n, Kr is
generated as a vector space over Kr−1 by ωr , ωr2 , . . . , ωrr−1 since r−1       k
                                                                            k=0 ωr = 0
for r > 1, and thus [Kr : Kr−1 ] ≤ r − 1. Since Gal(Kr /Kr−1 ) is abelian, it has
a composition series whose consecutive quotients are cyclic of prime order, the
prime order necessarily being ≤ [Kr : Kr−1 ] ≤ r − 1. Applying Galois theory,
form the chain of intermediate extensions between Kr−1 and Kr . The degree of
each extension is some prime p with p ≤ r − 1, the prime depending on the two
fields in the chain. The pth roots of unity are in the smaller of any two consecutive
fields because they are in Kr−1 . By Corollary 9.48, such a degree- p extension
between Kr−1 and Kr is generated by the smaller field and the pth root of an
element in the smaller field. Since K1 = k, we see inductively that Kr has a root
tower over Kr−1 for each r. Since K = Kn , K has a root tower over k.                §
          13. Solution of Certain Polynomial Equations with Solvable Galois Group    509

   PROOF OF SUFFICIENCY IN THEOREM 9.44 THAT Gal(K/k) BE SOLVABLE. Let
F(X) be in k[X], and suppose that K is a splitting field of F(X) over k. Under
the assumption that Gal(K/k) is solvable, we are to prove that there exists a finite
extension K0 of K having a root tower.
   Since G = Gal(K/k) is solvable, we can find a finite sequence of subgroups
of G, each normal in the next larger one, such that the quotient of any consecutive
pair is cyclic of prime order. We write
                     G = H0 ⊇ H1 ⊇ · · · ⊇ Hk−1 ⊇ Hk = {1}
with Hj /Hj+1 cyclic of prime order p j for 0 ≤ j < k. Let
                      k = K0 ⊆ K1 ⊆ · · · ⊆ Kk−1 ⊆ Kk = K
be the corresponding sequence of intermediate fields given by the Fundamen-
tal Theorem of Galois Theory (Theorem 9.38). Here K j = K Hj , and Hj =
Gal(K/K j ).
    According to Corollary 9.39, K j+1 is a normal extension of K j if and only if
Gal(K/K j+1 ) is a normal subgroup
                                ±    of Gal(K/K j ), and in this case we have a
group isomorphism Gal(K/K j ) Gal(K/K j+1 ) ∼     = Gal(K j+1 /K j ). Since Hj+1 is
a normal subgroup of Hj with quotient cyclic of order p j , it follows that K j+1 /K j
is indeed normal and the Galois group is cyclic of order p j .
                                                                                    0
    Let us use Theorem 9.22 to regard K as lying in a fixed algebraic closure K .
Let n be the product of all the primes p j , and let K00 be the splitting field over
       Qn                       0                                                     0
k for r=1    (X r − 1) within K . For 1 ≤ j ≤ k, let K0j be the subfield of K
generated by K j and K00 . We define K0 = Kk0 . Then we have
                     k ⊆ K00 ⊆ K01 ⊆ · · · ⊆ K0k−1 ⊆ K0k = K0 .
Lemma 9.49 shows that K00 has a root tower over K0 . To complete the proof, it is
enough to show for each j ∏ 0 that either K0j+1 = K0j or else [K0j+1 : K0j ] = p j
and K0j+1 is generated by K0j and the pth    j root of some member of K j .
                                                                            0

    For each j ∏ 0, suppose that K j+1 = K j (x j ). Let Fj (X) be the minimal poly-
nomial of x j over K j . Since K j+1 /K j is normal, K j+1 is the splitting
                                                                    Qn field    of Fj (X)
over K j . Then K0j+1 = K0j (x j ) is the splitting field of Fj (X) r=1    (X r − 1) over
K0j , and consequently K0j+1 /K0j is a normal extension. If g is in Gal(K0j+1 /K0j ),
  Ø g sends x j into a root of Fj (X) and is determined by this root. The restriction
then
g ØKj+1 therefore carries K j+1 into itself and is in Gal(K j+1 /K j ). Since g is
                                                                 Ø
determined by g(x j ), the group homomorphism g 7→ g Ø             K j+1
                                                                        is one-one. The
image of this homomorphism must be a subgroup of Gal(K j+1 /K j ) and therefore
must be trivial or have p j elements. In the first case, K0j+1 = K0j , and in the
second case, [K0j+1 : K0j ] = p j . In the latter case, K0j contains all p j of the pth
                                                                                     j
roots of 1 since these roots of 1 are in K00 ; by Corollary 9.48, K0j+1 is generated
by K0j and a pth
               j root of some member of K j . This completes the proof.
                                               0
                                                                                     §
510                              IX. Fields and Galois Theory

    We turn now to apply our methods to irreducible cubics over a field k of char-
acteristic 0. In effect we shall derive Cardan’s formula,15 which was mentioned
at the beginning of Section 11.
    The Galois group of a splitting field of a cubic polynomial has to be a subgroup
of the symmetric group S3 , and irreducibility of the cubic implies that the Galois
group has to contain a 3-cycle. Therefore the Galois group has to be either S3 or
the alternating group A3 ∼ = C3 .
    Let the cubic be X 3 +a2 X 2 +a1 X +a0 , the coefficients being in k. Substituting
X = Z − 13 a2 converts the polynomial into
  (Z − 13 a2 )3 + a2 (Z − 13 a2 )2 + a1 (Z − 13 a2 ) + a0
                                      = Z 3 + (a1 − 13 a22 )Z + (a0 − 13 a1 a2 +       2 3
                                                                                       27 a2 ),

and therefore we can assume whenever convenient that the given polynomial has
a2 = 0.
   Suppose for the moment that the Galois group is G = S3 . A composition
series is
                            G = S3 ⊇ A3 ⊇ {1},
and we can write the corresponding sequence of fixed fields as
                                         k ⊆ L ⊆ K,
where K is the splitting field and L is KA3 . The dimensions satisfy [L : k] = 2
and [K : L] = 3.
    Let the roots in K of the given cubic be r1 , r2 , r3 . Since G is solvable, Theorem
9.44 tells us that the roots are expressible in terms of radicals and members of
k. To derive explicit formulas for the roots, the idea is to use a two-step process
with Lagrange resolvents, arguing as in the proof of Corollary 9.48 at each step.
    The first step involves passing from k to L. The square roots of 1 are already
in k, and L is to be obtained from k by adjoining one of the square roots of
some element of k. In Proposition 9.47 the Galois group Gal(L/k) is a 2-element
quotient group, the sum is over members of the quotient group, and the element x
is in L. It is a little more convenient to pull the sum back to one over the 6-element
symmetric group, taking ω to be the sign function on S3 and taking x to be any
element of K. The formulas for the projection operators E 0 and E 1 are then
                                          P
                               E 0 x = 16     σ (x),
                                             σ ∈S3
                                         1    P
                                E1 x =   6           (sgn σ )σ (x),
                                             σ ∈S3

   15 We discuss only Cardan’s cubic formula, omitting any discussion of the corresponding quartic

formula, which often bears Cardan’s name and which can be handled with the same techniques. See
Van der Waerden, Vol. I, Section 58, for details.
          13. Solution of Certain Polynomial Equations with Solvable Galois Group   511

with x in K, and the proof of Corollary 9.48 tells us to adjoin to k the square root
of any element of image E 1 , i.e., any element with σ (x) = (sgn x)x for all σ in
S3 .
   The only elements of K for which we have good control of the action of the
Galois group, apart from the elements of k, are the elements that are expressed
directly in terms of the roots r1 , r2 , r3 of the polynomial. By renumbering the
roots if necessary, we may assume that the roots are permuted by S3 according to
their subscripts. An example of a polynomial function of r1 , r2 , r3 that transforms
according to the sign of the permutation played a role in Section I.4 in defining
the sign of a permutation. It is the difference product of the polynomial, namely
                                     Y
                                            (r j − ri ).
                                   1≤i< j≤3

This is a square root of the discriminant D of the polynomial, which is given by
                                     Y
                              D=          (r j − ri )2 .
                                      1≤i< j≤3

We shall compute D in terms of the coefficients of the cubicpshortly. Inpthe
meantime, the proof of Corollary 9.48 thus tells us that L = k( D ). Here D
is given by
             p
               D = (r3 − r2 )(r3 − r1 )(r2 − r1 )
                    = (r1r22 + r2r32 + r3r12 ) − (r12r2 + r22r3 + r32r1 ).

   The second step is to pass from L to K. Corollary 9.48 says to expect K
to be obtained by adjoining the cube root of something if the cube roots of 1
are already present in L. The proof of the second half of Theorem 9.44, which
follows Corollary 9.48, indicates how we can incorporate the cube roots of 1 into
                         Q a root tower. What we can do is to replace k at the start
the fields in order to have
by a splitting field for 1≤r≤3 (X r − 1). Since ±1 are already in k, we are to
adjoin the nontrivial cube roots of 1, i.e., the roots of X 2 + X + 1, if they p are not
already present. In other words, what  p  we  do is replace k at the start by k(   −3 ).
Changing notation, we assume that −3 lies in k from the outset.
   We can now use Lagrange resolvents. Let σ be the generator    p      (1 2 3) of A3 ,
sending r1 to r2 , r2 to r3 , and r3 to r1 . Let ω = 12 (−1 + −3 ) be a primitive
cube root of 1. Then we have

                         E 0 x = 13 (x + σ x + σ 2 x),
                         E 1 x = 13 (x + ω−1 σ x + ω−2 σ 2 x),
                         E 2 x = 13 (x + ω−2 σ x + ω−1 σ 2 x).
512                               IX. Fields and Galois Theory

Again we can use any x, but the roots of the cubic are the simplest nontrivial
elements for which we know the action of σ . Corollary 9.48 shows that K =
L(E 1 x) if E 1 x 6= 0. Proposition 9.47 says that (E 1 x)3 is fixed by σ , and it
therefore lies in L. Hence K is identified as obtained from L by adjoining a cube
root of the element (E 1 x)3 of L.                                        p
   Taking x = r1 , we have σ x = r2 and σ 2 x = r3 . Also, ω±1 = 12 (−1 ± −3 ).
                                                 p
Using the formula for E 1 x and substituting for D and ω±1 then gives

        (3E 1r1 )3 = r13 + r23 + r32 + 6r1r2r3
                    + 3ω−1 (r12r2 + r22r3 + r32r1 ) + 3ω(r1r22 + r2r32 + r3r12 )
                    P                     P 2           p p
                   = ri3 + 6r1r2r3 − 32      ri r j + 32 −3 D.
                          i                     i6= j

   To proceed further, we shall want to substitute expressions involving the co-
efficients of the cubic for the above symmetric expressions in the roots.16 These
expressions will be considerably simplified if we assume that the coefficient of
X 2 in the cubic is 0. We know that this assumption involves no loss of generality.
Thus we assume for the remainder of this section that the cubic is X 3 + p X + q.
The relevant formulas relating the roots and the coefficients are

                                       r1 + r2 + r3 = 0,
                                 r1r2 + r1r3 + r2r3 = p,
                                             r1r2r3 = −q.

   Aiming for the right side of the displayed formula for (3E 1r1 )3 , we have
                                  P         P 2
        0 = (r1 + r2 + r3 )3 = ri3 + 3         ri r j + 6r1r2r3 ,
                                         i              i6= j
                                                                     P
           0 = (r1 + r2 + r3 )(r1r2 + r1r3 + r2r3 ) = − 92                   ri2r j −   27
                                                                                        2 r1r2r3 ,
                                                                     i6= j

      − 27
        2q =
                 27
                 2 r1r2r3 .

Addition of these three lines and comparison with the expression for 3(E 1r1 )3
yields
                P 3 3P 2                                                 p p
       − 27
          2 q =     r i − 2    r i r j + 6r 1 r 2 r 3 = (3E 1 r 1 )3
                                                                     − 3
                                                                       2 −3 D.
                      i          i6= j

Consequently                                                    p p
                              (3E 1r1 )3 = − 27
                                             2q +
                                                            3
                                                            2    −3 D.
    16 Problems 36–39 at the end of Chapter VIII assure us that this rewriting is possible. For our

derivation this assurance is not logically necessary, since we will be producing explicit formulas.
            13. Solution of Certain Polynomial Equations with Solvable Galois Group   513

Similarly                                                   p p
                            (3E 2r1 )3 = − 27
                                           2q −
                                                        3
                                                        2    −3 D.
Since 3E 0r1 = r1 + r2 + r3 = 0, we have expressions for E 0r1 , E 1r1 , and E 2r1 ,
apart from the choices of the cube roots. Proposition 9.47b says that we recover
r1 by addition: r1 = E 0r1 + E 1r1 + E 2r1 . Thus we have found a root explicitly
as soon as we sort out the ambiguity in the choices of cube roots and determine
the value of D in terms of the coefficients p and q.

       p 9.50 (Cardan’s formula). Let k be a field of characteristic 0 con-
   Theorem
taining −3, and let X 3 + p X + q be an irreducible cubic in k[X]. For this
polynomial the discriminant D is given by

                                      D = −4 p3 − 27q 2 .

The Galois group of a splitting field of the cubic is S3 if D is a nonsquare in k
p is A3 if D is
and              a square in k.pIn either case, fix a square root of D, denote it by
  D, and let ω±1 = 12 (−1 ± −3) be the primitive cube roots of 1. Then it is
possible to determine cube roots of the form
             q            p p                            q              p p
   3E 1r1 = − 27        3
                                                            − 27      3
              3                                           3
                  2 q + 2   −3    D     and    3E   r
                                                   2 1 =       2 q − 2 −3 D

in such a way that their product is (3E 1r1 )(3E 1r2 ) = −3 p, and in this case the
three roots of X 3 + p X + q are given by

                                  r1 = E 1r1 + E 2r1 ,
                                  r2 = ωE 1r1 + ω2 E 2r1 ,
                                  r3 = ω2 E 1r1 + ωE 2r1 .

   PROOF. Define σk = r1k + r2k + r3k for 1 ≤ k ≤ 4. By inspection we have
            √              !  1 r r2  √                      !
               1 1 1                  1    1       3 σ1 σ2
               r1 r2 r3  1 r2 r22  = σ1 σ2 σ3 .
              r12 r22 r32       1 r3 r32           σ2 σ3 σ4

Taking the determinant of both sides and applying Corollary 5.3, we obtain
                             √                   !
                                 3     σ1   σ2
                   D = det       σ1    σ2   σ3       = 3σ2 σ4 − σ23 − 3σ32 .
                                 σ2    σ3   σ4
The given cubic shows that σ1 = r1 + r2 + r3 = 0. For the other σi ’s, we have
514                            IX. Fields and Galois Theory

 σ2 = r12 + r22 + r32 = (r1 + r2 + r3 )2 − 2(r1r2 + r1r3 + r2r3 ) = −2 p,
 σ3 = r13 + r23 + r33 = (r1 + r2 + r3 )(r12 + r22 + r32 )
                         − (r12r2 + r12r3 + r22r1 + r22r3 + r32r1r32r2 )
                       = −(r1 + r2 + r3 )(r1r2 + r1r3 + r2r3 ) + 3r1r2r3 = −3q,
 σ4 = r14 + r24 + r34 = (r12 + r22 + r32 )2 − 2(r12r22 + r12r32 + r22r32 )
                       = (−2 p)2 − 2(r1r2 + r1r3 + r2r3 )2
                          + 4r1r2r3 (r1 + r2 + r3 ) = (−2 p)2 − 2( p)2 = 2 p2 .

Substituting, we obtain D = −12 p3 + 8 p3 − 27q 2 = −4 p3 − 27q 2 . This proves
the formula for D. In particular, it confirms that D lies in k.
    The Galois group of the splitting fieldpof the polynomial must be S3 or A3 . If
it is S3 , then we saw above that L = k( D) and that [L : k] = 2. Hence D is a
nonsquare in k. If the Galois group is A3 , then (r3 − r2 )(r3 − r1 )(r2 − r1 ) is fixed
by the Galois group and lies in k. The square of this element is D, and hence D
is a square in k.
    With either Galois group the calculations with the cubic extension that precede
the statement of the theorem are valid. If r1 is one of the roots, then we know that
                         r1 = E 0r1 + E 1r1 + E 2r1 = E 1r1 + E 2r1 ,
                                          p p
                  (3E 1r1 )3 = − 27     3
                                 2 q + 2 −3 D,
                                          p p
                  (3E 2r1 )3 = − 27
                                 2  q − 3
                                        2 −3 D.

The uniqueness of simple extensions (Theorem 9.11) says that we can make any
choice of cube root to determine 3E 1r1 . Then
      (3E 1r1 )(3E 2r1 ) = (r1 + ω−1 σ r1 + ω−2 σ 2r1 )(r1 + ω−2 σ r1 + ω−1 σ 2r1 )
                       = (r1 + ω−1r2 + ωr3 )(r1 + ωr2 + ω−1r3 )
                       = (r12 + r22 + r32 ) + (ω + ω−1 )(r1r2 + r1r3 + r2r3 )
                       = (r12 + r22 + r32 ) − (r1r2 + r1r3 + r2r3 ).
The first term on the right side we calculated in the first paragraph of the proof
as σ2 = −2 p, and the second term gives − p. Thus (3E 1r1 )(3E 2r1 ) = −3 p as
asserted. Since σ operates on image E 1 as multiplication by ω and on image E 2
as multiplication by ω2 , the fact that r1 = E 1r1 + E 2r1 implies that
                          r2 = σ (r1 ) = ωE 1r1 + ω2 E 2r1
and                       r3 = σ 2 (r1 ) = ω2 E 1r1 + ωE 2r1 .
This completes the proof.                                                             §
                              14. Proof That π Is Transcendental                             515

                         14. Proof That π Is Transcendental

In this section and the next three, we combine Galois theory with some of the
ring theory in the second half of Chapter VIII. This combination will allow us to
prove some striking theorems, see how Galois groups can be used effectively in
practice, and develop some techniques for identifying Galois groups explicitly.
   The present section is devoted to the proof of the following theorem.

   Theorem 9.51 (Lindemann, 1882). The number π is transcendental over Q.

   The argument we give is based on that in a book by L. K. Hua.17 For purposes
of having a precise theorem, π is defined as the least positive real number such
that eπi = −1. In addition to Galois theory in the form of Proposition 9.35,
the proof here will make use of a few facts about algebraic integers. Algebraic
integers were defined in Section VIII.1 and again in Section VIII.9 (as well as in
Section VII.4) as complex numbers that are roots of monic polynomials in Z[X].
The algebraic integers form a ring by Corollary 8.38 (or alternatively by Lemma
7.30), the only algebraic integers in Q are the members of Z by Proposition 8.41
(or alternatively by Lemma 7.30), and any algebraic number x has the property
that nx is an algebraic integer for some integer n 6= 0 by Proposition 8.42.
   We begin with a lemma.
                                                                   Pn
  Lemma 9.52. Let f (X) in C[X] be given by f (X) =                   k=0 ak X
                                                                                 k
                                                                                     , and define
F(X) to be the sum of the derivatives of f (X):

                                              n
                                              P
                                   F(X) =          f (l) (X).
                                             l=0

                                                                                     Pn
If Q(z) is defined as Q(z) = F(0)e z − F(z) for z ∈ C, then F(0) =                      k=0 ak k!
and
                                         n
                                         P
                           |Q(z)| ≤ e|z|   |ak ||z|k .
                                                k=0

   PROOF. We calculate directly that

                n
              n P
              P           ak k! k−l   Pn     Pk     k!             Pn     Pk k!
    F(z) =                       z  =     ak               z k−l =     ak        zl .
              l=0 k=l   (k − l)!      k=0    l=0 (k −  l)!         k=0    l=0 l!

   17 Introduction to Number Theory, pp. 484–488. In the same pages Hua establishes the earlier

theorem of Hermite that e is transcendental, using a related but simpler argument.
516                              IX. Fields and Galois Theory
                                    P
Evaluation at z = 0 gives F(0) = nk=0 ak k!. Then
                       ØP                        k k! Ø
                       Ø n      P∞ k!        n P
                                             P            Ø
              |Q(z)| ≤ Ø     ak        zl −            zl Ø
                         k=0    l=0 l!      k=0 l=0 l!
                       ØP n       P k! l Ø
                                  ∞        Ø
                       Ø
                     =Ø      ak           zØ
                         k=0    l=k+1 l!

                           n
                           P              ∞
                                          P       |z|l             ° l ¢−1
                       ≤         |ak |                     since    k        ≤1
                           k=0           l=k+1 (l − k)!
                           n
                           P                   P∞ |z|m
                       =         |ak ||z|k
                           k=0                 m=1 m!
                                 n
                                 P
                       ≤ e|z|          |ak ||z|k .                                  §
                                 k=0
   PROOF OF THEOREM 9.51. Arguing by contradiction, suppose that π is al-
gebraic over Q, so that α = πi is algebraic over Q as well. Let M(X) be the
minimal polynomial of α over Q, and let K be the splitting field of M(X) in C.
This exists since C is algebraically closed. We write α1 , . . . , αm for the roots of
M(X) in K, with α1 = α. These are distinct algebraic numbers, and they are
permuted by the Galois group, G = Gal(K/Q). What we shall show is that
                                   Qm
                             R=        (1 + eαj ) 6= 0.
                                              j=1

This will be a contradiction since 1 + eα1 = 0 for α1 = iπ.
  We expand the product defining R, obtaining
                                 P        P
                       R = 1 + eαj + eαj +αk + · · · ,
                                          j          j,k

Whenever one of the exponentials has total exponent 0, we lump that term with
the constant 1. Otherwise we write the term as eβl , allowing repetitions among
terms eβl . Thus
                       R = N + eβ1 + eβ2 + · · · + eβr ,
with N an integer ∏ 1, with each βl 6= 0, and with N + r = 2m .
   Each member of G = Gal(K/Q) permutes α1 , . . . , αm , and it therefore per-
mutes the βl ’s that are single α j ’s, permutes the βl ’s that are the nonzero sums of
two α j ’s, permutes the βl ’s that are the nonzero sums of three α j ’s, and so on.
   Choose an integer a > 0 such that aα1 , . . . , aαm are algebraic integers, let p
be a prime number large enough to satisfy some conditions to be specified shortly,
and define
                            (a X) p−1 Q  r                    Pn
                  f (X) =                  (a X − aβl ) p =       ak X n .
                            ( p − 1)! l=1                    k=0
                               14. Proof That π Is Transcendental                           517

The members σ of G act on f (X) as usual by acting on the coefficients. Each βl
that is the nonzero sum of a certain number of α j ’s is sent into another βl 0 of the
same kind, and thus σ just permutes the factors of the product defining f , leaving
 f (X) unchanged. The coefficients of ( p − 1)! f (a −1 X) are algebraic integers in
K. Being fixed by G, they are in Q by Proposition 9.35d, and hence they are in
Z. Therefore
                                  A p−1 a p−1 X p−1 + A p a p X p + · · ·
                         f (X) =
                                                ( p − 1)!
                                               Qr
with A p−1 , A p , . . . in Z. Since A p−1 = l=1 (−aβl ) p , we can arrange
                                                                          Ø Qthat p does
                                                                                       Ø
not divide A p−1 a p−1 by choosing p greater than a and greater than Ø rl=1 (aβl )Ø.
If we look at the l th factor in the product defining f (X), we see that (X − βl ) p
divides f (X) in K[X]. Therefore we have further formulas for f (X), namely
                ∞ p,l (X − βl ) p + ∞ p+1,l (X − βl ) p+1 + · · ·
      f (X) =                                                            for 1 ≤ l ≤ r.
                                   ( p − 1)!
   As in Lemma 9.52, we define
                        n
                        P
            F(X) =            f (l) (X)    and        Q(z) = F(0)e z − F(z).
                        l=0
                     P
Then we have F(0) = nk=0 ak k!. For 1 ≤ l ≤ r, the definition of Q(z) gives
F(0)eβl = F(βl ) + Q(βl ). Substituting from the definition of R, we obtain
                    °    r
                         P    ¢           r
                                          P          r
                                                     P
        F(0)R = F(0) N +   eβl = N F(0) +   F(βl ) +   Q(βl ).                              (∗)
                                    l=1                     l=1            l=1

A further condition that we impose on the size of p is that p > N . Then the
computation
                n
                P
 N F(0) = N           ak k! = N (A p−1 a p−1 + p A p a p + p( p + 1)A p+1 a p+1 + · · · )
                k=0

and the properties of A p−1 , A p , . . . together imply that N F(0) is an integer and
is not divisible by p.
    Let us compute F(βl ). The derivatives through order p − 1 of f (X) are 0 at
βl . For the pth derivative we have
                                             P ( p + j) · · · ( j + 1)                 j
       p∞ p,l = f ( p) (βl ) = p A p a p +                             A p+ j a p+ j βl .
                                             j∏1    ( p −    1)!
                                      j
The coefficient of A p+ j a p+ j βl inside the sum equals
                                                      µ      ∂
                      ( p + j) · · · ( j + 1) j! p      p+ j
                                                   =p         ,
                             p( p − 1)! j!                j
518                                 IX. Fields and Galois Theory

and thus
                                         °        P ° p+ j ¢          ¢
              p∞ p,l = f ( p) (βl ) = a p p A p +  p j A p+ j (aβl ) j .
                                                       j∏1

The higher-order derivatives are computed and simplified similarly. For the
( p + k)th derivative with k ∏ 1, we find that

( p + k) · · · ( p + 1) p∞ p+k,l = f ( p+k) (βl )
                                         °
                                 = a p+k ( p + k) · · · ( p + 1) p A p+k                 (∗∗)
                                      P                          ° p+ j+k ¢                  ¢
                                   + ( p + k) · · · ( p + 1) p        j     A p+ j+k (aβl ) j .
                                         j∏1
                Pr
Put C p+k =        l=1 ∞ p+k,l .   Summing the left and right members of (∗∗) over l
gives
                             °          P ° p+ j+k ¢         l
                                                             P        ¢
                C p+k = a p+k r A p+k +        j     A p+ j+k (aβl ) j .
                                               j∏1                   j=1
           Pl
The sum j=1 (aβl ) j is an algebraic integer fixed by G, and it is therefore an
integer. Consequently each C p+k is an integer. Summing the left and middle
members of (∗∗) over k and l gives
                      r
                      P                 P
                            F(βl ) =        ( p + k) · · · ( p + 1) pC p+k ,
                      l=1              k∏0

and this is an integer divisible by p.                    P
   Since N F(0) is an integer not divisible by p, N F(0) + rl=1 F(βl ) is an
integer not divisible by p, and we have
                                   Ø          r       Ø
                                   Ø N F(0) + P F(βl )Ø ∏ 1.
                                                l=1

In view of (∗), we will have a contradiction to R = 0 if we show that
                                        ØPr       Ø
                                        Ø   Q(βl )Ø < 1.
                                         l=1
                                                        P              Q
An easy argument by induction on m shows that if m         k=0 dk z k = sj=1 (z − c j ),
    P                 Qs
then m             k
       k=0 |dk ||z| ≤   j=1 (|z| + |c j |). Applying this observation to the sum and
product defining f (X) and using Lemma 9.52, we see that
                                                     Q
           −|z|         Pn
                                     k     (a|z|) p−1 rl=1 (a|z| + a|βl |) p
         e |Q(z)| ≤          |ak ||z| ≤                                      .
                        k=0                           ( p − 1)!
                                      15. Norm and Trace                            519

For each fixed z, the right side is the ( p − 1)st term of the convergent series for an
exponential function at an appropriate point, and hence the right side is less than
r −1 e−|z| for p sufficiently large, p depending on z. Choosing p large enough to
                                  −1 −|z|
                 Ø Prless than Ør e
make the right side                       for z = β1 , . . . , βl and summing over these
z’s, we obtain   Ø              Ø
                      l=1 Q(βl ) < 1, and we have arrived at the contradiction we
anticipated.                                                                          §


                                   15. Norm and Trace

This is the second of four sections in which we combine Galois theory with
some of the ring theory in the second half of Chapter VIII. We shall make use
of a little more linear algebra than we have used thus far in this chapter, and we
shall conclude the section by completing the proof of Theorem 8.54 concerning
extensions of Dedekind domains.
   Let k be a field, not necessarily of characteristic 0, and let K be a finite
algebraic extension. We take advantage of the fact that K is a vector space over
k. If a is in K, let us write M(a) for the k linear mapping from K to K given by
multiplication by a. The characteristic polynomial det(X I − M(a)) is called the
field polynomial of a and is a monic polynomial in k[X] of degree [K : k]. The
norm and trace of a relative to K/k are defined to be the determinant and trace
of the linear mapping M(a). In symbols,

                                  NK/k (a) = det(M(a)),
                                  TrK/k (a) = Tr(M(a)).

Both NK/k and TrK/k are functions from K to k. If n = [K : k], then NK/k (a)
is (−1)n times the constant term of det(X I − M(a)), and TrK/k (a) is minus the
coefficient of X n−1 . The subscript K/k may be omitted when there is no chance
of ambiguity.
                                  p          p                        p
   EXAMPLE. k = Q, K = Q( 2 ), a = 2. If we use 0 = (1, ≥ 2 ) as¥ an
ordered basis of K over k, then the matrix of M(a) relative to 0 is M(a)00
                                                                              =
≥ ¥
  02
  10
       . Since characteristic polynomials are independent of the choice of basis,
the field polynomial of a can be computed in this basis and is given by
                          ≥               ¥           ≥           ¥
                              X I −M(a)                    X −2
                    det           00
                                              = det       −1 X
                                                                      = X 2 − 2.

We can read off the norm and trace as N (a) = −2 and Tr(a) = 0.
520                             IX. Fields and Galois Theory

  Proposition 9.53. If K/k is a finite extension of fields with n = [K : k], then
norms and traces relative to K/k have the following properties:
   (a) N (ab) = N (a)N (b),
   (b) N (ca) = cn N (a) for c ∈ k,
   (c) N (1) = 1, and consequently N (c) = cn for c ∈ k,
   (c) Tr(a + b) = Tr(a) + Tr(b),
   (d) Tr(ca) = c Tr(a) for c ∈ k,
   (e) Tr(1) = n, and consequently Tr(c) = nc for c ∈ k.
   PROOF. Properties (a) and (b) follow from properties of the determinant in
combination with the identities M(ab) = M(a)M(b) and M(ca) = cM(a).
Properties (c) and (d) follow from properties of the trace in combination with the
identities M(a + b) = M(a) + M(b) and M(ca) = cM(a). Since M(1) is the
identity, the norm and trace of 1 are 1 and n, respectively. The other conclusions
in (c) and (e) are then consequences of this fact in combination with (b) and (d).
                                                                                §
   Proposition 9.54. Let K/k and L/K be finite extensions of fields with
[K : k] = n and [L : K] = m, and let a be in K. The element a acts by
multiplication on K and also on L, yielding k linear maps in each case that will
be denoted by MK/k (a) and ML/k (a). Then in suitable ordered vector-space bases
the matrix of ML/k (a) is block diagonal, each block being the matrix of MK/k (a).
   PROOF. We choose the bases as in Theorem 7.6. Thus let 0 = (ω1 , ω2 , . . . )
be an ordered basis of K over k, and let 1 = (ξ1 , ξ2 , . . . ) be a basis of L over K.
Theorem 7.6 observes that the mn products ξi ω j form a basis of L over k, and we
make this set into an ordered basis ƒ by sayingP that (i 1 , j1 ) < (i 2 , j2 ) if i 1 < i 2
or if i 1 = i 2 and j1 < j2 . Let MK/k (a)ω j = l cl j ωl . Then
                                    n
                                   °P            ¢       n
                                                       m P
                                                       P
                ML/k (a)ξi ω j =          cl j ωl ξi =     (δki cl j )ξk ωl ,
                                    l=1               k=1 l=1
                                                                            ≥              ¥
                                                                                ML/k (a)
where δki is 1 when k = i and is 0 otherwise. The matrix                         ƒƒ
                                                                                               has
((k, l), (i, j))th entry δki cl j , and this is 0 unless the primary indices k and i are
equal. Thus the matrix is block diagonal, the entries of the i th diagonal block
being cl j .                                                                          §

   Corollary 9.55. Let K/k and L/K be finite extensions of fields with
[L : K] = m, and let a be in K. Let MK/k (a) and ML/k (a) denote multiplication
by a on K and on L, and let FK/k (X) and FL/k (X) be the corresponding field
polynomials. Then                     °         ¢m
                            FL/k (X) = FK/k (X) .
Consequently NL/k (a) = (NK/k (a))m and TrL/k (a) = m TrK/k (a).
                                15. Norm and Trace                             521

   PROOF. Proposition 9.54 shows that the matrix of X I − ML/k (a) may be
taken to be block diagonal with each of the m diagonal blocks equal to the
matrix of X I − MK/k (a). The determinant of X I − ML/k (a) is the product
of the determinants of the diagonal blocks, and the formula relating the field
polynomials is proved.
   The formulas for the norms and the traces are consequences of this relationship.
In fact, let
                  FK/k (X) = X n + cn−1 X n−1 + · · · + c0
and               FL/k (X) = X mn + dmn−1 X mn−1 + · · · + d0 .
                                      °        ¢m
Comparing coefficients of FL/k (X) and FK/k (X) , we see that dmn−1 = mcn−1
and d0 = c0m . Therefore
               NL/k (a) = (−1)mn d0 = ((−1)n c0 )m = (NK/k (a))m
and           TrL/k (a) = −dmn−1 = −mcn−1 = m TrK/k (a).
This completes the proof.                                                       §

   Corollary 9.56. Let K/k be a finite extension of fields, and let a be in K. Then
the field polynomial of a relative to K/k is a power of the minimal polynomial of
a over k, the power being [K : k(a)]. In the special case K = k(a), the minimal
polynomial of a coincides with the field polynomial.
   REMARKS. In the theory of a single linear transformation as in Chapter V,
the minimal polynomial of a linear map divides the characteristic polynomial, by
the Cayley–Hamilton Theorem (Theorem 5.9). For a multiplication operator in
the context of fields, we get a much more precise result—that the characteristic
polynomial is a power of the minimal polynomial.
  PROOF. If F(X) is in k[X], then the operation M of multiplication has

               M(F(a))b = F(a)b = F(M(a))b              for b ∈ K,             (∗)

as we see by first considering monomials and then forming k linear combinations.
The minimal polynomial of a over k is the unique monic F(X) of lowest degree
in k[X] for which F(a) = 0, hence such that M(F(a)) = 0. Meanwhile, the
minimal polynomial of the linear map M(a) is the unique monic F(X) of lowest
degree such that F(M(a)) = 0. These two polynomials coincide because of (∗).
   The degree of the minimal polynomial of M(a) thus equals the degree of the
minimal polynomial of a, which is [k(a) : k]. The Cayley–Hamilton Theorem
(Theorem 5.9) shows that the minimal polynomial of M(a) divides the charac-
teristic polynomial of M(a), i.e., the field polynomial of a. When the field K is
k(a), the minimal polynomial of a and the field polynomial of a have the same
522                                IX. Fields and Galois Theory

degree; since they are monic, they are equal. This proves the second conclusion
of the corollary.
   For the first conclusion we know from Corollary 9.55 that the field polynomial
of a relative to a general K is the [K : k(a)]th power of the field polynomial of a
relative to k(a). Since we have just seen that the latter polynomial is the minimal
polynomial of a, the first conclusion of the corollary follows.                    §
                                                p            p
   EXAMPLE, CONTINUED. k = Q, K = Q( 2 ), a = 2. We have seen that
the field polynomial of a is X 2 − 2, that the norm and trace are N (a) = −2 and
                                     ≥ multiplication
Tr(a) = 0, and that the≥matrix¥ of the    ¥            operator M(a)
                                                                  ≥ in¥the ordered
                 p                     02
                                                                                 p
                           M(a)
basis 0 = (1, 2 ) is 00 = 1 0 . The eigenvalues of M(a)              00
                                                                           are ± 2,
namely the roots of the field polynomial. These are not in the field k. Indeed,
they could not possibly be in the field, or we would have M(a)x = ∏x for some
xp6= 0 in K and some ∏ in k, and this would mean that ∏ = a. Since the roots
≥ 2 of
±      ¥ the field polynomial each have multiplicity
                                                  ≥p     1 ¥and lie in K, the matrix
  M(a)                                               2 p0
         is similar over K to the diagonal matrix            . Since similar matrices
   00                                               0 − 2
have the same trace and the same norm, we can compute the trace and norm of
M(a) from this diagonal matrix, namely by adding or multiplying its diagonal      p
entries. The significance of the diagonal entries is that they are the images of 2
under the members of the Galois group Gal(K/k). We shall now generalize these
considerations. Additional complications arise when K/k fails to be separable
and normal.18

   Proposition 9.57. Let k be a field, let k(a) be an algebraic extension of k, and
suppose that the minimal polynomial F(X) of a over k is separable. Let K be a
splitting field of F(X), and factor F(X) over K as

                         F(X) = (X − a1 )(X − a2 ) · · · (X − an )

with all a j ∈ K and with a1 = a. Then the matrix of the multiplication operator
M(a)k(a)/k of a on k(a) is similar over K to a diagonal matrix with diagonal
entries a1 , . . . , an . Consequently
                                Yn                          Xn
                 Nk(a)/k (a) =     aj  and   Trk(a)/k (a) =     aj .
                                 j=1                                        j=1

    18 The above argument used a matrix with entries in k and considered the entries as in the larger

field K. The reader may wonder what the corresponding construction is for the k linear map M(a). p It
is not to treat M(a) as a K linear map on K, since then M(a) would have just the one eigenvalue 2,
which would have multiplicity 1. Instead, it is to use tensor products as in Chapter VI, knowledge
of which is not being assumed at present. The idea is to extend scalars, replacing K by K ⊗k K and
replacing M(a) by M(a) ⊗ 1. The K linearity occurs in the second member ofpthe tensor product,
not the first, and the operator M(a) ⊗ 1 is the K linear map with eigenvalues ± 2.
                                  15. Norm and Trace                                523

   REMARKS. The elements a1 , . . . , an of K, with a1 = a, are called the
conjugates of a over k. The conjugates of a are the images of a under the
Galois group when k(a) is Galois over k, but they extend outside k when k(a)/k
is not normal.
   PROOF. Corollary 9.56 shows that F(X) equals the field polynomial of a
relative to k(a)/k, i.e., is the characteristic polynomial of the multiplication
operator Mk(a)/k (a). Let A be the matrix of Mk(a)/k (a) in some ordered basis of
k(a) over k. If we regard A as a matrix with entries in K, then the characteristic
polynomial of A splits in K, and the roots of the characteristic polynomial have
multiplicity 1, by separability. Consequently A has a basis of eigenvectors, the
eigenvectors being column vectors with entries in K and the eigenvalues being the
members a1 , . . . , an of K. It follows that A is similar over K to a diagonal matrix
with diagonal entries a1 , . . . , an . The determinant and trace of this diagonal
matrix equal the determinant and trace of A, and therefore the norm and trace of
a are the product and sum of the members a1 , . . . , an of K.                      §

   Corollary 9.58. Let K be a finite Galois extension of the field k, let G =
Gal(K/k), let L be an intermediate field with k ⊆ L ⊆ K, and let H = Gal(K/L)
as a subgroup of G. Fix an ordered basis 0 of L over k. Then the expression “σ (a)
for σ ∈ G/H ” is well defined for a in L, and there exists a nonsingular
                                                                      ≥     matrix
                                                                                ¥
C of size [L : k] with entries in K such that every a in L has C −1 ML/k  00
                                                                            (a)
                                                                                  C
diagonal with diagonal entries σ (a) for σ ∈ G/H . In particular, every member
a of L has norm and trace given by
                       Y                                     X
          NL/k (a) =        σ (a)     and       TrL/k (a) =        σ (a).
                       σ ∈G/H                                   σ ∈G/H

   PROOF. Let a be in L, σ be in G, and τ be in H . Then τ (a) = a, and therefore
σ τ (a) = σ (a). Consequently all members of the coset σ H of G/H have the
same value on a, and “σ (a) for σ ∈ G/H ” is well defined.
   Let n = [L : k] = |G/H |. Fix an ordered basis 0 of L over k. For each a ∈ L,
let A(a) be the matrix of the multiplication operator M(a)L/k relative to 0.
   The Theorem of the Primitive Element (Theorem 9.34) shows that L = k(x)
for some x. Proposition 9.57 applies to this element x and to a splitting field
within K for its minimal polynomial, showing that there is a nonsingular matrix
C with entries in K such that C −1 A(x)C is a diagonal matrix whose diagonal
entries are the n conjugates x1 , . . . , xn of x in K, x1 being x; the diagonal entries
are necessarily distinct by separability. For each i with 1 ≤ i ≤ n, there exists σi
in G with σi (x) = xi by Theorems 9.11 and 9.23. Since H fixes L, every member
of the coset σi H carries x to xi . On the other hand, every σ in G must carry x to
some conjugate, hence must have σ (x) = σi (x) for some i. Then σi−1 σ fixes x
524                               IX. Fields and Galois Theory

and hence L, and it follows that σi−1 σ is in H . Thus σ is in σi H . In other words,
the conjugates x1 , . . . , xn may be regarded exactly as the images of the n cosets
σj H .
   In this terminology the diagonal entries of C −1 A(x)C are the n elements σ (x)
for σ in G/H . For each j with 0 ≤ j ≤ n−1, we have A(x j ) = A(x) j , and hence
C −1 A(x j )C = C −1 A(x) j C is diagonal with diagonal entries σ (x) j = σ (x j ) for
σ in G/H . Forming k linear combinations, we see for every polynomial P(X)
in k[X] of degree ≤ n − 1 that C −1 A(P(x))C is diagonal with diagonal entries
σ (P(x)). Every element a of K is of the form P(x) for some such P(X), and
the existence of C in the statement of the corollary is proved. The formulas for
the norm and trace follow by taking the determinant and trace.                      §

   Corollary 9.59. If K is a finite separable extension of the field k, then the
trace function TrK/k is not identically 0.
   REMARKS. This result is trivial in characteristic 0 because TrK/k (1) = [K : k]
is not zero. The result is not so evident in characteristic p, and the assump-
tion of separability is crucial. An example for which separability fails and
the trace function is identically 0 has k = F(x), where F is a finite field of
characteristic p and x is transcendental, and K = k(x 1/ p ). The basis elements
1, x 1/ p , x 2/ p , . . . , x ( p−1)/ p all have trace 0, and therefore the trace is identically 0.
  PROOF. By the Theorem of the Primitive Element (Theorem 9.34), we can
write K = k(a) for some a 6= 0. Let K0 be a splitting field for the minimal
polynomial of a over k. Then K0 /k is a separable extension by Corollary 9.30
and hence is a finite Galois extension. Proposition 9.57 shows that the matrix of
MK/k (a) in any ordered basis of K over k is similar over K0 to a diagonal matrix
with entries a1 , . . . , an , where a1 , . . . , an are the conjugates of a with a1 = a.
These conjugates are necessarily distinct by separability. For 1 ≤ k ≤ n, the
matrix of MK/k (a k ) is similar via the same matrix over K0 to a diagonal matrix
with entries a1k , . . . , ank . If TrK/k (a k ) = 0 for 1 ≤ k ≤ n, then we obtain the
homogeneous system of linear equations

                              a1 x1 + a2 x2 + · · · + an xn = 0,
                              a12 x1 + a22 x2 + · · · + an2 xn = 0,
                                                               ..
                                                                .
                              a1n x1 + a2n x2 + · · · + ann xn = 0,

with (x1 , . . . , xn ) = (1, . . . , 1) as a nonzero solution. The coefficient matrix must
therefore have determinant 0. This coefficient matrix, however, is a Vandermonde
matrix except that the j th column is multiplied by a j for each j. Since a1 , . . . , an
                                       15. Norm and Trace                                  525

are distinct, Corollary 5.3 shows that the determinant of the coefficient matrix
can be 0 only if a1 a2 · · · an = 0. Since a 6= 0, we have arrived at a contradiction,
and we conclude that TrK/k (a k ) 6= 0 for some k.                                  §

  With the aid of Corollary 9.59, we can complete the proof of Theorem 8.54 in
Section VIII.11. Let us restate the part that still needs proof.

    THEOREM 8.54. If R is a Dedekind domain with field of fractions F and if K
is a finite separable extension field of F, then the integral closure T of R in K is
finitely generated as an R module and consequently is a Dedekind domain.
   REMARKS. What needs proof is that T is finitely generated as an R module.
It was shown in Section VIII.11 how to deduce as a consequence that T is a
Dedekind domain.
   PROOF. Since R is Noetherian (being a Dedekind domain), Proposition 8.34
shows that it is enough to exhibit T as an R submodule of a finitely generated R
module in K . Let {u 1 , . . . , u n } be a vector-space basis of K over F. Proposition
8.42 shows that we may assume that each u i is in T .
   Define an F linear map from K into its F vector-space dual K 0 by y 7→ ` y ,
where ` y (x) = Tr K /F (x y) for x ∈ K . This map is one-one by Corollary 9.59,
and the equality of dimensions of K and K 0 over F therefore implies that the
map is onto. We can thus view every member of K 0 as uniquely of the form ` y
for some y in K . With this understanding, let {`v1 , . . . , `vn } be the dual basis of
K 0 with `vj (u i ) = δi j for all i and j. Then we have

                         Tr K /F (u i v j ) = δi j   for all i and j.

Applying Proposition 8.42, choose c 6= 0 in R with cv j in T for all j. We shall
complete the proof by showing that

                             T ⊆ Rc−1 u 1 + · · · + Rc−1 u n .                             (∗)

   Before doing so, let us observe that

                              Tr K /F (t) is in R if t is in T.                          (∗∗)

In fact, Proposition 9.57 shows that Tr F(t)/F (t) is the sum of all the conjugates of t,
whether or not they are in K . The conjugates have the same minimal polynomial
over F that t has, and hence they are integral over R. Their sum Tr F(t)/F (t) must
be integral over R by Corollary 8.38, and it must lie in F. Since R is integrally
closed (being a Dedekind domain), Tr F(t)/F (t) lies in R. This proves (∗∗).
   Now we can return to the proof of (∗). Let x be given in T . Since T is a ring,
cxv j is in T for each j, and Tr K /F (cxv j ) is in R by (∗∗). Since {u 1 , . . . , u n } is a
526                                IX. Fields and Galois Theory
                               P
basis, we can write x =           i   di u i with each di in F. Since Tr(cxv j ) is in R, the
computation
                                                        n
                                                        P
                  Tr(cxv j ) = c Tr K /F (xv j ) = c          di Tr(u i v j ) = cd j
                                                        i=1
                                                   P
shows that cd j is in R. Then the expansion x = i (cdi )c−1 u i exhibits x as in
Rc−1 u 1 + · · · + Rc−1 u n and completes the proof of (∗).                   §


                     16. Splitting of Prime Ideals in Extensions

Section VIII.7 was a section of motivation showing the importance for number
theory and geometry of passing from factorization of elements to factorization
of ideals. The later sections of Chapter VIII set the framework for this study,
examining the notions of Noetherian domain, integral closure, and localization
and putting them together in the notion of Dedekind domain. Only just now
were we able to complete the proof of the fundamental result (Theorem 8.54) for
constructing Dedekind domains out of other Dedekind domains. However, that
proposition does not complete the task of extending what is in Section VIII.7 to a
wider context. Much of Section VIII.7 concerned the relationship between prime
ideals in one domain and prime ideals in an extension. In the present section we
put that relationship in a wider context, showing how the examples of Section
VIII.7 are special cases of the present theory.
   In two of the examples in Section VIII.7, we worked with the ring Z of integers
inside its field of fractions Q and with the ring T of algebraic integers within a
quadratic extension K of Q. In the third example in that section, we worked
with the ring C[x], for transcendental x, inside its field of fractions C(x) and
with a certain integral domain T within a quadratic extension of C(x). For all
three examples we saw a correspondence between prime ideals P in T and prime
ideals ( p) in Z or C[x], and that correspondence was formalized in a more general
setting in Propositions 8.43 and 8.53. The objective now is to understand that
correspondence a little better.
   The notation for this section is as follows: Let R be a Dedekind domain, such
as Z or C[x], and let F be its field of fractions.19 Let K be a finite separable
extension of F, and let T be the integral closure of R in K . Theorem 8.54,
including the part just proved in the previous section, shows that T is a Dedekind
domain. We make repeated use of the fact about Dedekind domains that every
nonzero prime ideal is maximal.
       might seem more natural to assume that R is a principal ideal domain, as it is with Z and
   19 It

C[x]. But that extra assumption will not help us, and it will often not be satisfied when the present
results are used in the proof of the important Theorem 9.64 in the next section.
                          16. Splitting of Prime Ideals in Extensions                     527

    Proposition 8.43 shows that if P is any nonzero prime ideal of T , then p = R∩P
is a nonzero prime ideal of R. In the reverse direction Proposition 8.53 shows that
if p is any nonzero prime ideal in R, then pT 6= T , and there exists at least one
prime ideal P of T with p = R ∩ P. The unique factorization of ideals in T (given
as Theorem 8.55) explains this correspondence better. If p is given, then pT is a
proper ideal, hence is contained in some maximal ideal P. Since “to contain is
to divide” (by Theorem 8.55d), such P’s (and only such P’s) are factors in the
decomposition of pT as the product of nonzero prime ideals. Accordingly let us
write
                                           Yg
                                   pT =       Piei ,
                                              i=1

where the Pi are the distinct prime ideals of T containing pT , or equivalently the
distinct prime ideals of T satisfying R ∩ Pi = p. The ei are positive integers
called the ramification indices.
   For each Pi , we can form the composition R ⊆ T → T /Pi of inclusion
followed by passage to the quotient. Since p ⊆ Pi , this composition descends to
a ring homomorphism R/p → T /Pi . The ideal p is maximal in R, and the ideal
Pi is maximal in T . Thus the mapping R/p → T /Pi is in fact a field map. We
regard it as an inclusion. Define

                                    f i = [T /Pi : R/p],

allowing the dimension for the moment possibly to be +∞. It will follow from
Theorem 9.60, however, that f i is finite. The integer f i is called the residue class
degree.

    Theorem 9.60. Let R be a Dedekind domain, let F be its field of fractions, let
K be a finite separable extension of F with [K : F] = n, and let T beQgthe integral
closure of R in K . If p is a nonzero prime ideal in R and pT = i=1 Piei is a
decomposition of pT as the product of powers of distinct nonzero prime ideals in
T , then the ramification indices ei and residue class degrees f i = [T /Pi : R/p]
are related by
                                   X g
                                       ei f i = n.
                                       i=1

   REMARKS. Consequently each f i is finite. The cases of interest for our earlier
examples have R = Z or R = C[x]. When R = Z, each R/p is a finite field.
However, when R = K[x] for some field K of characteristic 0 like K = C, then
each R/p is a finite extension of K, hence is an infinite field.20
    20 When R = C[x], then T /P = R/p ∼ C since C is algebraically closed. The last example of
                                  i   =
the present section will elaborate.
528                          IX. Fields and Galois Theory

   PROOF. Corollary 8.63 gives a ring isomorphism
                                       e                 eg
                        T /(pT ) ∼
                                 = T /P1 1 × · · · × T /Pg .                   (∗)

Recall from the definition of residue class degree that we have a field mapping of
R/p into each T /Pi . Since p ⊆ Pie for 1 ≤ e ≤ ei and since p ⊆ pT , it follows
similarly that we have a one-one ring homomorphism of R/p into each T /Pie
with 1 ≤ e ≤ ei and another one-one ring homomorphism of R/p into T /(pT ).
                                                                                e
Consequently each T /Pie with 1 ≤ e ≤ ei , the product T /P1e1 × · · · × T /Pg g ,
and T /(pT ) may all be regarded as unital R/p modules, i.e., as vector spaces
over the field R/p. Fix i. For 1 ≤ e ≤ ei , let us prove by induction on e that

                               dim R/p (T /Pie ) = e f i ,                    (∗∗)

the case e = 1 being the base case of the induction. Assume inductively that (∗∗)
holds for exponents from 1 to e − 1. We know from Corollary 8.60 that Pie−1 /Pie
is a vector space over the field T /Pi with

                             dimT /Pi (Pie−1 /Pie ) = 1.                       (†)

The First Isomorphism
                    ±    Theorem (as in the remark with Theorem 8.3) gives
         = (T /Pie ) (Pie−1 /Pie ) as vector spaces over R/p, and it follows that
T /Pie−1 ∼

            dim R/p (T /Pie ) = dim R/p (T /Pie−1 ) + dim R/p (Pie−1 /Pie )
                              = (e − 1) f i + f i = e f i ,

the next-to-last equality following from (†) and the inductive hypothesis for the
cases e − 1 and 1. This completes the induction and the proof of (∗∗).
   In view of the decomposition (∗) and the formula (∗∗) when e = ei , the
theorem will follow if it is shown that

                              dim R/p (T /(pT )) = n.                         (††)

To prove (††) we localize. Let S be the complement of the prime ideal p of R.
Corollary 8.48 shows that S −1 R is a Dedekind domain, Corollary 8.50 shows
that S −1 p is its unique maximal ideal, and Corollary 8.62 shows that S −1 R is a
principal ideal domain.
   The composition R ⊆ S −1 R → S −1 R/S −1 p descends to a field mapping
R/p → S −1 R/S −1 p. Let us see that this mapping is onto. If s0−1r0 + S −1 p in
S −1 R/S −1 p is given, then s0 is not in p, and the maximality of p as an ideal in
R implies that (s0 ) + p = R. Therefore we can choose r in R and x in p with
rs0 + x = r0 . Under the mapping R/p → S −1 R/S −1 p, the image of r + p is
                          16. Splitting of Prime Ideals in Extensions                  529

r + S −1 p = r + s0−1 x + S −1 p = s0−1 (rs0 + x) + S −1 p = s0−1r0 + S −1 p. Thus
our mapping is onto S −1 R/S −1 p, and we have an isomorphism of fields

                                     R/p ∼
                                         = S −1 R/S −1 p.                              (‡)

   Similarly the composition T ⊆ S −1 T → S −1 T /(S −1 pT ) descends to a ho-
momorphism of rings T /pT → S −1 T /(S −1 pT ). Let us show that this map too
is one-one onto.
   If t + pT is in the kernel, then the member t of T is in S −1 pT , and st is in
                                                            e
pT for some s in S. Hence we have (s)(t) ⊆ P1e1 · · · Pg g , and we can write
            e1     eg
(s)(t) = P1 · · · Pg Q for some ideal Q. Factoring the principal ideals (s) and
(t) and using the uniqueness of factorization of ideals gives
                                 u                                         v
              (s) = P1u 1 · · · Pg g Q 1     and        (t) = P1v1 · · · Pg g Q 2

with Q = Q 1 Q 2 and with u j + v j = e j for all j. If u j > 0, then we must have
(s) ⊆ Pj and s R ⊆ Pj ∩ R = p. This says that s is in p, in contradiction to
the fact that S equals the set-theoretic complement of p in R. We conclude that
                                               e                     e
u j = 0 for all j. Therefore (t) = P1e1 · · · Pg g Q 2 ⊆ P1e1 · · · Pg g = pT , and t is in
pT . Consequently the kernel consists of the 0 coset alone.
    Let us show that T /pT maps onto S −1 T /(S −1 pT ). If s0−1 t0 + S −1 pT in
  −1
S T /S −1 pT is given, then s0 is not in p, and the maximality of p as an ideal
in R implies that (s0 ) + p = R. Therefore we can choose r in R and x in p
with rs0 + x = 1, hence with rs0 t0 + xt0 = t0 . Under the mapping T /pT →
S −1 T /(S −1 pT ), the image of rt0 + pT is

                     rt0 + S −1 pT =rt0 + s0−1 xt0 + S −1 pT
                                       = s0−1 (rs0 t0 + xt0 ) + S −1 pT
                                       = s0−1 t0 + S −1 pT.

Thus our mapping is onto S −1 T /S −1 pT , and we conclude that we have an
isomorphism of rings
                       T /pT → S −1 T /(S −1 pT ).                    (‡‡)
   Since T is finitely generated as an R module (Theorem 8.54), S −1 T is finitely
generated as an S −1 R module with the same generators. Since S −1 R is a principal
ideal domain, Theorem 8.25c shows that S −1 T is the direct sum of cyclic S −1 R
modules. Each of these cyclic modules must in fact be isomorphic to S −1 R since
S −1 T has no zero divisors, and therefore S −1 T is a free S −1 R module of some
finite rank m. If t1 , . . . , tm are free generators, then we have

                           S −1 T = S −1 Rt1 + · · · + S −1 Rtm .                      (§)
530                            IX. Fields and Galois Theory
                                                                            P
Let us see that {t1 , . . . , tm } is an F vector-space basis of K . Suppose j c j t j = 0
with all c j in F. Proposition     P 8.42 shows that there is an r 6= 0 in R with
rc1 , . . . , rcm in R. Then j (rc j )t j = 0, and the independence of t1 , . . . , tm
over S −1 R implies that rc j = 0 for all j. Thus c j = 0 for all j, and we obtain
linear independence over F. If x ∈ K is given, we can choose r 6= 0 in R with
                                                             −1         −1
r x in T by Proposition 8.42. Since t1 , . . . , tP m span S T over S PR, we can find
                                  −1
members d1 , . . . , dm of S R with r x = j d j t j . Then x = j r −1 d j t j with
each coefficient r −1 d j in F. This proves the spanning. Hence {t1 , . . . , tm } is an
F vector-space basis, and m = n.
    To complete the proof of (††) and hence the theorem, it is enough, in view of the
isomorphisms (‡) and (‡‡), to prove that the cosets t j + S −1 pT in S −1 T /(S −1 pT )
formPa vector-space basis over S −1 R/S −1 p. If t is in S −1 T , then (§) says that
t = c j t j with c j in S −1 R. Hence
                                         P
                      t + S −1 pT = (c j + S −1 p)(t j + S −1 pT ),
                                P                                               P
and we have spanning. If j (c j + S −1 p)(t j + S −1 pT ) = 0+ S −1 pT , then j c j t j
                                          P          P
is in S −1 pT . Thus we can write j c j t j = i ai ti0 with ai ∈ p and ti0 ∈ S −1 T .
Expanding each ti0 according to (§), substituting, and using the uniqueness of the
expansion (§), we see for each j that c j is a sum of products of the ai ’s by members
of S −1 R. Therefore each c j is in S −1 p. This proves the linear independence and
establishes (††).                                                                       §

   The case of greatest interest is that K is a finite Galois extension of F. In this
case the statement of Theorem 9.60 simplifies and will be given in its simplified
form as Theorem 9.62. We begin with a lemma.

   Lemma 9.61. Let R be a Dedekind domain, let F be its field of fractions,
let K be a finite separable extension of F, and let T be the integral closure of R
in K . QSuppose that K is Galois over F. If p is a nonzero prime ideal in R and
          g
pT = i=1 Piei is a decomposition of pT as the product of nonzero prime ideals
in T , then Gal(K /F) is transitive on the set of ideals {P1 , . . . , Pg }.
    PROOF. Arguing by contradiction, suppose that Pj is not of the form σ (P1 )
for some σ in Gal(K /F). By the Chinese Remainder Theorem we can choose
an element t of T with t ≡ 0 mod Pj and t ≡ 1 mod σ (P1 ) for all σ . Every σ
in Gal(K /F) carries t to a member of T since t and σ (t) have Q the same minimal
polynomial over F. Corollary 9.58 shows that N K /F (t) = σ ∈Gal(K /F) σ (t), and
consequently N K /F (t) is in T ∩ F = R. Since the factor Qg t itself is in Pj , N K /F (t)
                                                                  ei
is in Pj . Therefore N K /F (t) is in R ∩ Pj = p ⊆ i=1 Pi . The right side is
contained in P1 . Since P1 is prime, some factor σl (t) of N K /F (t) is in P1 . Then
t is in σl−1 (P1 ), in contradiction to the fact that t ≡ 1 mod σ (P1 ) for all σ . §
                            16. Splitting of Prime Ideals in Extensions                          531

     Theorem 9.62. Let R be a Dedekind domain, let F be its field of fractions,
let K be a finite separable extension of F with [K : F] = n, and let T be the
integral closure of R in K . Suppose
                                 Qg       that K is Galois over F. If p is a nonzero
                                         ei
prime ideal in R and pT = i=1 Pi is a decomposition of pT as the product
of powers of distinct nonzero prime ideals in T , then the ramification indices
have e1 = · · · = eg , and the residue class degrees f i = [T /Pi : R/p] have
 f 1 = · · · = f g . If e and f denote the common value of the ei ’s and of the f j ’s,
then
                                        efg = n .
                                                                                       Qg
  PROOF. For σ in Gal(K /F), apply σ to the factorization pT =                            i=1   Piei ,
obtaining
                                      g
                                      Q
                       pT = σ (P1 )e1   σ (Pi )ei .
                                                   i=2

Lemma 9.61 shows that σ (P1 ) can be any Pj , and unique factorization of ideals
(Theorem 8.55) therefore implies that e1 = e j . With the same σ , the fact that σ
respects the field operations implies that

                              T /P1 ∼
                                    = σ (T )/σ (P1 ) = T /Pj ,

and thus f 1 = f j . Substituting the values of the ei ’s and the f j ’s into the formula
of Theorem 9.60, we obtain e f g = n.                                                  §

   EXAMPLES WITH n = 2 CONTINUED FROM SECTION VIII.7.
                           p
   (1) R = Z and T = Z[ −1 ]. In this case, Z and T are both principal ideal
domains. We found three possible behaviors21 for the prime factorization of a
principal ideal ( p)T in T generated by a prime p > 0 in Z:
      (a) ( p)T is prime in T if p = 4m + 3. Here e = g = 1; so f = 2.
      (b) ( p)T = (a + ib)(a − ib) with p = a 2 + b2 if p = 4m + 1. Here e = 1
           and g = 2; so f = 1.
      (c) (2)T = (1 + i)2 . Here e = 2 and g = 1; so f = 1.
                            p
   (2) R = Z and T = Z[ −5 ]. In this case, T is not a unique factorization
domain and is in particular not a principal ideal domain. We gave examples of
three possible behaviors for the prime factorization of a principal ideal ( p)T in
T generated by a prime p > 0 in Z:
      (a) (11)T is prime in             p g = 1; so f = 2.
                           p T . Here e =
      (b) (2)T = (2, p 1 + −5)(2, 1 − −5). Here e = 1 and g = 2; so f = 1.
      (c) (5)T = ( −5 )2 . Here e = 2 and g = 1; so f = 1.
   21 The notation here fits with the notation in Theorem 9.62 and is different from the notation in

Section VIII.7.
532                           IX. Fields and Galois Theory
                                 p
   (3) R = C[x] and T = C[x, (x − 1)x(x + 1) ]. In this case, R is a principal
ideal domain, and we saw that T is not a unique factorization domain. We found
two possible behaviors for the prime factorization of a principal ideal ( p)T in T
generated by a prime p in C[x]:
       (a) (x − x0 )T = (x − x0 , y − y0 )(x − x0 , y + y0 ) if the equal expressions
           y02 = (x0 − 1)x0 (x0 + 1) are not 0. Here e = 1 and g = 2; so f = 1.
      (b) (x − x0 )T = (x − x0 , y)2 if x0 is in {−1, 0, +1}. Here e = 2 and
           g = 1; so f = 1.
The third type, with (x − x0 )T prime in T , does not arise. It cannot arise since
 f > 1 would point to a quadratic extension of C, yet C is algebraically closed.


                17. Two Tools for Computing Galois Groups

In Section 8 we mentioned that the effect of the Fundamental Theorem of Galois
Theory is to reduce the extremely difficult problem of finding intermediate fields
to the less-difficult problem of finding a Galois group. In the intervening sections
we have seen some illustrations of the power of this reduction, all in cases in
which the Galois group was close at hand.
    The problem of finding a Galois group in a particular situation is usually not
as easy as in those cases, and it by no means can be considered as solved in
general. In this section we combine Galois theory with some of the ring theory
in the second half of Chapter VIII in order to develop two tools that sometimes
help identify particular Galois groups.
    Let us think in terms of a finite Galois extension K of the rationals Q. The
field K is the splitting field of some irreducible monic polynomial with rational
coefficients, and we can scale this polynomial’s indeterminate (in effect by multi-
plying its roots by some nonzero integer) so that the polynomial is monic and has
integer coefficients. Thus let F(X) be a monic irreducible polynomial in Z[X] of
some degree d, and let K be its splitting field over Q. The members of Gal(K /Q)
are determined by their effect on the d roots of F(X), and hence Gal(K /Q) may
be regarded as a subgroup of the symmetric group Sd . If r1 , . . . , rd are the roots
of F(X), then the discriminant of F(X) is the member of K defined by
                                       Q
                               D=            (r j − ri )2 .
                                    1≤i< j≤d

This was defined in Section 13 in the cases d = 2 and d = 3, and we computed the
value of D in those cases. The discriminant is an integer under our hypotheses,
and it is computable even though the roots r1 , . . . , rd of F(X) are not at hand. In
fact, the proof of Theorem 9.50 indicates that the discriminant D is given by the
determinant
                         17. Two Tools for Computing Galois Groups                        533
                                                                   
                              d          a1      a2     ···    ad−1
                             a1         a2      a3     ···     ad 
                                                                   
                    D = det  a          a3      a4     ···    ad+1  ,
                             2                          ..         
                                                         .         
                                  ad−1   ad    ad+1     ···   a2d−2
                j    j            j
where a j = r1 + r2 + · · · + rd . Problems 36–39 at the end of Chapter VIII show
that each of a1 , . . . , a2d−1 can be expressed as a polynomial in the elementary
symmetric polynomials in r1 , . . . , rd , i.e., in the coefficients of F(X), and doing
so in a symbolic manipulation program is manageable for any fixed degree.22
    The first of the two tools that sometimes help in identifying particular Galois
groups directly concerns the discriminant: the discriminant is a square if and only
if the Galois group is a subgroup of the alternating group. Let us state the result
in the context of a general finite Galois extension even though we shall use it only
for our Galois extension K /Q.

    Proposition 9.63. Let K/k be a finite Galois extension, and suppose that K
is the splitting field of a separable polynomial F(X) in k[X] of degree d. Let
D be the discriminant of F(X), and regard G = Gal(K/k) as a subgroup of the
symmetric group Sd . Then D is in k, and G is a subgroup of the alternating
group Ad if and only if D is the square of an element of k.
   REMARK. The proof will use Galois theory to show that D is in k, and Problems
36–39 at the end of Chapter VIII do not need to be invoked.
                                                                    Q
   PROOF. Let r1 , . . . , rd be the roots of F(X), and put 1 = i< j (r j − ri ).
Under the identification of G with a subgroup of the permutation group Sd on
{1, . . . , d}, each σ in G has
            Q                   Q                          Q
σ (1) = (σ (r j )−σ (ri )) = (rσ ( j) −rσ (i) ) = (sgn σ )   (r j −ri ) = (sgn σ )1.
         i< j                     i< j                           i< j

   22 For example, when d = 3, let F(X) = X 3 − c X 2 + c X − c . In Mathematica the following
                                                 1       2     3
program produces a1 , a2 , a3 , a4 as output:
e1={a1==r1+r2+r3, r1+r2+r3==c1, r1 r2+r2 r3+r1 r3==c2,
     r1 r2 r3==c3}
Eliminate[e1,{r1,r2,r3}]
e2={a2==r1∧2+r2∧2+r3∧2, r1+r2+r3==c1, r1 r2+r2 r3+r1 r3==c2,
     r1 r2 r3==c3}
Eliminate[e2,{r1,r2,r3}]
e3={a3==r1∧3+r2∧3+r3∧3, r1+r2+r3==c1, r1 r2+r2 r3+r1 r3==c2,
     r1 r2 r3==c3}
Eliminate[e3,{r1,r2,r3}]
e4={a4==r1∧4+r2∧4+r3∧4, r1+r2+r3==c1, r1 r2+r2 r3+r1 r3==c2,
     r1 r2 r3==c3}
Eliminate[e4,{r1,r2,r3}]
534                           IX. Fields and Galois Theory

In particular, the element D = 12 has σ (D) = D. By Proposition 9.35d, D is
in k.
   If some σ in G has sgn σ = −1, then σ does not fix 1, and 1 is not in k.
Since 1 is a square root of D and since any two square roots of an element in a
field differ at most by a sign, D is not the square of any element of k.
   Conversely if every σ in G has sgn σ = +1, then every σ fixes 1, and
Proposition 9.35d shows that 1 is in k. Since D = 12 , D is the square of the
member 1 of k.                                                               §

   The second tool is complicated to prove but simple to state. We reduce the
polynomial F(X) modulo p for each prime number p and form the associated
finite splitting field. The Galois group for a finite extension of finite fields is
cyclic by Proposition 9.40, and we thus obtain a cyclic subgroup of Sd . The
second tool is this: if p does not divide the discriminant of F(X), then this cyclic
group as a permutation group is a subgroup of Gal(K /Q) as a permutation group,
up to a relabeling of the symbols. In other words, the order and cycle structure
of a generator of the cyclic group are the same as the order and cycle structure of
some element of Gal(K /Q).
   Let us formulate the result precisely. In the setting of Theorem 9.62, fix a prime
ideal P of T lying in the factorization of pT . Each member σ of G = Gal(K /F)
carries T to itself, but not every σ in G carries P to itself. Let G P be the isotropy
subgroup of G at P, i.e., let G P = {σ ∈ G | σ (P) = P}. The subgroup
G P is called the decomposition group at P. Each σ in G P descends to an
automorphism of the field T /P that fixes the subfield R/p, since σ fixes each
element of R. Thus σ defines a member σ of G = Gal((T /P)/(R/p)) by the
formula
                 σ (x̄) = σ (x),      where ȳ = y + P for y ∈ T.

It is apparent that σ 7→ σ is a homomorphism of G into G. This homomorphism
turns out to yield the result stated informally in the previous paragraph. It has the
key property given in Theorem 9.64.

    Theorem 9.64. Let R be a Dedekind domain, let F be its field of fractions, let
K be a finite separable extension of F with [K : F] = n, and let T be the integral
closure of R in K . Suppose that K is Galois over F. Let p be a nonzero
                                                                 Qg     prime ideal
in R, let P = P1 be a prime factor in a decomposition pT = i=1 Piei of pT as
the product of powers of distinct nonzero prime ideals in T , and suppose that T /P
is a Galois extension of R/p. Let G = Gal(K /F), G P = {σ ∈ G | σ (P) = P},
and G = Gal((T /P)/(R/p)). Then the group homomorphism σ 7→ σ of G P
into G carries G P onto G.
                          17. Two Tools for Computing Galois Groups                         535

   REMARKS. In our application with R = Z, T /P and R/p are finite fields, and
Proposition 9.40 shows that T /P is a Galois extension of R/p with no further
assumptions.
   PROOF. Let K d be the fixed field of G P within K ; Theorem 9.38 shows that
Gal(K /K d ) = G P . Let T d be the integral closure of R in K d ; this is a Dedekind
domain, and T is the integral closure of T d in K . We are going to apply Theorem
9.62 with R in the theorem replaced23 by T d .
   Proposition 8.43 shows that P = T d ∩ P is a nonzero prime ideal of T d . Since
every member of G P carries P to itself and since G P is the full Galois group
of K over K d , Lemma 9.61 shows that P is the only nonzero prime ideal of T
                                                         0
whose intersection with T d is P. Therefore PT d = P e for some integer e0 ∏ 1.
   As always, we have a field mapping R/p → T d /P. Let us show that this
mapping is onto T d /P. For any given u in T d , we are to produce r in R with

                                       r ≡ u mod P.                                         (∗)

Each σ in G that is not in G P has σ −1 P 6= P, and the previous paragraph shows
that the nonzero prime ideal Pσ = T d ∩σ −1 P of T d has Pσ 6= T d ∩ P. Therefore
Pσ + P = T d , and the Chinese Remainder Theorem (Theorem 8.27) shows that
we can find an element v of T d with

                      v ≡ u mod P            and        v ≡ 1 mod Pσ

for all σ that lie in G but not G P . The first congruence implies that v − u is in
P = T d ∩ P ⊆ P, hence that

                                       v ≡ u mod P,                                       (∗∗)

while the second congruence implies that v − 1 is in Pσ = T d ∩ σ −1 P ⊆ σ −1 P,
hence that σ (v − 1) lies in P. Therefore

                   σ (v) ≡ 1 mod P           for all σ in G but not G P .                   (†)

Put r = N K d /F (v). Since the splitting field of the minimal polynomial of v over
F is contained in K , Corollary 9.58 shows that r is the product of the elements
σ (v) for σ in G/G P . Each of these is in T , and hence N K d /F (v) is in T . Since
N K d /F (v) is also in F, r = N K d /F (v) is in T ∩ F = R. If we use σ = 1 as the
representative of the identity coset of G/G P , then we have
                                                ° Q          ¢
                          r = N K d /F (v) = v          σ (v) .
                                                    some σ ’s
                                                    not in G P

    23 Consequently it would not have been sufficient to prove Theorem 9.62 when the ring R is a

principal ideal domain.
536                            IX. Fields and Galois Theory

The factor of v is congruent to u mod P by (∗∗), and each factor in parentheses
is congruent to 1 mod P by (†). Therefore r ≡ u mod P, and r − u is in P.
Since r − u is in T d , r − u is in T d ∩ P = P. This proves (∗). Consequently we
can identify G = Gal((T /P)/(R/p)) with Gal((T /P)/(T d /P)).
    Choose x̄1 in T /P with T /P = (T d /P)[x̄1 ]; this choice is possible by the
assumed separability of (T /P)/(R/p). Let x1 be a member of T with x̄1 = x1 + P,
and let M(X) be the minimal polynomial of x1 over K d . Since x1 is in T , the
coefficients of M(X) are in T d . Let M(X) be the corresponding member of
(T d /P)[X], given by the substitution homomorphism that takes T d to T d /P and
takes X to X. Since K /K d is normal, M(X) splits over K . Write x1 , . . . , xn for
its roots; these are in T .
    Let τ be given in G, and suppose that τ (x̄1 ) = x̄ j . Since M(X) is irreducible
over K d , the Galois group Gal(K /K d ) = G P is transitive on its roots. Choose σ
in G P with σ (x1 ) = x j . Then σ (x̄1 ) = x̄ j . Since σ and τ agree on the generator
x̄1 of T /P over T d /P, they agree on T /P. Therefore τ is exhibited as the image
of σ under the homomorphism of the theorem, and the proof is complete.               §

    A first consequence of Theorem 9.64 is that we get interpretations of the
integers e, f , and g, and they will be helpful to us. Galois theory gives us
|G| = n, and Theorem 9.62 says that e f g = n. The transitivity in Lemma 9.61
says that G acts transitively on the set {P1 , . . . , Pg }, and the isotropy subgroup at
P = P1 is G P . Hence g|G P | = |G|, and |G P | = n/g = e f . Galois theory gives
us |G| = f , and the fact that G P maps onto G says that G P /kernel ∼     = G; therefore
|kernel| = |G P |/|G| = (e f )/ f = e. We conclude that g is the number of cosets
modulo G P , e is the order of the kernel of the homomorphism in Theorem 9.64,
and f is the order of the cyclic group G.
    In the setting of interest for current purposes, we are taking R = Z, F = Q,
and K equal to the splitting field of a given monic irreducible polynomial F(X) of
degree d in Z[X]. We will be using Theorem 9.64 for various choices of p = ( p)
in Z to make progress on identifying Gal(K /Q). In order to identify G with the
subgroup G P of G, we need the kernel of the homomorphism of G P onto G to
be trivial. From the previous paragraph we know that the condition in question
is that e = 1. We postpone to Chapter V of Advanced Algebra any justification
of the assertion that e = 1 if p does not divide the discriminant of F(X).
    In previous sections we have identified Gal(K /Q) in some cases when the
Galois group is relatively small compared with the degree d of the polynomial.
The method now is helpful when the Galois group is relatively large compared
with d.
    Let us be sure when e = 1 that the theorem is telling us not only that G P
is isomorphic to G as an abstract group, but also that the cycle structure of the
elements of G is the same as the cycle structure of the elements of G P . For this
                        17. Two Tools for Computing Galois Groups                      537

purpose we ignore the proof of the theorem and concentrate only on the statement.
Assuming that p does not divide the discriminant, let F(X) be the reduction of
F(X) modulo p, let r1 , . . . , rd be the roots of F(X) in T , and let r̄1 , . . . , r̄d be
the images of r1 , . . . , rd under the quotient homomorphism T → T /P. The
elements r̄1 , . . . , r̄d are distinct since p does not divide the discriminant of F(X).
Any member σ of G = Gal(K /Q) permutes r1 , . . . , rd and is determined by the
resulting permutation since K is assumed to be generated by r1 , . . . , rd . Under
the assumption that σ is in G P , σ descends to an automorphism σ of T /P. This
automorphism σ acts on the set of elements r̄1 , . . . , r̄d , permuting them. Since
the mapping of the r j ’s to the r̄ j ’s is one-one, the resulting permutation of the
subscripts 1, . . . , d is the same.
    When p varies, we cannot match the elements r̄1 , . . . , r̄d for one value of
p with those for another value of p, because we have no direct knowledge of
r1 , . . . , rd . Thus we cannot directly compare the permutation groups G that we
obtain for different p’s. But at least we know their cycle structure.
    To apply the theory, we factor F(X) quickly with a symbolic manipulation
program, and we obtain the Galois group of a splitting field of F(X) by inspection,
together with the cycle structure of its elements. Specifically an irreducible factor
of degree m contributes an m-cycle for the element, and the cycles corresponding
to distinct irreducible factors are disjoint. Then we put together the information
from various p’s and see what elements must be in Gal(K /Q), up to a relabeling
of indices.

  EXAMPLE 1. F(X) = X 5 − X − 1. The discriminant is D = 2869 = 19 · 151.
Thus the method may be used with any prime number other than 19 and 151.
Here is the factorization for a few primes, together with the cycle structure within
S5 for a generator of G:
      p                             F(X)                             Cycle lengths

     2               (X 2 + X + 1)(X 3 + X + 1)                            2, 3
     3                       X 5 + 2X + 2                                    5
     17        (X + 9)(X + 11)(X 3 + 14X 2 + 12X + 6)                     1, 1, 3
     23         (X + 9)(X 4 + 14X 3 + 12X 2 + 7X + 5)                      1, 4
For comparison, p = 19 gives F(X) = (X + 6)2 (X 2 + 7X 2 + 13X + 10), but
we cannot use this prime since it divides the discriminant. It is enough to use
the information from p = 2 and p = 3. The irreducibility modulo 3 implies
irreducibility over Q. From p = 3, we obtain a 5-cycle in Gal(K /Q). From
p = 2, we obtain the product of a 2-cycle and a 3-cycle, and the cube of this
element is a 2-cycle. In the example in Section 11 following the statement of
Theorem 9.44, we saw in effect that the only subgroup of S5 containing a 5-cycle
and a 2-cycle is S5 itself. Therefore Gal(K /Q) = S5 .
538                         IX. Fields and Galois Theory

   EXAMPLE 2. F(X) = X 5 + 10X 3 − 10X 2 + 35X − 18. The discriminant
is D = 3025000000 = 26 58 112 , a perfect square. Thus the Galois group is a
subgroup of the alternating group A5 . The method using reduction modulo p
may be used with any prime other than 2, 5, and 11. Here is the factorization for
a few primes, together with the cycle structure within S5 for a generator of G:

      p                             F(X)                              Cycle lengths

      3           X (X + 2)(X 3 + X 2 + 2X + 1)                          1, 1, 3
      7               X 5 + 3X 3 + 4X 2 + 3                                 5
      17     (X + 14)(X 2 + 5X + 14)(X 2 + 15X + 15)                     1, 2, 2

It is enough to use the information from p = 3 and p = 7. The irreducibility
modulo 7 implies irreducibility over Q. From p = 7, we obtain a 5-cycle in
Gal(K /Q). From p = 3, we obtain a 3-cycle. Any 5-cycle and any 3-cycle
together generate all of A5 . In fact, the generated subgroup must have order
divisible by 15, hence must have order 15, 30, or 60. It cannot be of order 15
because every group of order 15 is cyclic and A5 has no elements of order 15. It
cannot be of order 30 because A5 is simple and subgroups of index 2 have to be
normal. Hence it is all of A5 .

   EXAMPLE 3. Galois group Sd . Given d ∏ 4, let us see how to form an
irreducible F(X) for which Gal(K /Q) is all of Sd . For any degree d and any
prime number `, there exists at least one irreducible monic polynomial of degree
d in F` [X]; the reason is that the finite field F`d is a simple extension of F` by
Corollary 9.19. Let Hd,2 (X) be such a polynomial of degree d for ` = 2, and let
Hd−1,3 (X) be such a polynomial of degree d − 1 for ` = 3. Then let p be a prime
greater than d, and let H2, p (X) be an irreducible monic polynomial of degree 2
in F p [X]. We can regard each of Hd,2 (X), Hd−1,3 (X), and H2, p (X) as in Z[X]
by reinterpreting their coefficients as integers. Consider the congruences

                   F[X] ≡ Hd,2 (X)                         mod (2),
                   F[X] ≡ X Hd−1,3 (X)                     mod (3),
                            ° d−3
                              Q            ¢
                   F[X] ≡           (X − k) H2, p (X) mod ( p),
                             k=0

in Z[X]. Since the sum of any two of the three ideals (2), (3), and ( p) of Z[X] is
Z[X], the Chinese Remainder Theorem (Theorem 8.27) implies that there exists a
simultaneous solution F[X] to these congruences in Z[X], and we may take F[X]
to be monic of degree d. Let K be a splitting field for F[X] over Q. Our method
applies to the primes 2, 3, and p since none of the three polynomials has any
                                     18. Problems                                   539

repeated factors. The result of applying the method is that Gal(K /Q) contains
a d-cycle, a (d−1)-cycle, and a 2-cycle. Let us see that the subgroup generated
by these three elements is all of Sd . We may assume that the (d − 1)-cycle is
(1 2 · · · d−1). Without loss of generality, the 2-cycle is either (1 j) with j < d
or is (k d) with k < d. In the first case some power of the d-cycle is a permutation
τ with τ (1) = d; if σ denotes the 2-cycle (1 j), then Lemma 4.41 shows that
τ σ τ −1 is the 2-cycle (d τ ( j)), and this is of the form (k d) with k < d. Thus
we may assume in any event that Gal(K /Q) contains (1 2 · · · d−1) and some
2-cycle (k d) with k < d. Conjugating (k d) by powers of (1 2 · · · d−1), we
see that Gal(K /Q) contains every 2-cycle (k d) with k < d. For 1 ≤ k < d − 1,
we then find that Gal(K /Q) contains
                       (k d)(k + 1 d)(k d) = (k k + 1).
So Gal(K /Q) contains (1 2), (2 3), . . . , (d−2 d−1), and we have already seen
that it contains (d−1 d). These d − 1 transpositions generate the full symmetric
group, and therefore Gal(K /Q) = Sd .

                                    18. Problems
1.   Take as known that the polynomial X 3 − 3X + 4 is irreducible over Q, and let
     r be a complex root of it. In the field Q(r), find a multiplicative inverse for
     r 2 + r + 1 and express it in the form ar 2 + br + c with a, b, c in Q.
2.   Suppose that R is an integral domain and that F is a subring that is a field, so
     that R can be considered as a vector space over F. Prove that if dim F R is finite,
     then R is a field.
3.   Let K be a subfield of C that is not a subfield of R. Prove that K is topologically
     dense in C.
4.   Let K = k(x) be a transcendental extension of the field k, and let y be a member
     of K that is not in k. Prove that k(x) is an algebraic extension of k(y).
5.   What is a necessary and sufficient condition on an integer N > 0 for the positive
                                               p
     square root of N to be in the subfield Q( 3 2 ) of R?
6.   The polynomials F(X) = X 3 + X + 1 and G(Y ) = Y 3 + Y 2 + 1 are irreducible
     over F2 . Let K be the field K = F2 [X]/(F(X)), and let L be the field L =
     F2 [Y ]/(G(Y )). Since K and L are two fields of order 8, they must be isomorphic.
     Find an explicit isomorphism.
7.   Can a field of order 8 have a subfield of order 4? Why or why not?
8.   If K is a finite field, prove that the product of the nonzero elements of K is −1.
     (Educational note: When K is F p , this result reduces to Wilson’s Theorem, given
     as Problem 8 at the end of Chapter IV.)
9.   Suppose that K/k is a finite extension of the form K = k(r) with [K : k] odd.
     Prove that K = k(r 2 ).
540                           IX. Fields and Galois Theory

10. Suppose that K/k is a finite extension of fields and that K = k[r, s]. Prove that
    if [k(r) : k] is relatively prime to [k(s), k], then
    (a) the minimal polynomial of r over k is irreducible over k(s),
    (b) [K : k] = [k(r) : k] [k(s) : k].
                    p                   p
11. In C, let β = 3 2, ω = 12 (−1 + −3), and α = ωβ.
    (a) Prove for all c in Q that ∞ = β +cα is a root of some sixth-degree polynomial
         of the form X 6 + a X 3 + b.
    (b) Prove that the minimal polynomial of β + α over Q has degree 3.
    (c) Prove that the minimal polynomial of β − α over Q has degree 6.
12. Suppose that k is a finite field and that F(X) is a member of k[X] whose derivative
    is the 0 polynomial. Prove that F(X) is reducible over k.
13. Let k be a field, let F(X) be a separable polynomial in k[X], let K be a splitting
    field of F(X) over k, and let r1 , . . . , rn be the roots of F(X) in K. Regard
    Gal(K/k) as a subgroup of the symmetric group Sn .
    (a) Prove that Gal(K/k) is transitive on {r1 , . . . , rn } if and only if F(X) is
         irreducible over k.
    (b) Show that the cyclotomic polynomial 88 (X) is an example with k = Q and
         n = 4 for which Gal(K/k) is transitive but Gal(K/k) contains no 4-cycle.
    (c) Prove that if n is prime and F(X) is irreducible over k, then Gal(K/k)
         contains an n-cycle.
14. Let a1 , . . . , an be relatively prime square-free integers ∏ 2, and define Lk =
       p              p
    Q( a1 , . . . , ak ) for 0 ≤ k ≤ n.
    (a) Show for each k that [Lk : Q] = 2l with 0 ≤ l ≤ k.
    (b) Suppose for a particular k that [Lk : Q] = 2k . Exhibit a vector-space basis
        of Lk over Q, and describe the members of Gal(Lk /Q) by telling the effect
        of each member on all basis vectors of Lk over Q.
                                                                            p
    (c) Suppose for a particular k < n that [Lk : Q] = 2k . Assume that ak+1 lies
                           p
        in Lk , and let ak+1 be expanded in terms of the basis of (b). Show that
        application of the members of Gal(Lk /Q) leads to a contradiction.
    (d) Deduce that [Ln : Q] = 2n .
15. Let p be a prime number, and suppose that a is a member of Q such that X p − a
    has no root in Q. If r is a member of C with r p = a, prove that
    (a) the cyclotomic polynomial 8 p (X) is irreducible in Q(r),
    (b) the splitting field K of X p − a over Q has degree [K : Q] = p( p − 1),
    (c) the Galois group Gal(K/Q) is isomorphic to a semidirect product of the
        multiplicative group of F p and the additive group of F p , with the action of
        a member m of the multiplicative group on the members n of the additive
        group being given by m(n) = mn.
                                      18. Problems                                   541

16. Let F(X) be a polynomial in k[X] of degree n, where k is a field of character-
    istic 0, and let K be a splitting field for F(X) over k. Prove that [K : k] divides
    n!.

17. Let k be a field, and let K be a quadratic extension k(r), where r 2 = a is a
    member of k.
    (a) If k has characteristic 0, determine all elements of K whose squares are in k.
    (b) What happens differently if the characteristic is different from 0?

18. Let G be a finite group. Show that there exist two finite extensions k and K
    of Q such that K is a Galois extension of k and the Galois group Gal(K/k) is
    isomorphic to G.

19. Let K/k be a finite normal extension. For F(X) in K[X] and σ in Gal(K/k), let
    F σ (X) be the result of the substitution homomorphism K[X] → K[X] carrying
    X to X and extending the action of σ on K, Qi.e., let F σ (X) be obtained by applying
    σ to the coefficients of F(X). Prove that σ ∈Gal(K/k) F σ (X) is in k[X].

20. Corollary 9.37 concerns a separable algebraic extension K/k and a finite sub-
    group H of Gal(K/k), showing that K/K H is a finite Galois extension with
    H = Gal(K/K H ) and [K : K H ] = |H |. By going over its proof, obtain the
    conclusion that if {x1 , . . . , xn } is the H orbit of x1 in K, then
                                                          Q
    (a) the minimal polynomial of x1 over K H is nj=1 (X − x j ).
    (b) n divides |H |.
    (c) K = K H (x1 ) if the isotropy subgroup of H at x1 is trivial.

21. Let K be the transcendental extension C(z) of C.
    (a) Prove that any linear fractional transformation ϕ(z) = az+b
                                                               cz+d with ad−bc 6= 0
        in C extends uniquely to a C automorphism of K.
    (b) Let H be the 4-element subgroup of Gal(K/C) generated by the extensions
        of σ (z) = −z and τ (z) = 1/z. Show that w = z 2 + z −2 is invariant under
        H , and conclude that every member of C(w) lies in K H .
    (c) Applying the previous problem to the element x1 = z of K, show that the
        minimal polynomial of z over C(w) has degree 4.
    (d) Conclude that K H = C(z 2 + z −2 ).

22. In characteristic 0, let L/K and K/k be quadratic extensions.
    (a) Show that there exists an irreducible polynomial F(X) = X 4 + bX 2 + c in
         k[X] such that F(r) = 0 for some r in L.
    (b) Show that the element r in (a) has L = k(r).
    (c) Show that L is a normal extension of k with Galois group C2 × C2 if and
         only if c is a square in k for some polynomial as in (a), if and only if c is a
         square in k for every polynomial as in (a).
542                            IX. Fields and Galois Theory

      (d) Show that L is a normal extension of k with Galois group C4 if and only if
          c−1 (b2 − 4c) is a square in k for some polynomial as in (a), if and only if
          c−1 (b2 − 4c) is a square in k for every polynomial as in (a).
      (e) Give an example of quadratic extensions L/K and K/k in characteristic 0
          such that L/k is not normal.
23. Determine Galois groups for splitting fields over Q for the two polynomials
    X 3 − 3X + 1 and X 3 + X + 1.
24. Suppose that F(X) is an irreducible cubic polynomial in Q[X] whose splitting
    field K has Gal(K/Q) isomorphic to S3 . What are the possibilities, up to
    isomorphism, for the Galois group of a splitting field of (X 3 − 1)F(X) over Q?
25. Let K/k be a finite Galois extension whose Galois group is isomorphic to S3 .
    Is K necessarily a splitting field of some irreducible cubic polynomial in k[X]?
    Why or why not?
26. Is Cardan’s cubic formula valid for finding roots of reducible cubics X 3 + p X +q
    in characteristic 0?
27. Prove that the discriminant of a real cubic with distinct roots is positive if all the
    roots are real, and is negative if two of the roots are complex.
28. Let F(X) = X 3 + p X + q be irreducible in Q[X], and suppose that X − r is a
    factor for some r in C.
    (a) Show that F(X) factors in Q(r)[X] as F(X) = (X −r)(X 2 +r X +(r 2 + p)).
    (b) We know that Q(r) is a splitting field for F(X) over Q if and only if
         the discriminant −4 p3 − 27q 2 is a square in Q. On the other hand, it is
         evident from the factorization of F(X) that it splits is Q(r) if and only if the
         discriminant r 2 −4(r 2 + p) is a square in Q(r). Show by a direct calculation
         that these two conditions are equivalent.
29. Let K be a splitting field of an irreducible cubic polynomial F(X) in Q[X]. If
    Gal(K/Q) is S3 , does it follow that K contains all three cube roots of 1? Why
    or why not?
30. In characteristic 0, let K be the splitting field over k of an irreducible polynomial
    in k[X] of degree 5. Assuming that the discriminant of the polynomial is a square
    in k, what are the possibilities for Gal(K/k) up to a relabeling of the indices?
31. Determine the Galois group of a splitting field over Q for the polynomial
    X 5 + 6X 3 − 12X 2 + 5X − 4. Use of a computer may be helpful for this
    problem.
32. The proof of Theorem 9.64 introduced a positive integer e0 in its second paragraph.
    Prove that e0 equals the integer e1 in the statement of the theorem.
                                     18. Problems                                   543

33. Let R be a Dedekind domain, let F be its field of fractions, let K be a finite
    separable extension of F, and let L be a finite separable extension of K . Let T
    be the integral closure of R in K , and let U be the integral closure of R in L. Let
    p, P, and Q be nonzero prime ideals in R, T , and U , respectively, and let the
    ramification indices and decomposition degrees for the extensions L/K , L/F,
    and K /F be
            e(Q|P), e(P|p), e(Q|p)         and      f (Q|P), f (P|p), f (Q|p).
    Prove that
           e(Q|p) = e(Q|P)e(P|p)           and      f (Q|p) = f (Q|P) f (P|p).



Problems 34–40 concern norms and traces.
34. Let m be a square-free integer, and let N and Tr denote the norm and trace from
       p
    Q( m ) to Q.
                            p                                p
    (a) Show that N (a + b m ) = a 2 − mb2 and Tr(a + b m ) = 2a.
                                                      p
    (b) Let T be the ring of algebraic integers in Q( m ). It was shown in Section
                                             p
         VIII.9 that T consists of all a + b m with a, b in Z if m ≡ 2 mod 4 or
                                            p
         m ≡ 3 mod 4, and of all a + b m with a, b in Z or a, b in Z + 12 if
                                        p         p                 p
         m ≡ 1 mod 4. Prove
                         p for a + b m p     in Q( m ) that a + b m is in T if and
         only if N (a + b m ) and Tr(a + b m ) are both in Z.
                           p                                p
    (c) Assume that a + b m is in T . Prove that N (a + b m ) is in Z× if and only
                 p
         if a + b m is in T × .
    (d) For m = 2, give an example of a member of T × other than ±1.
                         p
35. For the extension Q( 3 2 )/Q, find the value of the norm N and the trace Tr on a
                          p         p           p
    general element a + b 3 2 + c( 3 2 )2 of Q( 3 2 ); here a, b, c are in Q.
36. Let N ( · ) be the norm relative to the extension Q(≥ )/Q, where ≥ is a primitive
    n th root of 1.
    (a) Show that N (1−≥ ) = 8n (1), where 8n (X) is the n th cyclotomic polynomial.
                               Q
    (b) Using the formula d|n, d>1 8d (X) = X n−1 + X n−2 + · · · + 1, show that
          N (1 − ≥ ) = 8n (1) equals p if n is a power of the positive prime p and
          equals 1 if n is divisible by more than one positive prime.
37. Let p > 0 be a prime in Z of the form 4n + 1. It was shown in Problem 31
    at the end of Chapter VIII that such a prime is the sum of two squares. This
    problem
       p      gives a shorter proof. Take as known from Section VIII.4 that the ring
    Z[ −1 ] of Gaussian integers is a Euclidean domain, and from Problem 30 at
    the end of Chapter VIII that x 2 ≡ −1 mod p has an integer solution x. Carry
    out the following steps:
544                                IX. Fields and Galois Theory

      (a) Write
                                           p
                                      x±     −1  1   1p
                                                = x±   −1.
                                           p     p   p
                                 p
          If p were prime in Z[ −1 ], then p it would follow from   p the divisibility of
          x 2 + 1 by p that p divides x + −1 or p divides x − −1. Deduce from
          the displayed equationpthat neither alternative is viable, and conclude that p
          cannot be prime in Z[ −1 ].
                                                                                  p
      (b) Using the conclusion of (a) to write p as a nontrivial product in Z[ −1]
          and applying the norm function, prove that there exist integers a and b such
          that p = a 2 + b2 .
38. Let p > 0 be a prime in Z of the form 8n + 1. Take as known from Problem
                                         p
    13 at the end of Chapter VIII that Z[ −2 ] is a Euclidean domain, and from the
    law of quadratic reciprocity (to be proved in Chapter I of Advanced Algebra)
    that x 2 ≡ −2 mod p has an integer solution x. Guided by the argument for the
    previous problem, prove that there exist integers a and b such that p = a 2 + 2b2 .
39. Let p > 0 be a prime in Z of the form 6n + 1. Take as known from Problem
                                        £        p       §
    26 at the end of Chapter VIII that Z 12 (1 + −3 ) is a Euclidean domain, and
    from the law of quadratic reciprocity (to be proved in Advanced Algebra) that
    x 2 ≡ −3 mod p has an integer solution x. Guided by the argument for the
    previous problem, prove that there exist integers a and b such that p = a 2 + 3b2 .
40. Let k ⊆ L ⊆ L0 be fields such that L0 /k is a finite separable extension. Using
    Corollary 9.58, prove that the norm and trace satisfy

                NL0 /k = NL/k ◦ NL0 /L           and      TrL0 /k = TrL/k ◦ TrL0 /L .

Problems 41–45 make use of the theory of symmetric polynomials, which was intro-
duced in Problems 36–39 at the end of Chapter VIII.
41. Let k be a field, let F(X) be a polynomial in k[X], let K be an extension field
    in which F(X) splits, and let r1 , . . . , rn be the roots of F(X) in K, repeated
    according to their multiplicities. If P(X 1 , . . . , X n ) is a symmetric polynomial
    in k[X 1 , . . . , X n ], prove that P(r1 , . . . , rn ) is a member of k.
42. Let k be a field, let F(X) and G(X) be polynomials over k, let K be an extension
    field in which F(X) and G(X) both split, and let r1 , . . . , rm and s1 , . . . , sn
    be the respective roots of F(X) and G(X) in K, repeated according to their
    multiplicities. Deduce from the previous problem that the polynomials
                       n
                     m Q
                     Q                                                    n
                                                                        m Q
                                                                        Q
         H1 (X) =              (X − ri − s j )     and      H2 (X) =             (X − ri s j )
                     i=1 j=1                                           i=1 j=1

      lie in k[X].
                                       18. Problems                                      545
                                                                   p    p
43. (a) Find a nonzero polynomial with rational coefficients having 2 + 3 as a
                                                p      p
        root. What is the minimal polynomial of 2 + 3 over Q?
                                                                   p    p
    (b) Find a nonzero polynomial with rational coefficients having 2 + 3 2 as a
                                                p      p
        root. What is the minimal polynomial of 2 + 3 2 over Q?
44. Let k be a field of characteristic 0, and let K = k(r1 , . . . , rn ) be the field of
    fractions of the polynomial ring k[r1 , . . . , rn ] in n indeterminates. Show that
    any σ in the symmetric group Sn defines a member of Gal(K/k) such that
    σ (r j ) = rσ ( j) for all σ in Sn . Then define F(X) to be the polynomial
                               F(X) = (X − r1 ) · · · (X − rn )
    in K[X], and show that
    (a) F(X) is irreducible over the fixed field KSn ,
    (b) K is a splitting field for F(X) over KSn ,
    (c) KSn = k(u 1 , . . . , u n ), where u 1 , . . . , u n are given by
                        P                     P                                Q
                 u1 = r I ,            u2 =         ri r j ,     ...,     u n = ri ,
                          i                  i< j                             i
    (d) the Galois group of the splitting field of F(X) over k(u 1 , . . . , u n ) is Sn .
45. (Cubic resolvent) This problem carries out one step in finding the roots of an ar-
    bitrary quartic polynomial. Let k be a field of characteristic 0, let K = k( p, q, r)
    be the field of fractions of the polynomial ring k[ p, q, r] in n indeterminates,
    and let L be a splitting field of the polynomial

                               F(X) = X 4 + p X 2 + q X + r

    in K[X]. The Galois group Gal(L/K) is S4 by the previous problem. Let
    B4 = {(1), (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)}. In the composition series
    S4 ⊇ A4 ⊇ B4 ⊇ {(1),         p (1 2)}(3 4)} ⊇ {1}, Proposition 9.63 shows that the
    fixed field of A4 is K( D ),p       where D is the discriminant. To obtain the fixed
    field of B4 , we adjoin to K( D ) an element of L invariant under B4 but not
    under A4 . If s1 , s2 , s3 , s4 denote the roots of F(X) in L, then such an element is
    (s1 + s2 )(s3 + s4 ). Its three conjugates under A4 /B4 are

                                   θ1 = (s1 + s2 )(s3 + s4 ),
                                   θ2 = (s1 + s3 )(s2 + s4 ),
                                   θ3 = (s1 + s4 )(s2 + s3 ),

    which are the three roots of the “cubic resolvent” polynomial

                                    θ 3 − c1 θ 2 + c2 θ − c3 ,

    where c1 , c2 , c3 are the elementary symmetric polynomials in θ1 , θ2 , θ3 given by
546                               IX. Fields and Galois Theory
                              P                   P                        Q
                       c1 =       θi ,     c2 =          θi θ j ,   c3 =       θi .
                              i                   i< j                     i
      (a) Show that c1 , c2 , c3 are symmetric polynomials in s1 , s2 , s3 , s4 , hence are
          polynomials in the coefficients p, q, r.
      (b) Verify that c1 = 2 p, c2 = p2 − 4r, and c3 = q 2 .
      (c) Show that the discriminant of the cubic resolvent equals the discriminant of
          the original quartic polynomial.

Problems 46–50 concern Galois groups of splitting fields of quartic polynomials. Take
as known that the discriminant of a quartic polynomial F(X) = X 4 + p X 2 + q X + r
is given by

              −4 p3 q 2 − 27q 4 + 16 p4r + 144 pq 2r − 128 p2r 2 + 256r 3 .

Let K be a splitting field for F(X) over Q, and let G = Gal(K/Q). Regard G as a
subgroup of the symmetric group S4 .
46. (a) Identify all transitive subgroups of the alternating group A4 , up to a relabeling
        of the four indices.
    (b) Identify all transitive subgroups of the symmetric group S4 other than those
        in (a), up to a relabeling of the four indices.
47. Suppose q = 0.
    (a) Show that G is a subgroup of A4 if and only if r is a square in Q.
    (b) Show by solving F(X) = 0 explicitly that [K : Q] is a power of 2, and
        conclude that G has no element of order 3.
    (c) Deduce when r is a square that G = {(1), (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)}
        if F(X) is irreducible over Q.
    (d) Deduce when r is a nonsquare that G is cyclic of order 4 or is dihedral of
        order 8 if F(x) is irreducible over Q; in the dihedral case, G is generated by
        a 4-cycle and the group listed in (c). (Problem 22 shows how to distinguish
        between the two cases.)
48. For F(X) = X 4 + X + 1, show by considering reduction modulo 2 and modulo
    3 that G = S4 .
49. Let F(X) = X 4 + 8X + 12.
    (a) Compute the discriminant of F(X), and verify that it is a square.
    (b) Show that F(X) ≡ (1 + X)(2 + X + 4X 2 + X 3 ) mod 5 with the two factors
        on the right side irreducible in F5 .
    (c) Show from (a) and (b) that if F(X) is reducible over Q, then it must have a
        root that is an integer. Check that there is no such root.
    (d) Conclude that G = A4 .
                                       18. Problems                                      547

50. For each transitive group G as in Problem 46, find a polynomial F(X) of degree 4
    over Q whose splitting field K over Q has Gal(K/Q) isomorphic to G.


Problems 51–56 continue the introduction to error-correcting codes begun in Problems
63–73 at the end of Chapter IV and continued in Problems 25–28 at the end of Chapter
VII. The current problems will not make use of the problems in Chapter VII. As in
the problems in Chapter IV, we work with the field F = Z/2Z, with Hamming space
Fn , and with linear codes C in Fn . The minimal distance of C is denoted by δ(C).
Problem 72 in Chapter IV introduced cyclic redundancy codes, which are determined
by a generating polynomial G(X) of some degree g suitably less than n. Such a code
C is built from all polynomials G(X)B(X) with B(X) = 0 or deg B(X) ≤ n − g − 1.
A given polynomial c0 + c1 X + · · · becomes the n-tuple (c0 , c1 , . . . ) of C; the code
C has dimension n − g. This set of problems will discuss a special class of cyclic
redundancy codes called cyclic codes, and then a special subclass called BCH codes.
51. A linear code C in Fn is called a cyclic code if whenever (c0 , c1 , . . . , cn−1 ) is in
    C, then so is (cn−1 , c0 , c1 , . . . , cn−2 ).
    (a) Prove that a linear code C is cyclic if and only if the set of all polynomials
         c0 + c1 X + · · · + cn−1 X n−1 corresponding to members (c0 , c1 , . . . , cn−1 )
         of C is an ideal in the ring F[X]/(X n − 1). (In this case the members of C
         will be identified with the set of such polynomials.)
    (b) Prove that if C is cyclic and nonzero, then there exists a unique G(X) in
         C of lowest possible degree. Moreover, G(X) divides X n − 1 in F[X],
         and C consists exactly of the polynomials G(X)F(X) mod (X n − 1) such
         that F(X) = 0 or deg F(X) ≤ n − deg G(X) − 1, and C has dimension
         n − deg G(X). (The polynomial G(X) is called the generating polynomial
         of C. A cyclic code C over the field Z/2Z having block length n and
         dimension k is called a binary cyclic (n, k) code.)
    (c) Prove that if G(X) has degree n − k, then a basis of C consists of the
         polynomials G(X), X G(X), X 2 G(X), . . . , X k−1 G(X).
    (d) Under the assumption that C is cyclic and nonzero, (b) says that it is possible
         to write X n − 1 = G(X)H (X) for some H (X) in F[X]. Prove that a
         member B(X) of F[X]/(X n − 1) lies in C if and only if H (X)B(X) ≡
         0 mod (X n − 1).
                                                           µ1 0 0 1 0 1 1∂
52. (a) Show that the row space C of the matrix G = 0 1 0 1 1 1 0 is a cyclic
                                                                  0010111
        (7, 3) code with generating polynomial G(X) =                  + X 4.
                                                                1 + X2 + X3
    (b) Show directly from G that C has minimal distance δ = 4.
    (c) The polynomial H (X) = 1 + X 2 + X 3 has the property that G(X)H (X) =
        X 7 − 1 in F[X]. Find a 4-by-7 matrix H such that the column vectors v ∈ F7
        that lie in C are exactly the ones with Hv = 0.
548                             IX. Fields and Galois Theory

      (d) The matrix H in (c) is called the check matrix for the code. Describe a
          procedure for constructing the check matrix when starting from a general
          binary cyclic (n, k) code whose generating polynomial G(X) is known and
          whose polynomial H (X) with G(X)H (X) = X n − 1 is known. Prove that
          the procedure works.
53. Show that X n − 1 is a separable polynomial over F if n is odd but not if n is even.
54. Let C be a binary cyclic (n, k) code with generating polynomial G(X), and
    suppose that n is odd. Let K be a finite extension field of F in which X n − 1
    splits, and let α be a primitive n th root of 1, i.e., a root of X n − 1 in K such that
    α m 6= 1 for 0 < m < n. Suppose that r and s are integers with 0 ≤ s < n and
                         G(αr ) = G(αr+1 ) = · · · = G(αr+s ) = 0.
      (a) Let P(X) = G(X)F(X) with F(X) 6= 0 and deg F < k be an arbitrary
          nonzero member of C, so that P(αr ) = P(αr+1 ) = · · · = P(αr+s ) = 0.
          Write P(X) = c0 + c1 X + · · · + cn−1 X n−1 , and use the values of P(α j )
          for r ≤ j ≤ r + s to set up a homogeneous system of s + 1 linear equations
          with n unknowns c0 , . . . , cn−1 .
      (b) Using an argument with Vandermonde determinants, show that every (s+1)-
          by-(s+1) submatrix of the coefficient matrix of the system in (a) is invertible.
      (c) Obtain a contradiction from (b) if s + 1 or fewer of the coefficients of P(X)
          are nonzero.
      (d) Conclude that the minimal distance δ(C) is ∏ s + 2.
55. (BCH codes, or Bose–Chaudhuri–Hocquenghem codes) Let n be an odd
    positive integer, let e be a positive integer < n/2, let K be a finite extension
    field of F in which X n − 1 splits, and let α be a primitive n th root of 1 in
    K. For 1 ≤ j ≤ 2e, let Fj (X) be the minimal polynomial of α j over F, and
    define G(X) = (1 + X) LCM(F1 (X), . . . , F2e (X)). Prove that G(X) divides
    X n − 1 and that G(X) is the generating polynomial for a cyclic code C in Fn
    with minimal distance δ(C) ∏ 2e + 2. (Educational note: Therefore C has the
    built-in capability of correcting at least e errors.)
56. In the setting of the previous problem, let n = 2m − 1 for a positive integer m,
    and let K be a field of order 2m .
    (a) Prove that any irreducible polynomial in F[X] with a root in K has order
         dividing m, and conclude that the order of the generating polynomial G(X)
         in the previous problem is at most 2em + 1.
    (b) Prove that there exists a sequence Cr of binary cyclic (nr , kr ) codes of BCH
         type such that kr /nr tends to 1 and the minimal distance δ(Cr ) tends to
         infinity. (Educational note: The fraction kr /nr tells the fraction of message
         bits to total bits in each transmitted block. Thus the problem says that there
         are linear codes capable of correcting as large a number of errors as we
         please while having as large a percentage of message bits as we please.)
                                      18. Problems                                   549

57. Take as known that F1 (X) = 1 + X + X 4 is irreducible over F. Let K be the
    field F[X]/(F1 (X)) of order 16, and let α be the coset X + (F1 (X)) in K.
    (a) Explain why F1 (X) factors as F1 (X) = (X − α)(X − α 2 )(X − α 4 )(X − α 8 )
         over K.
    (b) Find the minimal polynomial F3 (X) of α 3 .
    (c) Show in F15 that the binary cyclic code C with generating polynomial
         G(X) = (1 + X)F1 (X)F3 (X) has dim C = 6 and δ(C) ∏ 6.

Problems 58-63 combine Problems 12–13 in Chapter V with the notion of extension
of scalars from Chapter VI and some Galois theory from Chapter IX to prove the
general Jordan–Chevalley decomposition. Let k be a field, and let V be a finite-
dimensional vector space over k. A linear map N : V → V is called nilpotent if
N k = 0 for some k. A linear map S : V → V is called semisimple if there is
some finite extension K of k for which the linear map S K : V K → V K obtained by
extension of scalars has a basis of eigenvectors. The theorem is that if L : V → V is a
linear map with the property that every irreducible factor of the minimal polynomial
of L over k is separable, then L has a unique decomposition L = S + N with S
semisimple, N nilpotent, and S N = N S. The theorem applies without restriction
to a linear L : V → V if k is finite or has characteristic 0 because the separability
condition is automatically satisfied in these cases.

58. Let k be a field, let V be a vector space over k, and let K be an extension field of
    k. Extend scalars to form the K vector space given by V K = V ⊗k K, and let
    Gal(K/k) act on V K by saying that ϕ(v ⊗ c) = v ⊗ ϕ(c) for ϕ in Gal(K/k) and
    v ⊗ c in V K . Explain for V = kn that V K may be interpreted as Kn and that the
    action by ϕ reduces to (ϕ(u)) j = ϕ(u j ).
59. Let k be a field, let V be a finite-dimensional vector space over k, and let
    L : V → V be a linear map. Suppose that every irreducible factor of the minimal
    polynomial of L over k is separable. Prove the existence of a Jordan–Chevalley
    decomposition of L by following these steps:
    (a) Let K be a splitting field of k, so that K is a finite Galois extension of k. Use
        Problems 12–13 of Chapter V to show that L ⊗ 1 : V K → V K has a unique
        decomposition as a sum S + N of K linear maps of V K to itself such that
        SN = NS, N is nilpotent, and S has a basis of eigenvectors.
    (b) Prove that any K linear T : V K → V K such that (1 ⊗ ϕ)T = T (1 ⊗ ϕ) for all
        ϕ ∈ Gal(K/k) is of the form T = T ⊗ 1 for a unique k linear T : V → V .
    (c) Show that the K linear maps S and N of (a) satisfy (1 ⊗ ϕ)S = S (1 ⊗ ϕ)
        and (1 ⊗ ϕ)N = N (1 ⊗ ϕ) for all ϕ ∈ Gal(K/k), and deduce from (b) that
        S and N may be written as S = S ⊗ 1 and N = N ⊗ 1 for uniquely defined
        k linear maps S and N of V into itself.
    (d) Show that S is semisimple, N is nilpotent, and S N = N S, and conclude
        that L = S + N is a Jordan–Chevalley decomposition of L.
550                            IX. Fields and Galois Theory

      (e) Show that S and N are polynomials in L.
60. Let k be a field, let V be a finite-dimensional vector space over k, and let
    L : V → V be a linear map. Prove the uniqueness result that there is at most
    one decomposition L = S + N with S semisimple, N nilpotent, and S N = N S.
61. Let k = R, and let L : R4 → R4 be the linear map defined by the matrix
                                                      
                                          0 −1    0 0
                                    A=   1 0     1 0
                                                       .
                                          0 0     0 −1
                                          0 0     1 0


      The minimal polynomial of L or A is (X 2 + 1)2 . Calculate the Jordan–Chevalley
      decomposition of L in matrix form.
62. Let F2 be a field of two elements, and let k = F2 (x), where x is transcendental
                                                                             ≥ ¥
                                                                                   0x
      over F2 . Let L : k2 → k2 be the linear map defined by the matrix A =        10
                                                                                        .
      The characteristic polynomial of L or A is M(X) =       X2− x. This is irreducible
      over k and hence is also the minimal polynomial. The quadratic extension
      K = k[x 1/2 ] of k is a splitting field for M(X), and M(X) has a double root in
      k[x 1/2 ].
      (a) Show that A, regarded as a matrix in M2 (K), does not have a basis of
           eigenvectors. Conclude that L is not semisimple.
      (b) Calculate the most general 2-by-2 matrix commuting with A, and show that
           it cannot have characteristic polynomial X 2 unless it is the 0 matrix.
      (c) Conclude that L cannot have a Jordan–Chevalley decomposition.
63. Let k be a field, let V be a finite-dimensional vector space over k, and let
    L : V → V be an invertible linear map. Suppose that every irreducible factor
    of the minimal polynomial of L over k is separable. A linear map U : V → V
    is called unipotent if (U − I )k = 0 for some k. By suitably adjusting the proof
    of the Jordan–Chevalley decomposition, prove that there exist linear maps S and
    U of V into itself such that S is semisimple, U is unipotent, and L = SU = US.
Problems 64–73 introduce ordered fields, formally real fields, and real closed fields.
An ordered field k is a field with a specified subset P of “positive” elements that is
closed under addition and multiplication and is such that each nonzero element of k
is in exactly one of P and −P. The fields Q and R are examples. A formally real
field k is a field in which −1 is not the sum of squares. A real closed field k is a
formally real field such that no proper algebraic extension of k is formally real. The
problems together prove the existence part of the Artin–Schreier Theorem: If k is
an ordered field with P as its set of positive elements and if k is an algebraic closure,
then there exists a real closed field K between k and k that is an ordered field with P
contained in its set of positive
                          p      elements. Moreover, K is unique up to k isomorphism,
and k is of the form K( −1 ).
                                      18. Problems                                   551

64. Verify the following properties of an ordered field k when P is the set of positive
    elements:
    (a) 1 is in P,
    (b) every nonzero square is in P,
    (c) whenever a is in P, then so is a −1 ,
    (d) k is formally real,
    (e) k has characteristic 0.
65. In an ordered field k whose set of positive elements is P, define x > y and y < x
    to mean x − y is in P. Let a, b, c, d be in k. Check the following:
    (a) exactly one the relations a > b, a = b, and a < b holds,
    (b) if a > b and b > c, then a > c,
    (c) if a > b, then a + c > b + c,
    (d) if a > b and c > 0, then ac > bc,
    (e) if a > b > 0, then b−1 > a −1 ,
    (f) if a > b > 0 and c > d > 0, then ac > bd,
    (g) if a > b and c > d, then ac + bd > ad + bc.
66. Let k be an ordered field with P as its set of positive elements, let k(x) be a
    transcendental extension, and define the positive elements of k(x) to be those
    for which the quotient of the leading coefficient of the numerator by the leading
    coefficient of the denominator is in P. Show that with this definition of the set
    of positive elements, k(x) becomes an ordered field in which x > n for every
    positive integer n. (Then also 1/n > 1/x for every positive integer n by Problem
    65e.)
                        p
67. (a) Show that Q( 2 ) becomes an ordered field in two distinct ways.
    (b) If k is an ordered field with P as its set of positive elements and if c is a
                                                   p there are two ways of defining
         member of P that is not a square, show that
         the set of positive elements P 0 of K = k( c ) so that K becomes an ordered
         field with P ⊆ P 0 .
68. Let k be an ordered field, and let K be the extension that arises by adjoining the
    square roots of all the positive elements of K. Prove that K is a formally real
    field by carrying out the following steps:
    (a) Show that if n is chosen as small as possible so that an equation −1 =
         Pk           2
            j=1 p j ξ j holds in K with all p j positive in k and all ξ j in an extension
            p            p
         k( c1 , . . . , cn ) of k with all c j positive in k, then writing
                           p             p              p          p        p
                         k( c1 , . . . , cn ) = k( c1 , . . . , cn−1 )( cn )
         leads to an equation
                                Pk               Pk                p P  k
                         −1 =         p j a j2 +     p j cn b2j + 2 cn     pj aj bj   (∗)
                                j=1              j=1                   j=1
                                             p            p
         in which a j and b j are in k( c1 , . . . , cn−1 ).
552                              IX. Fields and Galois Theory

      (b) Consider the third term on the right side of (∗), and show that a contradiction
          results if this term is 0 and a different contradiction arises if this term is not 0.
69. Let k be a formally real field, and let k be an algebraic closure. Show that there
    exist maximal formally real subfields of k containing k, and show that any such
    is a real closed field.
70. Carry out the following steps to show that a real closed field k becomes an ordered
    field in one and only one way:
                                                           p
    (a) Suppose that c 6= 0 is not a square, hence that k( c ) is a quadratic extension
                              Pn            p 2
         of k. Why is −1 = j=1 (a j +b j c ) for suitable members a j and b j of k?
    (b) By expanding the identity in (a), show that c is not a sum of squares. In
         other words, every sum of squares in k is a square in k.
    (c) Solve for c in the expansion in (b), and conclude that −c is a square.
    (d) Conclude from the previous steps that the choice of P as the set of nonzero
         squares makes k into an ordered field and that there no other possible defi-
         nition for the set P of positive elements that makes k into an ordered field.
71. Carry out the following steps to show that in any real closed field k, every
    polynomial of odd degree has a root:
    (a) Show by induction that it is enough to handle irreducible polynomials of
        odd degree.
    (b) For an irreducible polynomial Q(X) of odd degree n, let k(α) be a simple
        algebraic extension of k such that Q(α) = 0. Show that an expression of −1
                                                      P
        as a sum of squares in k(α) forces an identity kj=1 R j (X)2 + Q(X)A(X) =
        −1 for suitable polynomials R j (X) in k[X] of degree ≤ n − 1 and some
        polynomial A(X) in k[X] of odd degree ≤ n − 2.
                                                                 P
    (c) If r is a root of the polynomial A(X) in (b), show that kj=1 R j (r)2 = −1,
        and deduce a contradiction.
72. By using the results of Problems 70–71 and taking into account the proof of
    Theoremp 1.18 that appears in Section IX.10, prove that if k is a real closed field,
    then k( −1 ) is algebraically closed.
73. Put the above results together to give a proof of the existence in the Artin–Schreier
    Theorem: if an ordered field k has P as its set of positive elements and k as an
    algebraic closure,
                 p     then there exists a real closed field K with k ⊆ K ⊂ k such
    that k = K( −1 ) and such that P is contained in the set of squares in k, i.e.,
    such that the set of positive elements in the natural ordered-field structure on k
    contains P.
                                      CHAPTER X

                 Modules over Noncommutative Rings



Abstract. This chapter contains two sets of tools for working with modules over a ring R with
identity. The first set concerns finiteness conditions on modules, and the second set concerns the
Hom and tensor product functors.
    Sections 1–3 concern finiteness conditions on modules. Section 1 deals with simple and semisim-
ple modules. A simple module over a ring is a nonzero unital module with no proper nonzero
submodules, and a semisimple module is a module generated by simple modules. It is proved that
semisimple modules are direct sums of simple modules and that any quotient or submodule of a
semisimple module is semisimple. Section 2 establishes an analog for modules of the Jordan–Hölder
Theorem for groups that was proved in Chapter IV; the theorem says that any two composition series
have matching consecutive quotients, apart from the order in which they appear. Section 3 shows
that a module has a composition series if and only if it satisfies both the ascending chain condition
and the descending chain condition for its submodules.
    Sections 4–6 concern the Hom and tensor product functors. Section 4 regards Hom R (M, N ),
where M and N are unital left R modules, as a contravariant functor of the M variable and as a
covariant functor of the N variable. The section examines the interaction of these functors with
the direct sum and direct product functors, the relationship between Hom and matrices, the role
of bimodules, and the use of Hom to change the underlying ring. Section 5 introduces the tensor
product M ⊗ R N of a unital right R module M and a unital left R module N , regarding tensor
product as a covariant functor of either variable. The section examines the effect of interchanging
M and N , the interaction of tensor product with direct sum, an associativity formula for triple tensor
products, an associativity formula involving a mixture of Hom and tensor product, and the use of
tensor product to change the underlying ring. Section 6 introduces the notions of a complex and an
exact sequence in the category of all unital left R modules and in the category of all unital right R
modules. It shows the extent to which the Hom and tensor product functors respect exactness for
part of a short exact sequence, and it gives examples of how Hom and tensor product may fail to
respect exactness completely.



                          1. Simple and Semisimple Modules

This chapter develops further theory for unital modules over a ring with identity
beyond what is in Section VIII.1. Results about modules that take advantage of
commutativity of the ring were included in Chapter VIII. In the present chapter
the ring may or may not be commutative. We shall be interested in those modules
whose structure is especially easy to analyze and in constructions that create new
modules from old ones. The chapter consists of tools for working with such
                                                 553
554                         X. Modules over Noncommutative Rings

modules and their related rings and algebras. There are no major theorems in the
chapter, but the material here is essential for the developments in several of the
chapters of Advanced Algebra.
    Throughout this chapter, R will denote a ring with identity. We shall work
with the category C of all unital left R modules. Specifically the objects of
C are left unital R modules, and the space of morphisms between two such
modules M and N consists of all R homomorphisms from M into N . It is
customary to write Hom R (M, N ) for this set of morphisms.1 In the special case
that R is a field, the notation Hom R (M, N ) reduces to notation we introduced in
Section II.3 for the set of linear maps from one vector space over R to another.
For general R, the set Hom R (M, N ) is an abelian group under addition of the
values: (ϕ1 + ϕ2 )(m) = ϕ1 (m) + ϕ2 (m). Without some further hypothesis on R,
Hom R (M, N ) does not have a natural R module structure.
    However, there is some residual action by scalars. Any element z in the center
Z of R, i.e., any element with cr = rc for all r in R, acts on Hom R (M, N ). The
definition is that (cϕ)(m) = ϕ(cm). The function cϕ certainly respects addition,
and it respects action by a scalar r in R because (cϕ)(rm) = ϕ(crm) = ϕ(rcm) =
rϕ(cm) = r(cϕ)(m); thus cϕ is in Hom R (M, N ), and Hom R (M, N ) becomes a
Z module. The center Z automatically contains the multiplicative identity 1 and
its integer multiples Z 1.
    We shall tend to ignore this action by the center except in two special cases.
One is that R is commutative, and then Hom R (M, N ) is an R module. The other
is that R is an associative algebra (with identity) over a field F. In this case the
action of members of F on the identity of R embeds F into R, and F may thus
be identified with a subfield of the center of R. The result is that when R is an
associative algebra over a field F, then Hom R (M, N ) is a vector space over F.
    We write End R (M) for Hom R (M, M). This abelian group has the structure
of a ring with identity, multiplication being composition: (ϕ√)(m) = ϕ(√(m)).
The distributive laws need to be checked: the formula (ϕ1 + ϕ2 )√ = ϕ1 √ + ϕ2 √
is immediate from the calculation
         ((ϕ1 + ϕ2 )√)(m) = (ϕ1 + ϕ2 )(√(m))
                          = ϕ1 (√(m)) + ϕ2 (√(m)) = (ϕ1 √ + ϕ2 √)(m),

while the formula ϕ(√1 + √2 ) = ϕ√1 + ϕ√2 makes use of the fact that ϕ respects
addition and is proved by the calculation

         (ϕ(√1 + √2 ))(m) = ϕ(√1 (m) + √2 (m))
                          = ϕ(√1 (m)) + ϕ(√2 (m)) = (ϕ√1 + ϕ√2 )(m).
    1 The notation Hom(M, N ) with no subscript is sometimes used for Hom (M, N ), i.e., to denote
                                                                         Z
the group of homomorphisms from one abelian group to another.
                           1. Simple and Semisimple Modules                       555

If Z is the center of R, then End R (M) is a Z module, as well as a ring, and
the two structures are compatible; the result is that End R (M) is an associative Z
algebra in the sense of Example 15 in Section VIII.1. In particular, when R is an
associative algebra over a field F, then End R (M) is an associative F algebra.
   There is usually no need to re-prove for right R modules an analog of each
result about left R modules. The reason is that we can make use of the opposite
ring R o of R, defined to be the same underlying abelian group but with reversed
multiplication: a ◦ b = ba. Any left R module M then becomes a right R o
module M o under the definition mr o = rm for r in R, m in M, and r o equal to
the same set-theoretic member of R o as r. The theory of unital left R modules
for all R thereby yields a theory of unital right R modules for all R.
   A unital left R module M is said to be simple, or irreducible, if M 6= 0 and if
M has no proper nonzero R submodules. If M is simple, then M = Rx for each
x 6= 0 in M; conversely if M 6= 0 has M = Rx for each x 6= 0 in M, then M is
simple. Whenever M = Rx for an element x, then M is isomorphic as a unital
left R module to R/I , where I is the left ideal I = {r ∈ R | r x = 0}.
   A unital left R module M is said to be semisimple if M is generated by simple
left R submodules, i.e., if it is the sum of simple left R submodules. In this
definition, the sum may be empty (and then M = 0), it may be finite, or it may
be infinite. Evidently simple implies semisimple for unital left R modules.
   We come to examples in a moment. First we prove that the sum of simple left
R modules in a semisimple module may always be taken to be a direct sum, i.e.,
that semisimple modules are completely reducible.

   Proposition 10.1. If the unital left R module M is semisimple, then M
is the direct sum of some family of simple R submodules. In more detail if
{Ms | s ∈ S} is a family of simple R submodules of the unital left R module M
whose sum is M, then there is a subset T of S with the property that
                                      M
                                 M=       Mt .
                                          t∈T
                                                                 P
   PROOF. Call a subset U of S “independent” if the sum u∈U Mu is direct.
This condition means that for every finite subset {u 1 , . . . , u n } of U and every
set of elements m i ∈ Mu i , the equation m 1 + · · · + m n = 0 implies that each
m i is 0. From this formulation it follows that the union of any increasing chain
of independent subsets of S is itself independent. By Zorn’s Lemma         Pthere is a
maximal independent subset T of S. By definition the sum M0 = t∈T Mt is
direct. Consequently it suffices to show that M0 is all of M. By the hypothesis
on S, it is enough to show that each Ms is contained in M0 . For s in T , this
conclusion is clear. Thus suppose s is not in T . By the maximality of T , T ∪ {s}
is not independent. Consequently the sum Ms + M0 is not direct, and it follows
556                       X. Modules over Noncommutative Rings

that Ms ∩ M0 6= 0. But this intersection is an R submodule of Ms . Since Ms is
simple, a nonzero R submodule of Ms must be all of Ms . Thus Ms ∩ M0 = Ms ,
and Ms is contained in M0 .                                                 §
   EXAMPLES OF SEMISIMPLE MODULES.
   (1) Let F be a field. Left and right amount to the same thing for modules when
the underlying ring is commutative. We know that the unital F modules are just
the vector spaces over F. Such a vector space V is a simple F module if and
only if it is 1-dimensional, since 1-dimensionality is the necessary and sufficient
condition to have V 6= 0 be of the form V = F x for all x 6= 0 in V . Any vector
space V is the sum of all of its 1-dimensional subspaces, and consequently every
unital F module is semisimple. Theorem 2.42 shows that each vector space V
has a basis; this theorem is therefore a special case of Proposition 10.1, which
says that any semisimple module is the direct sum of simple modules.
   (2) Let D be a division ring. Division rings were defined in Section IV.4 as
rings with identity 1 6= 0 such that the nonzero elements form a group under
multiplication. Every field is a division ring, and the quaternions form a division
ring that is not a field. Let M be a unital left D module, and let x 6= 0 be in
M. Then the left D module Dx is simple because if N ⊆ Dx is a nonzero D
submodule and if y is in N , then we can write y = d x with d in D and see from
the formula d −1 y = x that x is in N and N = Dx. Any unital left D module is
the sum of its D submodules Dx for x in M, and therefore every unital left D
module is semisimple. From Proposition 10.1 we can conclude that every unital
left D module M is the direct sum of simple modules. In other words, M has a
basis, just as if D were a field. Consequently it is customary to refer to unital left
D modules as left vector spaces over D. A notion of (left) dimension, equal to
a well-defined nonnegative integer or ∞, will emerge from the discussion in the
next section.
   (3) Let D be a division ring. Section V.2 introduced the ring of n-by-n matrices
over any commutative ring with identity, and Example 4 of rings in Section VIII.1
extended the definition to the case that the ring is noncommutative. Thus let R
be the ring Mn (D). Let M = D n be the abelian group of n-component column
vectors with entries in D. Under multiplication of matrices times column vectors,
M becomes a unital left R module. Let us prove that M is simple. It is enough to
show that Rm = M for every nonzero m in M. Let m 0 be in M with entries m i0 ,
and suppose that the i 0th component m i0 of m is 6= 0. Then we can multiply on
the left of m by the matrix r whose (i, j)th entry ri j is m i0 m i−1
                                                                   0
                                                                      if (i, j) = (i, j0 )
and is 0 otherwise, and the product is the column vector m 0 . Thus m 0 is in Rm,
and Rm = M as required. Hence M = D n is an example of a simple R module.
   (4) Again let D be a division ring, and let R = Mn (D). Let us see that the left
R module R is semisimple. In fact, if R j is the additive subgroup of R whose
                          1. Simple and Semisimple Modules                      557

nonzero entries are all in the j th column, then R j is a left R submodule of R that
is R isomorphic to D n . Thus we see that R = R1 ⊕ · · · ⊕ Rn as left R modules,
and the left R module R is semisimple as a consequence of Example 3.
   (5) Let G be a group, and let CG be the complex group algebra defined
in Example 16 in Section VIII.1. Let V be a vector space over C, and let
8 : G → G L(V ) be a representation of G on V . The universal mapping
property of complex group algebras described in that example and pictured in
Figure 8.4 shows that the representation 8 of G extends to CG and makes V
into a unital left CG module. Conversely if the complex vector space V is a
unital left CG module, then we obtain a representation of G by restriction from
CG to G. What needs to be checked here is that each member of G acts by an
invertible linear mapping. This is a consequence of the unital property; since
1 acts as 1, the action by g −1 inverts the action of g. Thus we have a one-one
correspondence of representations of G on complex vector spaces with unital left
CG module structures. Under this correspondence, irreducible representations
of G (i.e., nonzero representations having no proper nonzero invariant subspace)
correspond to simple CG modules. Now suppose that G is finite. Readers
who have looked at Section VII.4 know from Corollary 7.21 that every finite-
dimensional representation of a finite group G on a complex vector space is the
direct sum of irreducible representations; the corresponding CG modules are
therefore semisimple. But more is true. If V is any CG module for the finite
group G and if x is in V , then CGx is a vector subspace spanned by {gx | g ∈ G}
and consequently is finite-dimensional. Applying what is known from Section
VII.4, we can write CGx as the direct sum of simple CG modules. Therefore
the sum of all simple CG modules in V is all of V , and V is semisimple. From
Proposition 10.1 we conclude that every unital left CG module is semisimple if
G is a finite group.

   The next proposition shows that decompositions of semisimple modules as
direct sums of simple modules behave in a fashion analogous to decompositions
of vector spaces as direct sums of 1-dimensional vector subspaces. However,
the simple modules need not all be isomorphic to one another, as is shown by
Example 5. A theory that takes the isomorphism types of simple modules into
account appears in Problems 12–20 at the end of the chapter.

  Proposition
      L          10.2. Let M be a semisimple left R module, and suppose that
M =      s∈S M s  is the direct sum of simple R modules Ms . Let N be any R
submodule of M. Then
                                                         L there is a subset
   (a) the quotient module M/N is semisimple. In more detail
       T of S with the property that the submodule MT = t∈T Mt of M maps
       R isomorphically onto M/N .
558                      X. Modules over Noncommutative Rings

      (b) N is a direct summand of M. In more detail, M = N ⊕ MT , where MT
          is as in (a).
      (c) N is semisimple. In more detail choose T as in (a), and write T 0 for the
          complement of T in S. Then the quotient mapping M → M/MT restricts
          to an R isomorphism of N onto M/MT , and M/MT is R isomorphic to
          MT 0 .

  PROOF. Each simple R submodule Ms of M maps to an R submodule M s of
M/N . This image either is simple (and then is R isomorphic to Ms ) or is zero.
We let U be the subset of S for which it is simple. Then M/N is evidently the
sum of the simple R submodules {M s | s ∈ U }. By Proposition 10.1 there is a
subset T of U such that                 M
                               M/N =         Mt .
                                           t∈T

This proves (a).
   For (b), we use the following elementary observation: if N and N 0 are R
submodules of M, then M = N ⊕ N 0 if and only if the quotient map M → M/N
carries N 0 isomorphically onto the quotient M/N . Taking N 0 = MT and applying
(a), we obtain (b).
   For (c), the same observation when applied first to M = N ⊕ MT and then to
M = MT 0 ⊕ MT shows that the quotient map M → M/MT carries N isomor-
phically onto M/MT and carries MT 0 isomorphically onto M/MT . Therefore
N= ∼ M/MT ∼    = MT 0 , and (c) is proved.                                   §

    In the context of simple modules, Hom R (M, N ) has special properties. Read-
ers who have looked at Section VII.4 have seen these special properties in the
context of representations of finite groups on complex vector spaces. There they
were captured by Schur’s Lemma (Proposition 7.18). If we pass from represen-
tations on complex vector spaces to CG modules, following the prescription in
Example 5, we obtain a result about HomCG (M, N ) when G is a finite group.
Lemma 10.3 and Proposition 10.4 generalize this to a result about Hom R (M, N )
for arbitrary R.

LLemma 10.3. Suppose that E is a simple left R module and that M =
   a∈A Ma is a direct-sum decomposition of the unital left R module M into
arbitrary R submodules, not necessarily simple. Then
                                          M
                        Hom R (E, M) ∼
                                     =          Hom R (E, Ma )
                                          a∈A


as an isomorphism of abelian groups.
                           1. Simple and Semisimple Modules                      559

   REMARKS. The hypothesis that E is simple is critical here. Without it a
map into a direct sum might have nonzero projections into infinitely many of the
summands, and then it could not be represented as a finite sum of maps into sum-
mands. Proposition 10.12 below will Q               Q correct identity without a
                                     point out that the
special hypothesis on E is Hom R (E, a∈A Ma ) ∼  = a∈A Hom R (E, Ma ).
    PROOF. Suppose ϕ is in Hom R (E, M). Write ϕa for the composition of ϕ with
the projection M → Ma . The map from left to right in the displayed isomorphism
is to be ϕ 7→ {ϕa }a∈A . Suppose for the moment that the image is contained in the
direct sum on the right. The mapping is one-one since M is the sum of the Ma ’s,
and it is onto since the mapping is the identity on each subgroup Hom R (E, Ma )
of Hom R (E, M).
    Thus we must show for each ϕ that only finitely many of the maps ϕa are
nonzero. Choose e in E with ϕ(e) 6= 0, and write
                   ϕ(e) = m 1 + · · · + m n      with m i ∈ Mai .
Since E is simple, E = Re. Therefore
           ϕ(E) = Rϕ(e) = R(m 1 + · · · + m n ) ⊆ Rm 1 + · · · + Rm n
                ⊆ Ma 1 ⊕ · · · ⊕ M a n .
Consequently only ϕa1 , . . . , ϕan can be nonzero.                               §

   Lemma 10.3 enables us to study maps between semisimple modules in terms
of maps between simple modules. The latter are described by the next result.

  Proposition 10.4 (Schur’s Lemma). Suppose that M and N are simple left R
modules.
  (a) If M and N are not R isomorphic, then Hom R (M, N ) = 0.
  (b) End R (M) is a division ring.
  (c) (Dixmier) If R is an associative algebra over an algebraically closed field
F and if the vector-space dimension of M over F is less than the cardinality of
F, then End R (M) consists of the F multiples of the identity.
    REMARK. In the setting of representations of a finite group G as in Section
VII.4, or in the case that G is a finite group and R = CG in the current setting, any
singly generated R module such as M or N is finite-dimensional over C. Part (a)
in that case reduces to the statement that the vector space of intertwining operators
between two inequivalent irreducible representations is 0. Part (c) in that case
says that the space of self-intertwining operators for an irreducible representation
consists of the scalar multiples of the identity. For a general R, we get only the
weaker conclusion of (b) that End R (M) is a division ring. If R is an associative
algebra over a field F, we have seen that End R (M) is an associative algebra over
F, and (c) gives a condition under which we can improve upon (b).
560                     X. Modules over Noncommutative Rings

   PROOF. Suppose that ϕ is nonzero in Hom R (M, N ). Then ker ϕ is a proper
R submodule of M, and we must have ker ϕ = 0 since M is simple. Similarly
image ϕ is a nonzero R submodule of N , and we must have image ϕ = F since
N is simple. Therefore ϕ is an R isomorphism of M onto N . This proves (a) and
(b).
    For (c) let m be a nonzero element of M. The map ϕ 7→ ϕ(m) is F linear and
one-one from End R (M) into M by (b). Thus End R (M) as an associative division
algebra over F has vector-space dimension at most the vector-space dimension of
M, and the latter by hypothesis is strictly less than the cardinality of F. Arguing
by contradiction, let us assume that End R (M) is not equal to F; say End R (M)
contains an element ϕ not in F.
   The smallest division subalgebra of End R (M) containing F and ϕ is the field
F generated by F and ϕ. Since F is algebraically closed, ϕ is not a root of any
nonzero polynomial with coefficients in F. Thus the substitution homomorphism
equal to the identity on F and carrying X to ϕ is one-one from F[X] into F.
By the universal mapping property of fields of fractions (Proposition 8.6), the
substitution homomorphism factors through the field of fractions F(X). Thus
we may regard F(X) as a subfield of F. In the field F(X), the set of elements
{(X − c)−1 | c ∈ F} is linearly independent over F, as we see by assuming a
nontrivial linear dependence and clearing fractions, and hence dim F F(X) is ∏
the cardinality of F. Since End R (M) ⊇ F ⊇ F(X) under our identification, the
dimension of End R (M) over F is ∏ the cardinality of F. This conclusion contra-
dicts the observation of the previous paragraph that the dimension of End R (M) is
strictly less than the cardinality of F. So the assumption that End R (M) contains
an element not in F must be false, and (c) follows.                              §


                             2. Composition Series

We continue with R as a ring with identity, and we work with the category of
all unital left R modules. In this section we shall say what is meant by a unital
left R module of “finite length,” and we shall investigate semisimplicity for such
modules.
   A finite filtration of a unital left R module M is a finite descending chain

                         M = M0 ⊇ M 1 ⊇ · · · ⊇ M n = 0

of R submodules. We do not insist on this particular indexing, and with the
obvious adjustments, we allow also a finite increasing chain to be called a fi-
nite filtration. Relative to the displayed inclusions, the modules Mi /Mi+1 for
0 ≤ i ≤ n − 1 are called the consecutive quotients of the filtration. The finite
filtration is called a composition series if the consecutive quotients are all simple
                                 2. Composition Series                                     561

R modules; in particular, they are to be nonzero. The consecutive quotients in
this case are called composition factors.
   We encountered an analogous notion with groups in Section IV.8, but there
was a complication in that case. The complication was that each subgroup had
to be normal in the next-larger subgroup in order for the consecutive quotients to
be groups. The overlap between the current treatment and the earlier treatment
occurs for abelian groups, which on the one hand are unital Z modules and on
the other hand are groups whose subgroups are automatically normal.
   We are going to obtain analogs for the category of unital left R modules of the
group-theoretic results of Zassenhaus, Schreier, and Jordan–Hölder in Section
IV.8. The ones here will be a little easier to prove than those in Section IV.8 since
we do not have the complication of checking whether subgroups are normal. Let
                         M = M0 ⊇ M1 ⊇ · · · ⊇ Mm = 0
and                      M = N0 ⊇ N1 ⊇ · · · ⊇ Nn = 0
be two finite filtrations of M. We say that the second is a refinement of the first
if there is a one-one function f : {0, . . . , m} → {0, . . . , n} with Mi = N f (i) for
0 ≤ i ≤ m. The two finite filtrations of M are said to be equivalent if m = n and
if the order of the consecutive quotients M0 /M1 , M1 /M2 , . . . , Mm−1 /Mm may
be rearranged so that they are respectively isomorphic to N0 /N1 , N1 /N2 , . . . ,
Nm−1 /Nm .
  Lemma 10.5 (Zassenhaus). Let M1 , M2 , M10 , and M20 be R submodules of a
unital left R module M with M10 ⊆ M1 and M20 ⊆ M2 . Then
  ((M1 ∩ M2 ) + M10 )/((M1 ∩ M20 ) + M10 )
                                 ∼
                                 = ((M1 ∩ M2 ) + M20 )/((M10 ∩ M2 ) + M20 ).
   PROOF. By the Second Isomorphism Theorem (Theorem 8.4),
      (M1 ∩ M2 )/(((M1 ∩ M20 ) + M10 ) ∩ (M1 ∩ M2 ))
                 ∼
                 = ((M1 ∩ M2 ) + (M1 ∩ M 0 ) + M 0 )/((M1 ∩ M 0 ) + M 0 )
                                               2          1                2       1
                                        0               0
                   = ((M1 ∩ M2 ) +     M1 )/((M1   ∩   M2 )   +   M10 ).
Since we have
        ((M1 ∩ M20 ) + M10 ) ∩ (M1 ∩ M2 ) = ((M1 ∩ M20 ) + M10 ) ∩ M2
                                               = (M1 ∩ M20 ) + (M10 ∩ M2 ),
we can rewrite the above isomorphism as
  (M1 ∩ M2 )/((M1 ∩ M20 ) + (M10 ∩ M2 ))
                                 ∼
                                 = ((M1 ∩ M2 ) + M 0 )/((M1 ∩ M 0 ) + M 0 ).
                                                              1                2       1
The left side of this isomorphism is symmetric under interchange of the indices 1
and 2. Hence so is the right side, and the lemma follows.                      §
562                       X. Modules over Noncommutative Rings

  Theorem 10.6 (Schreier). Any two finite filtrations of a module M in C have
equivalent refinements.
   PROOF. Let the two finite filtrations be
                          M = M 0 ⊇ M1 ⊇ · · · ⊇ Mm = 0
and                       M = N0 ⊇ N1 ⊇ · · · ⊇ Nn = 0,
and define

         Mi j = (Mi ∩ N j ) + Mi+1        for 0 ≤ i ≤ m − 1 and 0 ≤ j ≤ n,
          N ji = (Mi ∩ N j ) + N j+1      for 0 ≤ i ≤ m and 0 ≤ j ≤ n − 1.
Then
                M = M00 ⊇ M01 ⊇ · · · ⊇ M0n
                  ⊇ M10 ⊇ M11 ⊇ · · · ⊇ M1n ⊇ · · · ⊇ Mm−1,n = 0
and             M = N00 ⊇ N01 ⊇ · · · ⊇ N0m
                  ⊇ N10 ⊇ N11 ⊇ · · · ⊇ N1m ⊇ · · · ⊇ Nn−1,m = 0
are refinements of the respective given filtrations. The containments Min ⊇
Mi+1,0 and N jm ⊇ N j+1,0 are equalities here, and the only nonzero consecutive
quotients are therefore of the form Mi j /Mi, j+1 and N ji /N j,i+1 . For these we have

      Mi j /Mi, j+1
            = ((Mi ∩ N j ) + Mi+1 )/((Mi ∩ N j+1 + Mi+1 )        by definition
            ∼
            = ((Mi ∩ N j ) + N j+1 )/((Mi+1 ∩ N j ) + N j+1 )    by Lemma 10.5
            = N ji /N j,i+1                                      by definition,
and thus the above refinements are equivalent.                                      §

   Corollary 10.7 (Jordan–Hölder Theorem). If M is a unital left R module with
a composition series, then
    (a) any finite filtration of M in which all consecutive quotients are nonzero
        can be refined to a composition series, and
    (b) any two composition series of M are equivalent.
   PROOF. We apply Theorem 10.6 to a given filtration and a known composition
series. After discarding redundant terms from each refinement (those that lead to
0 as a consecutive quotient), we arrive at a refinement of our given finite filtration
that is equivalent to the known composition series. Hence the refinement is a
composition series. This proves (a). If we specialize this argument to the case
that the given filtration is a composition series, then we obtain (b).              §
                                2. Composition Series                           563

   Corollary 10.7 implies that the composition factors for a given composition
series depend only on M, not on the particular composition series. Moreover,
if M 0 ⊇ M 00 are R submodules of an M with a composition series such that
M 0 /M 00 is simple, then M 0 /M 00 is a composition factor of M. This fact follows
by eliminating redundant terms from the finite filtration M ⊇ M 0 ⊇ M 00 ⊇ 0 and
applying Corollary 10.7a to the result.
   If a unital left R module M has a composition series, then we say that M has
finite length. This notion is closed under passage to submodules and quotients.
In fact, if
                         M = M0 ⊇ M 1 ⊇ · · · ⊇ Mn = 0
is a composition series of M and if M 0 is an R submodule of M, then

               M 0 = M0 ∩ M 0 ⊇ M1 ∩ M 0 ⊇ · · · ⊇ Mn ∩ M 0 = 0

is a finite filtration of M 0 in which each consecutive quotient is simple or 0.
Discarding redundant terms (which lead to 0 as a consecutive quotient), we obtain
a composition series for M 0 . A similar argument works for M/M 0 .
   Let us see that if the unital left R modules M 0 and M/M 0 have finite length,
then so does M. In fact, we take a composition series for M/M 0 , pull it back to
M, and concatenate it to a composition series for M 0 . The result is a composition
series for M, and the assertion follows. In particular, the direct sum of two unital
left R modules of finite length has finite length.
   If M has a composition series of the form M = M0 ⊇ M1 ⊇ · · · ⊇ Mn = 0,
then we say that M has length n. If it has no composition series, we say it has
infinite length. According to Corollary 10.7, this notion of length is independent
of the particular composition series that we use. The argument in the previous
paragraph shows that if M 0 is an R submodule of M, then

                   length(M) = length(M 0 ) + length(M/M 0 ),

with the finiteness of either side implying the finiteness of the other side. One
consequence is that if M 0 is a length-n submodule of a length-n module M with
n finite, then M 0 = M. Another consequence is that if M is a semisimple left R
module, then M has a composition series if and only if M is the finite direct sum
of simple left R modules.
   From the last of these observations, we see that if F is a field, then the vector
spaces over F that have a composition series are the finite-dimensional vector
spaces, and in this case the length of the vector space is its dimension. The
structure of finite-dimensional vector spaces is so elementary that the Jordan–
Hölder Theorem is of no interest in this case, and it was for that reason that no
version of the Jordan–Hölder Theorem for vector spaces appeared earlier in the
book.
564                      X. Modules over Noncommutative Rings

     In the case that R = D is a division ring, matters are slightly subtler. We know
from Example 2 in Section 1 that every unital left D module is semisimple, and we
noted that such D modules are therefore called left vector spaces. Corollary 10.7
shows that the number of summands in any decomposition of a left vector space
V as the direct sum of simple D modules is either an integer n ∏ 0 independent
of the decomposition or is infinite, independently of the decomposition. This
number, the integer n or ∞, is called the dimension of the left vector space V .
     We saw one other example of a semisimple left R module. Specifically if D is a
division ring, then we saw in Example 4 of Section 1 that R = Mn (D) is semisim-
ple as a left R module. The number of simple summands is n, and hence R has
length n. So R has a composition series when considered as a left R module.
     There are two other cases in which composition series give something familiar.
One is the case that R is the ring Z of integers. A unital Z module is an abelian
group, and we know that the simple abelian groups are the cyclic groups of
prime order. For an abelian group with a composition series, the order of the
group is the product of the orders of the consecutive quotients and hence is finite.
Consequently an abelian group has a composition series if and only if it is a finite
abelian group. Such a group need not be semisimple; the group C4 , for example,
is not the direct sum of cyclic groups of prime order.
     The other case concerns triangular form, Jordan canonical form, and related
decompositions, as explained in Sections V.3 and V.6 and as reinterpreted after
Corollary 8.29. Let V be a finite-dimensional vector space over a field K, and let
L : V → V be a linear mapping from V to itself. Put R = K[X], and make V into
a unital R module by the definition A(X)(v) = A(L)v for any A(X) in K[X] and
v in V . The R submodules are the vector subspaces of V that are invariant under
L. The finite dimensionality of V forces V to have a composition series as an R
module. Let us suppose for a moment that K is algebraically closed. Proposition
5.6 says that the matrix of L in some ordered basis is upper triangular, and linear
combinations of the first k vectors in this basis form an invariant subspace under
L of dimension k. These subspaces are nested, and thus we obtain a composition
series. Thus obtaining a composition series when K is algebraically closed is
equivalent to obtaining triangular form. The existence of Jordan form is a finer
result. The discussion after Corollary 8.29 shows that V is a finite direct sum of R
modules R/(X − c j )k j with c j in K and k j > 0. For each of these, the discussion
at the end of Section VIII.6 shows how to refine R/(X − c j )k j to a composition
series for which there is an R submodule of each possible dimension from 0 to
k j ; the finer structure is hidden in the way that each invariant subspace is obtained
from the next smaller invariant subspace. If K is not necessarily algebraically
closed, then (X − c j )k j is to be replaced by Pj (X)k j for some prime polynomial
Pj (X), and the consecutive quotients for R/(Pj (X))k j have dimension equal to
the degree of Pj (X).
                                3. Chain Conditions                            565

                             3. Chain Conditions

We continue with R as a ring with identity, and we work with the category of
all unital left R modules. Except in special cases we did not address conditions
in Section 2 under which a unital left R module M has a composition series. In
this section we shall see that a necessary and sufficient condition for M to have a
composition series is that it satisfy two “chain conditions,” an ascending one and
a descending one, that we shall define. We already encountered the ascending
chain condition in Proposition 8.30 for the special case that R is a commutative
ring with identity, and the proof for general R requires only cosmetic changes.
   Proposition 10.8. If R is a ring with identity and M is a unital left R module,
then the following conditions on R submodules of M are equivalent:
    (a) (ascending chain condition) every strictly ascending chain of R sub-
        modules M1 $ M2 $ · · · terminates in finitely many steps,
    (b) (maximum condition) every nonempty collection of R submodules has
        a maximal element under inclusion,
    (c) (finite basis condition) every R submodule is finitely generated.
    PROOF. To see that (a) implies (b), let S be a nonempty collection of R
submodules of M. Take M1 in S. If M1 is not maximal, choose M2 in S properly
containing M1 . If M2 is not maximal, choose M3 in S properly containing M2 .
Continue in this way. By (a), this process must terminate, and then we have found
a maximal R submodule in S.
    To see that (b) implies (c), let N be an R submodule of M, and let S be
the collection of all finitely generated R submodules of N . This collection is
nonempty since 0 is in it. By (b), S has a maximal element, say N 0 . If x is in
N but x is not in N 0 , then N 0 + Rx is a finitely generated R submodule of N
that properly contains N 0 and therefore gives a contradiction. We conclude that
N 0 = N , and therefore N is finitely generated.
S∞  To see that (c) implies (a), let M1 $ M2 $ · · · be given, and put N =
   n=1 Mn . By (c), N is finitely generated. Since the Mn are increasing with n,
we can find some Mn 0 containing all the generators. Then the sequence stops no
later than at Mn 0 .                                                           §
  The corresponding result for descending chains is as follows.
   Proposition 10.9. If R is a ring with identity and M is a unital left R module,
then the following conditions on R submodules of M are equivalent:
    (a) (descending chain condition) every strictly descending chain of R
        submodules M1 % M2 % · · · terminates in finitely many steps,
    (b) (minimum condition) every nonempty collection of R submodules has
        a minimal element under inclusion.
566                    X. Modules over Noncommutative Rings

   PROOF. To see that (a) implies (b), let S be a nonempty collection of R
submodules of M. Take M1 in S. If M1 is not minimal, choose M2 in S properly
contained in M1 . If M2 is not minimal, choose M3 in S properly contained in
M2 . Continue in this way. By (a), this process must terminate, and then we have
found a minimal R submodule in S.
   To see that (b) implies (a), we observe that the members of any strictly de-
scending chain would be a family without a minimal element. Since (b) says that
any nonempty family has a minimal element, there can be no such chain.        §

  Proposition 10.10. Let R be a ring with identity, let M be a unital left R
module, and let N be an R submodule of M. Then
   (a) M satisfies the ascending chain condition if and only if N and M/N
       satisfy the ascending chain condition,
   (b) M satisfies the descending chain condition if and only if N and M/N
       satisfy the descending chain condition.
    PROOF. We prove (a), and the proof of (b) is completely similar. Suppose M
satisfies the ascending chain condition and hence also the maximum condition
by Proposition 10.8. The R submodules of N are in particular R submodules
of M and hence satisfy the maximum condition. The R submodules of M/N
lift back to R submodules of M containing N , and they too must satisfy the
maximum condition. By Proposition 10.8, N and M/N satisfy the ascending
chain condition.
    Conversely suppose that N and M/N satisfy the ascending chain condition.
Let {Ml } be an ascending chain of R submodules of M; we are to show that {Ml }
is constant from some point on. Since N and M/N satisfy the ascending chain
condition, we can find an n such that
      Mn+k ∩ N = Mn ∩ N          and      (Mn+k + N )/N = (Mn + N )/N
for all k ∏ 0. Combining the Second Isomorphism Theorem (Theorem 8.4) and
the first of these identities gives
          (Mn+k + N )/N ∼
                        = Mn+k /(Mn+k ∩ N ) = Mn+k /(Mn ∩ N )
for all k ∏ 0. Combining this result and two applications of the second of the
identities gives
                    Mn+k /(Mn ∩ N ) = Mn /(Mn ∩ N ).
The First Isomorphism Theorem (Theorem 8.3) shows that
              °                ¢±°            ¢
                Mn+k /(Mn ∩ N ) Mn /(Mn ∩ N ) ∼ = Mn+k /Mn .
Since the left side is the 0 module, the right side is the 0 module. Therefore
Mn+k = Mn for all k ∏ 0.                                                    §
                              4. Hom and End for Modules                           567

   Proposition 10.11. If R is a ring with identity and M is a unital left R module,
then M has a composition series if and only if M satisfies both the ascending
chain condition and the descending chain condition.

    PROOF. If M has a composition series of length n, then the Jordan–Hölder
Theorem (Corollary 10.7a) shows that every finite filtration of M with nonzero
consecutive quotients has length ≤ n, and hence M satisfies both chain conditions.
    Conversely suppose that M satisfies both chain conditions. By the maximum
condition, choose if possible a maximal proper R submodule N1 of M, then choose
if possible a maximal proper R submodule N2 of N1 , and so on. If all these choices
are possible, we obtain a strictly descending chain M % N1 % N2 % · · · , and the
consecutive quotients will be simple at each stage. The minimum condition says
that we cannot have such a chain, and thus the choice is impossible for the first
time at some stage k. That means that some Nk has no proper R submodule, and
Nk must be 0. Then M = N1 % N2 % · · · % Nk = 0 is a composition series. §



                         4. Hom and End for Modules

We continue to work with the category C of unital left R modules, where R is
a ring with identity, not necessarily commutative. Our interest in this section is
with Hom R (M, N ) and End R (M), where M and N are modules in C. Recall from
Section 1 that Hom R (M, N ) is a unital Z module, where Z is the center of R,
and that End R (M) is a Z algebra, the multiplication being composition. We shall
tend to ignore Z except when R is commutative or R is an associative algebra
over a field. However, Z will implicitly play a role in the context of bimodules,
which we introduce near the end of this section.
   In this section we shall be interested in interactions of Hom R (M, N ) and
End R (M) within the category C, in identities that they satisfy, in the naturality of
such identities, and in the use of Hom R (M, N ) in “change of rings,” also known
as “extension of scalars.” The next section will carry out a similar investigation
for a notion of tensor product that generalizes the tensor products in Chapter VI,
and we shall obtain in addition one important formula involving Hom and tensor
products at the same time. Finally in Section VI we shall examine the effect of
Hom and tensor product on “exact sequences.”
   The first observation is that Hom R is a functor, either a functor of one variable
with the other variable held fixed or, less satisfactorily, a functor of two variables.
To be precise, let D be the category of all abelian groups. For fixed M in Obj(C ),
we define
                              F(N ) = Hom R (M, N ).
568                         X. Modules over Noncommutative Rings
                                                   °                            ¢
If ϕ is in Hom R (N , N 0 ), we define F(ϕ) in HomZ Hom R (M,N ), Hom R (M,N 0 )
by the formula

                       F(ϕ)(τ ) = ϕτ           for τ ∈ Hom R (M, N ),

where ϕτ denotes the composition of τ followed by ϕ. In other words, F(ϕ)
is given by postmultiplication by ϕ. By inspection we see that F(1 N ) is the
identity from Hom R (M, N ) to itself if 1 N is the identity on N and that F(ϕ 0 ϕ) =
F(ϕ 0 )F(ϕ) if ϕ 0 is in Hom R (N 0 , N 00 ); the latter formula comes down to the asso-
ciativity formula (ϕ 0 ϕ)τ = ϕ 0 (ϕτ ) for functions under composition. Therefore F
is a covariant functor from the category C to the category D. We write Hom(1, ϕ)
for F(ϕ), so that Hom(1, ϕ)(τ ) = ϕτ .
    Similarly for fixed N in Obj(C ), we define

                                  G(M) = Hom R (M, N ).

On morphisms, G is given by premultiplication.
                                             °   Specifically for a morphism
                                                                           ¢ √
in Hom R (M, M 0 ), we define G(√) in HomZ Hom R (M 0 , N ), Hom R (M, N ) by
the formula
                   G(√)(τ ) = τ √      for τ ∈ Hom R (M 0 , N ).
We readily check that G is a contravariant functor from C to D. We write
Hom(√, 1) for G(√), so that Hom(√, 1)(τ ) = τ √.
    To create a single functor H from F and G, we can try to define a functor
H from C 2 to D by H (M, N ) = Hom R (M, N ). If ϕ ∈ Hom R (N , N 0 ) and
√ ∈ Hom R (M, M 0 ) are given, we can try the formula H (√, ϕ)(τ ) = ϕτ √ as a
definition for τ in Hom R (M 0 , N ). The trouble is that H is mixed as contravariant
in the first variable and covariant in the second variable. To get H to be covariant,
we can use the same formulas but regard H as defined on C opp × C, where C opp is
the opposite category of C, as defined in Problems 78–80 at the end of Chapter IV.
But this is getting to be a complicated structure for describing something simple,
and we shall simply avoid this construction altogether,2 working with F or G as
circumstances dictate.
    Even though we shall not work with H as a functor, it is convenient to
combine Hom(1, ϕ) and Hom(√, 1) into a single definition of Hom(√, ϕ) as
Hom(√, ϕ)(τ ) = ϕτ √. In particular, Hom(1, ϕ) and Hom(√, 1) commute with
each other; the commutativity follows from the associative law

Hom(√, 1) ◦ Hom(1, ϕ)(τ ) = (ϕτ )√ = ϕ(τ √) = Hom(1, ϕ) ◦ Hom(√, 1)(τ ).
   2 In category theory one sometimes proceeds in another way, defining a “bifunctor” to be a
functor-like thing depending on two variables, covariant or contravariant in each but maybe not the
same in each, and satisfying an appropriate commutativity property for the two variables.
                              4. Hom and End for Modules                           569

    Now let us turn to three identities involving Hom R and to their ramifications.
Each identity will assert some isomorphism involving Hom, and we consider each
side of the identity as the value of a functor. We shall be interested in knowing
that the isomorphism is natural in each case, the notion of naturality having been
defined in Section VI.6. The naturality need be proved in just one direction in
each case, since the inverse of an isomorphism that is natural is an isomorphism
that is natural.
    The first two identities concern the interaction of Hom R with direct products
and direct sums. Direct products and direct sums of unital left R modules were
defined in Examples 7 and 8 of modules in Section VIII.1, and they were seen to
be the product and coproduct
                         Q      functors for the category C. If S is a nonempty set,
then the direct product s∈S Ms of a family of unital left R modules {Ms | s ∈ S}
is the module whose underlying set is the Cartesian product of the setsLMs and
whose operations are defined coordinate
                                    Q      by coordinate. The direct sum s∈S Ms
is the R submodule of elements of s∈S Ms that are nonzero in only finitely many
coordinates.

   Proposition 10.12. Let S be a nonempty set, let Ms and Ns be unital left R
modules for each s ∈ S, and let M and N be unital left R modules. Then there
are isomorphisms of abelian groups
                °L          ¢ Q
    (a) Hom R ° s∈S Ms , N¢ ∼ =Q s∈S Hom R (Ms , N ),
                    Q
    (b) Hom R M, s∈S Ns ∼    = s∈S Hom R (M, Ns ).
Moreover, the isomorphism in (a) is natural in the variable {Ms }s∈S and in the
variable N , and the isomorphism in (b) is natural in the variable M and in the
variable {Ns }s∈S .
    REMARKS. In each instance the assertion of naturality is that some square
diagram is commutative, as illustrated in Figure 6.3. For example, if the mapping
from left to right in the isomorphism (a) is denoted for fixed N by 8{Ms }s∈S and
if a system of R homomorphisms ϕs : Ms → Ms0 is given, then one assertion of
naturality for (a) is that 8{Ms0 }s∈S ◦ {Hom(⊕ ϕs , 1)} = {Hom(⊕ ϕs , 1)} ◦ 8{Ms }s∈S .
The other says for fixed {Ms }s∈S and for an R homomorphism √ : N → N 0 that
8 N 0 ◦ Hom(1, √) = Hom(1, √) ◦ 8 N if the isomorphism (a) is denoted for fixed
L
    Ms by 8 N and if √ : N → N 0 is an R homomorphism. Two corresponding
assertions are made about (b). To simplify the notation, we shall usually drop the
subscripts from 8.
                                             L                   th
    PROOF
      L . For (a), let esth : Ms →              t Mt be the s       inclusion, and let
ps : t Mt → Ms be the s projection; the latter is defined as the restriction of
the projection associated with the direct product.° The L map from  ¢ left to right in
(a) is given by 8(σ ) = {σ ◦ es }s∈S for σ inPHom    R    s M s , N  , and the expected
formula for the inverse is 80 ({τs }s∈S ) = s (τs ◦ ps ). Then we have
570                     X. Modules over Noncommutative Rings
                                            P
             80 (8(σ )) = 80 ({σ ◦ es }s ) = (σ ◦ es ◦ ps ) = σ
                                            s
              0
                             °P            ¢ ©° P           ¢    ™
and        8(8 ({τs }s )) = 8   (τs ◦ ps ) =      (τs ◦ ps ) ◦ et t
                                 s                        s
                          = {τs ◦ ps ◦ es }s = {τs }s .

Hence 8 is an isomorphism with inverse 80 .
            the system of R homomorphisms ϕs : Ms0 → Ms °be
   Next let L                                                     Lgiven, let
                                                                           ¢
es : Ms → t Mt0 be the s th inclusion, and fix N . For σ in Hom R
 0    0
                                                                   s Ms , N ,
we have

 {Hom(⊕ ϕs , 1)}s (8(σ )) = {Hom(⊕ ϕs , 1)}s ({σ ◦ es }s ) = {σ ◦ es }s ◦ {ϕs }s
                           = {σ ◦ es ◦ ϕs }s = {σ ◦ ϕs ◦ es0 }s = {σ ◦ {ϕs }s ◦ et0 }t
                           = 8(σ ◦ {ϕs }s ) = 8({Hom(⊕ ϕs , 1)}s (σ )).
                                                                            0
                               ° L {Ms }¢s . If an R homomorphism ϕ : N → N
This proves naturality in the variable
is given and if σ is in Hom R      s Ms , N , then

       8(Hom(1, ϕ)(σ )) = 8(ϕ ◦ σ ) = {ϕ ◦ σ ◦ es }s
                        = Hom(1, ϕ)({σ ◦ es }s ) = Hom(1, ϕ)(8(σ )).

                     Q in the variable N .
This proves naturality
   For (b), let ps : Nt → Ns be the s th projection.° The Qmap ¢from left to right
in (b) is given by 8(σ ) = { ps ◦ σ }s for σ in Hom R M, s Ns , and the inverse
is given by 80 ({τs }s ) = τ , where τ (m) = {τs (m)}s . The proof of naturality is
similar to the corresponding proof in (a) and is omitted.                        §

   One ramification of Proposition 10.12 is the correspondence of “linear” maps
to matrices when the ring R of scalars is noncommutative. If R is a field and V is
an n-dimensional vector space over R, then we know that End R (V ) is isomorphic
as an R algebra to the space Mn (R) of n-by-n matrices over R, the isomorphism
being fixed once we choose an ordered basis of V . Things are more subtle when
R is noncommutative.

   Corollary 10.13. Let V be a unital left R module, and let S be the ring
S = End R (V ). For integers m ∏ 1 and n ∏ 1, there is a canonical isomorphism
of abelian groups
                          Hom R (V n , V m ) ∼
                                             = Mmn (S)
such that composition of R homomorphisms, given as a mapping

           Hom R (V n , V m ) × Hom R (V p , V n ) −→ Hom R (V p , V m ),
                                    4. Hom and End for Modules                             571

corresponds to matrix multiplication

                                Mmn (S) × Mnp (S) −→ Mmp (S).

In particular, in the special case that m = n, this canonical isomorphism becomes
an isomorphism of rings
                                 End R (V n ) ∼
                                              = Mn (S).
      REMARKS. For V = R, this isomorphism takes the form

                                  End R (R n ) ∼
                                               = Mn (End R (R))
and looks like something familiar from the case that R is a field. If End R (R)
were to be isomorphic as a ring to R, then the correspondence would be exactly
what we might expect between R linear mappings from a free R module of rank
n into itself, with n-by-n matrices with entries in R. However, End R (R) is not
ordinarily isomorphic to R, and the correspondence is something different and
unexpected. We shall sort out these matters in Proposition 10.14 and Corollary
10.15.
                                       Ln           Qn
   PROOF. Let e j : V → V n =            k=1 V =
                                                                       th
                                                      k=1 V be the j inclusion for
whatever n is under discussion, and let pi : V m → V be the i th projection
for whatever m is under discussion. For f in Hom R (V n , V m ), define f i j =
pi f e j . Then f i j is R linear from V into V , hence is in S = End R (V ). If also
Pis
g     in Hom R (V p , V n ), so that f ◦ g is in Hom R (V p , V m ), then the formula
   n                      n
   k=1 ek pk = 1 on V gives
                                             n
                                             P                         n
                                                                       P
                 ( f ◦ g)i j = pi f ge j =         pi f ek pk ge j =         f ik gk j .
                                             k=1                       k=1

Thus f ◦ g corresponds to the matrix product [ f i j ][gi j ], and the mapping is a ring
homomorphism. Since
       P                P                  °P      ¢ °P             ¢
          ei f i j p j = ei pi f e j p j =    ei pi f        e j p j = 1 f 1 = f,
          i, j           i, j                      i            j

the mapping is one-one.                                               P(S) is given, then
                                P If an arbitrary member [u i j ] of Mmn
we can define f = k,l ek u kl pl , obtain f i j = pi f e j = k,l pi ek u kl pl e j =
pi ei u i j p j e j = u i j , and conclude that the mapping is onto.                   §

  Proposition 10.14. The mapping ϕ 7→ ϕ(1) is a ring isomorphism End R (R) ∼
                                                                           =
  o
R of End R (R) onto the opposite ring R o of R.
   PROOF. The mapping ϕ 7→ ϕ(1) certainly respects addition. If ϕ maps to ϕ(1)
and τ maps to τ (1), then ϕτ maps to (ϕτ )(1) = ϕ(τ (1)) = ϕ(τ (1)1) = τ (1)ϕ(1)
since ϕ respects left multiplication by the element τ (1) of R. The order of
572                      X. Modules over Noncommutative Rings

multiplication is therefore reversed, and ϕ 7→ ϕ(1) is a ring homomorphism of
End R (R) into R o .
    If r is given in R o , define ϕr (s) = sr for s in R. Then ϕr respects addition, and
it respects left multiplication by R because ϕr (r 0 s) = r 0 sr = r 0 ϕr (s). Therefore
ϕr is a member of End R (R) such that ϕr (1) = r, and ϕ 7→ ϕ(1) is onto R o .
    If ϕ in End R (R) has ϕ(1) = 0, then the R linearity of ϕ implies that ϕ(r) =
ϕ(r1) = rϕ(1) = r0 = 0, so that ϕ = 0. Consequently the map ϕ 7→ ϕ(1) is
one-one.                                                                              §

  Corollary 10.15. For any integer n ∏ 1, End R (R n ) is ring isomorphic to
Mn (R o ).
   REMARKS. Now we can complete the remarks with Corollary 10.13: the case
in which R is commutative might lead us to believe that End R (R n ) is isomorphic
to Mn (R), but the correct isomorphism is with Mn (R o ) instead.
  PROOF. Corollary 10.13 shows that End R (R n ) is isomorphic to Mn (End R (R)),
and Proposition 10.14 shows that the latter ring is isomorphic to Mn (R o ).   §

   The third identity involving Hom R concerns Hom R (R, M), where M is a
unital left R module. Ordinarily Hom R (N , M), when N and M are two unital
left R modules, is not an R module, but in the case that N = R, it is. The
definition of the scalar multiplication by r ∈ R is (rϕ)(r 0 ) = ϕ(r 0r) for r 0 ∈ R
and ϕ ∈ Hom R (R, M). To see that rϕ is in Hom R (R, M), we let s be in R and
compute that (rϕ)(sr 0 ) = ϕ((sr 0 )r) = ϕ(s(r 0r)) = s(ϕ(r 0r)) = s((rϕ)(r 0 )), as
required. To see that (sr)ϕ = s(rϕ), we compute that ((sr)ϕ)(r 0 ) = ϕ(r 0 (sr)) =
ϕ((r 0 s)r) = (rϕ)(r 0 s) = (s(rϕ))(r 0 ). Proposition 10.16 identifies Hom R (R, M)
as an R module.

   Proposition 10.16. For any unital left R module M, there is a canonical R
isomorphism
                            Hom R (R, M) ∼ = M,
and this isomorphism is natural in the variable M.
   PROOF. The map 8 from left to right is given by 8(σ ) = σ (1), and the inverse
will be seen to be given by 80 (m) = τm with τm (r) = rm. The computation
8(rσ ) = (rσ )(1) = σ (1r) = σ (r1) = r(σ (1)) = r(8(σ )) shows that 8 is an
R homomorphism, and the computation τm (sr) = (sr)m = s(rm) = s(τm (r))
shows that τm is in Hom R (R, M).
   To see that 8 is an isomorphism with inverse 80 , we observe that 80 8 carries
Hom R (R, M) into itself and has (80 8)(σ ) = 80 (σ (1)) = τσ (1) , where τσ (1) (r) =
rσ (1) = σ (r); thus (80 8)(σ ) = σ , and 80 8 is the identity. Also, (880 )(m) =
                              4. Hom and End for Modules                          573

8(τm ) = τm (1) = 1m = m, and 880 is the identity.
  For the naturality let ϕ : M → M 0 be an R homomorphism. Then we have
8(Hom(1, ϕ)(σ )) = 8(ϕσ ) = ϕσ (1) = ϕ(8(σ )), and naturality is proved. §

   A relevant observation about the construction whose result is identified in
Proposition 10.16 is that we could get by with something more general than R
in the first variable of Hom R . In fact, the construction would have worked for
Hom R (P, M) for any unital (R, R) “bimodule” P, i.e., any abelian group P that
is a unital left R module and unital right R module in such a way that the two
actions commute: (r p)r 0 = r( pr 0 ). More generally let S be a second ring with
identity. We say that P is a unital (R, S) bimodule if P is simultaneously a unital
left R module and a unital right S module in such a way that (r p)s = r( ps) for
r ∈ R, s ∈ S, and p ∈ P. The following proposition shows that P allows us to
construct a unital left S module out of any unital left R module M.

   Proposition 10.17. If R and S are two rings with identity, if P is a unital
(R, S) bimodule, and if M is any unital left R module, then the abelian group
Hom R (P, M) becomes a unital left S module under the definition (sϕ)( p) =
ϕ( ps) for s ∈ S, ϕ ∈ Hom R (P, M), and p ∈ P.

   PROOF. To see that sϕ is an R homomorphism, we compute that (sϕ)(r p) =
ϕ((r p)s) = ϕ(r( ps)) = r(ϕ( ps)) = r((sϕ)( p)). It is clear that 1 acts as 1, and
the distributive laws are routine. What needs checking is the formula (ss 0 )ϕ =
s(s 0 ϕ) for s and s 0 in S and ϕ in Hom R (P, M). We compute that ((ss 0 )ϕ)( p) =
ϕ( p(ss 0 )) = ϕ(( ps)s 0 ) = (s 0 ϕ)( ps) = s((s 0 ϕ))( p), and the result follows. §

    An example of a unital (R, S) bimodule P is a ring S with identity such that
R is a subring of S with the same identity. Then we can take P = S, with the
result that R acts on the left, S acts on the right, and the two actions commute by
the associative law for multiplication in S. In this situation the passage from R
to Hom R (S, M) is called a change of rings, or extension of scalars, for M.
    In the special case that the rings are fields and the modules are vector spaces,
we saw a different kind of change of rings in Section VI.6. What we saw there
is that if K ⊆ L is an inclusion of fields and if E is a vector space over K, then
E L = E ⊗K L has a canonical scalar multiplication by members of L under the
definition that multiplication by c ∈ L is the linear mapping 1 ⊗ (l 7→ cl). In the
next section we shall see that this change of rings by means of tensor products
for vector spaces generalizes to give a second construction of a change of rings
for modules over a ring with identity.
574                          X. Modules over Noncommutative Rings

                             5. Tensor Product for Modules

In this section, R is still a ring with identity, and others rings will play a role
as well. We are going to generalize the discussion of tensor products of Section
VI.6, extending the notion from the tensor product of two vector spaces over a
field to the tensor product of a unital right R module and a unital left R module.
The tensor product will ordinarily not have the structure of an R module; it will
be just an abelian group. Additional structure on the tensor product will come
from a bimodule structure on one or both of the given R modules. For example it
will be seen that the tensor product, in the current sense, of two vector spaces over
a field F is a vector space over F because both vector spaces can be regarded as
unital bimodules over F. We return to this detail after giving the definition and
the theorem. Later in this section we shall obtain two fundamental associativity
formulas, one for triple tensor products and one involving tensor product and
Hom together.
    Let M be a unital right R module, and let N be a unital left R module. An R
bilinear function from M × N into an abelian group is a function b such that

   b(m 1 + m 2 , n) = b(m 1 , n) + b(m 2 , n) for all m 1 ∈ M, m 2 ∈ M, n ∈ N ,
   b(m, n 1 + n 2 ) = b(m, n 1 ) + b(m, n 2 ) for all m ∈ M, n 1 ∈ N , n 2 ∈ N ,
         b(mr, n) = b(m, rn) for all m ∈ M, n ∈ N , r ∈ R.

The first two conditions are summarized by saying that b is additive in each
variable. A tensor product of M and N over R is a pair (V, ∂) consisting of an
abelian group V and an R bilinear map ∂ : M × N → V having the following
universal mapping property: whenever b is an R bilinear function from M × N
into an abelian group A, then there exists a unique abelian-group homomorphism
L : V → A such that the diagram in Figure 10.1 commutes, i.e., such that L∂ = b
holds in the diagram. When ∂ is understood, one frequently refers to V itself as
the tensor product. The abelian-group homomorphism L : V → A is called
the additive extension of b to the tensor product.3 Theorem 10.18 below will
address existence and essential uniqueness of the tensor product. Because of the
essential uniqueness, it is customary to denote a tensor product by M ⊗ R N , and
Figure 10.1 incorporates this notation.4 The image ∂(m, n) of the member (m, n)
of M × N under ∂ is denoted by m ⊗ n.
    3 Warning. The name “additive extension” is in analogy with the situation for the tensor product

of vector spaces over a field, in which the extension is linear and really is an extension. Example 2
below will show that the tensor product of nonzero modules can be 0, and hence we do not always
get something for general R that we can regard intuitively as an extension.
    4 Sometimes the notation M ⊗ N refers to the constructed abelian group in the proof of Theorem
                                   R
10.18, and sometimes it refers to any abelian group as in the definition of tensor product.
                             5. Tensor Product for Modules                        575

                                               b
                                 M × N −−−→ A
                                   
                                   
                                  ∂y       L

                                M ⊗R N

         FIGURE 10.1. Universal mapping property of a tensor product
               of a right R module M and a left R module N .

   Theorem 10.18. Let R be a ring with identity. If M is a unital right R module
and N is a unital left R module, then there exists a tensor product (M ⊗ R N , ∂) of
M and N over R, and it is unique in the following sense: if (V1 , ∂1 ) and (V2 , ∂2 )
are two tensor products, then there exists a unique abelian-group homomorphism
8 : V1 → V2 such that 8 ◦ ∂1 = ∂2 , and it is an isomorphism. Any tensor product
is generated as an abelian group by the image of M × N in it. Moreover, tensor
product is a covariant functor from the category of pairs consisting of a unital
right R module and a unital left R module to the category of abelian groups under
the following definition: if ϕ : M → M 0 is a homomorphism of unital right R
modules and √ : N → N 0 is a homomorphism of unital left R modules, then there
exists a unique homomorphism of abelian groups ϕ ⊗ √ : M ⊗ R N → M 0 ⊗ R N 0
such that (ϕ ⊗ √)(m ⊗ n) = ϕ(m) ⊗ √(n) for all m ∈ M and n ∈ N .
   PROOF. Form the free abelian group G with a Z basis parametrized by the
elements of M × N . We write e(m, n) for the basis element in G corresponding
to the element (m, n) of M × N , and we regard e as a one-one function from
M × N onto the Z basis of G. Let H be the subgroup of G generated by all
elements of any of the forms

                      e(m 1 + m 2 , n) − e(m 1 , n) − e(m 2 , n),
                      e(m, n 1 + n 2 ) − e(m, n 1 ) − e(m, n 2 ),                 (∗)
                               e(mr, n) − e(m, rn),

where the elements m, m 1 , m 2 are in M, the elements n, n 1 , n 2 are in N , and the
scalar r is in R. We define M ⊗ R N to be the quotient group G/H , q : G → G/H
to be the quotient homomorphism, and ∂ to be the function (m, n) 7→ e(m, n)+ H
from M × N into G/H . The function ∂ is therefore given by ∂ = q ◦ e.
   Let us prove that (M ⊗ R N , ∂) is a tensor product of M and N over R. Each of
the elements in (∗) lies in H and hence is mapped by q into the 0 coset of G/H .
Since q is a homomorphism and since ∂ = q ◦ e, we obtain

                       ∂(m 1 + m 2 , n) = ∂(m 1 , n) + ∂(m 2 , n)
576                     X. Modules over Noncommutative Rings

from the first relation in (∗) and similar equalities from the other two relations.
Therefore ∂ : M × N → M ⊗ R N is an R bilinear function.
   Now let b : M × N → A be an R bilinear function from M × N into an
abelian group A. The universal mapping property in Figure 8.2 for free abelian
groups shows that there exists a unique group homomorphism e     L : G → A such
that e
     L(e(m, n)) = b(m, n) for all (m, n) in M × N . For the first expression in
(∗), we have
         °                                         ¢
       e
       L e(m 1 + m 2 , n) − e(m 1 , n) − e(m 2 , n)
                      =e                      L(e(m 1 , n)) − e
                        L(e(m 1 + m 2 , n)) − e                 L(e(m 2 , n))
                      = b(m 1 + m 2 , n) − b(m 1 , n) − b(m 2 , n).
The right side is 0 since b is R bilinear, and a similar conclusion applies to the
other two expressions in (∗). Therefore each member of (∗) lies in the kernel of e
                                                                                 L,
and the generated subgroup H lies in the kernel of e L. Consequently e L descends
to a group homomorphism L : G/H → A, i.e., there exists L with e       L = L ◦ q.
On any element (m, n) in M × N , we then have L ◦ ∂ = L ◦ q ◦ e = e     L ◦ e = b.
This proves the existence asserted by the universal mapping property for a tensor
product over R. For the asserted uniqueness, the formula L ◦ ∂ = b shows that L
is determined uniquely by b on ∂(M × N ). It is immediate from the definition of
M ⊗ R N that ∂(M × N ) generates M ⊗ R N , and thus L is determined uniquely
on all of M ⊗ R N .
   Therefore (M ⊗ R N , ∂) is a tensor product. Problems 18–22 at the end of
Chapter VI show that the uniqueness up to the asserted isomorphism follows
from general category theory.
   We are left with defining ϕ ⊗ √ when ϕ : M → M 0 and √ : N → N 0 are
given, and to showing that this definition makes tensor product into a covariant
functor. Define b : M × N → M 0 ⊗ R N 0 by b(m, n) = ϕ(m) ⊗ √(n). Then b is
R bilinear into an abelian group, the property b(mr, n) = b(m, rn) being verified
by the calculation
           b(mr, n) = ϕ(mr) ⊗ √(n) = ϕ(m)r ⊗ √(n)
                    = ϕ(m) ⊗ r√(n) = ϕ(m) ⊗ √(rn) = b(m, rn).
The additive extension of b to M ⊗ R N is taken to be ϕ ⊗ √. The formula is
therefore (ϕ ⊗ √)(m ⊗ n) = ϕ(m) ⊗ √(n). If we are given also ϕ 0 : M 0 → M 00
and √ 0 : N 0 → N 00 , then
 (ϕ 0 ⊗ √ 0 )(ϕ ⊗ √)(m ⊗ n) = (ϕ 0 ⊗ √ 0 )(ϕ(m) ⊗ √(n)) = ϕ 0 ϕ(m) ⊗ √ 0 √(n)
                             = (ϕ 0 ϕ ⊗ √ 0 √)(m ⊗ n).
Since the elements m ⊗ n generate M ⊗ R N , we obtain (ϕ 0 ⊗ √ 0 )(ϕ ⊗ √) =
ϕ 0 ϕ ⊗ √ 0 √. Similarly we check that 1 M ⊗ 1 N = 1 M⊗N . Therefore tensor product
is a covariant functor.                                                          §
                            5. Tensor Product for Modules                     577

   As in the last part of the above proof, the general procedure for constructing
an abelian-group homomorphism L : M ⊗ R N → A is somehow to define an
R bilinear function b : M × N → A and to take the additive extension from
Theorem 10.18 as the desired homomorphism. Once one has observed that the
expression b(m, n) is of a form that makes it R bilinear, then the homomorphism
L is defined and is uniquely determined by its values on elements m ⊗n, according
to the theorem.
   In practice, M or N often has some additional structure, and that structure
may be reflected in some additional property of the tensor product. The corollary
below addresses some situations of this kind.

   Corollary 10.19. Let R, S, and T be rings with identity, and suppose that M
is a unital right R module and N is a unital left R module. Under the additional
hypothesis that
     (a) M is a unital (S, R) bimodule, then M ⊗ R N is a unital left S module in
         a unique way such that s(m ⊗ n) = sm ⊗ n for all m ∈ M, n ∈ N , and
         s ∈ S,
    (b) N is a unital (R, T ) bimodule, then M ⊗ R N is a unital right T module
         in a unique way such that (m ⊗ n)t = m ⊗ nt for all m ∈ M, n ∈ N , and
         t ∈ T,
     (c) M is a unital (S, R) bimodule and N is a unital (R, T ) bimodule, then
         M ⊗ R N is a unital (R, T ) bimodule under the left R module structure
         in (a) and the right T module structure in (b).

   PROOF. In (a), let left multiplication by s ∈ S within M be given by ϕs : M →
M with ϕs (m) = sm. Then multiplication by s in S within M ⊗ R N is given
by ϕs ⊗ 1. The covariant-functor property makes ϕs ϕs 0 = ϕss 0 and ϕ1 = 1, and
the distributive properties follow from the definitions and the fact that each ϕs
is a homomorphism of the additive group M. This proves (a), and (b) is proved
similarly. For (c), if left multiplication by s ∈ S within M is given by ϕs and if
right multiplication by t ∈ T within N is given by √t , then the commutativity of
the operations on M ⊗ R N follows from the fact that the additive homomorphisms
ϕs ⊗ 1 and 1 ⊗ √t commute with each other.                                     §

  EXAMPLES.
   (1) R ⊗ R M ∼   = M as an isomorphism of left R modules whenever M is a left
R module. Here we regard R as a unital (R, R) bimodule, so that R ⊗ R M ∼     =M
has the structure of a unital left R module by Corollary 10.19a. The mapping of
left to right is the additive extension 8 of the R bilinear function b(r, m) = rm,
satisfying 8(r ⊗ m) = rm. It respects the left action by R. The two-sided
inverse 80 to 8 is given by 80 (m) = 1 ⊗ m. Then 80 ◦ 8 is the identity since
578                     X. Modules over Noncommutative Rings

80 (8(r ⊗ m)) = 80 (rm) = 1 ⊗ rm = r ⊗ m, and 8 ◦ 80 is the identity since
8(80 (m)) = 8(1 ⊗ m) = 1m = m. The R isomorphism R ⊗ R M ∼        = M is
natural in M. In fact, if ϕ : M → M 0 is given, then
             ϕ(8(r ⊗ m)) = ϕ(rm) = rϕ(m)
                                             °              ¢
                            = 8(r ⊗ ϕ(m)) = 8 (1 ⊗ ϕ)(r ⊗ m) .
    (2) R = Z. In this case, M ⊗Z N is the tensor product of abelian groups.
Let us consider what abelian group we obtain when M and N are both finitely
generated. Proposition 10.21 below shows that direct sums pull out of any tensor
product, and hence it is enough to treat the tensor product of two cyclic groups.
For Z ⊗Z A, we get A by Example 1, and Proposition 10.20 below shows that
A ⊗Z Z gives the same thing. Problem 3 at the end of the chapter identifies the
tensor product of two arbitrary finite cyclic groups (Z/kZ) ⊗Z (Z/lZ). For now,
let us verify in the special case that GCD(k, l) = 1 that (Z/kZ) ⊗Z (Z/lZ) = 0.
This tensor product is a unital Z module, being an abelian group, and Corollary
10.19a shows that the action by Z is given by c(a ⊗ b) = ca ⊗ b for any integer
c. Then we have 0 = (k1) ⊗ 1 = k(1 ⊗ 1) and 0 = 1 ⊗ (l1) = (1l) ⊗ 1 =
(l1) ⊗ 1 = l(1 ⊗ 1). Choosing integers x and y such that xk + yl = 1, we see
that 1 ⊗ 1 = x(k(1 ⊗ 1)) + y(l(1 ⊗ 1)) = 0 + 0 = 0. The tensor product is
generated by 1 ⊗ 1, and thus the tensor product is 0.
    (3) R equal to a commutative ring with identity. Then M is an (R, R) bimodule,
since any unital left module for a commutative ring is a right module under the
definition mr = rm and vice versa. Corollary 10.19 shows therefore that M ⊗ R N
is a unital R module. The special case that R is a field was treated in Section
VI.6.
    (4) M equal to a ring S with R as a subring with the same identity. Then we can
regard S as a unital (S, R) bimodule, and Corollary 10.19a shows that S ⊗ R M
is a unital left S module. The passage from M to S ⊗ R M is a second kind of
change of rings, or extension of scalars, the first kind being the passage from
M to Hom R (S, M) as in the previous section. Complexification of a real vector
space V as V ⊗R C is an instance of this change of rings by means of tensor
products. (Here we are taking into account the isomorphism V ⊗R C ∼      = C ⊗R V
given in Proposition 10.20 below.)
    (5) M and N equal to associative R algebras with identity over a commutative
ring R with identity. Proposition 10.24 below shows that M ⊗ R N is another
associative algebra with identity over R, with a multiplication such that
                     (m 1 ⊗ n 1 )(m 2 ⊗ n 2 ) = m 1 m 2 ⊗ n 1 n 2 .
In this case the additional structure on the tensor product is not a consequence of
Corollary 10.19, and additional argument is necessary.
                               5. Tensor Product for Modules                      579

   The rest of this section will be devoted to establishing some identities for tensor
product, together with their naturality, and to proving that the tensor product over
R of two R algebras, for a commutative ring R with identity, is again an R algebra.
Each identity involves setting up a homomorphism involving one or more tensor
products, and it is necessary to prove in each case that the homomorphism is an
isomorphism. For this purpose it is often inconvenient to prove directly that the
homomorphism has 0 kernel and is onto. In such cases one constructs what ought
to be the inverse homomorphism and proves that it is indeed a two-sided inverse.

   Proposition 10.20. Let R be a ring with identity, let M be a unital right R
module, and let N be a unital left R module. Let R o be the opposite ring of R,
let M o be M regarded as a left R o module, and let N o be N regarded as a right
R o module. Then
                           M ⊗R N ∼  = N o ⊗ Ro M o

under the unique homomorphism of abelian groups carrying m ⊗ n in M ⊗ R N
into n ⊗ m in N o ⊗ R o M o . The isomorphism is natural in the variables M and N .
   REMARK. To make the proof below a little clearer, we shall distinguish between
elements of M and M o , writing m in the first case and m o in the second case,
even though m o = m under our definitions. A similar notational convention will
be in force for N .
    PROOF. The map (m, n) 7→ n o ⊗ m o is additive in each variable and carries
(m, rn) to (rn)o ⊗ m o = n o r o ⊗ m o = n o ⊗r o m o = n o ⊗ (mr)o . This expression
is the image also of (mr, n), and hence (m, n) 7→ n o ⊗ m o is R bilinear and has
an additive extension 8 to M ⊗ R N . Arguing similarly, we readily construct a
homomorphism 80 : N o ⊗ R o M o → M ⊗ R N . It is immediate that 80 is a two-
sided inverse to 8, and the isomorphism follows. For the naturality in M, suppose
that ϕ : M → M 0 is an R homomorphism. Write ϕ o for the homomorphism
with ϕ o (m o ) = (ϕ(m))o . Then (1 ⊗ ϕ o )(8(m ⊗ n)) = (1 ⊗ ϕ o )(n o ⊗ m o ) =
n o ⊗ ϕ o (m o ) = n o ⊗ (ϕ(m))o = 8(ϕ(m) ⊗ n) = 8((ϕ ⊗ 1)(m ⊗ n)). This
proves the naturality in the M variable, and naturality in the N variable is proved
similarly.                                                                         §

  Proposition 10.21. Let R be a ring with identity, let S be a nonempty set, let
Ms be a unital right R module for each s ∈ S, and let N be a unital left R module.
Then                                      M
                       °M ¢
                           Ms ⊗ R N ∼   =      (Ms ⊗ R N )
                         s∈S                    s∈S

as abelian groups, and the isomorphism is natural in the tuple ({Ms }s∈S , N ).
580                      X. Modules over Noncommutative Rings

    REMARKS. A similar conclusion holds if the direct sum occurs in the second
member of the tensor product, as a consequence of Proposition 10.20. The
naturality carries with it some additional conclusions. For example, if each Ms is
a unital (T, R) bimodule for a ring T with identity, then the displayed isomorphism
is an isomorphism of left T modules.
                                                                    °L          ¢
    PROOF
      L . The map ({m s }s , n) 7→ {m s ⊗ n}s is R bilinear from         s∈S Ms × N
into s∈S (Ms ⊗ R N ), and its additive extension 8 is the homomorphism from
left to right in the displayed isomorphism.LIt has 8({m s }s ⊗ n) = {m s ⊗ n}s .
To construct the inverse, let i s : Ms →° Lt∈S Mt ¢be the s th inclusion. Then
(m s , n) 7→ i s (m s ) ⊗ n is R bilinear into        M ⊗ R¢ N and has an additive
                                                   L s
                                                 °s∈S
extension carrying m s ⊗ n to i s (m s ) ⊗ n in       s∈S Ms ⊗ R N . The universal
mapping property of direct sums of abelian
                                         L     groups          ° Lus a corresponding
                                                       then gives          ¢
                                      0
abelian-group homomorphism 8 : s∈S (Ms ⊗ R N ) →                   s∈S Ms ⊗ R N . It
has 80 ({m s ⊗ n}s ) = {m s }s ⊗ n. It is immediate that 80 ◦ 8 fixes each {m s }s ⊗ n
and hence is the identity, and that 8 ◦ 80 fixes each {m s ⊗ n}s and hence is the
identity.
    For the naturality let ϕs : Ms → Ms0 be an R homomorphism of right R
modules, and let √ : N → N 0 be an R homomorphism of left R modules. Then
 °                         ¢     °                  ¢
8 ({ϕs }s ⊗ √)({m s }s ⊗ n) = 8 {ϕs (m s )}s ⊗ √(n) = {ϕs (m s ) ⊗ √(n)}s
                             = {ϕs ⊗ √}s ({m s ⊗ n}) = {ϕs ⊗ √}s (8({m s } ⊗ n),

and naturality is proved.                                                          §

  Proposition 10.22. Let R and S be rings with identity, let M be a unital right
R module, let N be a unital (R, S) bimodule, and let P be a unital left S module.
Then
                    (M ⊗ R N ) ⊗ S P ∼= M ⊗ R (N ⊗ S P)
under the unique homomorphism 8 of abelian groups such that 8((m ⊗n)⊗ p) =
m ⊗ (n ⊗ p). The isomorphism is natural in the triple (M, N , P).
   REMARKS. As with Proposition 10.21, the naturality carries with it some
additional conclusions. For example, if T is a ring with identity and M is actually
a unital (T, R) bimodule, then the isomorphism is one of left T modules.
   PROOF. For fixed p, the map (m, n, p) 7→ m ⊗ (n ⊗ p) is R bilinear. In fact,
the map is certainly additive in m and in n. For the transformation law with an
element r of R, the calculation is (mr, n, p) 7→ mr ⊗ (n ⊗ p) = m ⊗ r(n ⊗ p) =
m ⊗ (rn ⊗ p), and this is the image of (m, rn, p).
   Thus for each fixed p, we have a unique well-defined extension, additive in
m and n, carrying (m ⊗ n, p) to m ⊗ (n ⊗ p). Using the uniqueness, we see
                            5. Tensor Product for Modules                    581

that this extended map is additive in the variables m ⊗ n and p. Also, if s is in
S, then ((m ⊗ n)s, p) = (m ⊗ ns, p) maps to m ⊗ (ns ⊗ p) = m ⊗ (n ⊗ sp),
which is the image of (m ⊗ n, sp), and therefore (m ⊗ n, p) 7→ m ⊗ (n ⊗ p) is
S bilinear. Consequently there exists a homomorphism 8 of abelian groups as in
the statement of the proposition.
   A similar argument produces a homomorphism 80 of abelian groups carrying
the right member of the display to the left member such that 80 (m ⊗ (n ⊗ p)) =
(m ⊗ n) ⊗ p. On the generating elements, we see that 80 ◦ 8 and 8 ◦ 80 are the
identity. This proves the isomorphism.
   For the naturality, let ϕ : M → M 0 , √ : N → N 0 , and τ : P → P 0 be maps
respecting the appropriate module structure in each case. Then
   °                             ¢      °                        ¢
 8 ((ϕ⊗√) ⊗ τ )((m ⊗ n) ⊗ p) = 8 (ϕ ⊗ √)(m ⊗ n) ⊗ τ ( p)
               °                        ¢
           = 8 (ϕ(m) ⊗ √(n)) ⊗ τ ( p) = ϕ(m) ⊗ (√(n) ⊗ τ ( p))
           = (ϕ ⊗ (√ ⊗ τ ))(m ⊗ (n ⊗ p)) = (ϕ ⊗ (√ ⊗ τ ))(8((m ⊗ n) ⊗ p)),

and naturality is proved.                                                      §

  Proposition 10.23. Let R and S be rings with identity, let M be a unital left
R module, let N be a unital (S, R) bimodule, and let P be a unital left S module.
Then
               HomS (N ⊗ R M, P) ∼   = Hom R (M, HomS (N , P))
under the homomorphism 8 of abelian groups defined by 8(ϕ)(m)(n) =
ϕ(n ⊗ m) for m ∈ M, n ∈ N , and ϕ ∈ HomS (M ⊗ R N , P). The isomorphism
is natural in the variables (N , M) and P.
   REMARKS. In the displayed isomorphism, N ⊗ R M on the left side is au-
tomatically a left S module, and hence HomS (N ⊗ R M, P) is a well-defined
abelian group. For the right side, Proposition 10.17 shows that HomS (N , P)
is a left R module under the definition (rτ )(n) = τ (nr); consequently
Hom R (M, HomS (N , P)) is a well-defined abelian group. The naturality in the
conclusion allows one to conclude, for example, that if M is in fact a unital
(R, T ) bimodule for a ring T with identity, then the displayed isomorphism is an
isomorphism of left T modules.
    PROOF. The homomorphism 8 is well defined. We construct its inverse. If √
is in Hom R (M, HomS (N , P)), then the map (n, m) 7→ √(m)(n) sends (nr, m)
to √(m)(nr) = (r(√(m))(n) = (√(rm))(n), and this is the image of (n, rm).
Hence (n, m) 7→ √(m)(n) is R bilinear and yields a map of N ⊗ R M into P such
that n ⊗ m maps to √(m)(n). The latter map is an S homomorphism since sn ⊗ m
maps to √(m)(sn) = s(√(m)(n)), which is s applied to the image of n ⊗ m. We
define 80 (√) to be the map defined on N ⊗ R M with 80 (√)(n ⊗ m) = √(m)(n).
582                      X. Modules over Noncommutative Rings

    Then 80 (8(ϕ))(n ⊗ m) = 8(ϕ)(m)(n) = ϕ(n ⊗ m) shows that 80 ◦ 8 is the
identity, and 8(80 (√))(m)(n) = 80 (√)(n ⊗ m) = √(m)(n) shows that 8 ◦ 80
is the identity. Hence 8 is an isomorphism of abelian groups.
    For naturality in (N , M), let σ : N 0 → N and τ : M 0 → M be given. Then

 8(Hom(σ ⊗ τ, 1)ϕ)(m 0 )(n 0 ) = (Hom(σ ⊗ τ, 1)(ϕ))(n 0 ⊗ m 0 )
             = ϕ(σ ⊗ τ )(n 0 ⊗ m 0 ) = ϕ(σ (n 0 ) ⊗ τ (m 0 )) = 8(ϕ)(τ (m 0 ))(σ (n 0 ))
             = Hom(τ, Hom(σ, 1))(8(ϕ))(m 0 )(n 0 ),

and naturality is proved in (N , M). For naturality in P, let σ : P → P 0 be given.
Then
  8(Hom(1, σ )ϕ)(m)(n) = (Hom(1, σ )ϕ)(n ⊗ m) = σ ϕ(n ⊗ m)
                   °            ¢
                = σ (8(ϕ))(m)(n) = Hom(1, Hom(1, σ ))(8(ϕ))(m)(n),

and naturality is proved in P.                                                        §

   Proposition 10.24. Let R be a commutative ring with identity, and let M and
N be associative R algebras with identity. Then M ⊗ R N is an associative R
algebra with identity under the unique multiplication law satisfying

                         (m ⊗ n)(m 0 ⊗ n 0 ) = mm 0 ⊗ nn 0 .

   PROOF. What we know from Example 3 is that M ⊗ R N is a unital R module.
We need to define the associative-algebra multiplication in M ⊗ R N and check
that it satisfies the required properties.
   Let µ(m) and ∫(n) be the left multiplication operators in M and N defined by
µ(m)(m 0 ) = mm 0 and ∫(n)(n 0 ) = nn 0 . The fact that R is central in M means
that µ(m)(rm 0 ) = mrm 0 = rmm 0 = rµ(m)(m 0 ) and hence that the mapping
µ(m) : M → M is a homomorphism of R modules. Similarly ∫(n) : N → N
is a homomorphism of R modules. Therefore µ(m) ⊗ ∫(n) is a well-defined
homomorphism of abelian groups for each (m, n) in M × N , and b(m, n) =
µ(m)⊗∫(n) is a well-defined map of M×N into the abelian group EndZ (M⊗ R N ).
The map b is certainly additive in the M variable and in the N variable. If r is in
R, then b(mr, n) = µ(mr) ⊗ ∫(n). Since

      (µ(mr) ⊗ ∫(n))(m 0 ⊗ n 0 ) = mrm 0 ⊗ nn 0 = mm 0r ⊗ nn 0
                                  = mm 0 ⊗ rnn 0 = (µ(m) ⊗ ∫(rn))(m ⊗ n 0 ),

we see that b(mr, n) = b(m, rn). Thus b is R bilinear and extends to a homo-
morphism L : M ⊗ R N → EndZ (M ⊗ R N ) of abelian groups.
                                      6. Exact Sequences                                     583

   For x and y in M ⊗ R N , we define a product by x y = L(x)(y). Since L(x) is in
EndZ (M ⊗ R N ), we have x(y1 + y2 ) = x y1 + x y2 . Since L is a homomorphism,
L(x1 + x2 ) = L(x1 ) + L(x2 ), and therefore (x1 + x2 )y = x1 y + x2 y. The
element 1 M ⊗ 1 N , where 1 M and 1 N are the respective identities of M and N , is a
two-sided identity for M ⊗ R N . Since M ⊗ R N is a two-sided unital R module,
we have r x = xr, and thus R(1 M ⊗ 1 N ) lies in the center of M ⊗ R N . Therefore
the product operation is R linear in each variable.
   Suppose that x = m ⊗ n and y = m 0 ⊗ n 0 . Then we have
             x y = L(x)(y) = L(m ⊗ n)(m 0 ⊗ n 0 ) = b(m, n)(m 0 ⊗ n 0 )
                 = (µ(m) ⊗ ∫(n))(m 0 ⊗ n 0 ) = mm 0 ⊗ nn 0
as asserted in the statement of the proposition. Consequently
          °                        ¢
 (m ⊗ n) (m 0 ⊗ n 0 )(m 00 ⊗ n 00 ) = (m ⊗ n)(m 0 m 00 ⊗ n 0 n 00 ) = m(m 0 m 00 ) ⊗ n(n 0 n 00 )
                                      = (mm 0 )m 00 ⊗ (nn 0 )n 00 = (mm 0 ⊗ nn 0 )(m 00 ⊗ n 00 )
                                        °                     ¢
                                      = (m ⊗ n)(m 0 ⊗ n 0 ) (m 00 ⊗ n 00 ).
This proves associativity of multiplication on elements of the form m ⊗ n. Since
these elements generate the tensor product as an abelian group and since the
distributive laws hold, associativity holds in general.                       §


                                   6. Exact Sequences

Consider a diagram of abelian groups and group homomorphisms of the form
                          ϕn−1          ϕn        ϕn+1           ϕn+2
                                   → Mn −−→ Mn+1 −−→ · · · ,
                    · · · −−→ Mn−1 −
where Mn−1 , Mn , Mn+1 , etc., are abelian groups and ϕn−1 , ϕn , ϕn+1 , ϕn+2 , etc.,
are homomorphisms. The diagram can be finite or infinite, and the particular kind
of indexing is not important. The sequence in question is called a complex if all
consecutive compositions are 0, i.e., if ϕk+1 ϕk = 0 for all k. This condition is
equivalent to having image(ϕk ) ⊆ ker(ϕk+1 ) and is the backdrop for the traditional
definitions of homology and cohomology groups, which are the various quotients
ker(ϕk+1 )/ image(ϕk ).

   EXAMPLES OF COMPLEXES.
   (1) The simplicial homology of a simplicial complex. For this situation the
indexing is reversed (say by replacing n by −n), so that the homomorphisms
lower the index. Each group Mn is a group whose elements are called “chains,”
and the homomorphisms are called “boundary maps.” The chains in the kernel
of one of the homomorphisms are said to be “closed,” and those in the image
584                         X. Modules over Noncommutative Rings

of a homomorphism are said to be “exact.” The quotient of the two, taking into
account the reversal of the indexing, is the system of simplicial homology groups
of the simplicial complex.
   (2) The de Rham cohomology of a smooth manifold. For this situation the
indexing goes upward as indicated, the group Mn is the vector space of smooth
differential forms of degree n, the homomorphisms are the restrictions to these
spaces of the linear de Rham operator d, ker(ϕn+1 ) is the vector subspace of
“closed” forms, image(ϕn ) is the vector subspace of “exact” forms, and the
quotient ker(ϕn+1 )/ image(ϕn ) is the n th de Rham cohomology space of the
manifold.
   (3) Cohomology of groups. This was defined in Section VII.6, knowledge
of which is not assumed in the present chapter. The result that shows that the
appropriate sequence is a complex is Proposition 7.39, for which we gave a direct
but complicated combinatorial proof.

    The above sequence is said to be exact at Mn if ker(ϕn+1 ) = image(ϕn ). It
is said to be an exact sequence if it is exact at every group in the sequence.
The condition of exactness may be viewed as having two parts to it. One is the
inclusion image(ϕn ) ⊆ ker(ϕn+1 ) that enters the definition of complex. Since
this condition says that ϕn+1 ϕn = 0, it is often easy to check. The other condition
is that ker(ϕn+1 ) ⊆ image(ϕn ), a condition that often is more difficult to check.
    The extent to which a complex fails to be exact plays a fundamental role in the
subject of homological algebra. This is a subject that for the most part is left to
Chapter IV of Advanced Algebra. That chapter will put the examples above into
a wider context, and it will develop techniques for working with homology and
cohomology. In the present section we shall give the barest hint of an introduction
to the subject by discussing some of the effects of the Hom functor and the tensor
product functor on exact sequences.
    Let us establish a setting for applying a functor F to an exact sequence or more
general complex. For current purposes we have in mind that F is Hom in one of
its two variables or is tensor product in one of its two variables. First we need
to have two categories available so that F carries the one category to the other.
These categories will have to satisfy some properties, but we shall not attempt to
list such properties at this time.5 Let us be content with some familiar examples of
categories whose objects are abelian groups with additional structure and whose
morphisms are group homomorphisms with additional structure. Specifically let
R be a ring with identity, let C R be the category of all unital left R modules, and
let D R be the category of all unital right R modules. We suppose that our functor
   5 The appropriate notion is that of an “abelian category,” which is defined in Section IV.8 of

Advanced Algebra.
                                  6. Exact Sequences                              585

F carries some C R or D R to another such category, possibly for a different ring.
The functor F can be covariant or contravariant. We require also of F that it be
an additive functor, i.e., that F(ϕ1 + ϕ2 ) = F(ϕ1 ) + F(ϕ2 ) for any maps ϕ1 and
ϕ2 that lie in the same Hom group.
   With the additional structure in place, we can now introduce the notions of
complex and exact sequence for the domain and range categories of F, not just
for the category of abelian groups. In this case the abelian groups in the sequence
are to be objects in the category, and the group homomorphisms in the sequence
are to be morphisms in the category; otherwise the definitions are unchanged.
The condition that F be additive implies that F carries any 0 map to a 0 map, and
that property will be key for us. In fact, we can apply F to any complex in the
domain category (by applying it to each object and morphism in the sequence);
after F is applied, the arrows point the same way if F is covariant, and they point
the opposite way if F is contravariant. If F is covariant, it sends any consecutive
composition 0 = ϕk+1 ϕk to 0 = F(0) = F(ϕk+1 ϕk ) = F(ϕk+1 )F(ϕk ); therefore
the consecutive composition of F of the maps is 0, and we obtain a complex.
If F is contravariant, we have 0 = F(0) = F(ϕk+1 ϕk ) = F(ϕk )F(ϕk+1 ); the
consecutive composition of F of the maps is still 0, and we still obtain a complex.
Thus the additive functor F sends any complex to a complex.
   However, not all additive functors invariably send exact sequences to exact
sequences, as we shall see with Hom and tensor product in the category CZ . Yet
they each preserve some features of certain exact sequences, even when Z is
replaced by a general ring with identity. To be precise we introduce the following
definition.
   A short exact sequence in our category is an exact sequence of the form
                                     ϕ        √
                         0 −→ M −→ N −−→ P −→ 0.
Exactness of this sequence incorporates three conditions:
     (i) ϕ is one-one,
    (ii) ker √ = image ϕ,
   (iii) √ is onto.
In fact, the three conditions are precisely the conditions of exactness at M, N ,
and P, respectively, since the maps at either end are 0 maps. If we think of ϕ as
an inclusion map, then the short exact sequence corresponds to the isomorphism
N/M ∼   = P obtained because √ factors through to the quotient N /M.

   Proposition 10.25. Let R be a ring with identity, let
                                      ϕ       √
                         0 −→ M −→ N −−→ P −→ 0
be a short exact sequence in the category C R , let E be a module in C R , and let E 0
be a module in D R . Then the following sequences in CZ are exact:
586                     X. Modules over Noncommutative Rings

                          1⊗ϕ                   1⊗√
              E 0 ⊗ R M −−−→ E 0 ⊗ R N −−−→ E 0 ⊗ R P −−−→ 0,

                               Hom(1,ϕ)                       Hom(1,√)
  0 −−−→ Hom R (E, M) −−−−−−→ Hom R (E, N ) −−−−−−→ Hom R (E, P),

                    Hom(ϕ,1)                       Hom(√,1)
  Hom R (M, E) √−−−−−− Hom R (N , E) √−−−−−− Hom R (P, E) √−−− 0.

   REMARKS. Similarly tensor product in the first variable, which carries D R to
CZ , retains the same exactness as in the first of these three sequences. In each
case when we specialize to R = Z, there are examples to show that exactness
fails if we try to include the expected remaining 0 in the above three sequences.
We give such examples after the proof of the proposition.
    PROOF. For the first sequence in CZ , we are to show that 1 ⊗ √ is onto E 0 ⊗ R P
and that every member of the kernel of 1 ⊗ √ is in the image of 1 ⊗ ϕ. (Recall that
ker(1 ⊗ √) ⊇ image(1 ⊗ ϕ) since the sequence is a automatically a complex.)
    Thus let p ∈ P be given. Since √ : N → P is onto, choose n ∈ N with
√(n) = p. Then (1 ⊗ √)(e ⊗ n) = e ⊗ p. The elements e ⊗ p generate E 0 ⊗ R P
as an abelian group, and hence 1 ⊗ √ is onto E 0 ⊗ R P.
    To show that ker(1 ⊗ √) ⊆ image(1 ⊗ ϕ), we observe from the exactness
of the given sequence at N that E 0 ⊗ R ker √ = E 0 ⊗ R image ϕ is generated
by all elements e ⊗ ϕ(m), hence by all elements (1 ⊗ ϕ)(e ⊗ m). Therefore
E 0 ⊗ R image ϕ = image(1 ⊗ ϕ), and it is enough to prove that

                           ker(1 ⊗ √) ⊆ E 0 ⊗ R ker √.                            (∗)

To prove (∗), we use the fact that √ is onto P. Define W = E 0 ⊗ R ker √ as a
subgroup of E 0 ⊗ R N , and let q : E 0 ⊗ R N → (E 0 ⊗ R N )/W be the quotient
homomorphism. Define b : E 0 × P → (E 0 ⊗ R N )/W by

      b(e, p) = (e ⊗ n) + W,              where n is chosen such that √(n) = p.

The expression b(e, p) does not depend on the choice of the element n having
√(n) = p since another choice n 0 will differ from n by a member of ker √ and will
affect the definition only by a member of W . The function b is certainly additive
in each variable, and it evidently has b(er, p) = b(e, r p) for r ∈ R as well. Thus
b is R bilinear. Let L : E 0 ⊗ R P → (E 0 ⊗ R N )/W be the additive extension.
From b(e, √(n)) = (e ⊗ n) + W , we see that L(e ⊗ √(n)) = (e ⊗ n) + W , hence
that L ◦ (1 ⊗ √) = q. This formula shows that ker(1 ⊗ √) ⊆ ker q = W , and
this is the inclusion (∗).
   For the second sequence in CZ , we are to show that Hom(1, ϕ) is one-one and
that every member of the kernel of Hom(1, √) is in the image of Hom(1, ϕ). If
                                       7. Problems                               587

σ is in Hom R (E, M) with Hom(1, ϕ)(σ ) = 0, then ϕ(σ (e)) = 0 for all e ∈ E.
Since ϕ is one-one, σ (e) = 0 for all e, and σ = 0.
    If τ in Hom R (E, N ) is in the kernel of Hom(1, √), so that √(τ (e)) = 0 for all
e ∈ E, then τ (e) = ϕ(m) for some m ∈ M depending on e, by exactness of the
given sequence at N ; the element m is unique because ϕ is one-one. Define τ 0
in Hom R (E, M) by τ 0 (e) = this m; the uniqueness of m for each e ensures that
τ 0 is in Hom R (E, M). Then we have τ (e) = ϕ(m) = ϕ(τ 0 (e)), and we conclude
that τ = Hom(1, ϕ)(τ 0 ).
    For the third sequence in CZ , we are to show that Hom(√, 1) is one-one and
that every member of the kernel of Hom(ϕ, 1) is in the image of Hom(√, 1). If
σ is in Hom R (P, E) with Hom(√, 1)(σ ) = 0, then σ (√(n)) = 0 for all n in N .
Since √ carries N onto P, σ = 0.
    If τ in Hom R (N , E) is in the kernel of Hom(ϕ, 1), then Hom(ϕ, 1)(τ ) = 0.
So τ (ϕ(m)) = 0 for all m ∈ M. Thus τ vanishes on image ϕ = ker √, and τ
descends to an R homomorphism τ : N / ker √ → E. That is, τ is of the form
τ = τ √ = Hom(√, 1)(τ ).                                                          §

     EXAMPLES OF FAILURE OF EXACTNESS IN CZ . We start from the exact sequence
                                   ϕ         √
                        0 −→ Z −→ Z −−→ Z/2Z −→ 0,
where ϕ is multiplication by 2 and √ is the usual quotient homomorphism.
    (1) We apply Z/2Z ⊗Z ( · ) to the given exact sequence, and the claim is that
1⊗ϕ : (Z/2Z⊗Z Z) → (Z/2Z⊗Z Z) is not one-one. In fact, Z/2Z⊗Z Z ∼      = Z/2Z,
and 1 ⊗ ϕ acts as multiplication by 2 under the isomorphism, hence is the 0 map
and is not one-one.
    (2) We apply HomZ (Z/2Z, · ) to the given exact sequence, and the claim is
that Hom(1, √) : HomZ (Z/2Z, Z) → HomZ (Z/2Z, Z/2Z) is not onto. In fact,
HomZ (Z/2Z, Z) = 0, and the identity map in HomZ (Z/2Z, Z/2Z) is nonzero;
therefore Hom(1, √) cannot be onto.
    (3) We apply HomZ ( · , Z/2Z)) to the given exact sequence, and the claim
is that Hom(ϕ, 1) : HomZ (Z, Z/2Z) → HomZ (Z, Z/2Z) is not onto. In fact,
Hom(ϕ, 1) is premultiplication by 2 and carries any σ in HomZ (Z, Z/2Z) to the
homomorphism k 7→ σ (2k) = 2σ (k) = 0. Since the usual quotient homomor-
phism Z → Z/2Z is a nonzero member of HomZ (Z, Z/2Z), Hom(ϕ, 1) is not
onto HomZ (Z, Z/2Z).

                                   7. Problems

1.    Suppose that the commutative ring R is an integral domain. As usual, the R
      submodules of R are the ideals. Prove that the ideals satisfy the descending
      chain condition if and only if R is a field.
588                        X. Modules over Noncommutative Rings

2.    Let F = F2 be a field with two elements.
      (a) Give an example of a representation of the cyclic group C2 on F2 with the
          property that there is a 1-dimensional invariant subspace U but there is no
          invariant subspace V with F2 = U ⊕ V .
      (b) How can one conclude from (a) that the group algebra R = F C2 has a unital
          left R module of finite length that is not semisimple? (Educational note:
          Compare this conclusion with Example 5 in Section 1, which shows that
          every unital left CG module is semisimple if G is a finite group.)

3.    Let G be the abelian group (Z/kZ) ⊗Z (Z/lZ), where k and l are nonzero
      integers.
      (a) Prove that G is generated by the element 1 ⊗ 1.
      (b) Prove that if k divides l, then (Z/kZ) ⊗Z (Z/lZ) ∼
                                                           = (Z/kZ) ⊗Z (Z/kZ).
      (c) Using multiplication as a Z bilinear form on (Z/kZ) × (Z/kZ), prove that
           (Z/kZ) ⊗Z (Z/kZ) has at least |k| elements.
      (d) Conclude that (Z/kZ) ⊗Z (Z/lZ) ∼    = Z/dZ, where d = GCD(k, l).

4.    (Fitting’s Lemma) Let R be a ring with identity, let M be a unital left R module,
      and suppose that M has a composition series. Let ϕ be a member of End R (M).
      (a) Prove for the composition powers ϕ n of ϕ that there exists an integer N such
           that ker ϕ n = ker ϕ n+1 and image ϕ n = image ϕ n+1 for all n ∏ N .
      (b) Let K and I be the respective R submodules of M obtained for n ∏ N in
           (a). Prove that K ∩ I = 0.
      (c) For x in M, show that there is some y in image ϕ N with ϕ N (x) = ϕ N (y).
      (d) Deduce from (c) that M = K + I, and conclude from  Ø    (b) that M = K ⊕ I.
      (e) Prove that ϕ carries I one-one onto I and that (ϕ ØK )n = 0 for some n.

5.    Let R be a ring with identity, and let
                                          ϕ       √
                              0 −→ M −−→ N −−→ P −→ 0

      be an exact sequence of unital left R modules. Prove that the following conditions
      are equivalent:
           (i) N is a direct sum N 0 ⊕ ker √ of R submodules for some N 0 ,
          (ii) there exists an R homomorphism σ : P → N such that √σ = 1 P ,
         (iii) there exists an R homomorphism τ : N → M such that τ ϕ = 1 M .
      (Educational note: In this case one says that the exact sequence is split.)

6.    (a) If R is the ring of quaternions, prove that End R (R) is isomorphic to R as a
          ring.
      (b) Give an example of a noncommutative ring with identity for which End R (R)
          is not isomorphic to R, and explain why it is not isomorphic.
                                       7. Problems                                    589

7.   Let R be a ring with identity, and let M be a unital left R module. Prove that M
     has a unique maximal semisimple R submodule N . (Educational note: The R
     submodule N is called the socle of M.)
8.   Let F ⊆ K be an inclusion of fields, and let A be an associative algebra with
     identity over F. Proposition 10.24 makes A ⊗F K into an associative algebra
     over F with a multiplication such that (a1 ⊗ k1 )(a2 ⊗ k2 ) = a1 a2 ⊗ k1 k2 . Show
     that A ⊗F K is in fact an associative algebra over K with scalar multiplication
     by k in K equal to left multiplication by 1 ⊗ k.
9.   A Lie algebra g over a field K is defined, according to Problems 31–35 at the
     end of Chapter VI, to be a nonassociative algebra over K with a multiplication
     written [x, y] that is alternating as a function of the pair (x, y) and satisfies
     [x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0 for all x, y, z in g. If L is a field
     containing K, prove that gL = g ⊗K L becomes a Lie algebra over L in a unique
     way such that its multiplication satisfies [x ⊗ c, y ⊗ d] = [x, y] ⊗ cd for x, y in
     g and c, d in L.
10. Let R be a ring with identity, let A be a unital right R module, and let B be a unital
    left R module. Since Z ⊆ R, A and B can be considered also as Z modules. Form
    a version of A ⊗ R B with associated R bilinear map b1 : A × B → A ⊗ R B, and
    form a version of A ⊗Z B with associated Z bilinear map b2 : A × B → A ⊗Z B.
    Let H be the subgroup of A ⊗Z B generated by all elements b2 (ar, b)−b2 (a, rb)
    with a ∈ A, b ∈ B, r ∈ R, and let q : A ⊗Z B → (A ⊗Z B)/H be the
    quotient homomorphism. Prove that there is an abelian group isomorphism
    8 : (A ⊗Z B)/H → A ⊗ R B such that 8(q(b2 (a, b))) = b1 (a, b) for all a ∈ A
    and b ∈ B.
11. Let R be a commutative ring with identity, and let C be the category of all
    commutative associative R algebras with identity. Prove that if A1 and A2 are in
    Obj(C), then (A1 ⊗ R A2 , {i 1 , i 2 }) is a coproduct, where i 1 : A1 → A1 ⊗ R A2 is
    given by i 1 (a1 ) = a1 ⊗ 1 and i 2 : A2 → A1 ⊗ R A2 is given by i 2 (a2 ) = 1 ⊗ a2 .

Problems 12–20 partition simple left R modules into isomorphism types, where R
is a ring with identity. For each simple left R module E and each unital left R
module M, one forms the sum M E of all simple R submodules that are isomorphic
to E and calls it an isotypic R submodule of M. The problems introduce a calculus
for working with the members of End R (M E ) in terms of right vector spaces over a
certain division ring. They show that if M is semisimple, then M is the direct sum of
all its isotypic R submodules, each of these is mapped to itself by every member of
End R (M), and consequently one can understand End R (M) in terms of right vector
spaces over certain division rings. These problems generalize and extend Problems
47–52 at the end of Chapter VII, which in effect deal with what happens for the ring
C G when G is a finite group; however, the material of Chapter VII is not prerequisite
for these problems. The following notation is in force: M is any unital left R module,
590                      X. Modules over Noncommutative Rings

E is a simple left R module, D E = Hom R (E, E) is the ring known from Proposition
10.4b to be a division ring,
         M E = (sum of all R submodules of M that are R isomorphic to E),
and                             M E = Hom R (E, M).
Unital right D E modules are right vector spaces over D E . In Problems 18–20, E
denotes a set of representatives of all R isomorphism classes of simple left R modules.
12. Prove that
    (a) M E is a direct sum of simple R modules that are R isomorphic to E,
    (b) the image of every mapping in M E belongs to M E ,
    (c) redefinition of the range from M to M E defines an isomorphism M E ∼         =
         Hom R (E, M E ) of abelian groups.
13. Prove that
    (a) M E is a unital right D E module under composition of R homomorphisms,
    (b) E is a unital left D E module under the operation of the members of D E ,
    (c) the left R module action and the left D E module action on E commute with
        each other.
14. Show that M E ⊗ D E E is a unital left R module in such a way that r(m ⊗ e) =
    m ⊗ re.
15. Prove that there is a well-defined R homomorphism 8 : M E ⊗ D E E → M such
    that 8(√ ⊗ e) = √(e) and such that 8 is an R isomorphism onto M E .
16. Prove that the left R submodules N of M E are in one-one correspondence with
    the right D E vector subspaces W of M E by the maps
                N 7→ Hom R (E, N ) ⊆ Hom R (E, M) = M E           if N ⊆ M E
      and         W 7→ W ⊗ D E E ⊆ M E ⊗ D E E ∼
                                               = ME           if W ⊆ M E .

17. Prove for any unital left R module N that there is a canonical isomorphism
                                            = Hom D E (M E , N E )
                         Hom R (M E , N E ) ∼
      of abelian groups defined as follows. Suppose ϕ is in Hom R (M E , N E ). Com-
      position with ϕ carries Hom R (E, M) into Hom R (E, N ); this map respects the
      right action of D E and hence induces a map
                                ϕ E ∈ Hom D E (M E , N E ).

      The isomorphism is given in terms of the isomorphisms 8 M for M and 8 N for
      N in Problem 15 by
                  ϕ(8 M (√ ⊗ e)) = 8 N (ϕ E (√) ⊗ e)          for √ ∈ M E .
                                      7. Problems                                591

18. If M is semisimple, prove that
                               M        M
                          M=       ME ∼
                                      =   (M E ⊗ D E E).
                                E∈E          E∈E

19. Still with M semisimple, prove that the left R submodules of M are in one-one
    correspondence with families {W E | E ∈ E} of right D E vector subspaces of
    ME.
20. Suppose that M and N are two semisimple left R modules. Prove that there is a
    canonical isomorphism of abelian groups
                                          Y
                        Hom R (M, N ) ∼=    Hom D E (M E , N E ).
                                             E∈E
    More precisely prove that an R module map from M to N is specified by giving,
    for a representative E of each class of simple left R modules, an arbitrary right
    vector-space map from M E to N E .
                                        APPENDIX



Abstract. This appendix treats some topics that are likely to be well known by some readers and less
known by others. Most of it already comes into play by Chapter II. Section A1 deals with set theory
and with functions: it discusses the role of formal set theory, it works in a simplified framework that
avoids too much formalism and the standard pitfalls, it establishes notation, and it mentions some
formulas. Some emphasis is put on distinguishing the image and the range of a function, since this
distinction is important in algebra and algebraic topology.
    Section A2 defines equivalence relations and establishes the basic fact that they lead to a parti-
tioning of the underlying set into equivalence classes.
    Section A3 reviews the construction of rational numbers from the integers, and real numbers
from the rational numbers. From there it concentrates on the solvability within the real numbers of
certain polynomial equations.
    Section A4 is a quick review of complex numbers, real and imaginary parts, complex conjugation,
and absolute value.
    Sections A5 and A6 return to set theory. Section A5 defines partial orderings and includes Zorn’s
Lemma, which is a powerful version of the Axiom of Choice, while Section A6 concerns cardinality.



                                   A1. Sets and Functions

Algebra typically makes use of an informal notion of set theory and notation for
it in which sets are described by properties of their elements and by operations
on sets. This informal set theory, if allowed to be too informal, runs into certain
paradoxes, such as Russell’s paradox: “If S is the set of all sets that do not
contain themselves as elements, is S a member of S or is it not?” The conclusion
of Russell’s paradox is that the “set” of all sets that do not contain themselves as
elements is not in fact a set.
    Mathematicians’ experience is that such pitfalls can be avoided completely by
working within some formal axiom system for sets, of which there are several
that are well established. A basic one is “Zermelo–Fraenkel set theory,” and the
remarks in this section refer specifically to it but refer to the others at least to
some extent.1
    The standard logical paradoxes are avoided by having sets, elements (or “en-
tities”), and a membership relation ∈ such that a ∈ S is a meaningful statement,
    1 Mathematicians have no proof that this technique avoids problems completely. Such a proof

would be a proof of the consistency of a version of mathematics in which one can construct the
integers, and it is known that this much of mathematics cannot be proved to be consistent unless it
is in fact inconsistent.

                                                 593
594                                           Appendix

true or false, if and only if a is an element and S is a set. The terms set, element,
and ∈ are taken to be primitive terms of the theory that are in effect defined by
a system of axioms. The axioms ensure the existence of many sets, including
infinite sets, and operations on sets that lead to other sets. To make full use of this
axiom system, one has to regard it as occurring in the framework of certain rules
of logic that tell the forms of basic statements (namely, a = b, a ∈ S, and “S
is a set”), the connectives for creating complicated statements from simple ones
(“or,” “and,” “not,” and “if . . . then”), and the way that quantifiers work (“there
exists” and “for all”).
    Working rigorously with such a system would likely make the development
of mathematics unwieldy, and it might well obscure important patterns and di-
rections. In practice, therefore, one compromises between using a formal axiom
system and working totally informally; let us say that one works “informally but
carefully.” The logical problems are avoided not by rigid use of an axiom system,
but by taking care that sets do not become too “large”: one limits the sets that one
uses to those obtained from other sets by set-theoretic operations and by passage
to subsets.2
    A feature of the axiom system lying behind working informally but carefully
is that it does not preclude the existence of additional sets beyond those forced to
exist by the axioms. Thus, for example, in the subject of coin-tossing within prob-
ability, it is normal to work with the set of possible outcomes as S = {heads, tails}
even though it is not immediately apparent that requiring this S to be a set does
not introduce some contradiction.
    It is worth emphasizing that the points of the theory at which one takes particu-
lar care vary somewhat from subject to subject within mathematics. For example
it is sometimes of interest in calculus of several variables to distinguish between
the range of a function and its image in a way that will be mentioned below, but it
is usually not too important. In homological algebra, however, the distinction is
extremely important, and the subject loses a great deal of its impact if one blurs
the notions of range and image.
    Some references for set theory that are appropriate for reading once are
Halmos’s Naive Set Theory, Hayden–Kennison’s Zermelo–Fraenkel Set Theory,
and Chapter 0 and the appendix of Kelley’s General Topology. The Kelley book
is one that uses the word “class” as a primitive term more general than “set”; it
develops von Neumann set theory.

  All that being said, let us now introduce the familiar terms, constructions,
and notation that one associates with set theory. To cut down on repetition, one
   2 Not every set so obtained is to be regarded as “constructed.” The Axiom of Choice, which we
come to shortly, is an existence statement for elements in products of sets, and the result of applying
the axiom is a set that can hardly be viewed as “constructed.”
                                       A1. Sets and Functions                                       595

allows some alternative words for “set,” such as family and collection. The word
“class” is used by some authors as a synonym for “set,” but the word class is used
in some set-theory axiom systems to refer to a more general notion than “set,”
and it will be useful to preserve this possibility. Thus a class can be a set, but we
allow ourselves to speak, for example, of the class of all groups even though this
class is too large to be a set. Alternative terms for “element” are member and
point; we shall not use the term “entity.” Instead of writing ∈ systematically, we
allow ourselves to write “in.” Generally, we do not use ∈ in sentences of text as
an abbreviation for an expression like “is in” that contains a verb.
    If A and B are two sets, some familiar operations on them are the union A ∪ B,
the intersection A ∩ B, and the difference A − B, all defined in the usual way in
terms of the elements they contain. Notation for the difference of sets varies from
author to author; some other authors write A \ B or A ∼ B for difference, but this
book uses A − B. If one is thinking of A as a universe, one may abbreviate A − B
as B c , the complement of B in A. The empty set ∅ is a set, and so is the set of
all subsets of a set A, which is sometimes denoted by 2 A . Inclusion of a subset A
in a set B is written A ⊆ B or B ⊇ A; then B is a superset of A. Inclusion that
does not permit equality is denoted by A $ B or B % A; in this case one says
that A is a proper subset of B or that A is properly contained in B.
    If A is a set, the singleton {A} is a set with just the one member A. Another
operation is unordered pair, whose formal definition is {A, B} = {A} ∪ {B} and
whose informal meaning is a set of two elements in which we cannot distinguish
either element over the other. Still another operation is ordered pair, whose
formal definition is (A, B) = {{A}, {A, B}}. It is customary to think of an
ordered pair as a set with two elements in which one of the elements can be
distinguished as coming first.3
    Let A and B be two sets. The set of all ordered pairs of an element of A and
an element of B is a set denoted by A × B; it is called the product of A and B
or the Cartesian product. A relation between a set A and a set B is a subset of
A × B. Functions, which are to be defined in a moment, provide examples. Two
examples of relations that are usually not functions are “equivalence relations,”
which are discussed in Section A2, and “partial orderings,” which are discussed
in Section A5.
    If A and B are sets, a relation f between A and B is said to be a function,
written f : A → B, if for each x ∈ A, there is exactly one y ∈ B such that
(x, y) is in f . If (x, y) is in f , we write f (x) = y. In this informal but careful
definition of function, the function consists of more than just a set of ordered
    3 Unfortunately a “sequence” gets denoted by {x , x , . . . } or {x }∞ . If its notation were really
                                                    1 2                n n=1
consistent with the above definitions, we might infer, inaccurately, that the order of the terms of
the sequence does not matter. The notation for unordered pairs, ordered pairs, and sequences is,
however, traditional, and it will not be changed here.
596                                         Appendix

pairs; it consists of the set of ordered pairs regarded as a subset of A × B. This
careful definition makes it meaningful to say that the set A is the domain, the set
B is the range,4 and the subset of y ∈ B such that y = f (x) for some x ∈ A is
the image of f . The image is also denoted by f (A). Sometimes a function f
is described in terms of what happens to typical elements, and then the notation
is x 7→ f (x) or x 7→ y, possibly with y given by some formula or by some
description in words about how it is obtained from x. Sometimes a function f is
written as f ( · ), with a dot indicating the placement of the variable; this notation
is especially helpful in working with restrictions, which we come to in a moment,
and with functions of two variables when one of the variables is held fixed. This
notation is useful also for functions that involve unusual symbols, such as the
absolute value function x 7→ |x|, which in this notation becomes | · |. The
word map or mapping is used for “function” and for the operation of a function,
especially when a geometric setting for the function is of importance.
    Often mathematicians are not so careful with the definition of function. De-
pending on the degree of informality that is allowed, one may occasionally refer
to a function as f (x) when it should be called f or x 7→ f (x). If any confusion is
possible, it is wise to use the more rigorous notation. Another habit of informality
is to regard a function f : A → B as simply a set of ordered pairs. Thus two
functions f 1 : A → B and f 2 : A → C become the same if f 1 (a) = f 2 (a) for
all a in A. With the less-careful definition, the notion of the range of a function is
not really well defined. The less-careful definition can lead to trouble in algebra
and topology, but it does not often lead to trouble in analysis until one gets to
a level where algebra and analysis merge somewhat. One place where it comes
into play in algebra is in the notion of an exact sequence of three abelian groups
      ϕ          √
A −→ B −→ C, which is defined as a system of three abelian groups and
homomorphisms as indicated such that the kernel of √ equals the image of ϕ. In
this definition one is not free to adjust B to be the image of ϕ since that adjustment
will affect the kernel of √ as well.
   The set of all functions from a set A to a set B is a set. It is sometimes denoted
by B A . The special case 2 A that arises with subsets comes by regarding 2 as a
set {1, 2} and identifying a function f from A into {1, 2} with the subset of all
elements x of A for which f (x) = 1.
   If a subset B of a set A may be described by some distinguishing property
P of its elements, we may write this relationship as B = {x ∈ A | P}. For
example the function f in the previous paragraph is identified with the subset
{x ∈ A | f (x) = 1}. Another example is the image of a general function
 f : A → B, namely f (A) = {y ∈ B | y = f (x) for some x ∈ A}. Still more
generally along these lines, if E is any subset of A, then f (E) denotes the set

   4 Some   authors refer to B as the codomain.
                                 A1. Sets and Functions                            597

{y ∈ B | y = f (x) for some x ∈ E}. Some authors use a colon or semicolon or
comma instead of a vertical line in this notation.                    S
   This book frequently uses sets denoted by expressions like x∈S A x , an in-
dexed union, where S is a set that is usually nonempty. If S is the set {1, 2}, this
reduces to A1 ∪ A2 . In the general case it is understood that we have an unnamed
function, say f , given by x 7→ AS    x , having domain S and range the set of all
subsets of an unnamed set T , and x∈S A x is the set of all y ∈ST such that y is
S A x for some x ∈ S. When ST
in                                 is understood, we may write x A x instead of
   x∈S xA . Indexed  intersections   x∈S A x are defined similarly, and this time it is
essential to disallow S empty because otherwise the intersection cannot be a set
in any useful set theory.
   There is also an indexed Cartesian product ×x∈S A x that specializes in the
case that S = {1, 2} to A1 × A2 . Usually S is assumedS nonempty. This Cartesian
product is the set of all functions f from S into x∈S A x such that f (x) is in
A x for all x ∈ S. In the special case that S is {1, . . . , n}, the Cartesian product
is the set of ordered n-tuples from n sets A1 , . . . , An and may be denoted by
A1 × · · · × An ; its members may be denoted by (a1 , . . . , an ) with a j ∈ A j for
1 ≤ j ≤ n. When the factors of a Cartesian product have some additional
algebraic structure, the notation for the Cartesian productQis often altered; for
example the Cartesian product of groups A x is denoted by x∈S A x .
   It is completely normal in algebra, and it is the practice in this book, to take
the following axiom as part of one’s set theory; the axiom is customarily used
without specific mention.

   Axiom of Choice. The Cartesian product of nonempty sets is nonempty.

   If the index set is finite, then the Axiom of Choice reduces to a theorem of set
theory. The axiom is often used quite innocently with a countably infinite index
set. For example a theorem of analysis asserts that any bounded sequence {an } of
real numbers has a subsequence converging to lim sup an , and the proof constructs
one member of the sequence at a time. When the proof is written in such a way that
these members have some flexibility in their definitions, the Axiom of Choice
is usually being invoked. The proof can be rewritten so that the members of
the subsequence have specific definitions, such as “the term an such that n is the
smallest integer satisfying such-and-such properties.” In this case the axiom is not
being invoked. In fact, one can often rewrite proofs involving a countably infinite
choice so that they involve specific definitions and therefore avoid invoking the
axiom, but there is no point in undertaking this rewriting. In algebra the axiom
is often invoked in situations in which the index set is uncountable; selection of
a representative from each of uncountably many equivalence classes is such a
choice if all equivalence classes have more than one element.
598                                   Appendix

   From the Axiom of Choice, one can deduce a powerful tool known as Zorn’s
Lemma, whose use it is customary to acknowledge. Zorn’s Lemma appears in
Section A5.
   If f : A → B is a function and B is a subset of B 0 , then f can be regarded
as a function with range B 0 in a natural way. Namely, the set of ordered pairs is
unchanged but is to be regarded as a subset of A × B 0 rather than A × B.
   Let f : A → B and g : B → C be two functions such that the range of f
equals the domain of g. The composition g ◦ f : A → C, written sometimes as
g f : A → C, is the function with (g ◦ f )(x) = g( f (x)) for all x. Because of the
construction in the previous paragraph, it is meaningful to define the composition
more generally when the range of f is merely a subset of the domain of g.
   A function f : A → B is said to be one-one if f (x1 ) 6= f (x2 ) whenever x1
and x2 are distinct members of A. The function is said to be onto, or often “onto
B,” if its image equals its range. The terminology “onto B” avoids confusion: it
specifies the image and thereby guards against the use of the less careful definition
of function mentioned above. A mathematical audience often contains some
people who use the more careful definition of function and some people who use
the less careful definition. For the latter kind of person, a function is always onto
something, namely its image, and a statement that a particular function is onto
might be regarded as a tautology. A function from one set to another is said to
put the sets in one-one correspondence if the function is one-one and onto.
   When a function f : A → B is one-one and is onto B, there exists a function
g : B → A such that g ◦ f is the identity function on A and f ◦ g is the identity
function on B. The function g is unique, and it is defined by the condition, for
y ∈ B, that g(y) is the unique x ∈ A with f (x) = y. The function g is called
the inverse function of f and is often denoted by f −1 .
   Conversely if f : A → B has an inverse function, then f is one-one and
is onto B. The reason is that a composition g ◦ f can be one-one only if f is
one-one, and in addition, that a composition f ◦ g can be onto the range of f
only if f is onto its range.
   If f : A → B isØ a function and E is a subset of A, the restriction of f
to E, denoted by f Ø E , is the function f : E → B consisting of all ordered
pairs (x, f (x)) with x ∈ E, this set being regarded as a subset of E × B, not of
A× B. One especially common example of a restriction is restriction to one of the
variables of a function of two variables, and then the idea of using a dot in place
of a variable can be helpful notationally. Thus the function of two variables might
be indicated by f or (x, y) 7→ f (x, y), and the restriction to the first variable,
for fixed value of the second variable, would be f ( · , y) or x 7→ f (x, y).
   We conclude this section with a discussion of direct and inverse images of
sets under functions. If f : A → B is a function and E is a subset of A, we
have defined f (E) = {y ∈ B | y = f (x) for some x ∈ E}. This is the same
                                   A2. Equivalence Relations                              599
                   Ø
as the image of f Ø E and is frequently called the image or direct image of E
under f . The notion of direct image does not behave well with respect to some
set-theoretic operations: it respects unions but not intersections. In the case of
unions, we have                ≥[ ¥ [
                             f      Es =       f (E s );
                                           s∈S          s∈S
                                       ≥S               ¥
the inclusion ⊇ follows since f      s∈S E s ⊇ f (E s ) for each s, and the inclusion
⊆ follows because any member of the left side is f of a member of some E s . In
                                                   ?
the case of intersections, the question f (E ∩ F) = f (E) ∩ f (F) can easily have
a negative answer, the correct general statement being f (E ∩ F) ⊆ f (E)∩ f (F).
An example with equality failing occurs when A = {1, 2, 3}, B = {1, 2}, f (1) =
 f (3) = 1, f (2) = 2, E = {1, 2} and F = {2, 3} because f (E ∩ F) = {2} and
 f (E) ∩ f (F) = {1, 2}.
    If f : A → B is a function and E is a subset of B, the inverse image of E
under f is the set f −1 (E) = {x ∈ A | f (x) ∈ E}. This is well defined even if f
does not have an inverse function. (If f does have an inverse function f −1 , then
the inverse image of E under f coincides with the direct image of E under f −1 .)
    Unlike direct images, inverse images behave well under set-theoretic opera-
tions. If f : A → B is a function and {E s | s ∈ S} is a set of subsets of B,
then
                                 ≥\ ¥ \
                            f −1     Es =       f −1 (E s ),
                                           s∈S          s∈S
                                       ≥[         ¥     [
                                  −1
                              f                  Es =         f −1 (E s ),
                                           s∈S          s∈S
                                       f   −1
                                             (E sc )   =(f −1
                                                                (E s ))c .

In the third of these identities, the complement on the left side is taken within
B, and the complement on the right       ° T side is   ¢ taken−1within A. To prove the
                                      −1
first identity,
             °T  we   observe
                         ¢    that  f        s∈S   E s   ⊆ f (E s ) for each s ∈ S and
                               T
hence f −1        s∈S sE   ⊆     s∈S  f −1
                                           (E s ).  For   the reverse inclusion, if x is in
T        −1                       −1
   s∈S f              T x is in f (E s ) for each° T
            (E s ), then                               s and thus¢ f (x) is in E s for each s.
Hence f (x) is in s∈S E s , and x is in f −1             s∈S E s  . This proves the reverse
inclusion. The second and third identities are proved similarly.


                              A2. Equivalence Relations

An equivalence relation on a set S is a relation between S and itself, i.e., is a
subset of S × S, satisfying three defining properties. We use notation like a ' b,
600                                   Appendix

written “a is equivalent to b,” to mean that the ordered pair (a, b) is a member of
the relation, and we say that “'” is the equivalence relation. The three defining
properties are
     (i) a ' a for all a in S, i.e., ' is reflexive,
    (ii) a ' b implies b ' a if a and b are in S, i.e., ' is symmetric.
   (iii) a ' b and b ' c together imply a ' c if a, b, and c are in S, i.e., ' is
         transitive.
   An example occurs with S equal to the set Z of integers with a ' b meaning
that the difference a − b is even. The properties hold because (i) 0 is even, (ii)
the negative of an even integer is even, and (iii) the sum of two even integers is
even.
   There is one fundamental result about abstract equivalence relations. The
equivalence class of a, written [a] for now, is the set of all members b of S such
that a ' b.

   Proposition. If ' is an equivalence relation on a set S, then any two equiv-
alence classes are disjoint or equal, and S is the union of all the equivalence
classes.
    PROOF. Let [a] and [b] be the equivalence classes of members a and b of S.
If [a] ∩ [b] 6= ∅, choose c in the intersection. Then a ' c and b ' c. By (ii),
c ' b, and then by (iii), a ' b. If d is any member of [b], then b ' d. From
(iii), a ' b and b ' d together imply a ' d. Thus [b] ⊆ [a]. Reversing the
roles of a and b, we see that [a] ⊆ [b] also, whence [a] = [b]. This proves the
first conclusion. The second conclusion follows from (i), which ensures that a is
in [a], hence that every member of S lies in some equivalence class.           §

   EXAMPLE. With the equivalence relation on Z that a ' b if a − b is even,
there are two equivalence classes—the subset of even integers and the subset of
odd integers.

    The first two examples of equivalence relations in this book arise in Section
II.3. The first example, which is captured in the definition of square matrices
that are “similar,” yields equivalence classes exactly as above. A square matrix
A is similar to a square matrix B if there is a matrix C with B = C −1 AC. The
text does not mention in Chapter II that similarity is an equivalence relation, but
it is routine to check that it is reflexive, symmetric, and transitive. The second
example is a relation “is isomorphic to” and implicitly is defined on the class of all
vector spaces. This class is not a set, and Section A1 of this appendix suggested
avoiding using classes that are not sets in order to avoid the logical paradoxes
mentioned at the beginning of the appendix. There is not much problem with
using general classes in this particular situation, but there is a simple approach
                                       A3. Real Numbers                                      601

in this situation for eliminating classes that are not sets and thereby following
the suggestion of Section A1 without making an exception. The approach is to
work with any subclass of vector spaces that is a set. The equivalence relation
is well defined on the set of vector spaces in question, and the proposition yields
equivalence classes within that set. This set can be an arbitrary subclass of the
class of all vector spaces that happens to be a set, and the practical effect is the
same as if the equivalence relation had been defined on the class of all vector
spaces.


                                    A3. Real Numbers

Real numbers are taken as known, as are the rational numbers from which they
are constructed. It will be useful, however, to review the constructions of both
these number systems so as to be able to discuss the solvability of polynomial
equations better.
     We take the set Z of integers as given, along with its ordering and its operations
of addition, subtraction, and multiplication. The set Q of rational numbers is
constructed rigorously from Z as follows. We start from the set of ordered pairs
(a, b) of integers such that b 6= 0. The idea is that (a, b) is to correspond to
a/b and that we want (na, nb) to correspond to the same a/b if n is any nonzero
integer. Thus we say that two such pairs have (a, b) ∼ (c, d) if ad = bc.
This relation is evidently reflexive and symmetric, and it will be an equivalence
relation if it is transitive. If (a, b) ∼ (c, d) and (c, d) ∼ (e, f ), then ad = bc
and c f = de. So ad f = bc f = bde. Since d 6= 0, a f = be and ∼ is transitive.
     From Section A2 the set of such pairs is partitioned into equivalence classes
by means of ∼. Each equivalence class is called a rational number. To de-
fine the arithmetic operations on rational numbers, we first define operations on
pairs, and then we check that the operations respect the partitioning into classes.
For addition, the definition is (a, b) + (c, d) = (ad + bc, bd). What needs
checking is that if (a, b) ∼ (a 0 , b0 ) and (c, d) ∼ (c0 , d 0 ), then (ad + bc, bd) ∼
(a 0 d 0 + b0 c0 , b0 d 0 ). This is a routine matter: (ad + bc)(b0 d 0 ) = ab0 dd 0 + bb0 cd 0 =
a 0 bdd 0 + bb0 c0 d = (a 0 d 0 + b0 c0 )bd, and thus addition of rational numbers is
well defined. The operations on pairs for negative, multiplication, and reciprocal
are −(a, b) = (−a, b), (a, b)(c, d) = (ac, bd), and (a, b)−1 = (b, a), and we
readily check that these define corresponding operations on rational numbers.
Finally one derives the familiar associative, commutative, and distributive laws
for these operations on Q.
     The above construction is repeated, with more details, in the more general
construction of “fields of fractions” in Chapter VIII.
     Inequalities on rational numbers are defined from inequalities on integers, tak-
602                                   Appendix

ing into account that an inequality between integers is preserved when multiplied
by a positive integer. Each rational number has a representative pair (a, b) with
b > 0 because any pair can always be replaced by the pair of negatives. Thus
let (a, b) and (c, d) be given with b > 0 and d > 0. We say that (a, b) ≤ (c, d)
if ad ≤ bc. One readily checks that this ordering respects equivalence classes
and leads to the usual properties of the ordering on Q. The positive rationals are
those greater than 0, and the negative rationals are those less than 0.
    The formal definition is that a real number is a cut of rational numbers, i.e.,
a subset of rational numbers that is neither Q nor the empty set, has no largest
element, and contains all rational numbers less than any rational that it contains.
The set of cuts, i.e., the set of real numbers, is denoted by R. The idea of the
construction is as follows: Each rational number q determines a cut q ∗ , namely
the set of all rationals less than q. Under the identification of Q with a subset of
R, the cut defining a real number consists of all rational numbers less than the
given real number.
    The set of cuts gets a natural ordering, given by inclusion. In place of ⊆, we
write ≤. For any two cuts r and s, we have r ≤ s or s ≤ r, and if both occur,
then r = s. We can then define <, ∏, and > in the expected way. The positive
cuts r are those with 0∗ < r, and the negative cuts are those with r < 0∗ .
    Once cuts and their ordering are in place, one can go about defining the usual
operations of arithmetic and proving that R with these operations satisfies the
familiar associative, commutative, and distributive laws, and that these interact
with inequalities in the usual ways. The definitions of addition and subtraction are
easy: the sum or difference of two cuts is simply the set of sums or differences of
the rationals from the respective cuts. For multiplication and reciprocals one has
to take signs into account. For example the product of two positive cuts consists
of all products of positive rationals from the two cuts, as well as 0 and all negative
rationals. After these definitions and the proofs of the usual arithmetic operations
are complete, it is customary to write 0 and 1 in place of 0∗ and 1∗ .
    This much allows us to define n th roots. The following proposition gives the
precise details.

   Proposition. If r is a positive real number and n is a positive integer, then
there exists a unique positive real number s such that s n = r.
  REMARK. In the terminology and notation introduced in Section I.3, the
polynomial X n − r in R[X] has a unique positive root if r is positive in R.
   SKETCH OF PROOF. Let s consist of all positive rationals q such that q n < r,
together with all rationals ≤ 0. One checks that s is a cut and that s n = r. This
proves existence. For uniqueness any positive cut s 0 with (s 0 )n = r must contain
exactly the same rationals and hence must equal s.                               §
                                         A3. Real Numbers                                         603

     To make efficient use of cuts in connection with arithmetic and algebra, one
needs to develop a certain amount of real-variable theory. This theory will not
be developed in any detail here; let us be content with a sketch, giving a proof of
the one specific result that we shall need.5
     The first step in the process is to observe that any nonempty subset of reals
with an upper bound has a least upper bound (the supremum, written as sup).
This is proved by taking the union of the cuts for each of the given real numbers
and showing that the result is a cut. Similarly any nonempty subset of reals with
a lower bound has a greatest lower bound (the infimum, written as inf). This
property follows by applying the least-upper-bound property to the negatives of
the given reals and then taking the negative of the resulting least upper bound.
     Meanwhile, we can introduce sequences of real numbers and convergence
of sequences in the usual way. In terms of convergence, the key property of
sequences of real numbers is given by the Bolzano–Weierstrass Theorem: any
bounded sequence has a convergent subsequence. In fact, if the given bounded
sequence is {sn }, it can be shown that there is a subsequence convergent to the
greatest lower bound over m of the least upper bound for k ∏ m of the numbers
sk .
     Next one introduces continuity of functions in the usual way. The Bolzano–
Weierstrass Theorem may readily be used to prove that any continuous real-valued
function on a closed bounded interval takes on its maximum and minimum values.
With a little more effort the Bolzano–Weierstrass Theorem may be used also
to show that any continuous real-valued function on a closed bounded interval
is uniformly continuous. That brings us to the theorem that we shall use in
developing basic algebra.

   Theorem (Intermediate Value Theorem). Let a < b be real numbers, and let
f : [a, b] → R be continuous. Then f , in the interval [a, b], takes on all values
between f (a) and f (b).
   PROOF. Let f (a) = α and f (b) = β, and let ∞ be between α and β. We may
assume that ∞ is in fact strictly between α and β. Possibly by replacing f by
− f , we may assume that also α < β. Let

     A = {x ∈ [a, b] | f (x) ≤ ∞ }              and        B = {x ∈ [a, b] | f (x) ∏ ∞ }.

These sets are nonempty since a is in A and b is in B, and f is bounded since
any continuous function on a closed bounded interval takes on finite maximum
and minimum values. Thus the numbers ∞1 = sup { f (x) | x ∈ A} and ∞2 =
inf { f (x) | x ∈ B} are well defined and have ∞1 ≤ ∞ ≤ ∞2 .
   5 Details of the omitted steps may be found, for example, in Section I.1 of the author’s book Basic

Real Analysis.
604                                       Appendix

   If ∞1 = ∞ , then we can find a sequence {xn } in A such that f (xn ) converges to ∞ .
Using the Bolzano–Weierstrass Theorem, we can find a convergent subsequence
{xn k } of {xn }, say with limit x0 . By continuity of f , { f (xn k )} converges to f (x0 ).
Then f (x0 ) = ∞1 = ∞ , and we are done. Arguing by contradiction, we may
therefore assume that ∞1 < ∞ . Similarly we may assume that ∞ < ∞2 , but we do
not need to do so.
   Let ≤ = ∞2 − ∞1 , and choose, since the continuous function f is necessarily
uniformly continuous, δ > 0 such that |x1 − x2 | < δ implies | f (x1 ) − f (x2 )| <
≤ whenever x1 and x2 both lie in [a, b]. Then choose an integer n such that
2−n (b − a) < δ, and consider the value of f at the points pk = a + k2−n (b − a)
for 0 ≤ k ≤ 2n . Since pk+1 − pk = 2−n (b−a) < δ, we have | f ( pk+1 )− f ( pk )| <
≤ = ∞2 − ∞1 . Consequently if f ( pk ) ≤ ∞1 , then
          f ( pk+1 ) ≤ f ( pk ) + | f ( pk+1 ) − f ( pk )| < ∞1 + (∞2 − ∞1 ) = ∞2 ,
and hence f ( pk+1 ) ≤ ∞1 . Now f ( p0 ) = f (a) = α ≤ ∞1 . Thus induction shows
that f ( pk ) ≤ ∞1 for all k ≤ 2n . However, for k = 2n , we have p2n = b. Hence
 f (b) = β ∏ ∞ > ∞1 , and we have arrived at a contradiction.                 §



                               A4. Complex Numbers

Complex numbers are taken as known, and this section reviews their notation and
basic properties.
    Briefly, the system C of complex numbers is a two-dimensional vector space
over R with a distinguished basis {1, i} and a multiplication defined initially by
11 = 1, 1i = i1 = i, and ii = −1. Elements may then be written as a + bi or
a + ib with a and b in R; here a is an abbreviation for a1. The multiplication is
extended to all of C so that the distributive laws hold, i.e., so that (a + bi)(c + di)
can be expanded in the expected way. The multiplication is associative and
                                                  ° a identity,
commutative, the element 1 acts as a multiplicative                  ¢ every nonzero
                                                                     and
                                                                 b
element has a multiplicative inverse: (a + bi) a 2 +b  2 − i a 2 +b2   = 1.
    Complex conjugation is indicated by a bar: the conjugate of a + bi is a − bi
if a and b are real, and we write a + bi = a − bi. Then we have z + w = z̄ + w̄,
r z = r z̄ if r is real, and zw = z̄ w̄.
    The real and imaginary parts of z = a + bi are Re z = a and Im z = b.
These may be computed as Re z = 12 (z + z̄) and Im z = − 2i (z − z̄).
                                                                         p
    The absolute value function of z = a + bi is given by |z| = a 2 + b2 , and
this satisfies |z|2 = z z̄. It has the simple properties that |z̄| = |z|, | Re z| ≤ |z|,
and | Im z| ≤ |z|. In addition, it satisfies
                                 |zw| = |z||w|
because             |zw|2 = zwzw = zwz̄ w̄ = z z̄ww̄ = |z|2 |w|2 ,
                        A5. Partial Orderings and Zorn’s Lemma                 605

and it satisfies the triangle inequality

                               |z + w| ≤ |z| + |w|

because       |z + w|2 = (z + w)(z + w) = z z̄ + z w̄ + wz̄ + ww̄
                       = |z|2 + 2 Re(z w̄) + |w|2 ≤ |z|2 + 2|z w̄| + |w|2
                       = |z|2 + 2|z||w| + |w|2 = (|z| + |w|)2 .


                 A5. Partial Orderings and Zorn’s Lemma

A partial ordering on a set S is a relation between S and itself, i.e., a subset of
S × S, satisfying two properties. We define the expression a ≤ b to mean that the
ordered pair (a, b) is a member of the relation, and we say that “≤” is the partial
ordering. The properties are
      (i) a ≤ a for all a in S, i.e., ≤ is reflexive,
     (ii) a ≤ b and b ≤ c together imply a ≤ c whenever a, b, and c are in S, i.e.,
          ≤ is transitive.
   An example of such an S is any set of subsets of a set X, with ≤ taken to
be inclusion ⊆. This particular partial ordering has a third property of interest,
namely
   (iii) a ≤ b and b ≤ a with a and b in S imply a = b.
However, the validity of (iii) has no bearing on Zorn’s Lemma below. A partial
ordering is said to be a total ordering or simple ordering if (iii) holds and also
    (iv) any a and b in S have a ≤ b or b ≤ a or both.
For the sake of a result to be proved at the end of the section, let us interpolate
one further definition: a totally ordered set is said to be well ordered if every
nonempty subset has a least element, i.e., if each nonempty subset contains an
element a such that a ≤ b for all b in the subset.
   A chain in a partially ordered set S is a totally ordered subset. An upper
bound for a chain T is an element u in S such that c ≤ u for all c in T . A
maximal element in S is an element m such that whenever m ≤ a for some a in
S, then a ≤ m. (If (iii) holds, we can conclude in this case that m = a.)

   Zorn’s Lemma. If S is a nonempty partially ordered set in which every chain
has an upper bound, then S has a maximal element.
   REMARKS. Zorn’s Lemma will be proved below using the Axiom of Choice,
which was stated in Section A1. It is an easy exercise to see, conversely,
that Zorn’s Lemma implies the Axiom of Choice. It is customary with many
606                                        Appendix

mathematical writers to mention Zorn’s Lemma each time it is invoked, even
though most writers nowadays do not ordinarily acknowledge uses of the Axiom
of Choice. Before coming to the proof, we give an example of how Zorn’s Lemma
is used. This example uses vector spaces and is expanded upon in Section II.9.

   EXAMPLE. Zorn’s Lemma gives a quick proof that any real vector space V
has a basis. In fact, let S be the set of all linearly independent subsets of V , and
order S by inclusion upward as in the example above of a partial ordering. The
set S is nonempty because ∅ is a linearly independent subset of V . Let T be a
chain in S, and let u be the union of the members of T . If t is in T , we certainly
have t ⊆ u. Let us see that u is linearly independent. For u to be dependent
would mean that there are vectors x1 , . . . , xn in u with r1 x1 + · · · + rn xn = 0 for
some system of real numbers not all 0. Let x j be in the member t j of the chain
T . Since t1 ⊆ t2 or t2 ⊆ t1 , x1 and x2 are both in t1 or both in t2 . To keep the
notation neutral, say they are both in t20 . Since t20 ⊆ t3 or t3 ⊆ t20 , all of x1 , x2 , x3
are in t20 or they are all in t3 . Say they are both in t30 . Continuing in this way,
we arrive at one of the sets t1 , . . . , tn , say tn0 , such that all of x1 , . . . , xn are all
in tn0 . The members of tn0 are linearly independent by assumption, and we obtain
the contradiction r1 = · · · = rn = 0. We conclude that the chain T has an upper
bound in S. By Zorn’s Lemma, S has a maximal element, say m. If m is not
a basis, it fails to span. If a vector x is not in its span, it is routine to see that
m ∪ {x} is linearly independent and properly contains m, in contradiction to the
maximality of m. We conclude that m is a basis.

    We now begin the proof of Zorn’s Lemma. If T is a chain in a partially ordered
set S, then an upper bound u 0 for T is a least upper bound for T if u 0 ≤ u for all
upper bounds of T . If (iii) holds in S, then there can be at most one least upper
bound for T . In fact, if u 0 and u 00 are least upper bounds, then u 0 ≤ u 00 since
u 0 is a least upper bound, and u 00 ≤ u 0 since u 00 is a least upper bound; by (iii),
u 0 = u 00 . The proof follows that in Dunford–Schwartz’s Linear Operators I.

    Lemma. Let X be a nonempty partially ordered set such that (iii) holds, and
write ≤ for the partial ordering. Suppose that X has the additional property that
each nonempty chain in X has a least upper bound in X. If f : X → X is a
function such that x ≤ f (x) for all x in X, then there exists an x0 in X with
 f (x0 ) = x0 .
   PROOF. A nonempty subset E of X will be called admissible for purposes of
this proof if f (E) ⊆ E and if the least upper bound of each nonempty chain in
E, which exists in X by assumption, actually lies in E. By assumption, X is an
admissible subset of X. If x is in X, then the intersection of admissible subsets of
X containing x is admissible. Let A x be the intersection of all admissible subsets
                          A5. Partial Orderings and Zorn’s Lemma                      607

of X containing x. This is admissible, and since the set of all y in X with x ≤ y
is admissible and contains x, it follows that x ≤ y for all y ∈ A x . By hypothesis,
X is nonempty. Fix an element a in X, and let A = Aa . The main step will be to
prove that A is a chain.
    To do so, consider the subset C of members x of A with the property that there
is a nonempty chain C x in A containing a and x such that
      • a ≤ y ≤ x for all y in C x ,
      • f (C x − {x}) ⊆ C x , and
      • the least upper bound of any nonempty subchain of C x is in C x .
The element a is in C because we can take Ca = {a}. If x is in C, so that C x
exists, let us use the bulleted properties to see that
                                     A = Ax ∪ Cx .                                    (∗)
We have A ⊇ C x by definition; also A ∩ A x is an admissible set containing x and
hence containing A, and thus A ⊇ A x . Therefore A ⊇ A x ∪ C x . For the reverse
inclusion it is enough to prove that A x ∪C x is an admissible subset of X containing
a. The element a is in C x , and thus a is in A x ∪C x . For the admissibility we have to
show that f (A x ∪ C x ) ⊆ A x ∪ C x and that the least upper bound of any nonempty
chain in A x ∪ C x lies in A x ∪ C x . Since x lies in A x , A x ∪ C x = A x ∪ (C x − {x})
and f (A x ∪ C x ) = f (A x ) ∪ f (C x − {x}) ⊆ A x ∪ C x , the inclusion following
from the admissibility of A and the second bulleted property of C x .
    To complete the proof of (∗), take a nonempty chain in A x ∪ C x , and let u be
its least upper bound in X; it is enough to show that u is in A x ∪ C x . The element
u is necessarily in A since A is admissible. Observe that
           y≤x      and    x≤z         whenever y is in C x and z is in A x .        (∗∗)
If the chain has at least one member in A x , then (∗∗) implies that x ≤ u, and
hence the set of members of the chain that lie in A x forms a nonempty chain in
A x with least upper bound u. Since A x is admissible, u is in A x . Otherwise the
chain has all its members in C x , and then u is in C x by the third bulleted property
of C x .
    This completes the proof of (∗). Let us now prove that if C x and C x 0 exist with
x ≤ x 0 and x 6= x 0 , then
                                     Cx ⊆ Cx 0 .                                   (†)
In fact, application of (∗) to x 0 gives A = A x 0 ∪ C x 0 . Intersecting both sides with
C x shows that C x = (C x ∩ A x 0 ) ∪ (C x ∩ C x 0 ). On the right side, the first member
is empty by (∗∗), and thus C x = C x ∩ C x 0 . This proves (†).
    Let C be the set of all members x of A for which C x exists. We have seen that
a is in C. If we apply (∗) and (∗∗) first to a member x of C and then to a member
x 0 of C, we see that either x ≤ x 0 or x 0 ≤ x. That is, C is a chain.
608                                           Appendix

    Let us see that f (C) ⊆ C. If x is in C, then the set D = C x ∪ { f (x)} certainly
has a as a member. The second bulleted property of C x shows that f carries
C x − {x} into D, and also f carries x into D. Thus f carries D − { f (x)} into
D, and D satisfies the second bulleted property of C f (x) . If {xα } is a chain in D
with least upper bound u, there are two possibilities. Either u is f (x), which is
in D by construction, or u is in C, which contains the least upper bound of any
nonempty chain in it. Thus u is in D, D satisfies the third bulleted property of
C f (x) , and C f (x) exists. In other words, f (x) is in C, and f (C) ⊆ C.
    Finally let us see that the least upper bound u of an arbitrary chain {xα } in C,
which exists in X by assumption, is a member of C. If xα = u for some α, then
Cu = C xα exists, and uSis in C. So assume that xα 6= u for all α. Our candidate
for Cu will be D = ( α C xα ) ∪ {u}. This certainly contains a. We check that
D satisfies the second bulleted property of Cu . For each α, we can find a β with
xα ≤ xβ and xα 6= xβ , since u is the least upper bound of all the x’s. Then (†)
gives C xα ⊆ C xβ − {xβ }, and f (C xα ) ⊆ f (C xβ − {xβ }) ⊆ C xβ ⊆ D. Taking the
union over α shows that D satisfies the second bulleted property of Cu .
    To see that D satisfies the third bulleted property of Cu , let v be the least upper
bound in A of a chain {yβ } in Cu . If v 6= u, then v cannot be an upper bound of
{xα }. So we can choose some xα0 such that v ≤ xα0 . Each yβ is ≤ v, and thus
each yβ is ≤ xα0 . Referring to (∗), we see that all yβ ’s lie in C xα0 . By the third
bulleted property of C xα0 , v is in C xα0 . Thus v is in D, and D satisfies the third
bulleted property of Cu . Consequently the least upper bound u of an arbitrary
chain in C lies in C.
    In short, C is an admissible set containing a, and it also is a chain. Since A is
a minimal admissible set containing a, C = A and also A is a chain. Let u be the
least upper bound of A. We have seen that f (A) ⊆ A, and thus f (u) ≤ u. On
the other hand, u ≤ f (u) by the defining property of f . Therefore f (u) = u,
and the proof is complete.

    PROOF OF ZORN’S LEMMA. Let S be a partially ordered set, with partial
ordering ≤, in which every chain has an upper bound. Let X be the partially
ordered system, ordered by inclusion upward ⊆, of nonempty chains6 in S. The
                                                                         property
partially ordered system X, being given by ordinary inclusion, satisfies S
(iii). A nonempty chain C in X is a nested system of chains cα of S, and α cα is
a chain in S that is a least upper bound for C. The lemma is therefore applicable
to any function f : X → X such that c ⊆ f (c) for all c in X. We use the lemma
to produce a maximal chain in X.
    Arguing by contradiction, suppose that no chain within S is maximal under
   6 Here   a chain is simply a certain kind of subset of S, and no element of S can occur more than
once in it even if (iii) fails for the partial ordering. Thus if S = {x, y} with x ≤ y and y ≤ x, then
{x, y} is in X and in fact is maximal in X.
                          A5. Partial Orderings and Zorn’s Lemma                       609

inclusion. For each nonempty chain c within S, let f (c) be a chain with c ⊆ f (c)
and c 6= f (c). (This choice of f (c) for each c is where we use the Axiom of
Choice.) The result is a function f : X → X of the required kind, the lemma
says that f (c) = c for some c in X, and we arrive at a contradiction. We conclude
that there is some maximal chain c0 within S.
   By assumption in Zorn’s Lemma, every nonempty chain within S has an upper
bound. Let u 0 be an upper bound for the maximal chain c0 . If u is a member of S
with u 0 ≤ u, then c0 ∪ {u} is a chain and maximality implies that c0 ∪ {u} = c0 .
Therefore u is in c0 , and u ≤ u 0 . This is the condition that u 0 is a maximal
element of S.                                                                   §

   Corollary (Zermelo’s Well-Ordering Theorem). Every set has a well ordering.
   PROOF. Let S be a set, and let E be the family of all pairs (E, ≤ E ) such that E
is a subset of S and ≤ E is a well ordering of E. The family E is nonempty since
(∅, ∅) is a member of it. We partially order E by a notion of “inclusion as an
initial segment,” saying that (E, ≤ E ) ≤ (F, ≤ F ) if
     (i) E ⊆ F,
    (ii) a and b in E with a ≤ E b implies a ≤ F b,
   (iii) a in E and b in F but not E together imply a ≤ F b.
In preparation for applying Zorn’s Lemma, let C = {(ES           α , ≤α )} be a chain in E,
with the α’s running through some set I . Define E 0 = α E α and define ≤0 as
follows: If e1 and e2 are in E 0 , let e1 be in E α1 with α1 in I , and let e2 be in E α2
with α2 in I . Since C is a chain, we may assume without loss of generality that
(E α1 , ≤α1 ) ≤ (E α2 , ≤α2 ), so that E α1 ⊆ E α2 in particular. Then e1 and e2 are both
in E α2 and we define e1 ≤0 e2 if e1 ≤α2 e2 , and e2 ≤0 e1 if e2 ≤α2 e1 . Because of
(i) and (ii) above, the result is well defined independently of the choice of α1 and
α2 . Similar reasoning shows that ≤0 is a total ordering of E 0 . If we can prove
that ≤0 is a well ordering, then (E 0 , ≤0 ) is evidently an upper bound in E for the
chain C, and Zorn’s Lemma is applicable.
    Now suppose that F is a nonempty subset of E 0 . Pick an element of F, and
let E α0 be a set in the chain that contains it. Since (E α0 , ≤α0 ) is well ordered and
F ∩ E α0 is nonempty, F ∩ E α0 contains a least element f 0 relative to ≤α0 . We show
that f 0 ≤0 f for all f in F. In fact, if f is given, there are two possibilities. One
is that f is in E α0 ; in this case, the consistency of ≤0 with ≤α0 forces f 0 ≤0 f .
The other is that f is not in E α0 but is in some E α1 . Since C is a chain and
E α1 ⊆ E α0 fails, we must have (E α0 , ≤α0 ) ≤ (E α1 , ≤α1 ). Then f is in E α1 but
not E α0 , and property (iii) above says that f 0 ≤α1 f . By the consistency of the
orderings, f 0 ≤0 f . Hence f 0 is a least element in F, and E 0 is well ordered.
    Application of Zorn’s Lemma produces a maximal element (E, ≤ E ) of E. If
E were a proper subset of S, we could adjoin to E a member s of S not in E and
610                                          Appendix

define every element e of E to be ≤ s. The result would contradict maximality.
Therefore E = S, and S has been well ordered.                               §


                                        A6. Cardinality

Two sets A and B are said to have the same cardinality, written card A = card B,
if there exists a one-one function from A onto B. On any set A of sets, “having the
same cardinality” is plainly an equivalence relation and therefore partitions A into
disjoint equivalence classes, the sets in each class having the same cardinality. The
question of what constitutes cardinality (or a “cardinal number”) in its own right
is one that is addressed in set theory but that we do not need to address carefully
here; the idea is that each equivalence class under “having the same cardinality”
has a distinguished representative, and the cardinal number is defined to be that
representative. We write card A for the cardinal number of a set A.
    Having addressed equality, we now introduce a partial ordering, saying that
card A ≤ card B if there is a one-one function from A into B. The first result below
is that card A ≤ card B and card B ≤ card A together imply card A = card B.

   Proposition (Schroeder–Bernstein Theorem). If A and B are sets such that
there exist one-one functions f : A → B and g : B → A, then A and B have
the same cardinality.
   PROOF. Define the function g −1 : image g → A by g −1 (g(a)) = a; this
definition makes sense since g is one-one. Write (g ◦ f )(n) for the composition
of g ◦ f with itself n times, and define ( f ◦ g)(n) similarly. Define subsets An
and An0 of A and subsets Bn and Bn0 for n ∏ 0 by

               An = image((g ◦ f )(n) ) − image((g ◦ f )(n) ◦ g),
               A0n = image((g ◦ f )(n) ◦ g) − image((g ◦ f )(n+1) ),
               Bn = image(( f ◦ g)(n) ) − image(( f ◦ g)(n) ◦ f ),
               Bn0 = image(( f ◦ g)(n) ◦ f ) − image(( f ◦ g)(n+1) ),

and let
             ∞
             T                                                 ∞
                                                               T
      A∞ =         image((g ◦ f )(n) )        and       B∞ =         image(( f ◦ g)(n) ).
             n=0                                               n=0

Then we have
                     ∞
                     S           ∞
                                 S                                    ∞
                                                                      S            ∞
                                                                                   S
      A = A∞ ∪            An ∪         A0n    and       B = B∞ ∪            Bn ∪         Bn0 ,
                    n=0          n=0                                  n=0          n=0
                                       A6. Cardinality                              611

with both unions disjoint.
   Let us prove that f carries An one-one onto Bn0 . If a is in An , then a =
(g ◦ f )(n) (x) for some x ∈ A and a is not of the form (g ◦ f )(n) (g(y)) with
y ∈ B. Applying f , we obtain f (a) = ( f ◦ ((g ◦ f )(n) )(x) = ( f ◦ g)(n) ( f (x)),
so that f (a) is in the image of (( f ◦ g)(n) ◦ f ). Meanwhile, if f (a) is in the
image of ( f ◦ g)(n+1) , then f (a) = ( f ◦ g)(n+1) (y) = f ((g ◦ f )(n) (g(y))) for
some y ∈ B. Since f is one-one, we can cancel the f on the outside and obtain
a = (g ◦ f )(n) (g(y)), in contradiction to the fact that a is in An . Thus f carries
An into Bn0 , and it is certainly one-one. To see that f (An ) contains all of Bn0 , let
b ∈ Bn0 be given. Then b = ( f ◦ g)(n) ( f (x)) for some x ∈ A and b is not of the
form ( f ◦ g)(n+1) (y) with y ∈ B. Hence b = f ((g ◦ f )(n) (x)), i.e., b = f (a)
with a = (g ◦ f )(n) (x). If this element a were in the image of (g ◦ f )(n) ◦ g,
we could write a = (g ◦ f )(n) (g(y)) for some y ∈ B, and then we would have
b = f (a) = f ((g ◦ f )(n) (g(y))) = ( f ◦ g)(n+1) (y), contradiction. Thus a is in
An , and f carries An one-one onto Bn0 .
   Similarly g carries Bn one-one onto A0n . Since A0n is in the image of g, we can
apply g −1 to it and see that g −1 carries A0n one-one onto Bn .
   The same kind of reasoning as above shows that f carries A∞ one-one onto
B∞ . In summary, f carries each An one-one onto Bn0 and carries A∞ one-one
onto B∞ , while g −1 carries each A0n one-one onto Bn . Then the function
                            Ω
                                f          on A∞ and each An ,
                       h=
                                g −1       on each A0n ,

carries A one-one onto B.                                                            §

   Next we show that any two sets A and B have comparable cardinalities in the
sense that either card A ≤ card B or card B ≤ card A.
   Proposition. If A and B are two sets, then either there is a one-one function
from A into B or there is a one-one function from B into A.
   PROOF. Consider the set S of all one-one functions f : E → B with E ⊆ A,
the empty function with E = ∅ being one such. Each such function is a certain
subset of A× B. If we order S by inclusion upward, then the union of the members
of any chain is an upper bound for the chain. By Zorn’s Lemma let G : E 0 → B
be a maximal one-one function of this kind, and let F0 be the image of G. If
E 0 = A, then G is a one-one function from A into B. If F0 = B, then G −1
is a one-one function from B into A. If neither of these things happens, then
there exist x0 ∈ A − E 0 and y0 in B − F0 , and the function G  e equal to G on
                e
E 0 and having G(x0 ) = y0 extends G and is still one-one; thus it contradicts the
maximality of G.                                                                §
612                                   Appendix

   Corollary. If E is an infinite set, then E has a countably infinite subset.
   PROOF. The proposition shows that either there is a one-one function from the
set of positive integers into E, in which case we are done, or there is a one-one
function from E into the set of positive integers. In the latter case the image cannot
be finite since E is assumed infinite. Then the image must be an infinite subset
of the positive integers. This set can be enumerated and is therefore countably
infinite. Thus E is countably infinite.                                             §
   Cantor’s proof that there exist uncountable sets, done with a diagonal argument,
in fact showed how to start from any set A and construct a set with strictly larger
cardinality.

   Proposition (Cantor). If A is a set and 2 A denotes the set of all subsets of A,
then card 2 A is strictly larger than card A.
   PROOF. The map x 7→ {x} is a one-one function from A into 2 A . If we are
given a one-one function F : A → 2 A , let E be the set of all x in A such that x
is not in F(x). If we define E = F(x0 ), then x0 ∈ E implies x0 ∈  / F(x0 ) = E,
while x0 ∈/ E implies x ∈ F(x0 ) = E. We have a contradiction in any case, and
hence E cannot be of the form F(x0 ). We conclude that F cannot be onto 2 A . §

   Proposition. If E is an infinite set, then E is the disjoint union of sets that are
each countably infinite.

             S S be the set ofS
    PROOF. Let                  all disjoint unions of countably infinite subsets of
E. If A = α Aα and B = β Bβ are members of S, say that A ≤ B if each
Aα is some Bβ . The result is a partial ordering on S. If U is a chain in S, then
the collection C of all countably infinite sets that are Uα ’s in some member of U
is a collection of countably infinite subsets of E that contains each member of U.
If Uα and Uβ are distinct members of C, then Uα and Uβ must both be in some
member of U and hence must be disjoint. Thus C is an upper bound for U. Also,
the empty union is a member of S. By Zorn’s Lemma, S has a maximal element
M. Let F be the union of the members of M. If E − F were to be infinite, then
the corollary above would show that E − F has a countably infinite subset Z ,
and M ∪ {Z } would contradict the maximality of M. Thus E − F is finite. Since
E is infinite, the corollary shows that E contains at least one countably infinite
subset. Thus M has some member T . The set T 0 = T ∪ (E − F) is countably
infinite, and (M − {T }) ∪ T 0 is the required decomposition of E as the disjoint
union of countably infinite sets.                                                 §

   Corollary. Let S and E be nonempty sets with S infinite, and suppose that to
each element
     S       s of S is associated a countable subset E x of E in such a way that
E = s∈S E s . Then card E ≤ card S.
                                   A6. Cardinality                              613

   PROOF. The proposition allows us to write S as the Sdisjoint union of countably
infinite sets. If U is one of these sets, then EU = s∈U E s is countable, being
the countable union of countable sets. Therefore there exists a function from U
onto SEU . The union of these functions, as U varies, yields a function f from S
onto EU = E. Applying the Axiom of Choice, we can select, for each e ∈ E,
an element s ∈ f −1 ({e}) and call it g(e). The result is a one-one function g from
E into S, and consequently card E ≤ card S.                                       §

    Addition is well defined for cardinals: the sum of two cardinal numbers is
defined to be the cardinality of the disjoint union of the two sets in question. If
at least one of the two cardinals is infinite, the sum equals the larger of the two,
as an immediate consequence of the above corollary.
         HINTS FOR SOLUTIONS OF PROBLEMS




                                       Chapter I

   1. 582.
   2. The Euclidean algorithm gives 11 = 1 · 7 + 4, 7 = 1 · 4 + 3, 4 = 1 · 3 + 1,
3 = 3 · 1 + 0. So the GCD is 1. Reversing the steps gives 1 = 4 − 1 · 3 =
(11 − 1 · 7) − 1 · (7 − 1 · 4) = (11 − 1 · 7) − 1 · (7 − 1 · (11 − 1 · 7)) = 2 · 11 − 3 · 7.
So (x, y) = (2, −3) is a solution in (a). For (b), the difference of any two solutions
solves 11x + 7y = 0, and the solutions of this are of the form (x, y) = n(7, −11).
   3. Let dn = GCD(a1 , . . . , an ). The sequence dn is a monotone decreasing
sequence of positive integers, and it must eventually be constant. This eventual
constant value is d, and thus dn = d for suitably large n.
   4. These n’s divide x + y − 2 and the sum of 2x − 3y − 3 and −2 times the
x + y − 2, hence x + y − 2 and −5y + 1. A necessary and sufficient condition for
−5y + 1 = na to be solvable for the pair (a, y) is that GCD(5, n) = 1 by Proposition
1.2c. Let us see that the answer to the problem is GCD(5, n) = 1.
   The n’s we seek must further divide 5(x +y−2) = 5x +5y−10 and −5y+1, hence
also the sum 5x−9, as well as −5y+1. If GCD(5, n) = 1, then 5x−9 = nb is solvable
for (b, x). With our solutions (a, y) and (b, x), we have 5x + 5y − 10 = n(b − a).
Since 5 divides the left side and GCD(5, n) = 1, 5 divides b − a. Write b − a = 5c.
Then x + y − 2 = nc and −5y + 1 = na, and we obtain 2x − 3y − 3 = n(2c + a).
  5. Q(x) = (X − 1)P(X) + (X 3 + x 2 + X + 1), P(X) = X (X 3 + x 2 + X + 1)
+(X 2 +1), X 3 +x 2 + X +1 = (X +1)(X 2 +1)+0. Hence the GCD is D(X) = X 2 +1.
For (b), we retrace the steps, letting R(X) = X 3 + X 2 + X + 1. We have D(X) =
P(X)− X R(X) = P(X)− X (Q(X)−(X −1)P(X)) = (X 2 − X +1)P(X)− X Q(X).
Thus A(X) = X 2 − X + 1 and B(X) = −X.
   6. The computation via the Euclidean algorithm, done within C[X], retains
real numbers as coefficients throughout. By Proposition 1.15a one GCD has real
coefficients. By Proposition 1.15c any GCD is a complex multiple of this polynomial
with real coefficients.
                                                        Q that P has leading coeffi-
    7. In (a), we may assume, without loss of generality,
cient 1, so that P(X) = X n +an−1 X n−1 +· · ·+a0 = j (X −z j )m j . Define Q(X) =
Q               mj
                                Q               mj =
                                                     Q
   j (X − z̄ j ) . Then Q(z̄) =   j (z̄ − z̄ j )       j (z − z j ) = P(z). Replacing z̄
by z gives Q(z) = P(z̄) = z̄ n + an−1 z̄ n−1 + · · · + a0 = z n + an−1 z n−1 + · · · + a0 .
                                            615
616                              Hints for Solutions of Problems

Since P has real coefficients, Q(z) = P(z) for all z. Then Q − P has every z as a
root
Q and in particularmj =
                        Q has more than       n roots. Hence it must be the 0 polynomial. So
                                       m j , and the result follows from unique factorization
   j  (X  −  z̄ j )       j (X  − z j )
(Theorem 1.17).
     In (b), the result of (a) shows Q  that we may factor Q any real polynomial
                                                                       °           in C[X] with  ¢
leading coefficient 1 in the form x j real (X − x j )m j z j nonreal (X − z j )n j (X − z̄ j )n j .
                         Q                       Q          °                           ¢n
The right side equals x j real (X − x j )m j z j nonreal X 2 − (z j + z̄ j )X + z j z̄ j j . Every
factor on the right side is in R[X], and the only way that the polynomial can be prime
in R[X] is if only one factor is present. Thus the polynomial has degree at most 2.
     8. For (a), let deg A = d and form the equation A( p/q) = 0. Multiply through
by q d in order to clear fractions. Every term in the equation except the leading term
has q as a factor, and thus q divides the leading term pd . Since GCD( p, q) = 1, no
prime can divide q. Thus q = ±1, and n = p/q is an integer. Forming the equation
A(n) = 0, we see that n is a factor of each term except possibly the constant term a0 .
Thus n divides a0 .
     For (b), we apply (a) to both polynomials. The only possible rational roots of
X 2 − 2 are ±1 and ±2, while the only possible rational roots of X 3 + X 2 + 1 are ±1.
Checking directly, we see that none of these possibilities is actually a root. By the
Factor Theorem, neither X 2 − 2 nor X 3 + X 2 + 1 has a first-degree factor in Q[X].
If a polynomial of degree ≤ 3 has a nontrivial factorization, then it has a first-degree
factor. We conclude that X 2 − 2 and X 3 + X 2 + 1 are prime.
     9. Computation gives GCD(8645, 10465) = 455. Therefore 8645/10465 equals
19/23 in lowest terms.
     10. Apart from the identity, the cycle structures are those of (1 2) with 6 represen-
tatives, (1 2 3) with 8 representatives, (1 2 3 4) with 6 representatives, and (1 2)(3 4)
with 3 representatives. This checks, since there are 4! = 24 permutations in all.
     11. Check that the function σ 7→ σ (1 2) is one-one from the set of permutations
of sign +1 onto the set of permutations of sign −1.
                    µ 1∂                        µ −11/3 ∂       µ 1∂
     12. (a) x3 −2 . (b) None. (c)                 10/3 + x 3 −2 .
                   1                              0                1
   13. By the definition of “step,” an interchange of two rows (type (i)) takes n steps,
and a multiplication of a row by a nonzero scalar (type (ii)) takes n steps. Also,
replacement of a row by the sum of it and a multiple of another row (type (iii)) takes
2n steps. We proceed through the row-reduction algorithm column by column. For
each of the n columns, we do possibly one operation of type (i) and then possibly an
operation of type (ii). This much requires ≤ 2n steps. Then we do at most n − 1
operations of type (iii), requiring ≤ 2n(n − 1) steps. Thus a single column is handled
in ≤ 2n(n − 1) + 2n = n 2 steps, and the entire row reduction requires ≤ 2n 3 steps.
                 ≥         ¥            ≥        ¥
                    −2 11                 −11 25
   14. A + B =        3 8
                            , and AB  =   −21 47
                                                  .
   15. We induct on n, the result being clear for n = 1. Taking into account the
fact that B commutes with A, we have (A + B)n = (A + B)(A + B)n−1 = (A +
                                              Chapter I                                             617
    P     °n−1¢ n−1−k k       Pn−1 °n−1¢ n−k k   P       °n−1¢ n−1−k k+1
B) n−1k=0   k   A       B =     k=0  k  A    B + n−1 k=0      A       B      =
Pn−1 °n−1¢ n−k k Pn °n−1¢ n−k k            n
                                              Pn−1 £°n−1¢ °kn−1¢§ n−k k
   k=0   k A      B + k=1 k−1 A B = A + k=1           k + k−1 A       B +B n .
                              Pn °n ¢ n−k k
In turn, the right side equals k=0 k A    B by the Pascal-triangle identity for
binomial coefficients.
              µ      ∂                    µ    ∂
                   110                                    010
    16. Write 0 1 1 as I + B, where B = 0 0 1 , and apply Problem 15. Since
         µ 0 0 1 ∂0 0 1                 000

B = 0 0 0 and B = 0, we obtain (I + B)n = I + n B + 12 n(n − 1)B 2 =
  2                     3

µ           0 0 0∂
  1 n 12 n(n−1)
  01       n      .
  00      1
   17. (AD)i j = Ai j d j and (D A)i j = di Ai j . Thus AD = D A if and only if di = d j
for all (i, j) for which Ai j 6= 0.
   18. E kl E pq = δlp E kq .
                     ≥ ¥
   19. Check that ac db times the asserted inverse is the identity. Then the matrix
                                                  ≥ ¥≥x ¥ ≥ p¥
actually is the inverse. Apply the inverse to ac db      y = q to obtain the value
    ≥x¥
for y .
                                      µ −2/3 −4/3 1 ∂                µ 1 −1 0 ∂
                                 −1                           −1
   20. (a) No inverse. (b) A = −2/3 11/3 −2 . (c) A = −2 3 −1 .
                                                1    −2    1                          2 −5    4
    21. No. If the algorithm is followed, then the row of 0’s persists throughout the
row reduction, at worst moving to a different row at various stages.
    22. If C = (AB)−1 , then ABC = I shows that BC is the inverse of A and
C AB = I shows that C A is the inverse of B.
    23. (I + A)(I − A + A2 − A3 + · · · + (−A)k−1 ) = I − (−A)k = I shows that
I − A + A2 − A3 + · · · + (−A)k−1 is an inverse.
    24. Let S be the set of positive integers, and let f (n) = n + 1. Take g(n) to be
n − 1 for n > 1 and g(1) = 1. Then g ◦ f is the identity. But f is not onto S, and g
is not one-one.                       ≥ ¥                   ≥ ¥
   25. Take A = ( 1 0 ) and B = 10 . Then B A = 10 00 . More generally, if
                        °c¢                ≥       ¥
                                             ca cb
A = ( a b ) and B = d , then B A = da           db
                                                    . If the upper right entry is 0, then
c = 0 or b = 0. But then one of the two diagonal entries must be 0, and hence B A
cannot be the identity.
   26. The set of common multiples is a nonempty set of positive integers because
ab is in it. Therefore it has a least element.
   27. This is a restatement of Corollary 1.7.
   28. Let a and b have prime factorizations a = p1k1 · · · prkr and b = p1l1 · · · prlr .
Problem 27 shows that any positive common multiple N of a and b is of the form
p1m 1 · · · prm r q1n 1 · · · qsn s with m j ∏ k j , m j ∏ l j , and n j ∏ 0, and certainly any positive
618                                          Hints for Solutions of Problems

integer of this form is a common multiple. The inequalities for m j are equivalent
with the condition m j ∏ max(k j , l j ). The smallest positive integer of this kind has
m j = max(k j , l j ) and n j = 0. This proves (a). In combination with the form of N ,
the formula for LCM(a, b) proves (b). Conclusion (c) follows from Corollary 1.8
and the identity k j + l j = min(k j , l j ) + max(k j , l j ).
                          k            k
     29. If a j = p11, j · · · pr r, j is a prime factorization of a j , then LCM(a1 , . . . , at ) =
 max1≤ j≤r {k1, j }           max1≤ j≤r {kr, j }
p1                    · · · pr                     , just as with Corollary 1.11.


                                                        Chapter II
                                                              ©                         ™
    1. The methods at the end of Section 2 lead to the basis ( 23 , 1, 0), (− 53 , 0, 1) for
                     ©            ™
(a) and to the basis (1, − 12 , 2) for (b).
    2. For 0 ≤ k < n, the two recursive formulas and one application of associativity
give v(k+1) +v (k+2) = (v(k) +vk+1 )+v (k+2) = v(k) +(vk+1 +v (k+2) ) = v(k) +v (k+1) ,
and (a) follows.
    For (b), we proceed by induction on n, the cases n ≤ 3 being handled by associa-
tivity. Suppose that the result holds for sums of fewer than n vectors, with n ∏ 4.
In a sum of n vectors, there is some outer plus sign, and the inductive hypothesis
means that the sum is of the form (v1 + · · · + vk ) + (vk+1 + · · · + vn ), the expressions
v1 + · · · + vk and vk+1 + · · · + vn being unambiguous. The inductive hypothesis
means that we have v1 + · · · + vk = v(k) and vk+1 + · · · + vn = v (k+1) , and hence
the expression we are studying is of the form v(k) + v (k+1) . Part (a) shows that this
is independent of k, and hence (b) follows.
    3. From Section I.4, σ is a product of transpositions, and hence it is enough to
prove the result for a transposition. When r +1 < s, iteration of the identity (r s) =
(r r + 1)(r + 1 s)(r r + 1) shows that any transposition is a product of trans-
positions of the form (r r + 1), and hence it is enough to prove the formula for
σ = (r r + 1). This case is just the commutative law, and the result follows.
                                               Ωµ 1 ∂ µ 0 ∂æ
    4. (a) {( 1 2 −1 ) , ( 0 0 1 )}; (b)            2 ,     1     ; (c) 2.
                                                                     0      −1
    5. If R is a reduced row-echelon form of A, then we know that R = E A, where E
is a product of invertible elementary matrices. Since A has rank one, R has a single
nonzero row r and is of the form e1r, where e1 is the first standard basis vector. Then
A = E −1 R = (E −1 e1 )r, and we can take c = E −1 e1 .
    6. In (a), let u 1 , . . . , u s be the rows of R having at least one of the first r entries
nonzero, and let u s+1 , . . . , u m be the other rows. For each i with 1 ≤ i ≤ s, the
first nonzero entry of u i corresponds to a corner variable and occurs in the j (i)th
position with j (i) ≤ r. The most general member of the row space of A is of the
form c1 u 1 + · · · + cm u m , and the j (i)th entry of this is ci . For this row vector to be
in the indicated span, we must have ci = 0 for i ≤ s.
                                                  Chapter II                                   619

    In (b), let R 0 be a second reduced row-echelon form, and let its nonzero rows be
v1 , . . . , vm . From part (a), it follows that the linear span of u s+1 , . . . , u m equals the
linear span of vs+1 , . . . , vm for each s. Moreover, the value of each j (i) has to be
the same for u i as for vi . Inducting downward, we prove that u i = vi for each i. For
i = m, this follows since the first nonzero entry is 1 for both u m and vm . Assuming
the result for s + 1, we write vs = cs u s + cs+1 u s+1 + · · · + cm u m . We have cs = 1
since the first nonzero entry of u s and vs is 1, and we have cs+i = 0 for i > 0 since
the j (s + i)th entry of this equality of row vectors is 0 = cs+i . Thus vs = u s , and
the induction is complete.
    7. Let E = {x1 , . .  . , x N }, and let f 1 , . . . , f n be a basis of U . Form the matrix
         f 1 (x1 ) ··· f 1 (x N )
A=        ..     ..       ..       . By assumption, A has row rank n. Therefore it has column
            .          .    .
         f n (x1 ) ··· f n (x N )
rank n, and there exist n linearly independent columns, say columns j1 , . . . , jn . Then
D = {x j1 , . . . , x jn }.
                                                                                ≥ ¥
                                                                                    I
   8. Let the listed basis be 0, and let 6 be the standard basis. Then 60               =
≥        ¥                             ≥ ¥        ≥ ¥          ≥ ¥
   3 −4                                  I         34            L
          , the inverse matrix is 06         =         , and 00 is the product
≥ −2 ¥ 3≥           ¥≥           ¥ ≥ ¥             23
  34      −6 −12            3 −4    20
  23       6 11           −2 3
                                  = 03 .
     9. One could compute the matrix of I − D 2 in an explicit basis, but an easier way
is to observe that D 3 = 0 and hence (I − D 2 )(I + D 2 ) = I − D 4 = I .
     10. Since image(AB) ⊆ image A, we have rank(AB) = dim image(AB) ≤
dim image A = rank A. Similarly rank((AB)t ) = rank(B t At ) ≤ rank B t . Since a
matrix and its transpose have the same rank (by the equality of row rank and column
rank), rank(AB) ≤ rank B.
     11. Since A has n columns, rank A ≤ n. Applying Problem 10 gives rank(AB) ≤
rank A ≤ n. Since n < k = rank I , we cannot have AB = I .
                      ≥ ¥                 ≥ ¥
     12. Take A = 10 00 and B = 00 10 . Then AB = B has rank 1 while B A = 0
has rank 0.
     13. {cosh t, sinh t}.
     14. Let {vn | n ∈ Z} be a countably infinite basis. For each subset S of Z, define
v S0 to be the member of V 0 such that v S0 (vn ) is 1 if n is in S and is 0 if not. Choose
by Theorem 2.42 a subset of {v S0 } that is a basis for the linear span of all v S0 . Arguing
by contradiction, assume that this basis is countable. Number the S’s in question as
S1 , S2 , . . . . Any v S0 then has a unique expansion as v S0 = c1 v S0 1 + · · · + ck vs0 k for
some k. Fix k, and let v S0 be expandable for this k. Let E ⊆ {1, . . . , k}. Let m and n
be such that vm and vn are in Sj for j in E and are not in Sj for j in {1, . . . , k} − E.
Then v S0 j (vm ) = v S0 j (vn ) for j = 1, . . . , k, and hence v S0 (vm ) = v S0 (vn ). Thus with
k fixed, the number of S’s for which v S0 is expandable is at most 2k . In particular, it
is finite. Taking the union over k, we find that there are only countably many v S0 in
620                              Hints for Solutions of Problems

the linear span of v S0 1 , v S0 2 , . . . . But there are uncountably many subsets S of Z, and
we have thus arrived at a contradiction. We conclude that our subset of all v S0 that is
a basis for the linear span must have been uncountable.
   15. For (a), take L, M, and N to be the three 1-dimensional subspaces of R2
shown in Figure 2.1. Then L ∩ (M + N ) = L while (L ∩ M) + (L ∩ N ) = 0.
   For (b), we always have ⊇ since L ∩(M + N ) ⊇ L ∩ M and L ∩(M + N ) ⊇ L ∩ N .
   For (c), if l = m + n is in L ∩ (M + N ), then L ⊇ M implies that n = l − m is
in L. So l = m + n has m ∈ L ∩ M and n ∈ L ∩ N .
   16. Take M, N1 , and N2 to be the three 1-dimensional subspaces of V = R2
shown in Figure 2.1. Then M ⊕ N1 = M ⊕ N2 = R2 , but N1 6= N2 .
   17. (b) only.
   18. In V1 ⊕ · · · ⊕ Vn , let p j pick off the j th coordinate, and let i j carry v j to
(0, . . . , 0, v j , 0, . . . , 0). Then pr i s is I on Vs if r = s and is 0 on Vs if r 6= s. Also,
P  n
   k=1 i k pk = I on V1 ⊕ · · · ⊕ Vn .
   19. Corollary 2.15 shows that dim ker T + dim image T = n. Since ker T and
image T have 0 intersection, the union of bases of ker T and image T is a linearly
independent set of n vectors in Rn . This set must be a basis of Rn , and hence
Rn = ker T ⊕ image T . This proves (a).
   For (b), let T 2 = T and suppose that v is in ker T ∩image T . Since v is in image T ,
we have v = T (w) for some w. Then v = T (w) = T 2 (w) = T (T (w)) = T (v), and
the right side is 0 since v is in ker T . Consequently ker T ∩ image T = 0.
   20. Define L : V10 ⊕ V20 → (V1 ⊕ V2 )0 by L(µ1 , µ2 )(v1 , v2 ) = µ1 (v1 ) + µ2 (v2 ).
    21. Proposition 2.25 shows that y 7→ z is onto the subset of z’s in V 0 such that
M ⊆ ker z, i.e., is onto Ann M. Since q is onto V /M, y 7→ z is one-one.
                                                       Ø                   Ø
    22. The kernel of q is M, and thus the kernel of q Ø N is M ∩ N . So q Ø N is one-one
if and only if M ∩ N = 0.
    If M + N = V , then any v ∈ V is of the form m +n; so v has v+ M = m +n+ M =
n + M = q(n), and q carries N onto V /M. Conversely if q carries N onto V /M,
let v ∈ V be given, and choose n with q(n) = v + M. Then n + M = v + M, and
hence v − n is in M.Ø This says that V = M + N .
    Consequently q Ø N : N → V /M is an isomorphism if and only if M ∩ N = 0
and M + N = V , and we know from Proposition 2.30 that this pair of conditions is
equivalent to the single condition V = M ⊕ N .
   23. If A−1 has integer entries, then det A and det A−1 are integers that are recip-
rocals, and we conclude that det A = ±1. If det A = ±1, then Cramer’s rule shows
that A−1 has integer entries.
   24. When r = rank A, there exist r linearly independent rows. Say that these are
the ones numbered i 1 , . . . , ir . Let A1 be the r-by-n matrix obtained by deleting the
remaining rows. Since A1 has rank r, it has r linearly independent columns. Say
that these are the ones numbered j1 , . . . , jr . Let A2 be the r-by-r matrix obtained by
                                           Chapter II                                        621

deleting the remaining columns. Then A2 is a square matrix of rank r, is therefore
invertible, and must have nonzero determinant. In the reverse direction if some
s-by-s submatrix has nonzero determinant, then the rows of the submatrix are linearly
independent, and certainly the corresponding rows of A are linearly independent.
Thus s ≤ rank A.
                                                              Pn
   25. Let the expression in question be f (t) = i=1               ai eci t . Put ri = eci . The
numbers ri are distinct. The fact that f (0) = f (1) = · · · = f (n − 1) = 0 says
that the product of the Vandermonde matrix formed from r1 , . . . , rn times the column
vector (a1 , . . . , an ) is the 0 vector. Since the Vandermonde matrix is invertible, it
follows that (a1 , . . . , an ) is the 0 vector.
   26. The characteristic polynomial is ∏≥2 −5∏+6  ¥        = (∏−2)(∏−3). The eigenvectors
                                                 1
for ∏ = 2 are all nonzero multiples of 2 , and the eigenvectors for ∏ = 3 are all
                           ≥ ¥
nonzero multiples of 13 .
          P       −1
                                   P P          −1
                                                                  P           P           −1
   27.       i (C PAC)ii =           i    j,k (C )i j A jk C ki =   j,k A jk     i C ki (C )i j =
P
   j,k A jk δk j =     j Aj j .
   28. For n = 2, direct computation gives ∏2 − a1 ∏ − a0 . Similarly we obtain
∏3 − a2 ∏2 − a1 ∏ − a0 when n = 3. We are thus led to the guess in the general case
that the determinant is ∏n − an−1 ∏n−1 − · · · − a1 ∏ − a0 . This is proved by induction,
using expansion in cofactors about the first column. The term from the (1, 1) entry,
by the inductive hypothesis, is ∏(∏n−1 − an−1 ∏n−2 − · · · − a1 ), and the term from the
(1, n) entry is (−1)n+1 (−a0 ) det B, where B is a lower triangular matrix of size n − 1
with −1 in every diagonal entry. Then det B = (−1)n−1 , and substitution completes
the induction.
   29. In (a), we have det(∏I − AB) = det(A(∏A−1 − B)) = det A det(∏A−1 − B) =
det(∏A−1 − B) det A = det((∏A−1 − B)A) = det(∏I − B A).
   For (b), we know from the fact that the characteristic polynomial of A is a polyno-
mial that there are only finitely many ≤ for which A + ≤ I fails to be invertible. Thus
there is some ≤0 > 0 such that A + ≤ I is invertible when 0 < ≤ < ≤0 . By (a), these
≤’s have det(∏I − (A + ≤ I )B) = det(∏I − B(A + ≤ I )). Since det is a polynomial in
the entries of the matrix it is applied to, det(∏I − C) is a continuous function of the
entries of C. Taking C = (A + ≤ I )B and then C = B(A + ≤ I )), and letting ≤ tend
to 0, we obtain det(∏I − AB) = det(∏I − B A).
   30. In R1 , let the n th spanning set consist of {(r) | 0 < r < 1/n}. These each
span R1 , but their intersection is empty and the empty set does not span R1 .
   31. Let {vα } be a basis of V . For each α, define a member vα0 of V 0 by saying
that vα0 (vβ ) is 1 for β = α and is 0 for β 6= α. In addition, let w0 be the member
of V 0 that is 1 on each VαP  . Arguing by contradiction, suppose that w0 is in ∂(V ).
Then we can write w0 = β∈F cβ ∂(vβ ) for some finite set F, and for each α we
                    P                       P
have w0(vα0 ) = β∈F cβ ∂(vβ )(vα0 ) = β∈F cβ vα0 (vβ ). The right side is nonzero
only if some β ∈ F has vα0 (vβ ) 6= 0, i.e., only if α is in F. On the other hand, the
622                            Hints for Solutions of Problems

left side is 1 for every α. For this equality to happen for all α forces F to be infinite,
contradiction.
    32. Ann(M + N ) ⊆ Ann M, and Ann(M + N ) ⊆ Ann N ; thus Ann(M + N ) ⊆
Ann M ∩ Ann N . If v 0 is 0 on M and is 0 on N , then it is 0 on M + N . Hence
Ann(M + N ) ⊇ Ann M ∩ Ann N .
    33. Ann(M ∩ N ) ⊇ Ann M, and Ann(M ∩ N ) ⊇ Ann N ; thus Ann(M ∩ N ) ⊇
Ann M + Ann N . Let {u α } be a basis of M ∩ N , let vβ be vectors added to {u α } to
obtain a basis of M, and let w∞ be vectors added to {u α } to obtain a basis of N . Then
{u α } ∪ {vβ } ∪ {w∞ } is a basis of M + N . Let xδ be vectors added to this to obtain a
basis of V . If v 0 is given in Ann(M ∩ N ), define v10 to be v 0 on all the basis vectors
but the vβ , where it is to be 0, and define v20 = v 0 − v10 . Then v 0 = v10 + v20 with
v10 ∈ Ann M and v20 ∈ Ann N . So Ann(M ∩ N ) ⊆ Ann M + Ann N .
    34. Let v be in M, and let v 0 be in Ann M. Then ∂(v)(v 0 ) = v 0 (v) = 0. This
proves (a).
    For (b), Propositions 2.19 and 2.20a give dim Ann M = dim V 0 − dim M
and dim Ann(Ann M) = dim V 00 − dim Ann M = dim V 00 − (dim V 0 − dim M) =
dim M = dim ∂(M). This equality in the presence of the inclusion ∂(M) ⊆
Ann(Ann M) implies ∂(M) = Ann(Ann M) by Corollary 2.4.
    For (c), let V be as in Problem 31, and put M = V . Then Ann(M) = 0 and
Ann(Ann M) = V 00 6= ∂(V ).
    35. Parts (a) and (b) follow by writing out individual entries of the products as
appropriate sums.
    36. If A or D is not invertible, then suitable row operations on the matrix on the left
side exhibit the matrix on the left as not invertible, and hence both sides are 0. Thus
we may assume≥that A¥−1 and          −1
                              ≥ D¥ ≥ exist. ¥ ≥ Problem  ¥ 35c allows us to decompose the
                     A B        A 0     I 0      I A−1 B
given matrix as 0 D = 0 I               0 D
                                                          . The determinant of the product
                                                 0 I
is the product of the determinants. Using the defining formula for det, we see that the
first two determinants from the right side are det A and det D. The third determinant
is 1 since the matrix is triangular with 1’s on the diagonal.
    37. In effect,≥ we ¥do row ≥ reduction
                                    ¥≥       with¥ blocks,
                                                        ≥    ¥taking
                                                                ≥     ¥advantage
                                                                        ≥          of Problem
                                                                                       ¥
                                         I A−1 B                              A−1 B
35c. We have CA D      B
                           = A0 0I                  = A0 0I       I 0
                                                                  C I
                                                                          I
                                                                                  −1    . Tak-
                                        C D                               0 D−C A ≥  B     ¥
ing the determinant of both sides and using Problem 36, we obtain det CA D               B
                                                                                             =
(det A) det(D − C A−1 B) = det(AD − AC A−1 B), and this equals det(AD − C B)
since AC = C A.       ≥ ¥
   38. The matrices A0 and ( B 0 ) are of size n-by-n, and their products in
                  ≥      ¥                                    ≥       ≥      ¥¥
the two orders are AB
                    0 0
                       0
                           and B A. Problem 29 shows that det  ∏I n −   AB 0
                                                                         0 0
= det(∏In − B A). The left side equals ∏n−k det(∏Ik − AB), and the result follows.
   39. Substitute the definitions of the determinants of A(S) and A(S) b into the right
side, sort out the signs, and verify that the result is the defining expression for det A.
                                         Chapter II                                 623

                                                                            \
  40. Expansion in cofactors about the last row gives det An = (An )nn det (A n )nn −
                 \
(An )n−1,n det (An )n−1,n = 2 det A≥n−1 + det¥B, where B in block form is the square
                                     A
                                     n−2     0
matrix of size n − 1 given by B =    0 −1
                                             . Expansion by cofactors of det B about
the last row shows that det B = − det An−2 , and the stated formula results.
   41. Inspection gives det A1 = 2 and det A2 = 3. The function f with f (n) =
det An − (n + 1) thus has f (1) = f (2) = 0 and f (n) = 2 f (n − 1) − f (n − 2) for
n ∏ 3, and it must be 0 for all n ∏ 1.
   42. The only changes in (a) are notational. For (b), we compute det C2 = det C3 =
2, and the formula det Cn = 2 follows as in Problem 41.
   43. For (b), we interchange the first two rows and then interchange the first two
columns. The determinant does not change.
   44. For (b), we interchange the third and fourth rows and then interchange the
third and fourth columns. For (c) we change the list of rows and columns of An from
1, 2, 3, 4, 5 to 3, 5, 4, 2, 1.
  45. The area of the rectangle is (a + b)(c + d), the two trapezoids have areas
1                    1
2 d(a + (a + b)) and 2 a(d + (c + d)), and the two triangles have areas 12 ac and 12 bd.
The difference is bc − ad. The answer is independent of the picture except for a sign.
Thus the answer is the absolute value of the determinant.
    46. The geometric effect is to leave the left edge where it is and to translate the
                                                            ≥ is¥ unchanged
right edge parallel to itself in the same direction. The area       ≥ ¥     because the
                                                               0      a
parallelogram can still be regarded as having base from 0 to c and having the
same distance between the parallel sides. The algebraic effect is that of the column
operation of replacing the second column by the sum of it and s times the first column.
                                  ≥ ¥
    47. Right multiplication by 1t 01 leaves the bottom edge where it is and translates
the top edge parallel to itself in the same direction; algebraically it corresponds to
the column operation of replacing the first ≥ column
                                                 ¥     by the sum of it and t times the
second column. Right multiplication by 01 10 interchanges the left and bottom sides
of the parallelogram and≥corresponds
                                ¥       to interchange of the two columns of the matrix.
                            q 0
Right multiplication by 0 1 corresponds to stretching the left side by a factor of q
if q > 0, along with reversing the direction if q < 0, and the algebraic≥ effect
                                                                            ¥     is the
                                                                         10
column operation of multiplying the first column by q. The effect of 0 r is similar
but affects the bottom edge instead of the left edge.
   48. The roles of rows and columns are interchanged by the transpose operation,
and the determinant is unaffected by transpose according to Proposition 2.35. In
view of Proposition 1.29, A can thus be put in reduced column-echelon form by a
sequence of column operations, each of which corresponds to right multiplication
by a suitable elementary matrix. The result is an equality saying that the product
of A and some elementary matrices is the identity. Using inverses shows that A is
624                             Hints for Solutions of Problems

the product of elementary matrices. The product can be applied a step at a time to
the cube determined by the standard basis, and each step either preserves volume or
multiplies it by a known factor, up to a minus sign. The product of these numerical
factors is the determinant, up to a minus sign. Hence the volume of the parallelepiped
has to be the product of these factors, with its sign made positive.


                                       Chapter III
                           P
     1. Since Tr B ∗ A = i, j Ai j B i j , the inner product is the usual inner product on the
n 2 entries. Then (a) and (b) are immediate. For (c), (a) gives the result of Parseval’s
equality relative to the orthonormal basis in (b).
≥ For   ¥ (d), let U be the unitary matrix with columns u 1 , . . . , u n , i.e., the matrix
    I
  60
          , where 0 = (u 1 , . . . , u n ) and 6 is the standard ordered basis. Then kAk2HS =
                                             P                 2
                                                                  P          2
Tr(A∗ A) = Tr(U −1 A∗ AU ) =                   i, j |(AU )i j | =   j kAu j k , and this equals
P           ∗      2
    i, j |vi Au j | by Parseval’s equality.
     In (e), W ⊥ consists of all matrices that are 0 along the diagonal. It has dimension
  2
n − n.
   2. The system has unknowns c0 , c1 , . . . , cn , where pn (x) = c0 +c1 x +· · ·+cn x n ,
       k th equation, for 0 ≤ k ≤ n, comes from the equality for f (x) = x k , namely
and theP
2 = nj=0 ( j + k + 1)−1 c j .
 −k

   3. (L M)∗ = M ∗ L ∗ = M L is equal to L M if and only if L M = M L.
   4. A vector u is in ker L if and only if (L(u), v) = 0 for all v, if and only if
(u, L ∗ (v)) = 0 for all v, if and only if u is in (image L ∗ )⊥ .
  5. There are none. The characteristic polynomial has no real roots, but all roots
must be real if A is Hermitian.
   6. The map v1 7→ (L(v1 ), v2 )2 is a linear functional on V1 and hence is given by
the inner product with a unique member u 1 of V1 , i.e., ((L(v1 ), v2 )2 = (v1 , u 1 )1 , and
we define this element u 1 to be L ∗ (v2 ). We readily check that L ∗ is linear, and (a) is
then proved. The proof of (b) proceeds in the same way as in the case that V1 = V2 .
    7. In (a), if v is in S ⊥ ∩ T ⊥ , then v is in V ⊥ = 0. Thus S ⊥ + T ⊥ is a direct
sum. We have dim V = dim S + dim T = (dim V − dim S ⊥ ) + (dim V − dim T ⊥ ) =
2 dim V − dim S ⊥ − dim T ⊥ . Therefore dim V = dim(S ⊥ + T ⊥ ). The inclusion
plus the equality of the finite dimensions forces V = S ⊥ + T ⊥ .
    In (b), let ∏ be 0 or 1. Then E ∗ u = ∏u if and only if (E ∗ u, v) = ∏(u, v) for all
v, if and only if (u, Ev) = ∏(u, v) for all v. When ∏ = 1, this says that E ∗ u = u if
and only if u ⊥ (I − E)v for all v, hence if and only if u ⊥ T , hence if and only if u
is in T ⊥ . When ∏ = 0, it says that E ∗ u = 0 if and only if u ⊥ Ev for all v, hence if
and only if u ⊥ S, hence if and only if u is in S ⊥ .
                                            Chapter III                                           625

     8. The formulas of the Gram–Schmidt orthogonalization process have v j =
             P                                                    P
c j (ge j ) + i< j ai j vi with c j > 0. Therefore ge j = c−1
                                                           j vj +  i< j bi j vi , and
                     P                 P
     (k −1 g)i j =          −1
                        l (k )il gl j =
                                                −1
                                            l (k )il (ge j )l
                          P                     P   P
                =    c−1
                      j
                                −1
                            l (k )il (v j )l +    l
                                                              −1
                                                      m< j (k )il bm j (vm )l
                                       P                                    P
                =    c−1    −1
                      j (k v j )i +
                                                      −1         −1
                                          m< j bm j (k vm )i = c j δi j +     m< j   bm j δim .

If i = j, the right side is c−1
                              j and is positive. If i > j, then every term on the right
side is 0. Thus k −1 g is upper triangular with positive diagonal entries. Since k carries
the standard orthonormal basis to the orthonormal basis {v1 , . . . , vn }, k is unitary.
    9. For (a), the Spectral Theorem and Corollary 3.22 show that A is similar to a
diagonal matrix with positive diagonal entries. Thus det A > 0. In (b), we specialize
the inequality x̄ t Ax > 0 to x’s that are 0 except in the entries numbered i 1 , . . . , i n , and
we find that the submatrix is positive definite. Then the result follows from Corollary
3.22.
                     p                               p
    10. Take g = A in Problem 8 and obtain A =pkt with             p k unitary and t upper
triangular with positive diagonal entries. Then A = ( A)∗ ( A) = (kt)∗ (kt) = t ∗ t.
    11. The roots of the characteristic polynomial are 12 (a+d+s) and 12 (a+d−s), where
      p                                                              ≥1                           ¥
                                                                        (a+d+s)           0
s = (a − d)2 + 4|b|2 . Let r = 12 (−a + d + s). Then D = 2 0                        1
                           ≥      ¥                                                 2 (a+d−s)
             2      2 −1/2   b −r
and U = (b + r )             r b̄
                                    .
   12. In (a), the conditions ad −µ|b| 2
                                     p > 0 and a + d >∂0 together are necessary
                            p          1
                                       2 (a+d+s) p      0
and sufficient. In (b), let D =                     1
                                                                  , and let U be as in the
                                           0        2 (a+d−s)           p
previous problem. Then the positive definite square root of A is U D U −1 .
   13. The Spectral Theorem shows that A has a basis of eigenvectors, each with a
real eigenvalue. If v is an eigenvector with eigenvalue ∏, then v t Av = 0 says that
∏kvk2 = 0. So every eigenvalue is 0, and A, being similar to a diagonal matrix, has
to be 0.
   14. Choosing a basis of eigenvectors, we may solve the corresponding problem
for diagonal matrices. Thus let A be a diagonal matrix, and assume, without loss
of generality, that A11 = · · · = Akk = 1 and A j j 6= 1 for j > k. Then the given
equation (I − A)2 y = (I − A)x says that (1 − A j j )2 yj = (1 − A j j )x j for all j. Thus
define yj to be 0 if j ≤ k, and choose y j = (1 − A j j )−1 x j for j > k.
   15. L L ∗ = (U P)(U P)∗ = (PU )(PU )∗ = PUU ∗ P = P 2 = PU ∗ U P =
(U P)∗ (U P) = L ∗ L.
   16. The family has a basis of simultaneous eigenvectors, and the matrices are all
diagonal in this basis. So the answer is the dimension of the vector space of diagonal
matrices, namely n.
626                              Hints for Solutions of Problems
                                             P                           °P          P         ¢
    17. In (a), ct G(v1 , . . . , vn )c̄ =     i, j ci (vi , v j )c̄ j =   i ci vi ,   j cj vj   =
                           2
kc1 v1 + · · · + cn vn k . Thus Corollary 3.22 shows that G(v1 , . . . , vn ) is positive
semidefinite. Moreover, kc1 v1 + · · · + cn vn k2 = 0 for some c 6= 0 if and only
if v1 , . . . , vn are linearly dependent. Thus G(v1 , . . . , vn ) is definite if and only if
v1 , . . . , vn are linearly independent. We know that a positive semidefinite matrix is
definite if and only if it is invertible, and thus det G(v1 , . . . , vn ) > 0 if and only
if v1 , . . . , vn are linearly independent; this proves (b). In (c), equality holds in the
Schwarz inequality if and only if the two vectors are linearly dependent, i.e., if and
only if one of them is a multiple of the other.
     18. This is immediate by induction.
   19. For (a), the left side is D 2 (X n+1 ) = (n + 1)D(X n X 0 ). Comparing with the
expected right side, we see that we are to show that

                                     ?
                       n D(X n X 0 ) = (2n + 1)n X 00 X n + 4n 2 X n−1 .

The left side equals n X n−1 times n(X 0 )2 + X X 00 , while the right side equals
n X n−1 times (2n + 1)X 00 X + 4n. Since

        n(X 0 )2 + X X 00 = 4nx 2 + 2x 2 − 2
                          = (4n + 2)x 2 − (4n + 2) + 4n = (2n + 1)X 00 X + 4n,

(a) is proved.
   For (b), the Leibniz rule gives D n (X 0 Y ) = X 0 D n Y + n X 00 D n−1 Y for any Y .
Meanwhile, application of D n−1 to (a) yields

D n+1 (X n+1 ) = (2n + 1)D n (X 0 X n ) − n(2n + 1)X 00 D n−1 (X n ) − 4n 2 D n−1 (X n−1 ).

Substituting with Y = (2n + 1)X n , we obtain (b). The recursion in conclusion (c)
follows immediately by multiplying by (2n+1 n!)−1 .
   For (d), conclusion (c) and the definition of Pn show that Q n = Pn − Rn satisfies
Q 0 = Q 1 = 0 and (n +1)Q n+1 (x)−(2n +1)x Q n (x)−n Q n−1 (x). Thus Q n (x) = 0
for every n by induction.
   20–21. Write X = x 2 − 1. Since X n = (x − 1)n f (x), the function X n has all
derivatives through order n − 1 equal to 0 at x = 1. The same conclusion applies
also at x = −1. If m ≤ n, integration by parts gives
R1                                                           R1
 −1   D m (X m )D n (X n ) dx = [D m (X m )D n−1 (X n )]1−1 − −1 D m+1 (X m )D n−1 (X n ) dx
                                    R1
                              = − −1 D m+1 (X m )D n−1 (X n ) dx
                                              R1
                              = · · · = (−1)k −1 D m+k (X m )D n−k (X n ) dx
                                            Chapter III                                    627

for k ≤ n. If m < n, then taking k = m + 1 gives 0 on the right side because
                                                          R1
D 2m+1 (X m ) = 0. If m = n, then taking k = n gives (−1)n −1 X n D 2n (X n ) dx =
            R 1
(−1)n (2n)! −1 X n dx on the right side. Therefore

                                                                n   2     2(2n n!)2
             hD n (X n ), D n (X n )i = (−1)n (2n)!(−1)n 2(2 n!))
                                                         (2n+1)! =         2n+1 ,

                     2
and hPn , Pn i =   2n+1 .
   22. The expansion for (a) is

 D n+1 [(D(X n ))X]
       = D n+1 (D(X n ))X + (n + 1)D n (D(X n ))X 0 + 12 n(n + 1)D n−1 (D(X n ))X 00
       = X D 2 (D n (X n )) + (n + 1)X 0 D(D n (X n )) + 12 n(n + 1)X 00 D n (X n ),

and the expansion for (b) is

   D n+1 [(D(X n ))X] = D n+1 (n X n X 0 ) = D n+1 (n X n )X 0 + (n + 1)D n (n X n )X 00
                            = n D(D n (X n ))X 0 + n(n + 1)D n (X n )X 00 .

Thus, for (c), we get (x 2 − 1)D 2 (Pn (x)) + (n + 1)2x D(Pn (x)) + 12 n(n + 1)2Pn (x) =
n D(Pn (x))2x + n(n + 1)Pn (x)2. This simplifies to

        (x 2 − 1)Pn00 + 2(n + 1)x Pn0 + n(n + 1)Pn = 2nx Pn0 + 2n(n + 1)Pn

and then to (1 − x 2 )Pn00 − 2x Pn0 + n(n + 1)Pn = 0.
   24. In Problems 24–28, there is no difficulty with addition, and we have to check
something only about scalar multiplication. For Problem 24, we need to check in V
that (ab)v = a(bv), 1v = v, a(u + v) = au + av, and (a + b)v = av + bv. These
are satisfied in V because the identities (ab)v = ā(b̄v), 1v = v, ā(u + v) = āu + āv,
and (a + b)v = āv + b̄v hold in V .
  25. We are to see that L respects scalar multiplication, and the argument is that
L(cv) = L(c̄v) = c̄L(v) = cL(v).
   26. We have (au, bv)V = (b̄v, āu)V = a b̄(v, u)V = a b̄(u, v)V , as required.
   27. Let ` in V 0 correspond to v in V , so that `(u) = (u, v)V = (v, u)V . Then
` in V 0 corresponds to v in V , while (c`)(u) = c(v, u)V = (cv, u)V shows that c`
corresponds to cv in V .
   28. Let ` in V 0 correspond to v in V . Then L t (`)(u) = `(L(u)) = (v, L(u))V =
(v, L(u))V = ((L)∗ (v), u)V , and this says that L t (`) corresponds to (L)∗ (v), i.e., L t
corresponds to (L)∗ .
628                               Hints for Solutions of Problems

   29. In (a), it is enough to check the result for p and q equal to monomials, and
                                                    P
(b) is a direct calculation. In (c), let p(x) =          ck1 ,...,kn x1k1 · · · xnkn . The bilinearity
                              P
and (b) show that h p, pi = (ck1 ,...,kn )2 hx1k1 · · · xnkn , x1k1 · · · xnkn i, and this is positive
unless all the coefficients are 0.
   30. The polynomial p is in HN if and only if @(|x|2 ) p = 0, if and only if
h@(|x|2 ) p, qi = 0 for all q in VN −2 , if and only if @(q)(@(|x|2 ) p) = 0 for all q in
VN −2 , if and only if @(|x|2 q) p = 0 for all q in VN −2 , if and only if h p, |x|2 qi = 0
for all q in VN −2 , if and only if p is in (|x|2 VN −2 )⊥ .
   31. Problem 30 gives VN = HN ⊕ |x|2 VN −2 , and we iterate this decomposition.
   32. A basis of |x|2 V2 is {|x|2 x12 , |x|2 x1 x2 , |x|2 x22 }. Apply the Gram–Schmidt
orthogonalization procedure to obtain an orthonormal basis {|x|2 u 1 , |x|2 u 2 , |x|2 u 3 },
                              P
and write x14 + y14 = h 4 + 3j=1 (x14 + y14 , |x|2 u j )|x|2 u j . Then h 4 is harmonic by
Problem 30. A basis of |x|2 V0 is |x|2 , and hence an orthonormal basis consists of
the single vector w = k|x|2 k−1 |x|2 . Write u j = h 2, j + (u j , w)w for each j, and
substitute. Each h 2, j is harmonic. Then we have
                                 P3                               °                    ¢
            x14 + y14 = h 4 +            4    4      2        2
                                  j=1 (x 1 + y1 , |x| u j )|x| h 2, j + (u j , w)w
                                    P
                       = h 4 + |x|2 3j=1 (x14 + y14 , |x|2 u j )h 2, j
                                P
                         + |x|4 3j=1 (x14 + y14 , |x|2 u j )(u j , w)k|x|2 k−1

with h 4 in H4 , each h 2, j in H2 , and the last sum in H0 .
   33. Let P be the positive semidefinite square root of B. Then AB = A P P, and
hence det(∏I − AB) = det(∏I − P A P). Consequently AB has the same eigenvalues
as P A P. The latter is positive semidefinite since (P A Pv, v) = (A(Pv), Pv) ∏ 0.
Therefore all the eigenvalues of AB are ∏ 0.
   34. Since (P −1 ABC P −1 v, v) = (ABC(P −1 v), P −1 v), ABC is positive
semidefinite if and only if P −1 ABC P −1 is positive semidefinite, if and only if
P −1 ABC P −1 has all eigenvalues ∏ 0. But P −1 ABC P −1 has the same eigenvalues
as ABC P −1 P −1 = AB, which has all eigenvalues ∏ 0 by the previous problem.


                                           Chapter IV

   1. If a 2 = b2 = (ab)2 = 1, then a −1 = a, b−1 = b, and (ab)−1 = ab. So
ab = (ab)−1 = b−1 a −1 = ba.
   2. Number the vertices counterclockwise as 1, 2, 3, 4. The motions in D4 are then
given by permutations as 1, (1 2)(3 4), (1 4)(2 3), (1 3), (2 4), (1 2 3 4),
(1 3)(2 4), (1 4 3 2).
   2A. In (c), the result follows from (a) and (b) if r 6= 0. If r = 0, both sides are 1.
                                       Chapter IV                                     629

    3. Choose integers x and y with xl + y|G| = 1. Then a = a xl+y|G| =
           = (al )x since a |G| = 1, and this is a power of an element of H .
(al )x (a |G| ) y
  4. Define ϕ : G → G 0 by ϕ(a) = a. Then ϕ(a) ◦ ϕ(b) = a ◦ b = ba = ϕ(ba) =
ϕ(a ◦ b). From this equality it follows that G 0 is a group and that ϕ is an isomorphism.
  5. For n > 0, (ab)n = abab · · · ab = a n bn ; also (ab)−n = ((ab)−1 )n =
         = (a −1 b−1 )n = (a −1 )n (b−1 )n = a −n b−n . In S3 , a 7→ a 2 is not a
(b−1 a −1 )n
homomorphism since four elements are sent to 1 and since 4 does not divide |S3 | = 6.
   6. Define ϕ : H × K → H K by ϕ(h, k) = hk. What needs proof is that
members of H commute with members of K . If h is in H and k is in K , then
(hkh −1 )h = hk = k(k −1 hk). Since H and K are normal, hkh −1 is in K and
k −1 hk is in H . Then k −1 (hkh −1 ) = (k −1 hk)h −1 and H ∩ K = {1} together imply
k −1 (hkh −1 ) = 1 = (k −1 hk)h −1 = 1. From the first of these, k = hkh −1 . Therefore
hk = kh.
   7. Since GCD(1234, 8191) = 1, there exist x and y with 1234x + 8191y = 1,
and x and y can be found explicitly by the Euclidean algorithm of Section I.1. For
this x, 1234x ≡ 1 mod 8191.
   8. The members 1, 2, . . . , p − 1 of F p are roots of X p−1 − 1 = 0. By iterated use
of the Factor Theorem, X p−1 − 1 = (X − 1)(X − 2) · · · (X − ( p − 1))Q(X), and
Q(X) must have degree 0. Checking the coefficient of X p−1 on both sides shows
that Q(X) = 1. Evaluating at X = 0 gives −1 = (−1)(−2) · · · (−( p − 1)) mod p.
Since p is odd, this equation reads ( p − 1)! = −1 mod p.
    9. Corollary 4.39 shows that such a group has to be abelian, and Theorem 4.56
shows that it is the direct sum of cyclic groups. Thus it must be C p2 or C p × C p , up
to isomorphism.
   10. If y = axa −1 , then y n = ax n a −1 . This proves (a). Also, ba = a −1 (ab)a
shows that ba and ab are conjugate. This proves (b).
   11. There are four classes: C1 = {1}, C2 = {(1 2)(3 4), (1 3)(2 4), (1 4)(2 3)},
C3 = {(1 2 3), (3 4 1), (2 1 4), (4 3 2)}, C4 = {(1 3 2), (3 1 4), (2 4 1), (4 2 3)}.
The centralizer of the first element of each class is A4 for C1 , C1 ∪ C2 for C2 ,
{(1 2 3), (1 3 2)} for C3 and C4 . Since A4 has no element of order 6, it has no
subgroup C6 . In a subgroup S3 , an element of order 3 is conjugate to its square, but
no element of order 3 in A4 is conjugate to its square.
   12. A subgroup of order 30 would have index 2 and would thus be normal, in
contradiction to Theorem 4.47.
    13. This is a special case of Proposition 4.36.
   14. Since H is normal, G acts on H by conjugation. The number of elements in
an orbit has to be a divisor of |G|, and the smallest divisor of |G| apart from 1 is p, by
hypothesis. Since {1} is one orbit and there are only p − 1 other elements in H , each
orbit must contain one element. Therefore ghg −1 = h for each g ∈ G and h ∈ H ,
and each h is in Z G .
630                            Hints for Solutions of Problems

    15. Certainly the inner automorphisms are closed under composition and inversion
and therefore form a subgroup. If ϕ is an automorphism and √ is the inner automor-
phism √(x) = axa −1 , then ϕ◦√ ◦ϕ −1 (x) = ϕ(aϕ −1 (x)a −1 ) = ϕ(a)xϕ(a)−1 shows
that ϕ ◦ √ ◦ ϕ −1 is inner. Hence the subgroup of inner automorphisms is normal.
Define a mapping 8 of G into the inner automorphisms by 8(a) = {x 7→ axa −1 }.
Then 8(ab) = 8(a)8(b), and hence 8 is a homomorphism. Certainly 8 is onto the
inner automorphisms, and its kernel consists of all elements a ∈ G with axa −1 = x
for all x, hence consists of all a in Z G . Thus 8 exhibits G/Z G as isomorphic to the
group of inner automorphisms.
    16. Part (a) is proved in the same way as Lemma 4.45. For (b), choose m = 8;
then Aut Cm is C2 × C2 .
    17. In (a), each Ck is a conjugacy class, by Proposition 4.42, and it is evident that
the Ck ’s are the only conjugacy classes whose members have order 2. If x and y are
in Sn , then τ (x yx −1 ) = τ (x)τ (y)τ (x)−1 shows that τ carries any conjugate of y to
a conjugate of τ (y). Therefore conjugacy classes map to conjugacy classes under τ ,
and τ (C1 ) has to be some Ck .                                              °n¢
    In (b), the number of ways of selecting 2k elements from n is 2k             . For each
of these, the number of ways of selecting           k unordered
                                               ° 2k ¢ (2k)!      pairs of elements  from 2k
elements is the multinomial coefficient 2,...,2 = 2k . Although the individual pairs
are unordered, this enumeration counts one for each different ordering of the k pairs.
There are k! orderings, and hence the multinomial coefficient must be divided by° k!¢
                                                                                           n
to discount the enumeration of the pairs. Thus |Ck | is the product of the integer 2k
                   (2k)!
and the integer 2k k! .
    In (c), we saw in (b) that Nk = (2k)!2k k!
                                                is always an integer. Let us bound it below.
Canceling every even factor of the numerator by a factor of k! and a factor of 2k , we
see that Nk = (2k − 1)(2k − 3)(2k − 5) · · · (3)(1). Thus Nk ∏ 2k − 1 with equality
only if 2k − 1 = 1, in which case k = 1. Also, Nk ∏ (2k − 1)(2k − 3) with equality
holding for a value of k > 1 only if 2k − 3 = 1, in which case k = 2. ° ¢
                                                                                   n (2k)!
    Now let us compare |Ck | and |C1 |. We have N1 = 1. Also, |Ck | = 2k                    =
   °n¢                        °n ¢ °n ¢                                        ° n ¢ 2k k!
Nk 2k and |C1 | = N1 2 = 2 . The easy comparison is that |Ck | ∏ 2k and this
       ° ¢
is > n2 = |C1 | unless k = 1 or |n − 2k| ≤ 2. Thus |Ck | > |C1 | unless k equals 1
or 12 n or 12°(n¢ − 1) or    1                                1
                         °n ¢2 (n − 2). We can discard k = 2 (n − 2) because in this case
               n
|Ck | = Nk 2 > N1 2 = |C1 | except when k = 1.
   °Consider     k = 12 (n − 1) with k > 1. Then |C1 | = 12 n(n − 1) = nk and |Ck | =
       n ¢
Nk n−1 = n Nk . From above, the latter is > n(2k − 1) ∏ nk = |C1 |.
    Finally consider
              °n ¢       k = 12 n with k > 1. Then |C1 | = 12 n(n − 1) = (n − 1)k and
|Ck | = Nk n = Nk . From above, the latter for k > 1 is ∏ (2k − 1)(2k − 3) =
(n − 1)(n − 3), and this is > (n − 1)k = |C1 | unless k ∏ n − 3. When k ∏ n − 3,
we obtain 12 n ∏ n − 3 and n ≤ 6. Since k = 12 n, n has to be even with n ≤ 6.
The case n = 6 (with k = 3) we are allowing, and the case n = 4 with k = 2 has
|C2 | = 3 6= 6 = |C1 |. Thus the only exceptions have k = 1 or n = 6.
                                             Chapter IV                                               631

    18. In the composition series given for S4 in Section 8, take G to be A4 , N to
be the 4-element subgroup in the series, and M to be the 2-element subgroup. For
another example, take G to be the dihedral group D4 , N to be the cyclic subgroup of
the 4 rotations, and M to be the 2-element subgroup of N .
    19. If GCD(r, s) = 1, define a homomorphism ϕ : Z → (Z/rZ) × (Z/sZ)
by ϕ(n) = (n mod r, n mod s). This is 0 for n = rs. Thus it descends to a
homomorphism ϕ : Z/rsZ → (Z/rZ) × (Z/sZ). The kernel of ϕ consists of
all integers n divisible by r and s. Since r and s are relatively prime, such integers
are divisible by rs. Thus ker ϕ = rsZ, and ϕ is one-one. Since the domain and range
have the same number of elements, ϕ is onto.
    Conversely if GCD(r, s) 6= 1, then some prime p divides both r and s. The number
of elements in Crs of order p is then p − 1, while the number of elements in Cr × Cs
of order p is p( p − 1) + ( p − 1) = p 2 − 1. So Crs cannot be isomorphic to Cr × Cs .
    20. Three, namely C27 , C9 × C3 , and C3 × C3 × C3 .
                                                  µ3 2 5∂
    21. The matrix relating the bases is C = 0 1 3 . A row interchange and a
                                                    015                      µ1 0 3∂
column interchange move the entry 1 in the center to the upper left and give 2 3 5 .
                                                                                                 105
Two row operations and one column
                            µ     operation
                                   ∂        eliminate the other entries in the first
                                        10 0
column and first row, yielding          0 3 −1       . The remaining steps pass from there to
                                        00 2
                 µ1    00
                             ∂        µ1 0  0
                                                 ∂           µ1 0   0
                                                                        ∂        µ1 0 0∂
                    0 −1 3       7→    0 1 −3        7→        0 1 −3       7→    010      .
                    0 20               02 0                    00 6               006
Hence H = Z ⊕ Z ⊕ 6Z, and G/H ∼            = C6 .
   22. Let the four generators for G be x1 , x2 , x3 , x4 , and let the four generators for H
be y1 , y2 , y3 , y4 . Since each is linearly independent over Q, it is
                                                                       linearly independent
                                                                                     
                                                                                      2 −1 0 0
                                                                                     −1  2 −1 −1 
over Z. The matrix of the yi ’s in terms of the x j ’s is C =                      
                                                                                      0 −1 0 2
                                                                                                  .   The
                                                                                    0 −1 2 0
                                              1      0   0   0
reduction procedure on this leads to         0      1   0   0
                                                               .   Hence G/H ∼
                                                                             = C2 × C2 .
                                              0      0   2   0
                                              0      0   0   2
   23. Each step of row reduction or column reduction preserves the rank of the matrix
as a member of Mnn (Q) since row rank equals column rank. Following through the
steps of the procedure, we may assume that the matrix is diagonal with     L diagonal
entries D11 , . . . , Dnn with D j j 6= 0 exactly for 1 ≤ j ≤ r. Then H = rj=1 D j j Z,
and we can read off that H has rank r and the Q rank of the matrix is r.
                                                         L
                                                            g∈G Z. For each g, form the
   24. Let G be an abelian group, and let G        e =
homomorphism ϕg : Z → G given in additive notation by ϕg (n) = ng. Then the
universal mapping property of direct sums gives the desired homomorphism of the
free abelian group G   e onto G.
632                            Hints for Solutions of Problems

    25. For (a), right translation by any element of H ∩ K sends x H to itself and y K
to itself, hence sends x H ∩ y K to itself. Therefore x H ∩ y K is a union of left cosets
of H ∩ K . We are to see that at most one left coset is involved. Thus suppose we have
two elements g1 and g2 in x H ∩ y K . Write g1 = xh 1 = yk1 and g2 = xh 2 = yk2 .
Then g2−1 g1 = h −1             −1            −1
                    2 h 1 = k2 k1 , and g2 g1 is exhibited as in H ∩ K . So g1 is in
g2 (H ∩ K ).
    For (b), if the sets x1 H, . . . , xm H exhaust G and the sets y1 K , . . . , yn K exhaust
G, then G is the union of the mn sets xi H ∩ yj K . By (a), G is exhibited as the union
of ≤ mn left cosets of H ∩ K .
                                                         L
    26. Returning to Problem 23, we see that H = nj=1 D j j Z with each D j j 6= 0.
                                 Q
Then the index of H in G is nj=1 D j j .
   27. In (a), take H2 = {(1), (1 2)(3 4), (1 3)(2 4), (1 4)(2 3), (1 3), (2 4),
(1 2 3 4), (1 4 3 2)}. The number of such subgroups is 2k + 1 and divides 3.
Since H2 is not normal, the number is > 1. Therefore it is 3.
   In (b), take H3 = {(1), (1 2 3), (1 3 2)}. The number of such subgroups is
3k + 1 and divides 8. Since H3 is not normal, the number is 4.
   28. Disproof: In S3 , take H = {(1), (1 2)}. Then N (H ) = H , and this is not
normal.
   29. Since 168/7 = 24, the number of Sylow 7-subgroups is 7k + 1 and divides 24.
The group G is assumed simple, and so k 6= 0. Then k must be 1, and there are 8
distinct Sylow 7-subgroups. Any two of these intersect only in the identity, and each
contains 6 elements of order 7. Hence there are 48 elements of order 7.
   30. The number of Sylow q-subgroups is qk + 1 and divides p, hence must be 1.
So Sq is normal, and the set Sp Sq of products is a subgroup. An argument in the proof
of Proposition 4.60 shows that each element of G is uniquely a product of a member
of Sp and a member of Sq , and hence G is a semidirect product.
   31. Let 0 be the set of subgroups conjugate to H , and form the action G × 0 → 0
by conjugation. The isotropy subgroup at H is N (H ), which must have index 1 or
index p in G. If it has index 1, then H is normal, and |0| = 1. Otherwise it has index
p. Then N (H ) = H , the orbit of H has |G|/|H | = p elements, and |0| = p.
    32. In (a), the subgroup H is a Sylow 2-subgroup, and the number of its conjugates
must then be 2k +1 and divide 24/8 = 3. Since H is assumed not normal, the number
of conjugates has to be 3.
    In (b), call the conjugates H , H 0 , and H 00 . Each member g of G acts on the
set {H, H 0 , H 00 } by conjugation of the subgroups, sending H to g Hg −1 , H 0 to
g H 0 g −1 , and H 00 to g H 00 g −1 . The result is that we obtain a function 8 from G to the
permutation group S3 on {H, H 0 , H 00 }. This function 8 is a group homomorphism.
    In (c), the subgroup ker 8 is normal, and it is enough to show that this subgroup is
neither {1} nor G. The image of 8 is not the identity subgroup since some member
g of G has g Hg −1 = H 0 ; thus ker 8 6= G. Since 24/| ker 8| = |G|/| ker 8| =
| image 8| ≤ 6, we have 6| ker 8| ∏ 24 and | ker 8| ∏ 4; thus ker 8 6= {1}.
                                             Chapter IV                                            633

   33. Let H be a Sylow 3-subgroup, of order 9. If H is normal, then G/H is a
subgroup of order 4, necessarily either C2 × C2 or C4 . Both of these groups of order
4 are isomorphic to subgroups of S4 , and thus there is a nontrivial homomorphism
of G onto a subgroup of order 4 in S4 .
   If H is not normal, then the number of conjugates of H is 3k + 1 and divides
4. Then the number of conjugates must be 4. Arguing as in the previous problem
we obtain a homomorphism of G into S4 by having each element of g map to the
corresponding permutation of the conjugates of H . This homomorphism is nontrivial
since H can be moved to any of its conjugates by some element of G and since the
number of such conjugates is > 1.
    34. Let K be a Sylow q-subgroup. The number of conjugates of K is of the form
kq + 1 and divides 2 p. If k = 0, then K is normal. This conclusion disposes of (a)
and the first statement of (b) for this case. We come back to the remainder of (b) for
this case in a moment.
    If k > 1, then kq + 1 ≤ 2 p is impossible since p < q. Thus the only other
possibility besides k = 0 is k = 1. Then q + 1 divides 2 p. So q + 1 equals 1, 2, p, or
2 p. Since q > p, the only possibility is q + 1 = 2 p. This completes the argument
for (a).
    For the rest we may assume that q + 1 = 2 p. If either of H or K is normal, then
an argument in the proof of Proposition 4.60 shows that H K is a subgroup with pq
elements. Since 2 p = q + 1, p divides q + 1. If p also divides q − 1, then p divides
the difference, which is 2, and we obtain a contradiction. So p does not divide q − 1,
and Proposition 4.60 shows that H K is abelian, hence cyclic.
    Thus we are reduced to the situation that q +1 = 2 p and K is not normal; we are to
prove that H is normal. We have seen in this case that the number of conjugates of K
is q +1, and hence the number of elements of order q is (q +1)(q −1) = 2 p(q −1) =
2 pq−2 p. The number of conjugates of H is of the form lp+1 and divides 2q. If l = 0,
then H is normal, and we are done. If l ∏ 1, then the number of elements of order p is
(lp+1)( p−1) ∏ ( p+1)( p−1) = p2 −1. Thus the total number of elements of order
1, p, or q is ∏ 1+( p2 −1)+(2 pq −2 p) = 2 pq +( p−1)2 −1 ∏ 2 pq +22 −1 > 2 pq,
and we have obtained a contradiction.
   35. Certainly √ is one-one and onto. For (h, k) and (h 0 , k 0 ) in H ×ϕ2 K , we have

       √((h, k)(h 0 , k 0 )) = √(hh 0 , ((ϕ2 )h 0−1 (k))k 0 ) = (ϕ(hh 0 ), ((ϕ2 )h 0−1 (k))k 0 )

and

       √(h, k)√(h 0 , k 0 ) = (ϕ(h), k)(ϕ(h 0 ), k 0 ) = (ϕ(hh 0 ), ((ϕ1 )ϕ(h 0 )−1 (k))k 0 ).

The right sides are equal because (ϕ2 )h 0−1 = (ϕ1 ◦ ϕ)h 0−1 = (ϕ1 )ϕ(h 0−1 ) = (ϕ1 )ϕ(h 0 )−1 .
   36. Again √ is visibly one-one and °onto. The formula ¢ for ϕ2 in terms of ϕ1 is
given more concretely as (ϕ2 )h (k) = a (ϕ1 )h (a −1 (k)) . For (h, k) and (h 0 , k 0 ) in
634                              Hints for Solutions of Problems

H ×ϕ1 K , we then have

      √((h, k)(h 0 , k 0 )) = √(hh 0 , ((ϕ1 )h 0−1 (k))k 0 )
                              °       °                    ¢¢ °      °              ¢       ¢
                            = hh 0 , a ((ϕ1 )h 0−1 (k))k 0 = hh 0 , a (ϕ1 )h 0−1 (k) a(k 0 )

and
                                                          °     °                 ¢       ¢
        √(h, k)√(h 0 , k 0 ) = (h, a(k))(h 0 , a(k 0 )) = hh 0 , (ϕ2 )h 0−1 (a(k)) a(k 0 )
                               °     ° °                         ¢¢       ¢
                             = hh 0 , a (ϕ1 )h 0−1 (a −1 (a(k))) a(k 0 ) .

The right sides are equal because a −1 (a(k)) = k.
    37. An action of C p on Cq is a homomorphism of C p into Aut Cq ∼    = Cq−1 . If a
is a generator of C p and b is a generator of Cq−1 , we may assume that a 7→ bk for
some k. Since the action is nontrivial, 0 < k < q − 1. Then 1 = a p maps to bkp ,
and therefore bkp must be 1. This means that kp must be a multiple of q − 1. So
kp = r(q − 1). Since 0 < k < q − 1, we see that p > r. Therefore p does not
divide r and must divide q − 1.
    38. Put n = (q − 1)/ p. Let a be a generator of C p , and let b be a generator
of Aut Cq ∼   = Cq−1 . For reference, take τ (a) = bn . This defines a nontrivial
homomorphism of C p into Cq−1 . Any other one is of the form τ1 (a) = bk1 with
0 ≤ k1 < q − 1. As in the previous problem, we know that k1 p = r(q − 1). Hence
k1 = nr for some r with 1 ≤ r ≤ p − 1. The mapping ϕ(a s ) = a rs is then an
automorphism of C p , and τ1 (a) = bk1 = bnr = τ (ar ) = (τ ◦ ϕ)(a). So τ1 = τ ◦ ϕ.
Problem 35 applies and yields the desired isomorphism.
    40. For (a), D4 ⊇ C4 ⊇ C2 ⊇ {1}, where C4 is the subgroup of rotations. For (b),
H8 ⊇ C4 ⊇ C2 ⊇ {1}, where C4 is the subgroup {±1, ±i}.
    41. For (a), the trivial subgroup, the whole group, and all subgroups of index 2
are automatically normal. The only other possibility is order 2. Since −1 is the only
element of order 2, the only subgroup of order 2 is {±1}. This is the center of H8 and
hence is normal.
    For (b), the five conjugacy classes are {±i}, {±j}, {±k}, {−1}, and {1}.
    For (c), Problem 15 shows that the inner automorphisms form a normal subgroup
isomorphic to the quotient of H8 by its center. The center is {±1}, and thus the inner
automorphisms form a subgroup of the group of all automorphisms isomorphic to
C2 × C2 . The nontrivial inner automorphisms multiply two of i, j, k by −1 and fix
the third one. In addition, the cyclic map i 7→ j 7→ k 7→ i is an automorphism and
gives an automorphism of order 3. So is its square. One more automorphism fixes
i and has j 7→ k 7→ −j 7→ −k. Consequently the group of automorphisms G acts
transitively on the set of six elements of order 4, and |G| = 6|H |, where H is the
subgroup fixing i. With i fixed, an automorphism can carry j to any of ±j and ±k.
Thus |H | = 4|K |, where K is the subgroup fixing i and j. Since i and j generate H8 ,
K is trivial. Hence | Aut H8 | = 24.
                                       Chapter IV                                    635

   42. The only possible orders are the divisors of 8. If it were to have an element of
order 8, it would be cyclic, hence abelian. If all elements other than the identity were
to have order 2, it would be abelian by Problem 1. Hence it must have an element of
order 4.
   43. Let C2 be the subgroup generated by the element of order 2. Proposition 4.44
shows that G is a semidirect product C2 ×τ K , and τ has to be nontrivial for G to
be nonabelian. By Problem 16a, there is only one possibility for τ . Since D4 is one
such semidirect product, G must be isomorphic to D4 .
    44. Let the elements of K be the powers of i. By assumption every element
outside K has order 4. Thus i2 is the only element of order 2. Its conjugacy class
therefore contains no other element, and it is central. Let us write −1 for this element.
No element other than ±1 can be central since if the center has order 4, then it
commutes with any other element and together they generate an abelian G. So
Z G = {±1}. Next let j be an element of order 4 not in K . Define k = ij. We know
that j2 = k2 = −1, and thus the 8 elements are ±1, ±i, ±j, ±k. From k = ij, we
obtain kj = (i)(−1) = −i and similarly ik = −j. Finally we know that i and j do not
commute (since G would otherwise be abelian) and that neither ij nor ji is a power of
i or j. Thus ji has to be ±k and cannot be k. So ji = −k, and we then obtain jk = i
and ki = j. Thus the multiplication table in G matches that in H8 , and we have an
isomorphism.
    46. Suppose K ∼    = C4 . If H acts nontrivially on K , then there is a nontrivial
homomorphism of H ∼      = C3 into Aut K ∼  = Aut C4 ∼= C2 . Since C2 has no element of
order 3, this is impossible.
         = C2 × C2 , then Aut K ∼
    If K ∼                            = S3 , the automorphisms being the permutations
of the set {(1, 0), (0, 1), (1, 1)}. Thus there are two nontrivial homomorphisms of C3
into Aut K . Since the elements of order 3 in S3 are conjugate in S3 , Problem 36
applies and shows that the two resulting semidirect products are isomorphic. The
group A4 meets the conditions of this problem, and hence the given G must be
isomorphic to A4 .
   47. Certainly one of those conditions holds, and G is abelian if (i) holds. If (ii)
holds, then τ has order 2, and τ is determined by its kernel. Let us rewrite the group
K as C2 × C2 with the second factor as the kernel of τ , so that τ factors through to
a homomorphism of the first factor. Then (C2 × C2 ) ×τ C3 ∼    = C2 ×τ (C2 × C3 ) ∼ =
C2 ×τ C6 ∼ = D6 . If (iii) holds, we have a nonnormal subgroup of order 4 in G, and
this does not happen in A4 or D6 .
   48. If (iii) holds, the homomorphism C4 → Aut C3 has to be nontrivial and is then
uniquely determined since Aut C3 ∼  = C2 . This proves the uniqueness of the group
up to isomorphism. The group has 1 element of order 1, 3 elements of order 2, 2
elements of order 3, and 6 elements of order 4.
   49. Let H be a Sylow q-subgroup, and let K be a Sylow p-subgroup. The number
of conjugates of H is of the form qk + 1 and divides p 2 . Since p is prime, qk + 1
636                              Hints for Solutions of Problems

must be 1, p, or p2 . If H is not normal, then k > 0 and we cannot have qk + 1 = p
since p < q; therefore qk + 1 = p2 . In this case the number of elements of order q
is (qk + 1)(q − 1) = p 2 (q − 1) = p2 q − p2 , and a Sylow p-subgroup then accounts
for all the remaining elements. Consequently H not normal implies K normal.
    Now let us analyze what k must be when qk = p2 − 1. Since q is prime, q divides
p + 1 or q divides p − 1. But the condition q divides p − 1 is impossible since p < q,
and thus q divides p + 1. Since 2q > p + q > p + 1, we must in fact have q = p + 1.
Since all primes but 2 are odd, this says that p = 2 and q = 3. We conclude that
either p 2 q = 12 or else the condition qk = p2 − 1 is impossible; when qk = p2 − 1
fails, we have seen that H is normal.
    50. We form three distinct semidirect products, two with Sylow p-subgroup C p2
and one with Sylow p-subgroup C p × C p . For each a Sylow q-subgroup Cq is
to be normal. We know from Problem 16a and Corollary 4.27 that the group of
automorphisms of the cyclic group Cq is isomorphic with Cq−1 . We obtain one
homomorphism C p2 → Cq−1 by mapping a generator of C p2 to an element in Cq−1
of order p2 and a second homomorphism by mapping a generator of C p2 to an element
in Cq−1 of order p. The third semidirect product comes by having the first factor C p
of C p × C p act trivially on Cq and having the second factor act with a generator of
C p mapping to an element of order p in Cq−1 .
    51. The second and third groups constructed in the previous problem make sense
when p divides q − 1.
    52. If p does not divide q −1, then p2 q 6= 12. Problem 49 then shows that a Sylow
q-subgroup is normal. Hence the group has to be a semidirect product. The action
of a Sylow p-subgroup on Cq corresponds to a homomorphism of C p2 or C p × C p
into Cq−1 , and the condition that p not divide q − 1 means that C p2 or C p × C p must
map to the identity. Therefore the group is abelian.
    53. In (a) and (b), the automorphism group of Z/9Z is given by multiplication by
the members of (Z/9Z)× = {1, 2, 4, 5, 7, 8}. The element 4 has square 7 and cube 1
modulo 9, and hence the multiplications by 1, 4, 7 yield a group of automorphisms of
order 3 of C9 . Hence C3 has a nontrivial action by automorphisms on C9 , and there
exists a nonabelian semidirect product of C3 and C9 with C9 normal.
    In (c), let a be a generator of C9 , let b be a generator of C3 , and let τb be the
automorphism a 7→ a 7 . Then τb−1 is the automorphism a n 7→ a 4n , and τb− p (a n ) =
   p
a 4 n . Proposition 4.43 says that (bm , a n )(b p , a q ) = (bm+ p , (τb− p (a n ))a q ), and the
                              p
right side equals (bm+ p , a 4 n+q ). Taking m = −1, n = 1, p = 1, and q = 0, we
obtain (b−1 , a)(b, 1) = (1, a 4 ). Abbreviating (1, a) as a and (b, 1) as b, we obtain
a 9 = b3 = b−1 aba −4 = 1.
    54. In such a group the subgroup H is normal by Proposition 4.36, and thus the
group of order 27 is a semidirect product of C3 and C9 with C9 normal. A nonabelian
such semidirect product must have a generator of C3 mapping into an automorphism
of order 3 of C9 . There are two possibilities, and Problem 35 shows that they lead to
isomorphic semidirect products.
                                          Chapter IV                                        637

    55. |GL(2, F)| = (q 2 − 1)(q 2 − q) and |SL(2, F)| = (q − 1)−1 |GL(2, F)|
because |GL(2, F)| = | ker det | | image det |. This handles (a) and (b). For (c), the
scalar matrices of determinant 1 are those for which the scalar has square 1. Since the
characteristic is not 2, both ±1 qualify. Since F is a field, the polynomial X 2 − 1 can
have only two roots. So we factor by a group of order 2, and the number of elements
                                                   2       2 −q)
is cut in half. For (d), the order in general is (q −1)(q
                                                     2(q−1)      = 12 (q − 1)q(q + 1). Then
|PSL(2, F7 )| = 168.
                                               ≥ ¥mappings that
    56. Regard G as a group of invertible linear                    ≥ is to¥be written in the
                                                  M                    0 −1
standard basis 6. Let 0 = (u, v). If A = 00            , then A = 1 c , the upper right
                                                 ≥       ¥     ≥ ¥ ≥ ¥−1
                                                     M           M          M
entry being −1 because det M = 1. Then 66                  = 60        A 60      . Products
AB go into products of such expressions, and conjugates h Ah −1 by matrices of
determinant 1 go into expressions
         ° ≥ M ¥ ≥ M ¥−1 ¢° ≥ M ¥ ≥ M ¥−1 ¢° ≥ M ¥ ≥ M ¥−1 ¢−1
             60
                  h 60           60
                                    A 60          60
                                                      h 60

that are conjugates of such expressions. Thus if A and such expressions generate
SL(2, F), then the conjugates generate the conjugates, again giving SL(2, F).
    57. In (a), B −1 A−1 B A is the product of the conjugate B −1 A−1 B of the inverse of
A ≥by A itself and   ¥ hence is in G. Direct computation shows that the matrix in question
     a −2 c(a −2 −1)
is             2      . In (b), the diagonal entries are equal if and only if a −2 = a 2 , hence
     0     a
if and only if a 4 = 1. In (c), the result of (b) shows that there are at most 4 choices
of a to avoid. We must also avoid a = 0. Thus if the field has more than 5 elements,
a can be chosen nonzero so that a 4 6= 1.
    58. As in Problem 57a, the conditions that ≥C is in G     ¥ and det D = 1 imply that
                                                     1 x 2 −1
       −1  −1
C DC D is in G. The product in question is                     . Since x 6= ±1, ∏ = x 2 −1
                                                     0 1
is not 0.                                           ≥ ¥
                                                         1∏
   59. Let 3 be the set of ∏ such that E(∏) =            01
                                                               is in G. Since E(∏ + ∏0 ) =
E(∏)E(∏0 ) and E(∏)−1 = E(−∏), 3 is closed under addition and negation. Since
≥        ¥      ≥        ¥−1
  α 0             α 0
  0 α −1   E(∏)   0 α −1     = E(α 2 ∏), 3 is closed under multiplication by squares of
nonzero elements.
   60. The previous problems produce some ∏0 6= 0 in 3, and −∏0 is in 3 since 3 is
closed under negatives. If x 6= ±1, then 14 (x + 1)2 and 14 (x − 1) are nonzero squares,
and hence 14 (x + 1)2 ∏0 and 14 (x − 1)∏0 are in ∏. Subtracting, we see that x∏0 is in 3.
Thus all multiples of ∏0 except possibly for those by 0, +1, −1 are in 3. However,
we have seen separately that 0, ∏0 , −∏0 are in 3. Hence 3 = F.
                                        ≥      ¥≥ ¥≥           ¥      ≥      ¥
                                            01     1∏      0 1 −1        1 0
   61. The conjugacy follows from −1 0                    −1 0
                                                                   =    −∏ 1
                                                                              . Next we
       ≥ ¥≥ ¥≥ ¥                ≥              ¥ 01
         1a    10      1c         1+ab c+cab+a
have 0 1       b1      01
                             =      b    bc+1
                                                 , and it follows that every member of
638                            Hints for Solutions of Problems
                                                                       ≥      ¥
                                                                   01
SL(2, F) with lower left entry nonzero is in G. Conjugating by           , we obtain
                                                                  ≥ 0¥ ≥
                                                                  −1              ¥
                                                                   11      α 0
the same conclusion when the upper right entry is nonzero. Finally 0 1     0 α −1   =
≥ −1 ¥       ≥       ¥ ≥       ¥ ≥ −1 ¥                                ≥       ¥
                          1 −1
  α α
         says α0 α0−1 = 0 1       α α
                                          and shows that every matrix α0 α0−1 is in
  0 α −1                          0 α −1
G. Hence G = SL(2, F).
    62. Let ϕ : SL(2, F) → PSL(2, F) be the quotient homomorphism. If H is a
normal subgroup 6= {1} in PSL(2, F), then ϕ −1 (H ) is a normal subgroup of SL(2, F)
containing an element not in the center. By Problem 61, ϕ −1 (H ) = SL(2, F).
Therefore H = ϕ(ϕ −1 (H )) = ϕ(SL(2, F)) = PSL(2, F).
    63. If a differs from c in a set A of k places and if b differs from c in a set B of
l places, then a differs from b at most in the places of A ∪ B, hence in at most k + l
places. Therefore d(a, b) ≤ d(a, c) + d(c, b).
    If d(w, a) ≤ (D − 1)/2 and d(w, b) ≤ (D − 1)/2 with a and b distinct in C,
then it follows that d(a, b) ≤ (D − 1) and hence that δ(C) = minx6= y in C d(x, y) ≤
d(a, b) ≤ (D − 1) < D.
    64. Since C is linear, 0 is in C. Then δ(C) ≤ d(0, c) for every c in C, and we obtain
δ(C) ≤ minc∈C d(0, c). On the other hand, we certainly have d(a, b) = d(0, a − b)
for all a, b in Fn . If a and b are in C, then the linearity of C forces a − b to be in C,
and hence d(a, b) = d(0, a − b) ∏ minc∈C d(0, c). Taking the minimum over all a
and b, we obtain δ(C) ∏ minc∈C d(0, c). Hence equality holds.
    65. n + 1 and 0, 1 and n, n and 1, 2 and n − 1.
    66. In (a), a basis vector c is 1 in one of the entries corresponding to the corner
variables, and it is 0 in the other entries corresponding to corner variables. At worst
it could be 1 in every entry corresponding to an independent variable. The number
of independent variables is n minus the rank, i.e., n minus dim C. Thus wt(c) ≤
1 + n − dim C. Since δ(C) ≤ wt(c), dim C + δ(C) ≤ n + 1.
    For (b), one can take the parity-check code.
    For (c), the alternative would be dim C +δ(C) = n+1. Then dim C +wt(c) ∏ n+1
for every c in C. Consequently every basis vector of C must have a 1 in every position
corresponding to an independent variable. Since dim C ∏ 2, there are at least two such
basis vectors. Their sum gets a contribution of 2 to its weight from the corner variables
and can have a 0 in at most 1 position corresponding to an independent variable. But
their sum is 0 in every position corresponding to an independent variable. Hence there
is at most one such position, and we conclude that n − dim C = 1, in contradiction
to the hypothesis dim C ≤ n − 2.
    67. A direct check of all seven nonzero elements of C shows that each has weight
3. Therefore δ(C) = 3.
    68. In (a), the basis vectors each have one 1 in positions 3, 5, 6, 7, and at least
two of the parity bits in positions 1, 2, 4 are 1 since none of 3, 5, 6, 7 is a power of
2. Any sum of two distinct basis vectors has two 1’s in positions 3, 5, 6, 7, and the
parity bits cannot all be 0 since the parity bits for each of the basis vectors identify the
                                         Chapter IV                                       639

basis vector and since the two basis vectors in question are distinct. Finally the sum
of three or more basis vectors has 1 in three or more positions 3, 5, 6, 7 and hence has
weight ∏ 3. Thus all code words have weight ∏ 3, and therefore δ(C7 ) ∏ 3. Since
the first basis vector has weight 3, δ(C7 ) = 3.
    In (b), each word in C8 is a word of C7 plus a parity bit. The part from C7 has
weight ∏ 3, by (a), and the parity bit means that the weight has to be even. Thus the
weight of every word in C8 is ∏ 4.
    In (c) for C2r −1 , we distinguish between the r bits whose indices are a power of
two and the other 2r − 1 − r bits. The first are the check bits, and the others are the
message bits. The message bits are allowed to be arbitrary, and the check bits will
depend on them. Thus dim C8 = 2r − r − 1. For a given pattern of message bits,
the check bit in position 2 j counts, modulo 2, the number of 1’s in message bits that
occur is positions requiring 2 j in their binary expansions. Then C2r is obtained by
adjoining a parity bit to each word of C2r −1 .
    The first conclusion of (d) was proved in the course of answering (c), and the other
two conclusions follow by the same argument that was given for r = 3 in (a) and (b).
    69. In (a), the dimension of the null space of H is the number of columns minus
the rank, hence is 7 − 3 = 4. Since C7 lies in the null space and dim C7 = 4, the null
space equals C7 .
    In (b), let c be in C7 . If ei denotes the usual i th basis vector, then H (c + ei ) =
H c + H ei = H ei , and this is the i th column of H .
    70. Take a basis of C, write it as the rows of a matrix, row reduce the matrix,
and permute the variables so that all the corner variables precede all the independent
variables. The resulting matrix in block form is (I A) for some matrix A with dim C
rows and n − dim C columns. Since each basis vector has weight ∏ 3, each row of
A has at least two 1’s. Since each sum of two distinct basis vectors has weight ∏ 3,
the sum of two distinct rows of A cannot be 0. Thus the rows of A must be distinct.
    Arguing by contradiction, suppose that dim C > n − r, so that A has ≤ r − 1
columns. The number of possible rows in A with at least two 1’s is then ≤ 2r−1 − 1 −
(r − 1) = 2r−1 − r. Hence n − r < dim C ≤ 2r−1 − r, and n < 2r−1 , contradiction.
    71. For (a), the answers are X n , (X + Y )n , X n + Y n , 12 ((X + Y )n + 12 (X − Y )n ),
  6
X + 7X 3 Y 3 , X 7 + 7X 4 Y 3 + 7X 3 Y 4 + Y 7 , and X 8 + 14X 4 Y 4 + Y 8 . The last three
are by a direct count of the number of code words of each weight.
    In (b), the 0 word is the unique code word of weight 0, and it is present in every
linear code.
    In (c), the expression X n−wt(c) Y wt(c) makes a contribution of 0 to the coefficient
Nk (C) of X n−k Y k if wt(c) 6= k andP makes a contributionP        of 1 to the coefficient if
wt(c) = k. Summing on c yields nk=0 Nk (C)X n−k Y k = c∈C X n−wt(c) Y wt(c) .
   72. The equality (1+ X + X 2 + X 4 )(1+ X + X 3 ) = 1+ X 7 produces a member of C
with weight 2. Therefore δ(C) ≤ 2. On the other hand, the product of 1+ X + X 2 + X 4
with a polynomial can never be a monomial, and therefore no code word has weight
1. Thus δ(C) > 1.
640                             Hints for Solutions of Problems

   73. In essence we use the method suggested by the solution to Problem 70, except
that we put coefficients corresponding to low degrees on the left and we row reduce
the matrix into the form (A I ). Let 8 ≤ n ≤ 19. Form the images of as many of the
following polynomials as have degree ≤ n:
1, X, X 2 , X 3 , X 4 , X 5 , X 6 + 1, X 7 + X + 1, X k (X 8 + X 2 + X + 1) for k ∏ 0.
The list stops with k = n − 16. Assemble the coefficients of the image polynomials
as the rows of a matrix as in Problem 70. The images form a basis of C. They all
have weight 4, and thus every member of C has even weight. Since the image of 1
has weight 4, δ(C) must be 2 or 4.
   Imagine doing a row reduction as in the solution of Problem 70. We want to rule
out δ(C) = 2, and it is enough to show that the basis vectors and all sums of two
distinct basis vectors have weight > 2. To handle the basis vectors, it is enough to
show that the A part of the reduced matrix (A I ) never has just one 1 in a row. To
handle the sums of two distinct basis vectors, it is enough to show that the sum of
two rows of A is never 0, i.e., that the rows of A are distinct.
   The matrix A will have 8 columns, corresponding to powers X l with l ≤ 7. The
rows of (A I ) are thus to correspond to polynomials of the form X m + “lower,” where
each expression “lower” has degree at most 7 and m takes on the values 8, 9, . . . , n.
The polynomials whose images correspond to the rows of the reduced matrix are
      1, X, . . . , X 5 , X 6 + 1, X 7 + X + 1,
      X 8 + X 2 + X + 1, X (X 8 + X 2 + X + 1), . . . , X 3 (X 8 + X 2 + X + 1),
and the left part A of the reduced matrix is
                                    1 1 1 0 0 0 0 0
                                    0 1 1 1 0 0 0 0
                                    0 0 1 1 1 0 0 0
                                    0 0 0 1 1 1 0 0
                                                   
                                    0 0 0 0 1 1 1 0
                                    0 0 0 0 0 1 1 1
                                  A=               
                                    1 1 1 0 0 0 1 1.
                                    1 0 0 1 0 0 0 1
                                                   
                                    1 0 1 0 1 0 0 0
                                                   
                                    0 1 0 1 0 1 0 0
                                           00101010
                                           00010101
No row of A is 0, and no two distinct rows are equal. This completes the proof.
    74. Suppose that {X s }s∈S is an object in C S and that f s : X s → A for each s is
a function, A being a particular set. The disjoint union of the X s ’s consists of all
ordered pairs (xs , s) with s ∈ S and xs ∈ X s , and we define i s (xs ) = (xs , s). To
define a function f from the disjoint union of the X s ’s into A such that f i s = f s for
all s, we let f (xs , s) = f s (xs ). Then f i s (xs ) = f (xs , s) = f s (xs ). Thus f exists.
On the other hand, the condition that f i s = f s forces f (xs , s) to be f s (xs ), and hence
the f in the universal mapping property is unique, as it is required to be.
                                         Chapter V                                       641

   76. Peeking ahead to Problem 80, we take the category to be C opp , where C is the
category defined in Section 11 after Example 4 of products. The category C has no
product functor when S has two elements.
    77. The existence of the identity and associativity are part of the definition. The
existence of inverses is given in the hypothesis. The answer to the question is “yes”;
if a group G is given, define a category with one object, namely the set G, define
Morph(G, G) to be the set G, and let the law of composition be the group law.
    78. To see that ◦opp is well defined, let f be in MorphC opp (A, B), and let g be
in MorphC opp (B, C). The definition is g ◦opp f = f ◦ g, and this is meaningful
since g is in MorphC (C, B) and f is in MorphC (B, A). The associativity and the
existence of the identity are straightforward to check. It is clear from the definition
that (C opp )opp = C.
    In a diagram the vertices stay where they are, and so do the morphisms, since the
objects and the sets of morphisms do not change. However, the direction of each
arrow is reversed since “domain” and “range” are interchanged in passing from C to
C opp . Thus diagrams map to diagrams with the arrows reversed.
    Compositions correspond because of the definition of ◦opp , and it follows that
commutative diagrams map to commutative diagrams.
   79. Let A and B be sets such that A has three elements and B has one element.
The number of functions from A to B is then one, and the number of functions from B
to A is three. Since MorphC opp (A, B) = MorphC (B, A), MorphC opp (A, B) has three
elements and cannot be accounted for by functions from A to B.
   80. For (a), if (X, { ps }s∈S ) is a product of {X s }s∈S , we set up the diagram of the
universal mapping property of the product. Passing to C opp and using Problem 78,
we obtain the same diagram in C opp but with the arrows reversed. Then it follows that
(X, { ps }s∈S ), when interpreted in C opp , satisfies the condition of being a coproduct.
The other half proceeds in the same way.
   For (b), we start with two coproducts in C and pass to C opp , where they be-
come products, according to (a). Proposition 4.63 shows that the two products are
canonically isomorphic in C opp . This isomorphism, when reinterpreted in C, is still an
isomorphism, and the result is that the two coproducts in C are canonically isomorphic.


                                       Chapter V

   1. For (a), we have ((g1 , h 1 )((g2 , h 2 )x)) = (g1 , h 1 )(g2 xh −1             −1 −1
                                                                       2 ) = g1 g2 xh 2 h 1 =
                   −1                                          −1
(g1 g2 )x(h 1 h 2 ) = (g1 g2 , h 1 h 2 )x and (1, 1)x = 1x1 = x.
   For (b), left multiplications by GL(m, C) preserve the row space, hence the rank,
and right multiplications by GL(n, C) preserve the column space, hence the rank.
Hence all members of an orbit have the same rank.
   Row operations, which correspond to left multiplications by elementary matrices,
can be used to bring the matrix into reduced row-echelon form, and then column
642                             Hints for Solutions of Problems

operations, which correspond to right multiplications by elementary matrices, can be
used to bring the result into reduced column-echelon form. If r = min(m, n), then
the resulting matrix is 1 in entries (1, 1), (2, 2), . . . , (l, l) for some l ≤ r and 0
elsewhere. This has rank l and answers (c) and the remainder of (b).
    2. If A has minimal polynomial X k + ck−1 X k−1 + · · · + c1 X + c0 , with c0 6= 0,
then I = A(−c0−1 (Ak−1 + ck−1 Ak−2 + · · · + c1 I )), and A is invertible. Conversely
if c0 = 0, then X is a factor of the minimal polynomial and must be a factor of the
characteristic polynomial, by Corollary 5.10. Then 0 is an eigenvalue, and the null
space is nonzero. Hence A is not invertible.
    3. Proposition 5.12 shows that l j ∏ max(r j , s j ). For u in U , we know that
P1 (L)r1 · · · Pk (L)rk (u) = 0. For w in W , we know that P1 (L)s1 · · · Pk (L)sk (w) =
0. Thus any v in U or W has P1 (L)max(r1 ,s1 ) · · · Pk (L)max(rk ,sk ) (v) = 0. Forming
sums, we see that P1 (L)max(r1 ,s1 ) · · · Pk (L)max(rk ,sk ) (v) = 0 for all v in V . Thus the
minimal polynomial divides P1 (X)max(r1 ,s1 ) · · · Pk (X)max(rk ,sk ) , and we must have
l j ≤ max(r j , s j ).
   4. For any monomial P(X) = X j , the monomial Q(X) = X P(X) = X j+1
has Q(B A) = B A(B A) j = B(AB) j A = B P(AB)A. Taking suitable linear
combinations of this result as j varies, we obtain (a).
   For (b), let M AB (X) and M B A (X) be the minimal polynomials of AB and B A.
Part (a) implies that M B A (X) divides X M AB (X). Reversing the roles of A and B, we
see that M AB (X) divides X M B A (X). By unique factorization all the prime powers in
the prime factorizations of M AB (X) and M B A (X) are the same except for the power
of X. The powers of X in the factorizations of M AB (X) and M B A (X) differ at most
by 1.
   5. Theorem 5.14 allows us to write Kn = U1 ⊕ · · · ⊕Uk and Kn = W1 ⊕ · · · ⊕ Wl ,
where the U j are the eigenspaces for the distinct eigenvalues of D and the W j are
the eigenspaces for the distinct eigenvalues of D 0 . These decompositions are the
primary decompositions as in Theorem 5.19, and (e) of that theorem shows that
W j = (W j ∩ U1 ) ⊕ · · · ⊕ (W j ∩ Uk ) for 1 ≤ j ≤ l. Summing on j, we see that Kn is
the direct sum of all Ui ∩ W j . Each of D and D 0 is scalar on Ui ∩ W j , and (a) follows
                         ≥ into
by translating this result    ¥ a statement≥about¥ matrices.
                           01          0      02
   The matrices N = 0 0 and N = 0 0 commute, and both have N uniquely
as Jordan form. If C were to exist with C −1 N C and C −1 N 0 C both in Jordan form,
we would have C −1 N C = C −1 N 0 C and N = N 0 , contradiction. This answers (b).
   6. If E is the projection of V on U along W , then each member of U is an eigen-
vector with eigenvalue 1, and each member of W is an eigenvector with eigenvalue 0.
The union of bases of U and W is then a basis of eigenvectors for E, and (a) follows
from Theorem 5.14. In view of Proposition 5.15, two projections are given by similar
matrices if and only if they have the same rank.
   7. For (a), E F = F implies image F ⊆ image E, which implies E F = F.
Reversing the roles of E and F, we see that F E = E if and only if image E ⊆
                                          Chapter V                                       643

image F.
    For (b), E F = E implies ker F ⊆ ker E, while F E = F implies ker E ⊆ ker F.
So E F = E and F E = F implies ker E = ker F. Conversely if ker F ⊆ ker E, then
E F = E on ker F and E F = E on image F; so E F = E. Reversing the roles of E
and F, we see that ker E ⊆ ker F implies F E = F.
    8. If E F = F E, then (E F)2 = E F E F = E(F E)F = E(E F)F = E 2 F 2 =
                        ≥ ¥ This proves
E F. So E F is a projection.            ≥ (a).  ¥
    For (b), let E = 10 00 and F = 10 10 . Each is a projection, and E F = F, so
that E F is a projection. However, F E = E. Since E 6= F, E F 6= F E.
    9. If E is a projection, then U = 2E − I has U 2 = 4E 2 − 4E + I =
4E − 4E + I = I ; so U is an involution. If U is an involution, then E = 12 (U + I )
has E 2 = 14 (U 2 + 2U + I ) = 14 (I + 2U + I ) = 12 (U + I ) = E. So E is a projection.
The two formulas U = 2E − I and E = 12 (U + I ) are inverse to each other.
    10. Apply Theorem 5.19, and take U to be the primary subspace for the prime
polynomial X and W to be the sum of the remaining primary subspaces. Then (i),
(ii), and (iii) are immediate from the theorem. For (iv), let U j be the primary
                                                                           Ø       subspace
for some other prime polynomial P(X). The theorem shows that L ØUj has a power
of P(X) Ø as minimal polynomial.Ø Since X does not divide P(X), Problem 2 shows
that L ØUj is invertible. Hence L ØUj is invertible on the direct sum of the U j ’s other
than the one for the polynomial X.
    11. Let V = U1 ⊕ · · · ⊕ Uk be the primary decomposition, with U1 corresponding
to the prime X. By (ii) and Theorem 5.19e, U = (U1 ∩ U ) ⊕ · · · ⊕ (Uk ∩ U ) and
similarly for W . Then U j ∩ U = 0 for j ∏ 2 by (iii), and hence U ⊆ U1 . By (iv),
U1 ∩ W = 0, so that W ⊆ U2 ⊕ · · · ⊕ Uk . By (i), U = U1 and W = U2 ⊕ · · · ⊕ Uk .
    12. Part (a) is immediate, and a basis for (b) consists of the union of bases for the
individual U j ’s. Part (f) is evident.
    For (d) and (e), since D is a linear combination of the E j ’s and each E j is a
polynomial in L, D is a polynomial in L, say D = P(L). Then N = L − P(L)
commutes with L, and this is (d). Applying the division algorithm to P, we have
P = AM + R with R = 0 or deg R < deg M. Evaluating at L gives D = P(L) =
A(L)M(L) + R(L) = R(L) since M(L) = 0. Thus R will serve in place of P if
deg P ∏ deg M. This proves the existence in (e) of the polynomial for D. Since
N = L − D, N is a polynomial in L, and again we can take this polynomial to be
0 or to have degree < deg M. This proves the existence in (e) of the polynomial for
N . For uniqueness if P1 is a second polynomial that yields D, then 0 = D − D =
P(L) − P1 (L) shows that P − P1 is a multiple of M, and the condition on the
degrees of P and P1 forces P − P1 = 0. So P is unique. Similarly the polynomial
representing N is unique. This completes the proof of uniqueness in (e).
    If Q j (X) is the polynomial (X − ∏0 )l j , then N l j = (L − D)l j = Q j (L) on U j , and
Theorem 5.19f shows that Q j (L) is 0 on U j . Therefore a power of N is 0 on each
U j , and N is nilpotent. This proves (c). Part (g) now follows.
644                             Hints for Solutions of Problems

    13. Each eigenvector of D must lie in some U j by Theorem 5.19e. If Vi is the
eigenspace of D with eigenvalue ci , it follows that Vi ⊆ U j (i) for some j = j (i).
Thus each U j is the sum of full eigenspaces of D. Property (d) forces N to carry Vi
into itself. By (c), (L − D)n is 0 on Vi for n = dim V ; hence (L − ci I )n is 0 on
Vi . Since Vi ⊆ U j , (L − ∏ j I )n is 0 on U j . Application of Problem 10 to L − ci I
shows that L − ∏ j I is nonsingular on Vi if ci 6= ∏ j , in contradiction to the fact that
(L − ∏ j I )n is 0 on U j , and therefore ci = ∏ j . The conclusion is that Vi = U j (i) , and
the desired uniqueness follows.
    A slightly shorter argument is available if one takes the constructive proof of
existence of a decomposition L = D + N as known, so that Problem 12 is available
for that decomposition. If there is a second decomposition L = D 0 + N 0 satisfying
(a) through (d), then D 0 and N 0 commute with L and hence with all polynomials in
L. Thus they commute with D and N . The equality L = D + N = D 0 + N 0 implies
D − D 0 = N 0 − N . Problem 5a shows that D − D 0 has a basis of eigenvectors, and
N 0 −N is nilpotent because the commutativity of N and N 0 shows that the the Binomial
Theorem applies, in view of Problem 15 in Chapter I. Thus D − D 0 = N 0 − N = 0.
                                                                           0
    14. In (a), Lemma 5.22 says that det(X I − N 0 ) = X n . Consequently
                                                                0
det(X I − (N 0 + cI )) = det((X − c)I − N 0 ) = (X − c)n .
    In (b), form the primary decomposition of L as in Theorem 5.19, and let notation
be as in Problem 12. On the subspace U j , which is carried to itself by L, L = D + N
acts as ∏ j I + N , and the characteristic polynomial on that subspace is (X − ∏ j )n j ,
by (a). On the whole space V , the characteristic polynomial of L is the product
of the contributions from each U j , since as a consequence of Proposition 5.11, the
determinant of a block diagonal matrix is the product    Q     of the determinants of the
blocks. Therefore L has characteristic polynomial nj=1 (X − ∏ j )n j , and this matches
the characteristic polynomial of D.
    15. The characteristic polynomial is X 2 − 2X + 1 = (X − 1)2 . Since A − I 6= 0,
the minimal                              2
      ≥ ¥ polynomial is (X − 1) rather than X − 1. Thus the Jordan               ≥ form
                                                                                      ¥      is
        11                                                                        3/2
J =          . Solving shows that ker(A − I ) consists of the multiples of 1 . Use
≥ ¥ 01                                                        ≥ ¥                ≥ ¥
  3                                                            3                  1
  2
      as the  first column   of C, and  solve (A   −  I )X =   2
                                                                    to get   X =  1
                                                                                      as one
                                               ≥ ¥                ≥       ¥
                                                   31      −1        1 −1
answer for the second column. Then C = 2 1 , C = −2 3 , and one readily
checks that C −1 AC = J .
   16. The characteristic polynomial is P(X)                       3
                                               ∂ det(X I − A) = X . Thus A is
                                        µ 0 1 0=
nilpotent, and in fact A2 = 0. Then J = 0 0 0 , and the computation proceeds as
                                          000                         
                                        µ 4 1 −1 ∂                   1
                                                                0 0 8
                                                                      
in Example 1 in Section 7, yielding C = −8 0 4 and C −1 =  1 14 − 14 .
                                                  80    0                     1   1
                                                                            0 4   4
    17. The characteristic polynomial is (X − 2)6 (X − 3) by inspection. Thus there
is a primary subspace for X − 2 with dimension 6 and a primary subspace for X − 3
                                      Chapter V                                    645

with dimension 1. For the Jordan form let K j = ker(A − 2I ) j . By raising A − 2I
to powers and row reducing, we see that dim K 3 = 6, dim K 2 = 5, and dim K 1 = 3.
We do not have to proceed beyond K 3 since we have reached the full dimension 6 of
the primary subspace for X − 2. Therefore the number of Jordan blocks for X − 2 of
size ∏ 3 is 6 − 5 = 1, of size ∏ 2 is 5 − 3 = 2, and of size ∏ 1 is 3. Hence there is
one block of each size 1, 2, and 3, and
                                     2 1 0       
                                          21
                                          2
                                                            
                                                           
                                   J =        21           .
                                                           
                                               2           
                                                    2
                                                        3

Solving (A − 3I )X = 0, we find that the eigenvectors for eigenvalue 3 are the
multiples of (5, 2, 2, 3, 2, 1, 1). Thus this vector can be taken to be the last column
of C.
   The next step is to express K 1 , K 2 , and K 3 explicitly in terms of parameters by
using the standard solution procedure for systems of homogeneous linear equations.
The result is that
                  x1                    x1                   x1 
                 
                    x2   
                                          
                                              x2 
                                                                   
                                                                      x2 
                 
                                                                   
                  x3                  
                                           
                                             x3                  
                                                                          
                                                                      x3 
                   
          K1 =   ,  0                    
                                    K2 =  4  ,
                                               x                    
                                                             K3 =  4  .
                                                                       x  
                 
                                                                      
                  0 
                          
                           
                                           
                                            x5 
                                                                   x5 
                                                                          
                  0                       0                     x6 
                                                                          
                     0                         0                      0

Following the method of Example 1 in Section 7, we choose W2 such that K 3 =
K 2 ⊕ W2 , and then we form U1 = (A − 2I )(W2 ):
                         0                     0 
                        
                           0
                               
                                                 
                                                     x6 
                                                          
                         
                                                      
                         0                    
                                                   0 
                                                          
                          
                  W2 =  0         and                
                                              U1 =  x 6  .
                        
                          0 
                                                 
                                                    x6 
                                                          
                                                       
                        
                         x6                    
                                                         
                                                          
                               0                                 0

We choose W1 such that K 2 = K 1 ⊕U1 ⊕ W1 , and we form U0 = (A −2I )(U1 + W1 ):
                     0                          x +2x 
                                                        4   6
                    
                         0 
                                                  
                                                         0
                                                               
                                                               
                                                           
                      
                     0                         x6 
                                                             
                                                              
                       
              W1 =  x4          and               
                                             U0 =  0  .     
                    
                       0                      
                                                     0     
                    
                     0     
                                                  
                                                              
                                                               
                                                        0    
                           0                                     0

Finally we choose W0 such that K 1 = K 0 ⊕ U0 ⊕ W0 . Here K 0 = 0, and we can
take W0 = {(0, x2 , 0, 0, 0, 0, 0)}.
646                             Hints for Solutions of Problems

   To form C we take a basis of each W j , apply powers of A−2I in turn to its members,
and line up the resulting columns, along with the eigenvector for eigenvalue 3, as C:
                                        2 0 0 1 0 0 5
                                      10 01 00 00 00 01 22 
                                                           
                                  C =
                                     0 1 0 0 1 0 3.
                                                            
                                     0 1 0 0 0 0 2
                                           0010001
                                           0000001

   18. In (a), if every prime-power factor of the minimal polynomial is of degree 1,
then the matrix is similar to a diagonal matrix, and the multiplicities of the eigenvalues
                                                                                        2
                                              ∂ If the minimal polynomial is (X −c) ,
can be seen from the characteristic polynomial.
                                       µ   c 10
then the matrix has to be similar to        . If the minimal polynomial instead is
                                           0c 0
                                           00c           µc 1 0∂
(X − c)2 (X − d), then the matrix has to be similar to 0 c 0 . If the minimal
                                                             0d
                                                            0µ      ∂
                                                               c 1c
                     3
polynomial is (X − c) , then the matrix has to be similar to 0 c 1 . There are no
                                                                   00c
other possibilities.
                                    
              0100              0100
   For (b),  0 0 0 0  and  0 0 0 0  both have minimal polynomial X 2 and charac-
              0001              0000
              0000              0000
teristic polynomial X 4 , but they are not similar because their ranks are unequal.
   19. If the diagonal entries are c and N denotes the strictly upper-triangular part,
                         P ° ¢
then J k = (cI + N )k = kj=0 kj ck− j N j . The term from j = 1 is not canceled by
any other term, and hence J k is not diagonal.
    20. Choose J in Jordan form and C invertible with J = C −1 AC. Then J n =
C An C −1 = CC −1 = I . By Problem 19, every Jordan block in J is of size 1-by-1.
Thus A is similar to a diagonal matrix D, and each diagonal entry of D must be an
n th root of unity. Any n-tuple of n th roots of unity can form the diagonal entries, and
the corresponding matrices are similar if and only if one is a permutation of the other.
    21. The minimal polynomial has to divide X (X 2 − 1) = X (X + 1)(X − 1).
Hence there is a basis of eigenvectors, the allowable eigenvalues being 1, −1, and 0.
A similarity class is therefore given by an unordered triple of elements from the set
{1, −1, 0}. There are three possibilities for a single eigenvalue, six possibilities for
one eigenvalue of multiplicity 2 and one of multiplicity 1, and one possibility with
all three eigenvalues present. So the answer is ten.
   22. If A2 = N and N n = 0, then A2n = 0. So A is nilpotent and An = 0. Since
N n−1 6= 0, A2n−2 6= 0. Therefore n > 2n − 2, and n = 1.
    23. If J is of size n, then the matrix C with Ci,n+1−i = 1 for 1 ≤ i ≤ n and
Ci j = 0 otherwise has C −1 J C = J t .
                                        Chapter V                                 647

    24. Choose C with C −1 AC = J in Jordan form. Problem 23 shows that there
is a block-diagonal matrix B with B −1 J B = J t . Then B −1 C −1 AC B = J t and
C t At (C −1 )t = J t . So B −1 C −1 AC B = C t At (C −1 )t , and the result follows.
   25. The matrices A and B have A2 = B 2 = 0 and hence are nilpotent. Since each
of A and B has rank 2, dim ker A = dim ker B = 2. The numbers dim ker Ak and
dim ker B k being equal for all k, the two matrices have the same Jordan form and are
therefore similar.
   26. If M(X) is the minimal polynomial of L, then M(L)v = 0. Hence M(X) is
in Iv . Then Proposition 5.8 shows that Mv (X) exists.
                                                                             Ø
   27. The polynomial Mv (X) has to divide the minimal polynomial of L ØP(v) , and
the latter has degree ≤ dim P(v). Hence deg Mv (X) ≤ dim P(v). If v, L(v), . . . ,
L deg Mv −1 (v) are linearly dependent, then there is a nonzero polynomial Q(X) of
degree ≤ deg Mv − 1 with Q(L)(v) = 0, and that fact contradicts the minimality
of the degree of Mv (X). Hence they are independent, and deg Mv (X) ∏ dim P(v).
Thus equality holds, and the linearly independent set is a basis. This proves (a) and
(b).                                                      Ø
   Since Mv (X) divides the minimal polynomial of L ØP(v) , which divides the char-
                            Ø
acteristic polynomial of L ØP(v) , and since the end polynomials have degree dim P(v),
these three polynomials are all equal. This proves (c).
   28. Use the ordered basis (L d−1 (v), L d−2 (v), . . . , L(v), v).
   29. Since P(X) is prime and does not divide Q(X), there exist polynomials
A(X) and B(X) with A(X)P(X) + B(X)Q(X) = 1. Using the substitution that
sends X to L and applying both sides to v, we obtain B(L)Q(L)(v) = v. Hence
P(Q(L)(v)) ⊇ P(v). Since the reverse inclusion is clear, the result follows.
   30. In (a), the base case of the induction is that dim V = deg P(X), and then
the result follows from Problem 27. For the inductive step, the same problem shows
that there must be a nontrivial invariant subspace U . Proposition 5.12 shows that
the minimal polynomial for U and V /U is P(X), and induction shows that the
characteristic polynomial for U and V /U is a power of P(X). Proposition 5.11 then
shows that the characteristic polynomial for V is a power of P(X).
   For (b), we induct on l, using (a) to handle the case l = 1. For general l, form
the invariant subspace U = ker P(X)l−1 , for which the minimal polynomial is some
P(X)r with r < l. The minimal polynomial of V /U is certainly P(X). By induction,
U and V /U have characteristic polynomials equal to powers of P(X), and Proposition
5.11 shows that the same thing is true for V .
   In (c), (b) says that the characteristic polynomial is of the form P(X)r for some
r. Then the degree of the characteristic polynomial is rd, where d = deg P(X).
   32–34. These are proved word-for-word in the same way as Lemmas 5.23 through
5.25 except that n is to be replaced by l and N is to be replaced by P(L).
648                             Hints for Solutions of Problems

   35. If Q(X) is in K[X], we successively apply the division algorithm to write

                         Q = A0 P + B0        with deg B0 < deg P,
                        A0 = A1 P + B1        with deg B1 < deg P,
                        A1 = A2 P + B2        with deg B2 < deg P,

etc., and then we substitute and find that

        Q = A0 P + B0 = A1 P 2 + B1 P + B0 = A2 P 3 + B2 P 2 + B1 P + B0
           = · · · = A j P j+1 + B j P j + · · · + B2 P 2 + B1 P + B0

with each Bi equal to 0 or of degree < deg P. The fact that W j ⊆ K j+1 implies that
P j+1 (L)(v) = 0. Consequently

P(v) = {(Bj P j + · · · + B1 P + B0 )(L)(v) | Bi = 0 or deg Bi < d for 0 ≤ i ≤ j},

and the given set spans P(v).
   For the linear independence suppose that some such expression is 0 with not all
Bi (X) equal to 0. Fix i as small as possible with Bi (X) 6= 0. Since P(L) j+1 (v) = 0,
Br (L)P(L)r (v) is annihilated by P(L) j−i if r > i. Application of P(L) j−i to the
dependence relation yields

      P(L) j−i (B j (L)P(L) j (v) + · · · + Bi+1 (L)P(L)i+1 + Bi (L)P(L)i )(v) = 0

and therefore also Bi (L)P(L) j (v) = 0. Since deg Bi < deg P, Problem 29 shows
that P(L) j (v) = 0. Therefore v is in K j . Since W j ∩ K j , we conclude v = 0,
contradiction.
   36. We show at the same time that it is possible to arrange for each U j and W j to
be such that K j + U j and K j + W j are invariant under L. We proceed by induction
downward on j. The construction begins with Ul−1 = 0 and Wl−1 chosen such
that K l = K l−1 ⊕ Wl−1 . Then we have L(Wl−1 ) ⊆ Wl−1 + K l−1 and L(Ul−1 ) ⊆
Ul−1 + K l−1 . Select some v1(l−1) 6= 0 in Wl−1 . If there is a polynomial B(X) 6= 0 with
deg B < deg P such that B(L)(v1(l−1) ) is in K l−1 , then it follows from Problem 29
and the invariance of K l−1 under L that v1(l−1) is in K l−1 , contradiction. So there is
no such polynomial, and the vectors v1(l−1) , L(v1(l−1) ), . . . , L d−1 (v1(l−1) ) are linearly
independent with span T1(l−1) such that K l−1 + T1(l−1) is a direct sum.
   If K l−1 + T1(l−1) 6= K l , then we form v2(l−1) and T2(l−1) in the same way. If
there is a polynomial B(X) 6= 0 with deg B < deg P such that B(L)(v2(l−1) ) is in
K l−1 + T1(l−1) , then Problem 29 shows that v2(l−1) is in K l−1 + T1(l−1) , contradiction.
We conclude that K l−1 + T1(l−1) + T2(l−1) is a direct sum. Continuing in this way,
                                        Chapter V                                      649

we obtain enough linearly independent vectors to have a basis for a complement
Wl−1 = T1(l−1) + T2(l−1) + · · · to K l−1 .
   Now suppose inductively in the construction of U j and W j that j ≤ l − 2
and that U j+1 + K j+1 and W j+1 + K j+1 are invariant under L. We define U j =
P(L)(U j+1 ⊕ W j+1 ), and the assumed invariance implies that U j + K j is invariant
under L. We now construct W j in the same way that we constructed Wl−1 , insisting
                                               ( j)
that (U j + K j ) ∩ W j = 0. If we choose v1 in K j+1 but not U j + K j , then the
                                                          ( j)   ( j)                  ( j)
invariance of U j + K j under L implies that the vectors v1 , L(v1 ), . . . , L d−1 (v1 )
                                                    ( j)                          ( j)
are linearly independent and their linear span T1 is such that U j + K j + T1 is a
direct sum. Continuing in this way, we obtain the required basis of a complement W j
to K j ⊕ U j .
                                                      ( j)
    37. Problem 36 arranges that the vectors L r (vi j ) for 0 ≤ r ≤ d − 1 and all i j
form a basis of W j . We show by induction downward for j ≤ l − 1 that the vectors
            ( j+k)
L r P(L)k (vi j+k ) for 0 ≤ r ≤ d − 1, k > 0, and all i j+k form a basis of U j . This
holds for j = l − 1 since Ul−1 = 0. If it is true for j + 1, then U j+1 ⊕ W j+1 has a
                                     ( j+1+k)
basis consisting of all L r P(L)k (vi j+1+k ) for 0 ≤ r ≤ d − 1, k ∏ 0, and all i j+1+k .
Since Problem 33 shows that P(L) is one-one from U j+1 ⊕ W j+1 onto U j , U j has a
                                        ( j+1+k)
basis consisting of all L r P(L)k+1 (vi j+1+k ) for 0 ≤ r ≤ d −1, k ∏ 0, and all i j+1+k ,
                    ( j+k)
i.e., all L r P(L)k (vi j+k ) for 0 ≤ r ≤ d − 1, k > 0, and all i j+k . This completes the
induction.
                                                                              ( j)
    38. Problem 35 gives a basis for the cyclic subspace generated by vi j , Problem
37 shows that the members within Ui ⊕ Wi of the union of these bases, as j and i j
vary, form a basis of Ui ⊕ Wi , and Problem 34 allows us to conclude that as i varies,
we obtain a basis of V .
    39. Because of the linear independence proved in Problem 38, the left side of the
formula in question equals the number of vectors vi(k)    in any Wk with k ∏ j, which
         P                                              k
equals k∏ j (dim Wk )/d. Iterated application of Problem 33 gives

   dim K j+1 − dim K j = dim U j + dim W j = dim U j+1 + dim W j+1 + dim W j
                                P
                       = · · · = k∏ j dim Wk ,

and the result follows.
   40. The minimal polynomial for any cyclic subspace must divide the minimal
polynomial for V and hence must be a power of P(X). Problem 28 shows that the
restrictions of L to any two cyclic subspaces with the same minimal polynomial are
isomorphic. Hence the decomposition into cyclic subspaces will be unique up to
isomorphism as soon as it is proved that the number of cyclic direct summands with
minimal polynomial of the form P(X)k with k ∏ j +1 equals (dim K j+1 −dim K j )/d.
   Suppose that V is the direct sum of cyclic subspaces Ci , with vi as the generator
of Ci . Since each Ci is invariant under L, each K r is the direct sum of the subspaces
650                                      Hints for Solutions of Problems

K r ∩ Ci . Thus
                                              X°                                        ¢
              dim K j+1 − dim K j =                    dim(K j+1 ∩ Ci ) − dim(K j ∩ Ci ) .
                                               i


    If P(X)k is the minimal polynomial of Ci , it is enough to show that the right side
of this displayed formula equals d if k ∏ j + 1 and equals 0 if k ≤ j. By Problem
35, Ci has a basis consisting of all vectors L r P(L)s (vi ) with 0 ≤ r ≤ d − 1 and
0 ≤ s ≤ k − 1. The nonzero vectors among the L r P(L)s+ j+1 (vi ) are still linearly
independent; these are the ones with s + j + 1 < k, i.e., s < k − j − 1. The vectors
L r P(L)s (vi ) that are not sent to 0 by P(L) j+1 are a basis of K j+1 ∩ Ci . These are
the ones with s ∏ k − j − 1. This is the full basis of Ci if j + 1 > k, and there are
d( j + 1) such vectors if j + 1 ≤ k. Thus
                                                   Ω
                                                       dk              if j + 1 > k,
                         dim K j+1 ∩ Ci =
                                                       d( j + 1)       if j + 1 ≤ k.

Similarly                                               Ω
                                                             dk     if j > k,
                                    dim K j ∩ Ci =
                                                             dj     if j ≤ k.
Subtracting and taking the cases into account, we see that
                                                                   Ω
                                                                       d    if j + 1 ≤ k,
                   dim(K j+1 ∩ Ci ) − dim(K j ∩ Ci ) =
                                                                       0    otherwise.
               ≥                   ¥     ≥                   ¥
                     cos t sin t
   41. (a)        − sin t cos t
                                    , (b) cosh  t
                                           sinh t
                                                    sinh t
                                                    cosh t
                                                              , (c) the diagonal matrix with diagonal
entries   ed1 , . . . , edn .
    42. Suppose that J has diagonal entry c. Let N be the strictly upper-triangular
                                                                             1 2 2
part of J . Then et J = etcI +t N = etc et N . Here et N = I + t N + 2!       t N + ··· +
   1      n−1   n−1          n
(n−1)!  t     N     since  N   = 0.  The powers   of N  were  observed  to have the diagonal
of 1’s move one step at a time up and to the right.
           d tA
   43.       (e v) = (Aet A )v = A(et A v).
          dt
    44. Suppose that y(t) is a solution. The product rule for derivatives is valid in this
                                              d                 d
situation by the usual derivation. Hence dt     (e−t A y(t)) = dt (e−t A )y(t)+e−t A y 0 (t) =
−e  −t A  Ay(t)+e  −t A  0       −t A              0
                        y (t) = e (−Ay(t)+y (t)). The right side is 0 since y(t) solves
                                      d
the differential equation. Since dt     (e−t A y(t)) = 0, each component of e−t A y(t) is
constant. Thus for a suitable vector v of complex constants, e−t A y(t) = v, and the
conclusion is that y(t) = et A v.
   45. The first formula follows by making a term-by-term calculation with the
defining series. Multiplication of C has to be interchanged with the infinite sum, and
                                                      Chapter VI                                                      651

similarly for C −1 , but these operations are simply the operations of taking certain
linear combinations of limits.
                                d
   Suppose that z(t) satisfies dt z(t) = (C −1 AC)z(t) and z(0) = u. Multiplying
              d                                                   d
by C gives dt   C z(t) = AC z(t). Thus y(t) = C z(t) satisfies dt   y(t) = Ay(t) and
y(0) = C z(0) = Cu. We can invert the correspondence by using C −1 .
                                                                         µ3 1 0∂
   46. Example 3 in Section 7 says that C −1 AC = J holds for J = 0 3 0 and
       µ −1 1 0 ∂                     µ1∂       µ 0 −1 0 ∂ µ 1 ∂    µ −2 ∂ 0 0 2
C =      −1 0 0  . Define u = C    −1   2   =     1 −1 0     2   = −1 . Problems
          −1 0 1                                       3              0 −1 1             3        1
                                       d
42–43 show that the unique solution of dt z(t) = J z(t) with z(0) = u is z(t) = et J u.
                                                d
Problem 45 shows that the unique solution to dt   y(t) = Ay(t) with y(0) = Cu =
µ1∂
  2 is y(t) = C z(t) = Cet J u. By Problem 42, this is
  3

               ≥ −1 1 0 ¥ µ e3t       0 0
                                             ∂µ1   t 0
                                                           ∂≥
                                                                 −2
                                                                      ¥       ≥ −1 1 0 ¥ µ e3t te3t 0
                                                                                                        ∂≥
                                                                                                             −2
                                                                                                                  ¥
      y(t) =       −1 0 0          0 e3t 0        010            −1       =      −1 0 0      0 e3t 0         −1
                   −1 0 1          0 0 e2t        001             1              −1 0 1      0 0 e2t          1
               µ                        ∂≥        ¥        µ                     ∂
                   −e3t −te3t +e3t 0         −2                   e3t +te3t
          =        −e3t −te3t      0         −1       =          2e3t +te3t          .
                   −e3t −te3t e2t             1                2e3t +te3t +e2t




                                                  Chapter VI

   1. In (a), the linear function ϕ : V → V 0 given by ϕ(v) = hv, · i has kernel
equal to the left radical of the bilinear form, hence 0. Therefore ϕ is one-one,
and dim image ϕ = dim V = dim V 0 . Since dim V 0 < ∞, ϕ is onto V 0 . In (b),
v 7→ (v, · ) is a linear functional and by (a) is of the form (v, u) = hw, ui for some
unique w depending on v. Set w = L(v). The uniqueness shows that L(v1 + v2 ) =
L(v1 ) + L(v1 ) and L(cv) = cL(v). Hence L is linear.
   2. Since M t AM would have to be nonsingular,
                                               ≥ the ¥ only possibility would be
M t AM   equal to the identity. Writing M −1 as ac db , we obtain the conditions
a + c = b + d = 0 and ab + cd = 1. A check of cases shows that these have no
solution.        ≥     ¥
                            −1 1
   3. Take M =               11
                                    .
   5. Define (a + bi)w = aw + b J (w) for a and b real. The crucial property to
show in order to obtain a complex vector space is that ((a + bi)(c + di))(w) =
(a + bi)((c + di)w); expansion of both sides shows that both sides are equal to
(ac − bd)w + (bc + ad)J (w) since J 2 = −I . Thus W = VR for a suitable V .
   Next define (v, w) = hJ (v), wi + ihv, wi. This is bilinear over R. It is complex
linear in the first variable because (J (v), w) = hJ 2 (v), wi+ihJ (v), wi = −hv, wi+
652                               Hints for Solutions of Problems

ihJ (v), wi = i(v, w). It is Hermitian because (w, v) = hJ (w), vi − ihv, wi =
hJ 2 (w), J (v)i − ihw, vi = −hw, J (v)i − ihw, vi = hJ (v), wi + ihv, wi = (v, w).
    6. For (a), U isotropic implies U ⊥ ⊇ U . If v is a vector in U ⊥ but not U , then
U ⊕Kv is isotropic. Maximality thus implies that U ⊥ = U . Proposition 6.3 says that
dim V = dim U + dim U ⊥ , and we conclude that dim V = 2 dim U . So dim U = n.
    The proof of (b) goes by induction on the dimension, the base case being dimen-
sion 2, where there is no problem. Assuming the result for spaces of dimension less
than dim V , let S1 be maximal isotropic in V , so that dim S1 = 12 dim V by (a). Fix a
basis {v1 , . . . , vn } of S1 . Choose u 1Øwith hv1 , u 1 i = 1; this exists by nondegeneracy.
Put U = Kv1 ⊕ Ku 1 . Then h · , · iØU ×U is evidently nondegenerate, and Corollary
6.4 shows that V = U ⊕ U ⊥ . Certainly S1 ∩ U ⊥ is an isotropic subspace of U ⊥ .
It contains the n − 1 linearly independent elements v j − hv j , u 1 iv1 for 2 ≤ j ≤ n
and hence has dimension ∏ n − 1. Therefore it is maximal isotropic. By induction,
there is a maximal isotropic subspace T of U ⊥ with (S1 ∩ U ⊥ ) ∩ T = 0. Put
S2 = T ⊕ Ku 1 . Since hu 1 , U ⊥ i = 0, hu 1 , T i = 0. Therefore S2 is isotropic, hence
maximal isotropic in V . Suppose that the element t + cu 1 of S2 lies in S1 . From
hv1 , t + cu 1 i = 0, v1 ∈ U , t ∈ U ⊥ , and hv1 , u 1 i = 1, we obtain c = 0. Then t + cv1
lies in (S1 ∩ U ⊥ ) ∩ T , which is 0. We conclude that S1 ∩ S2 = 0.
                                                                     Ø S1 is maximal isotropic
    For (c), if h · , s2 i is the 0 function on S1 , then the fact that
implies that s2 = 0. Therefore the mapping s2 7→ h · , s2 iØ S1 is one-one. A count of
dimensions shows that it is onto S10 .
    In (d), choose any basis { p1 , . . . , pn } of S1 , and let {q1 , . . . , qn } be the dual basis
of S10 , which has been identified with S2 by (c).
                                        L
    7. In (a), first suppose that h : s Us → V is given. Then hi s is in HomK (Us , V ),
and the map from left to right may be taken to be h 7→ {hi s }s∈S . Next suppose             L that
h s : Us → L    V is given for each s. Then the universal mapping property of s Us
supplies h : s Us → V with hi s = h s for all s. The map from right to left may be
taken as {h s }s∈S 7→ h. These two maps invert each other.
    In (b), first suppose that h s : U → Vs is given for each Q             s. Then the universal
mapping property of the direct product produces h : U → s Vs . Q                      The map from
right to left may be taken as {h s }s∈S 7→ h. Next suppose that h : U → s Vs is given.
Q ps h is in HomK (U, Vs ) for each s ∈ S. Consequently the S-tuple {h s }s∈S is in
Then
   s HomK (U, Vs ). Then the map from left to right can be taken as h 7→ { ps h}s∈S .
These two maps invert each other.
    For (c), we treat (a) and (b) separately. In the case of    L(a), take S countably infinite
with each Us =LK and with V = K. Then HomK ( s∈S Us , V ) has uncountable
dimension and s∈S HomK (Us , V ) has countable dimension.
    In Lthe case of (b), take S to be countably infinite with each Vs = K and with
U = s∈S L      Vs . Each member of HomK (U, Vs0 ) has its values in Vs0 , and hence each
member of s HomK (U, Vs ) has L                                      L Vs . On the other hand,
                                           its values in finitely many
the identity function from U into s Vs is in HomK (U, Vs ) and takes values in
all Vs ’s.
                                                Chapter VI                                              653

    8. For (a), we have g1 (g2 (x)) = g1 (g2 xg2t ) = g1 g2 xg2t g1t = (g1 g2 )x(g1 g2 )t =
P1 g2 )(x). If x isPalternating, thenP
(g                                                (gxg t )t = gx t g t P  = −gxg t , and (gxg t )ii =
   j,k gi j x jk gik =      j<k gi j x jk gik +      j>k gi j x jk gik =      j<k gi j (x jk − x jk )gik = 0;
hence gxg t is alternating. If x is symmetric, then (gxg t )t = gx t g t = gxg t , and gxg t
is symmetric.
    For (b), certainly x and gxg t have the same rank if g is nonsingular. Theorem
6.7 shows that an alternating matrix x can be transformed by some                   ≥ nonsingular
                                                                                             ¥           g to
                    t                                                                     01
a matrix gxg that is block diagonal with k blocks of the form −1 0 , where 2k is
the rank, followed by 0’s down the diagonal. This proves that any two alternating
matrices of the same rank lie in the same orbit. It also gives an example of a matrix
in each orbit.
    For (c), certainly x and gxg t have the same rank if g is nonsingular. The Principal
Axis Theorem (Theorem 6.5) shows that any symmetric matrix over C can be trans-
formed by some nonsingular g to a matrix gxg t that is diagonal, say with diagonal
entries d1 , . . . , dn . We may assume that d1 , . . . , dk are nonzero and the others are 0.
                                                                             −1/2            −1/2
Taking h to be the diagonal matrix with diagonal entries (d1 , . . . , dk , 0, . . . , 0)
                          t   t
and forming h(gxg )h , we obtain a diagonal matrix in the same orbit whose first
k diagonal entries are 1 and whose other diagonal entries are 0. As k varies, these
matrices have different ranks and hence lie in different orbits. They provide examples
of matrices in each orbit.
                                          °P 0             ¢          P 0
    9. In (a), the formula is TU V            i (u i ⊗ vi ) (u) =       i u i (u)vi , and we may assume
that {vi } is linearly independent. If this is 0 for all u, then the linear independence
of                               0                                                      0
P the0 vi ’s implies that u i (u) = 0 for all i and all u. Then all u i are 0, and hence
     (u
   i i   ⊗    v i ) = 0. Thus   T UV    is one-one.
    In (b), Problem 7a shows that it is enough to handle U = K. Thus we are to
show that K0 ⊗K V maps onto HomK (K, V ) ∼                  = V . One member of K0 is the identity
function 10 on K, and 10 ⊗ V certainly maps onto V .
    For (c), if U = V and if dim U is infinite, every member of the image of TUU has
finite rank, but HomK (U, U ) contains the identity function, which has infinite rank.
    In (d), let L : U1 → U and M : V → V1 be given, so that F(L , M) carrying
(U 0 ⊗K V ) to (U10 ⊗K V1 ) is given by F(L , M)(u 0 ⊗v) = L t (u 0 )⊗ M(v) and G(L , M)
carrying HomK (U, V ) to HomK (U1 , V1 ) has (G(L , M)(ϕ))(u 1 ) = M(ϕ(L(u 1 ))
Then
                                                            °                    ¢
            TU1 V1 F(L , M)(u 0 ⊗ v)(u 1 ) = TU1 V1 L t (u 0 ) ⊗ M(v) (u 1 )
                                               = L t (u 0 )(u 1 )M(v) = u 0 (L(u 1 ))M(v),
            G(L , M)TU V (u 0 ⊗ v)(u 1 ) = M((TU V (u 0 ⊗ v))(L(u 1 )))
                                               = M(u 0 (L(u 1 ))v) = u 0 (L(u 1 ))M(v).
The right sides are equal, and hence {TU V } is a natural transformation.
    In (e), the answer is no because the maps TU V need not be isomorphisms, according
to (c).
654                                  Hints for Solutions of Problems

   10. To see that 9(E) is a vector space, one has to verify that (l + l 0 )ϕ = lϕ + l 0 ϕ,
l(ϕ + ϕ 0 ) = lϕ + lϕ 0 , and (ll 0 )ϕ = l(l 0 ϕ), and these are all routine. If µ is in
HomK (E, F), then 9(µ) : HomK (L, E) → HomK (L, F) has to be given by left-
by-µ, and the key step is to show that 9(µ) is L linear, not merely K linear. For ϕ
in HomK (L, E) and l, l 0 in L, we have (9(µ)(lϕ))(l 0 ) = µ((lϕ)(l 0 )) = µ(ϕ(ll 0 )) =
(9(µ)ϕ)(ll 0 ) = (l(9(µ)ϕ))(l 0 ). Hence 9(µ)(lϕ) = l(9(µ)ϕ) as required. It is
routine to check that 9(1) = 1 and that µ → 9(µ) respects compositions, and
hence 9 is a functor.
   11. Let 0 = (v1 , . . . , vn ) be an ordered basis of E, 1 = (w1 , . . . , wm ) be
an ordered basis of F, and A = [Ai j ] be the matrix of L in these ordered bases.
Put 0R = (v1 , iv1 , . . . , vn , ivn ) and 1R = (w1 , iw1 , . . . , wm , iwm ). Then the
matrix
≥      of L R in¥these ordered bases is obtained by replacing Ai j by the 2-by-2 block
  Re Ai j − Im Ai j
  Im Ai j Re Ai j     .
   12. Let 01 = (u 1 , . . . , u m ) and 11 = (v1 , . . . , vn ), and put

      ƒ1 = (u 1 ⊗ v1 , u 1 ⊗ v2 , . . . , u 1 ⊗ vn , u 2 ⊗ v1 , . . . , u 2 ⊗ vn , . . . , u m ⊗ vn ).

Form ƒ2 from the ordered bases 02 and 12 similarly. Members of ƒ1 are indexed by
pairs (i, j) with 1 ≤ i ≤ m and 1 ≤ j ≤ n, and members of ƒ2 are indexed similarly
by pairs (r, s). Then C(r,s),(i, j) = Ari Bs j .
    13. Define F to be the vector space KU ⊕ KV , and let l be the linear map
l : F → T (E) given by l(U ) = Y and l(V ) = X 2 + XY + Y 2 . Let L be the
extension of l to an algebra homomorphism L : T (F) → T (E) with L(1) = 1. The
subalgebra in question is the image of L, and the affirmative answer to the question
comes by showing that L is one-one. It is enough to show that the basis elements
consisting of all iterated products U i1 ⊗ V j1 ⊗ U i2 ⊗ · · · ⊗ V jn are carried by L
        P independent elements. The image of this element is homogeneous of
to linearly
degree nk=1 (i k + 2 jk ), P
                           and it is enough to consider only those images with the same
homogeneity, i.e., with nk=1 (i k + 2 jk ) constant. A failure of linear independence
would Pmean that among these, the ones with the highest total power of X, namely         P
with nk=1 2 jk maximal, must cancel together. These terms are monomials with i k
                   P
factors of Y and      jk factors of X 2 , and all such monomials, being also monomials
in X and Y , are linearly independent.
    14. Let ∂ E : E → S(E) be the one-one linear map that embeds E as S 1 (E) ⊆
S(E), and define ∂ F similarly. The composition ∂ F ϕ is a linear map of E into the
commutative associative algebra S(F), and Proposition 6.23b yields a homomorphism
8 : S(E) → S(F) of algebras with identity such that ∂ F ϕ = 8 ∂ E . We take 8 as S(ϕ),
and this addresses (a). Part (c) is part of the construction of S(ϕ). For (b), it is plain that
S(1 E ) = 1 S(E) . For compositions, suppose that √ : F → G is linear and that S(√)
is formed similarly. Proposition 6.23b says that S(√ϕ) is the unique homomorphism
of S(E) into S(G) carrying 1 into 1 and satisfying ∂G √ϕ = S(√ϕ) ∂ E . On the other
hand, S(√)S(ϕ) is another homomorphism of S(E) into S(G) carrying 1 into 1, and
                                          Chapter VI                                         655

it satisfies ∂G (√ϕ) = (∂G √)ϕ = (S(√) ∂ F )ϕ = S(√)(∂ F ϕ) = S(√)(S(ϕ) ∂ E ) =
(S(√)S(ϕ))∂ E . Therefore S(√ϕ) = S(√)S(ϕ) by uniqueness, and S is a functor.
    15. The homomorphism 8         e carries each T n (E) into itself. Since 8       e carries
                                       e                     e    n
commutators into commutators, 8(I ) ⊆ I . Thus 8(T (E) ∩ I ) ⊆ T (E) ∩ I .           n

Also, 8e commutes with the symmetrizer operator and hence carries e          S n (E) into itself.
                                 e                              n
We are given the equation q 8(x) = 8q(x) on Øall of T (E). Since 8           e carries e S n (E)
                                                  e Ø
into itself, we can interpret this as saying that 8 e         is well defined, and then all the
                                                      S n (E)
assertions in the problem have been addressed.
    16. Fix an ordered basis and check the result directly for L’s that correspond
                                                                             V
to elementary matrices. The determinant and the scalar effect on dim E (E) both
multiply under composition, and the result follows.
    17. Part (a) is a consequence of uniqueness. The formula for (b) is 8(g)P(v) =
8(g −1 v) for v in Kn .
    18. For (a), take A to be the category of commutative associative algebras over K
with identity, V to be the category of vector spaces over K, and F : A → V to be
the forgetful functor that takes an algebra and retains only the vector-space structure.
If a vector space E is given, then (S, ∂) is taken to be (S(E), ∂ E ), where S(E) is the
symmetric algebra of E and ∂ E : E → F(S(E)) is the identification of E with the
first-order symmetric tensors.
    For (b), take V again to be the category of vector spaces over K. Define A to be the
category whose objects are pairs (A, F) in which A is an associative algebra over K
with identity and F is a vector subspace of A such that every element f of F has f 2 =
0 and whose morphisms ϕ ∈ Morph((A, F), (A1 , F1 )) are algebra homomorphisms
ϕ : A → A0 such that ϕ(F) ⊆ F1 . The functorØ       F : A → V is to take the pair (A, F)
to F and is to take the morphism ϕ to ϕ Ø F : F → F1 . If a vector space E is given,
                          V V                                     V           V V
we take (S, ∂) to be (( E, 1 E), ∂ E ), where ∂ E : E → 1 E = F( E, 1 (E)) is
the identification of E with the first-order alternating tensors.
    For (c), let the nonempty index set be J . Take V = C J and A = C. The functor
F : C → C J is the “diagonal functor” taking an object A to the J -tuple whose j th
coordinate is A for every j; this functor takes any morphism ϕ ∈ MorphC (A, A0 ) to
the J -tuple whose j th coordinate is ϕ for ` every j. The given E is to be a J -tuple of
objects {X j } j∈J , S is to be the coproduct j∈J X j , and ∂ : {X j } j∈J → F(S) is to be
the given J -tuple {i j } j∈J of morphisms of X j into X.
    19. Let L be the unique member of MorphA (S, S 0 ) given as corresponding to
 0
∂ in MorphV (E, F(S 0 )), i.e., satisfying F(L)∂ = ∂0 . Similarly let L 0 be the unique
member of MorphA (S, S 0 ) corresponding to ∂ in MorphV (E, F(S)), i.e., satisfying
F(L 0 )∂0 = ∂. Then L 0 L and 1 S are in MorphA (S, S) and have F(1 S )∂ = 1F(S) ∂ = ∂
and F(L 0 L)∂ = (F(L 0 )F(L))∂ = F(L 0 )(F(L)∂) = F(L 0 )∂0 = ∂. By uniqueness,
1 S = L 0 L. Similarly L L 0 = 1 S 0 .
    20. By definition, T A satisfies T A (L) = F(L)∂ for L ∈ MorphA (S, A). For
ϕ in MorphA (A, A0 ), we are to show that G(ϕ)(T A (L)) = T A0 (F(ϕ)(L)). Sub-
656                              Hints for Solutions of Problems

stitution from the definitions gives G(ϕ)(T A (L)) = F(ϕ)F(L)∂ = F(ϕ L)∂ and
T A0 (F(ϕ)(L)) = T A0 (ϕ L) = F(ϕ L)∂. These are equal, and hence {T A } is a natural
transformation. Since each T A is one-one onto by hypothesis, the system {T A } is a
natural isomorphism.
    21. The previous problem shows that F is naturally isomorphic to G and that F 0
is naturally isomorphic to G. Hence F is naturally isomorphic to F 0 . The hypotheses
of Proposition 6.16 are satisfied, and the conclusion is that the object S is isomorphic
in A to the object S 0 by a specific isomorphism described in the proposition.
    22. Let E and F be in Obj(V ), and let ϕ be in MorphV (E, F). Then ∂ F ϕ is in
MorphV (E, F(S(F))), and the universal mapping property of (S(E), ∂ E ) produces a
unique 8 in MorphA (S(E), S(F)) such that F(8)∂ E = ∂ F ϕ. We define S(ϕ) = 8.
There is no difficulty in checking that S(1 E ) = 1 S(E) . Let us check that if we are
given also √ in MorphV (F, G), then S(√)S(ϕ) = S(√ϕ). We know that S(√ϕ) is
the unique member of MorphA (S(E), S(G)) satisfying ∂G √ϕ = F(S(√ϕ)) ∂ E . On
the other hand, S(√)S(ϕ) is another member of MorphA (S(E), S(G)), and it satisfies
∂G (√ϕ) = (∂G √)ϕ = (F(S(√)) ∂ F )ϕ = F(S(√))(∂ F ϕ) = F(S(√))(F(S(ϕ)) ∂ E ) =
(F(S(√))F(S(ϕ)))∂ E = F(S(√)S(ϕ))∂ E . Therefore S(√ϕ) = S(√)S(ϕ) by
uniqueness, and S is a functor.
   23. Pfaff(J ) = 1 because the only nonzero term comes from τ = 1.
    24. The terms in which σ contains a 1-cycle are each 0 because the diagonal
entries of X are 0. The remaining terms in which σ contains some cycle of odd
length will be grouped in disjoint pairs that add to 0. If such a σ is given, choose
the smallest label 1, . . . , 2n that is moved by a cycle of odd length within σ , and let
τ be that cycle. Let σ 0 be the product of τ −1 and the remaining cycles of σ . The
resulting unordered pairs {σ, σ 0 } are disjoint. For theQ    indices i moved by Q τ , xi,σ (i) =
xi,τ (i) while xi,σ 0 (i) = xi,τ −1 (i) = −xτ −1 (i),i . Then τ (i)6=i xi,σ (i) = τ (i)6=i xi,τ (i)
                  Q                      Q                                    Q
and we obtain τ (i)6=i xi,σ 0 (i) = τ (i)6=i xi,τ −1 (i) = (−1)length τ τ (i)6=i xτ −1 (i),i =
               Q                                    Q                        Q
(−1)length τ τ (i)6=i xi,τ (i) = (−1)length τ τ (i)6=i xi,σ (i) = − τ (i)6=i xi,σ (i) . If
                                                 Q                Q
τ (i) = i, then xi,σ (i) = xi,σ 0 (i) . Thus i xi,σ (i) = − i xi,σ 0 (i) . Since sgn σ =
sgn σ 0 , the terms for σ and σ 0 sum to 0.
    25. If σ is good, let A0 consist of the smallest index in each cycle of σ , let A be
the union of all σ 2k (A0 ) for k ∏ 0, and let B be the union of all σ 2k+1 (A0 ) for all
k ∏ 0. Certainly A ∪ B = {1, . . . , 2n}, σ (A) = B, and σ (B) = A. We have to
prove that A ∩ B = ∅. If the intersection is nonempty, we have σ 2k (a0 ) = σ 2l+1 (a00 )
for some a0 and a00 in A0 . Possibly by increasing l by an even multiple of the order
of σ , we may assume that l ∏ k. Then σ 2(l−k)+1 a00 = a0 . This says that a00 and a0
lie in the same cycle. Being least indices in cycles, they must be equal. Then some
odd power of σ fixes a0 , and the cycle of σ whose least element is a0 must have odd
length, contradiction.
    The definitions of A and B in terms of A0 are forced by the conditions in the
statement of the problem, and therefore A and B are unique.
                                                 Chapter VI                                        657

     26. Since A ∪ B = {1, . . . , 2n} and A ∩ B = ∅, we have y(σ )z(σ ) =
Q2n                                                                      Q
    l=1 xi,σ (i) . The
                   Q   definitions of τ and τ 0 make y(σ ) = s(τ ) nk=1 xτ (2k−1),τ (2k) and
z(σ ) = s 0 (τ 0 ) nk=1 xτ 0 (2k−1),τ 0 (2k) . The construction has made the integers τ (2k −1)
increasing and has made the inequalities τ (2k − 1) < τ (2k) hold, and similarly for
τ 0 . This proves the desired equality, apart from signs.
                                                           Q2n
     27. The previous problem shows that (sgn σ ) l=1            xi,σ (i) equals
                                           n
                                           Y                        n
                                                                    Y
                 (sgn σ )s(τ )s 0 (τ 0 )         xτ (2k−1),τ (2k)         xτ 0 (2k−1),τ 0 (2k) .
                                           k=1                      k=1

Thus we want to see that

                            (sgn σ )s(τ )s 0 (τ 0 ) = (sgn τ )(sgn τ 0 ).                          (∗)

In proving (∗), we retain the step in which factors xi j of y(σ ) and z(σ ) are replaced
by x ji with a minus sign if j < i, but we may disregard the step in which the factors
are then rearranged so that τ and τ 0 can be defined. In fact, this rearranging does not
affect the signs of τ and τ 0 . The reason is that if ρ is in Sn and if ρ e in S2n is defined
by ρe(2k − 1) = 2ρ(k) − 1 and ρ       e(2k) = 2ρ(k), then sgn ρ      e = +1; it is enough to
check this fact when ρ is a consecutive transposition, and in this case ρ     e is the product
of two transpositions and is even.
    Turning to (∗), we first consider the case in which σ , when written as a disjoint
product of cycles, takes the integers 1, . . . , 2n in order. In this case we compute
directly that τ = 1, that s(τ ) involves no sign changes, and that τ 0 is the product of
cycles of odd length, with an individual cycle of τ 0 permuting cyclically all but the
last member of a cycle of σ . Thus τ 0 is even. In the adjustment of factors of z(σ ),
one minus sign is introduced because of each cycle in σ and comes from the last and
first indices in the cycle. Thus s 0 (τ 0 ) is (−1) p , where p is the number of cycles in σ ,
and this is also the value of sgn σ . Hence (∗) holds for this σ .
    A general σ is conjugate in S2n to the one in the previous paragraph. Thus it is
enough to show that if (∗) holds for σ , then it holds for σ 0 = (a a + 1)σ (a a + 1).
First suppose that σ (a) 6= a + 1 and σ (a + 1) 6= a. Then a factor of y(σ ) gets
replaced with a minus sign for σ if and only if it gets replaced for σ 0 , and similarly
for z(σ ). Hence s(τ ) and s 0 (τ 0 ) are unchanged in passing from σ to σ 0 . The effect
on τ and τ 0 , in view of the observation immediately after (∗), is to multiply each on
the left by (a a + 1). Thus sgn τ and sgn τ 0 are each reversed. Since sgn σ = sgn σ 0 ,
(∗) remains valid for σ 0 .
    Now suppose that σ (a) = a+1. We may assume that σ (a+1) 6= a since otherwise
σ 0 = σ . To fix the ideas, first suppose that a is in A. Then one factor in y(σ ) is
xa,a+1 , and the corresponding factor of y(σ 0 ) is xa+1,a . As a result τ is unchanged
under the passage from σ to σ 0 , but the number of minus signs contributing to s(τ )
is increased by 1 and s(τ ) is therefore reversed. Meanwhile, τ 0 is left multiplied by
(a a + 1), and s 0 (τ 0 ) is unchanged. Thus (∗) remains valid for σ 0 . If a instead is in
658                               Hints for Solutions of Problems

B, then the roles of τ and τ 0 are reversed in the above argument, but the conclusion
about (∗) is not affected. Finally suppose that σ (a + 1) = a and σ (a) 6= a + 1.
Then the argument is the same except that the number of signs contributing to s(τ )
or s 0 (τ 0 ) is decreased by 1. In any event, (∗) remains valid for σ 0 .
    28. What is needed is an inverse construction that passes from the pair (τ, τ 0 ) to
σ . Define ω ∈ S2n to be the commuting product of the n transpositions (2k − 1 2k)
for 1 ≤ k ≤ n.
    Assuming for the moment that we know that some index a is to be in A, we see
from the definitions above that b = σ (a) is to be given by b = τ (ω(τ −1 (a))) and b
is to be in B. If, on the other hand, we know that some index b is to be in B, then
σ (b) is to be given by τ 0 (ω(τ 0−1 (b))) and is to be in A. Thus the cycle within σ to
which a belongs has to be given by applying alternately τ ωτ −1 and then τ 0 ωτ 0−1 .
    The critical fact is that this cycle is necessarily even. In the contrary case we
would have τ ωτ −1 (τ 0 ωτ 0−1 τ ωτ −1 )k (a) = a for some k. If k = 2l, then this equality
gives (τ ωτ −1 τ 0 ωτ 0−1 )l (τ ωτ −1 )(τ 0 ωτ 0−1 τ ωτ −1 )l (a) = a, which we can rewrite as
(τ ωτ −1 )(τ 0 ωτ 0−1 τ ωτ −1 )l (a) = (τ 0 ωτ 0−1 τ ωτ −1 )l (a); this equation is contradictory
since τ ωτ −1 is a permutation that moves every index. If k = 2l + 1, then this equal-
ity gives (τ ωτ −1 )(τ 0 ωτ 0−1 τ ωτ −1 )l (τ 0 ωτ 0−1 )(τ ωτ −1 τ 0 ωτ 0−1 )l (τ ωτ −1 )(a) = a and
hence (τ 0 ωτ 0−1 )(τ ωτ −1 τ 0 ωτ 0−1 )l (τ ωτ −1 )(a) = (τ ωτ −1 τ 0 ωτ 0−1 )l (τ ωτ −1 )(a);
this equation is contradictory since τ ωτ −1 is a permutation that moves every index.
    What we know is that the smallest index in each cycle is to be in A. Thus we can
use this process to construct σ from (τ, τ 0 ), one cycle at a time. For the first cycle
the index 1 is to be in A; for the next cycle the smallest remaining index is to be in
A, and so on. We have seen that the constructed σ will be the product of even cycles,
and we can define A as the union of the images of the even powers of σ on the least
indices of each cycle, with B as the complement. In this way we have formed σ and
its disjoint decomposition {1, . . . , 2n} = A ∪ B, and it is apparent that τ and τ 0 are
indeed the permutations formed in the usual passage from σ to (τ, τ 0 ) via (A, B).
    29. It is enough to prove that ϕ|Vn : Vn → Vn# is an isomorphism for every n.
We establish this property by induction on n, the trivial case for the induction being
n = −1. Suppose that
                                        #
                       ϕ|Vn−1 : Vn−1 → Vn−1          is an isomorphism.                         (∗)
By assumption
                                             #
                grn ϕ : (Vn /Vn−1 ) → (Vn# /Vn−1 ) is an isomorphism.                         (∗∗)
If v is in ker(ϕ|Vn ), then (grn ϕ)(v + Vn−1 ) = 0 + Vn−1# , and (∗∗) shows that v

is in Vn−1 . By (∗), v = 0. Thus ϕ|Vn is one-one. Next suppose that v # is in Vn# .
By (∗∗) there exists vn in Vn such that (grn ϕ)(vn + Vn−1 ) = v # + Vn−1  # . Write
             #    #            #        #
ϕ(vn ) = v + vn−1 with vn−1 in Vn−1 . By (∗) there exists vn−1 in Vn−1 with
               # . Then ϕ(v − v             #
ϕ(vn−1 ) = vn−1               n    n−1 ) = v , and thus ϕ|Vn is onto. This completes
the induction.
                                               Chapter VI                                             659

   30. We define a product (Am /Am−1 ) × (An /An−1 ) → Am+n /Am+n−1 by

                       (am + Am−1 )(an + An−1 ) = am an + Am+n−1 .

This is well defined since am An−1 , Am−1 an , and Am−1 An−1 are all contained in
Am+n−1 . It is clear that this multiplication is distributive and associative as far as it
is defined. We extend the definition of multiplication to all of gr A by taking sums
of products of homogeneous elements, and the result is an associative algebra. The
identity is the element 1 + A−1 of A0 /A−1 .
   31. [x, x] = x x − x x = 0, and also [x, [y, z]] + [y, [z, x]] + [z, [x, y]] =
(x yz − x zy − yzx + zyx)+(yzx − yx z − zx y + x zy)+(zx y − zyx − x yz + yx z) = 0.
   32. In (a), let x and y be in g. Then we have

  [x, y]t A + A[x, y] = (x y − yx)t A + A(x y − yx)
           = y t x t A − x t y t A + Ax y − Ayx
           = y t (x t A + Ax) − x t (y t A + Ay) + (x t A + Ax)y − (y t A + Ay)x = 0.

Part (b) is the special case A = I .
    33. Uniqueness follows from the fact that 1 and ∂(g) generate U (g). For existence
let e
    L : T (g) → A be the extension given by the universal mapping property of T (g)
in Proposition 6.22. To obtain L, we are to show that eL annihilates the ideal I 00 . It is
enough to consider e                              00
                     L on a typical generator of I , where we have

   L(∂X ⊗ ∂Y − ∂Y ⊗ ∂X − ∂[X, Y ]) = e
   e                                      L(∂Y ) − e
                                     L(∂X)e              L(∂X) − e
                                                   L(∂Y )e       L(∂[X, Y ])
                                               = l(X)l(Y ) − l(Y )l(X) − l[X, Y ]
                                               = 0.

    34. First one proves the following: if Z 1 , . . . , Z p are in g and σ is a permutation
of {1, . . . , p}; then (∂Z 1 ) · · · (∂Z p ) − (∂Z σ (1) ) · · · (∂Z σ ( p) ) is in U p−1 (g). In fact, it
is enough to prove this statement when σ is the transposition of j with j + 1. In
this case the statement follows from the identity (∂Z j )(∂Z j+1 ) − (∂Z j+1 )(∂Z j ) =
∂[Z j , Z j+1 ] by multiplying through on the left by (∂Z 1 ) · · · (∂Z j−1 ) and on the right
by (∂Z j+2 ) · · · (∂Z p ).                                                                 P
    For the assertion in the problem, if we use all monomials with m jm ≤ p,
we certainly have a spanning set, since the obvious preimages in T (g) span
⊕k≤ p Tk (g). The result of the previous paragraph then implies inductively that the
monomials with monotone increasing indices suffice.
    35. We shall construct the map in the opposite direction without using the
Poincaré–Birkhoff–Witt Theorem, appeal to the theorem to show that we have
an isomorphism,
             L          and then compute what the map is in terms of a basis. Let
Tn (g) = nk=0 T k (g) be the n th member of the usual filtration of T (g). Define
660                           Hints for Solutions of Problems

Un (g) to be the image in U (g) of Tn (g) under the passage T (g) → T (g)/I 00 . Form
the composition

              Tn (g) → (Tn (g) + I 00 )/I 00 = Un (g) → Un (g)/Un−1 (g).

This composition is onto and carries Tn−1 (g) to 0. Since T n (g) is a vector-
space complement to Tn−1 (g) in Tn (g), we obtain an onto linear map T n (g) →
Un (g)/Un−1 (g). Taking the direct sum over n gives an onto linear map

                                 e : T (g) → gr U (g)
                                 √

that respects the grading.
   Let I be the two-sided ideal in T (g) such that S(g) = T (g)/I . It is generated
by all X ⊗ Y − Y ⊗ X with X and Y in T 1 (g). Let us show that the linear map
e : T (g) → gr U (g) respects multiplication and annihilates the defining ideal I
√
for S(g); then we can conclude that √ descends to an algebra homomorphism

                                 √ : S(g) → gr U (g)

that respects the grading.
   To do so, let x be in T r (g) and let y be in T s (g). Then x + I 00 is in Ur (g), and
                 e
we may regard √(x)     as the coset x + Tr−1 (g) + I 00 in Ur (g)/Ur−1 (g), with 0 in
all other coordinates of gr U (g) since x is homogeneous. Arguing in a similar
fashion with y and x y , we obtain

            e
            √(x) = x + Tr−1 (g) + I 00 , e
                                         √(y) = y + Ts−1 (g) + I 00 ,
                    and      e y) = x y + Tr+s−1 (g) + I 00 .
                             √(x

                             e
                        e √(y)
Since I 00 is an ideal, √(x)       e y). General members x and y of T (g) are
                                 = √(x
sums of homogeneous elements, and hence √    e respects multiplication.
                        e                                    e ⊇ I , it is enough
   Consequently ker √ is a two-sided ideal. To show that ker √
                   e
to show that ker √ contains all generators X ⊗ Y − Y ⊗ X . We have

               e ⊗ Y − Y ⊗ X) = X ⊗ Y − Y ⊗ X + T1 (g) + I 00
               √(X
                                      = [X, Y ] + T1 (g) + I 00
                                      = T1 (g) + I 00 ,

and thus √                                 e descends to a homomorphism √
          e maps the generator to 0. Hence √
as asserted.
   Finally we show that this homomorphism is an isomorphism. Let {X i } be
an ordered basis of g. We know that the monomials X ij11 · · · X ijkk in S(g) with
                                         Chapter VI                                  661
                          P
i 1 < · · · < i k and with m jm = n form a basis of S n (g). Let us follow the effect
of √ on such a monomial. A preimage of this monomial in T n (g) is the element

                      X i1 ⊗ · · · ⊗ X i1 ⊗ · · · ⊗ X ik ⊗ · · · ⊗ X ik ,

in which there are jm factors of X im for 1 ≤ m ≤ k . This element maps to the
monomial in Un (g) that we have denoted by X ij11 · · · X ijkk , and then we pass to the
quotient Un (g)/Un−1 (g). The Poincaré–Birkhoff–Witt Theorem shows that such
monomials modulo Un−1 (g) form a basis of Un (g)/Un−1 (g). Consequently √ is
an isomorphism.
   36. This is quite similar to Problem 33.
   37. This is similar to Problem 34.
   38. What is needed here is a description of a triple product of generators in
terms of permuting indices and replacing repeated pairs of indices by a scalar;
the description does not depend on the way that the parentheses are inserted in a
triple product, and then associativity follows. The details are omitted.
   39. Using the universal mapping property of Problem 36, construct an algebra
homomorphism L : Cliff(E, h · , · i) → C carrying 1 into 1 and extending the
mapping ei 7→ ei . Since the ei ’s and 1 generate C , L is onto C . Problem 37
shows that dim Cliff(E, h · , · i) ≤ 2n , and we know that dim C = 2n . Since L is
onto, L must be one-one, as well as onto.
   40. This is similar to Problem 35. The substitute for the Poincaré–Birkhoff–
Witt Theorem is the fact established by Problem 39 that the spanning set of 2n
elements in Problem 37 is actually a basis.
   41. The matrix that corresponds to X 0 has r = −2.
   42. To see that e   ∂ has the asserted properties, form the quotient map
T (H (V )) → T (V ) by factoring out the two-sided ideal generated by X 0 − 1. The
composition T (H (V )) → W (V ) is obtained by factoring out the two-sided ideal
generated by X 0 −1 and all u⊗v−v⊗u−hu, vi1, hence by all u⊗v−v⊗u−hu, viX 0
and by X 0 − 1. Thus T (H (V )) → W (V ) factors into the standard quotient map
T (H (V )) → U (H (V )) followed by the quotient map of U (H (V )) by the ideal
generated by X 0 − 1. By uniqueness in the universal mapping property for
universal enveloping algebras, e  ∂ is given by factoring out by X 0 − 1.
   43. Let P be the extension of ϕ to an associative algebra homomorphism of
U (H (V )) into A. Then P(X 0 ) = 1 since ϕ(X 0 ) = 1. The previous problem shows
that P descends to W (V ), i.e., that there exists eϕ with P = e    ϕ ◦e∂. Restriction to
V gives ϕ = e ϕ ◦ ∂.
   44. This is immediate from Problem 42 and the spanning in Problem 34.
   46. The linear combination L j = ϕ( p j ) + 2πϕ(q j ) of the two given linear
                                                                               2
mappings ϕ( p j ) = @/@ x j and ϕ(q j ) = m j replaces P(x) in e−π|x| P(x) by
662                                   Hints for Solutions of Problems

@ P/@ x j . Take a nonzero e−π|x| P(x) in an invariant subspace U , let x1k1 · · · xnkn be
                                          2


a monomial of maximal total degree in P(x), and apply L k11 · · · L knn to e−π|x| P(x)
                                                                                                          2

                           2
to see that e−π|x| is in U . Then apply products of powers of the various m j ’s to
this to see that all of V is contained in U .
                                                            °            2¢                             2
    47. Let ri = pi + 2πqi , so that ϕ(ri ) Pe−π|x| = (@ P/@ xi )e−π|x| . It is
enough to prove that no nontrivial linear combination of the members of the
spanning set q1k1 · · · qnkn r1l1 · · · rnln maps to 0 under e          ϕ . Let a linear combination
of such terms map to 0 under e                    ϕ . Among all the terms that occur in the lin-
ear combination with nonzero coefficient, let (L 1 , . . . , L n ) be the largest tuple
of exponents (l1 , . . . , ln ) that occurs; here “largest” refers to the lexicographic
ordering taking l1 first, then l2 , and so on. Put P(x1 , . . . , xn ) = x1L 1 · · · xnL n . If
                                                                                              °         2¢
(l1 , . . . , ln ) < (L 1 , . . . , L n ) lexicographically, then e       ϕ (r1l1 · · · rnln ) Pe−π|x| = 0.
                                              °       2¢
Thus e    ϕ (q1k1 · · · qnkn r1l1 · · · rnln ) Pe−π|x| is 0 if (l1 , . . . , ln ) < (L 1 , . . . , L n ) lexico-
graphically and equals x1k1 · · · xnkn L 1 ! · · · L n !e−π|x| if (l1 , . . . , ln ) = (L 1 , . . . , L n ).
                                                                     2


The linear independence follows immediately.
    48. This is similar to Problems 35 and 40. The key fact needed is the linear
independence established in the previous problem.
    52. In (a), for [a, b, c] to be alternating means that [a, a, c] = [a, b, a] =
[b, a, a] = 0. These say that (aa)c − a(ac) = (ab)a − a(ba) = (ba)a − b(aa) = 0.
For (b), [a, a, c] = [b, a, a] = 0 and the 3-linearity together imply that [a, b, a] =
[a, b, a]+[b, b, a] = [a +b, b, a] = [a +b, b, a]+[a +b, a, a] = [a +b, a +b, a] =
0.
    53. For (a), (1, 0)(c, d) = (c, d) and (a, b)(1, 0) = (a, b) directly from
the definition. Also, the definition (a, b)∗ = (a ∗ , −b) makes (1, 0)∗ = (1, 0),
(a, b)∗∗ = (a ∗ , −b)∗ = (a ∗∗ , b) = (a, b), and (c, d)∗ (a, b)∗ = (c∗ , −d)(a ∗ , −b) =
(c∗ a ∗ −bd ∗ , −c∗∗ b−a ∗ d) = ((c∗ a ∗ −bd ∗ )∗ , a ∗ d +cb)∗ = (ac−db∗ , a ∗ d +cb)∗ =
((a, b)(c, d))∗ .
    For (b), (c), and (d), we observe that
((a, b)(c, d))(e, f ) = (ac·e−db∗ ·e− f ·d ∗ a+ f ·b∗ c∗ , c∗ a ∗ · f −bd ∗ · f +e·a ∗ d+e·cb)

and
(a, b)((c, d)(e, f )) = (a·ce−a· f d ∗ −c∗ f ·b∗ −ed·b∗ , a ∗ ·c∗ f +a ∗ ·ed+ce·b− f d ∗ ·b),

and the results are immediate.
   In (e), (i) is the usual construction, and (ii) has 1 = (1, 0), i = (i, 0), j = (0, 1),
and k = (0, −i), with the identity of H written now as 1.
   54. For (a), (a, b)∗ +(a, b) = (a ∗ , −b)+(a, b) = (a ∗ +a, 0), which is a real mul-
tiple of (1, 0). Also, (a, b)(a, b)∗ = (a, b)(a ∗ , −b) = (aa ∗ + bb∗ , a ∗ (−b) + a ∗ b)
= (aa ∗ + bb∗ , 0), and this is a positive multiple of (1, 0) since aa ∗ and bb∗ are ∏ 0
and at least one of them is positive. A similar argument applies to (a, b)∗ (a, b).
                                         Chapter VI                                      663

   In (b), certainly (a, b) is bilinear over R, the expression for (a, b) is manifestly
symmetric, and we know that (a, a) = aa ∗ is ∏ 0 with equality only for a = 0.
   In (c), we are to prove that (x x)y = x(x y) and (yx)x = y(x x) in B . It is
enough to prove the first identity since application of ∗ to it gives the second
identity. We use (c, d) = (a, b) and substitute into the displayed formulas above
for Problem 53. We find that ((a, b)(a, b))(e, f ) equals

  (aa · e − bb∗ · e − f · b∗ a − f · b∗ a ∗ , a ∗ a ∗ · f − bb∗ · f + e · a ∗ b + e · ab)

and that (a, b)((a, b)(e, f )) equals

  (a · ae − a · f b∗ − a ∗ f · b∗ − eb · b∗ , a ∗ · a ∗ f + a ∗ · eb + ae · b − f b∗ · b).

Taking into account the associativity of A, we see that it is enough to show that
(bb∗ )e = e(bb∗ ), f b∗ (a + a ∗ ) = (a + a ∗ ) f b∗ , (bb∗ ) f = f (bb∗ ), and e(a + a ∗ ) =
(a + a ∗ )e. These all follow from the fact that A is nicely normed.
   55. Part (a) follows from (a) and (c) of the previous problem.
   In (b), we have (x x ∗ )y = (x(c1 − x))y = cx y − (x x)y = cx y − x(x y) =
x(cy − x y) = x((c1 − x)y) = x(x ∗ y). The equality x(yy ∗ ) = (x y)y ∗ follows by
applying ∗ and renaming the variables.
   In (c), use of (b) and the definitions of the norm and ∗ gives kabk2 a =
((ab)(ab)∗ )a = (ab)((ab)∗ a) = (ab)((b∗ a ∗ )a) = (ab)(b∗ (a ∗ a)) = kak2 ((ab)b∗ )
= kak2 a(bb∗ ) = kak2 kbk2 a .
   For (d), the norm equality of (c) implies that the R linear maps left-by-a and
right-by-a are one-one, and the finite dimensionality of O allows us to conclude
that they are onto. Hence they are invertible.
   For (e), use of (b) gives a(kak−2 a ∗ b) = kak−2 a(a ∗ b) = kak−2 (aa ∗ )b =
kak−2 kak2 b = b. This proves the result for left multiplication, and the argument
for right multiplication is similar.
   For (f), the table is as follows, with each entry representing the product of the
element at the left (the row index) by the element at the top (the column index):
   (1, 0)    (i, 0)     (j, 0)      (k, 0)     (0, 1)      (0, i)     (0, j)      (0, k)
   (i, 0)   −(1, 0)     (k, 0)     −(j, 0)    −(0, i)      (0, 1)    −(0, k)      (0, j)
   (j, 0)   −(k, 0)    −(1, 0)      (i, 0)    −(0, j)      (0, k)     (0, 1)     −(0, i)
   (k, 0)    (j, 0)    −(i, 0)     −(1, 0)    −(0, k)     −(0, j)     (0, i)      (0, 1)
   (0, 1)    (0, i)     (0, j)      (0, k)    −(0, 1)     −(0, i)    −(0, j)     −(0, k)
   (0, i)   −(1, 0)    −(k, 0)      (j, 0)     (0, i)     −(0, 1)    −(0, k)      (0, j)
   (0, j)    (k, 0)    −(1, 0)     −(i, 0)     (0, j)      (0, k)    −(0, 1)     −(0, i)
   (0, k)   −(j, 0)     (i, 0)     −(1, 0)     (0, k)     −(0, j)     (0, i)     −(0, 1)
   56. Although B is nicely normed, the steps of (b) in Problem 55 are not justified
for it because we cannot conclude that B is alternative. Since the argument for
(b) breaks down, so do the arguments for (c) and (d).
664                                  Hints for Solutions of Problems

                                             Chapter VII

   1. The only integer < 60 that is not the product of powers of at most two primes is
30. Thus Burnside’s Theorem assures us that the only possible order less than 60 for
a nonabelian simple group is 30. The integer 30 is of the form 2 pq with p = 3 and
q = 5, and q + 1 = 2 p. Part (b) of Problem 34 at the end of Chapter IV is applicable
and shows that the group has a subgroup of index 2; subgroups of index 2 are always
normal.
   2. For (a) and (b), (x yx −1 y −1 )−1 = yx y −1 x −1 is a commutator, and so is
a(x yx −1 y −1 )a −1 = (axa −1 )(aya −1 )(axa −1 )−1 (aya −1 )−1 .
   3. Let H be generated by a and b, and let K be generated by bab2 and bab3 .
Certainly K ⊆ H . Since bab2 and bab3 are in K , so is (bab2 )−1 (bab3 ) = b and
then so is (b−1 )(bab2 )(b−2 ) = a. Hence H ⊆ K .
   4. If H is characteristic, then in particular every inner automorphism x → gxg −1
carries H to itself, and H is normal. If ϕ : G → G is an automorphism and z is
in Z G , then the equality ϕ(z)ϕ(g) = ϕ(zg) = ϕ(gz) = ϕ(g)ϕ(z) and the fact that
ϕ is onto G show that ϕ(z) is in Z G . If √ : G → G is an automorphism, then
√(x yx −1 y −1 ) = √(x)√(y)(√(x))−1 (√(y))−1 shows that √ carries commutators
to commutators; hence √ carries the generated subgroup G 0 to itself.
   5. H8 , Z H8 , and {1} are characteristic. But the subgroups of order 4 are not,
because, for example, there exists an automorphism of H8 carrying i to j.
    6. Yes. The proof of Proposition 7.7, which takes S = G, gives a finite presenta-
tion.
                 µp       ∂≥ ¥µp              ∂−1 ≥ ¥              ≥ ¥
                    2 0                  2 0               −1
                              1 t                     1 t            1 t
    7. In (a), 0 p1           01        0 p1
                                                      01
                                                                =    01
                                                                            .
                       2                    2
                               ≥        ¥≥ s       ¥≥        ¥−1 ≥ s        ¥−1           ≥ −2s ¥
                                   01      e 0           01         e 0                     e     0
    In (b), we have also −1 0                   −s                       −s         =                   and
                                            0 e        −1 0         0 e                       0 e2s
µ          ∂≥ ¥µ               ∂−1 ≥ ¥              ≥     ¥          ≥        ¥  ≥      ¥         ≥        ¥
   p1   0              p1   0              −1
                10                    10              10                10          1 t              a 0
     2 p
                r 1
                         2 p
                                      r 1
                                                =     r 1
                                                           .   Thus     r 1
                                                                               ,    01
                                                                                          , and      0a −1
    0    2              0    2
                                    ≥ ¥          ≥        ¥≥        ¥≥           ¥                   ≥ ¥
                                      a  b          1  0      a  0       1  b/a                        a b
are in G 0 for a > 0. Since c d = c/a 1                          −1               ,  the   matrix      c d
                                                   ≥ ¥ ≥ 0 a¥          ≥0 1         ¥
                                                                          a+br b
is in G 0 if a > 0. If a < 0, we have ac db                  10
                                                                    =                 ;  if b  =
                                                                                               6    0, then
                                                             r 1     ≥ c+dr  ¥ d≥                ¥≥        ¥
                                                                                        a+br b         10
a + br > 0 for suitable r and therefore the equality ac db = c+dr d                                   −r 1
           ≥ ¥
              a b          0
exhibits c d as in G . Similarly if c 6= 0, then a + cr > 0 for suitable r and hence
≥ ¥≥ ¥ ≥                          ¥           ≥ ¥
  1r       a b        a+cr b+dr                  a b
  01       c d
                  =     c ≥d
                                    exhibits     c d
                                                       as in G 0 . Thus all members of G are in
                                    ¥                                                         ≥         ¥
                                                                                                 −1 0
G 0 except possibly for a0 a0−1 with a < 0. So it is enough to prove that                         0 −1
                                                                                                           is
                               ≥       ¥
                                   01
in G 0 . This follows since −1 0 has been shown to be in G 0 and has square equal to
≥         ¥
  −1 0
    0 −1
            .
                                           Chapter VII                                          665
                                               ≥           ¥
                                                   −1 0
   In (c), suppose that (x yx −1 )y −1 =            0 −1
                                                            . Then x yx −1 = −y. Taking the
trace of both sides and using the fact that Tr≥x yx −1¥ = Tr y, we see that Tr y = − Tr y
                            °r s ¢              a b
and Tr y = 0. Put x = t u and y = c −a , and substitute into the equality
x y = −yx. The entry-by-entry equations are ra + sc = −ra − tb, rb = −ub,
uc = −rc, and tb − ua = −sc + ua. The first and fourth equations together say
that 2ra = −tb − sc = −2ua. Thus we have (r + u)a = 0, (r + u)b =≥ 0, and              ¥
                                                                                   r s
(r + u)c = 0. Since at least one of a, b, c is nonzero, r + u = 0 and x = t −r .
Writing out the equality x y = −yx, we obtain the necessary and sufficient condition

                                       2ra = −sc − tb.                                          (∗)

The determinant conditions are −r 2 − st = 1 and −a 2 − bc = 1. Multiplying
(∗) by sc and substituting st = −1 − r 2 and bc = −1 − a 2 , we obtain 2rsac =
−s 2 c2 − (−1 − r 2 )(−1 − a 2 ) and then 0 = −s 2 c2≥− 2rsac          2  2
                                                            ¥ −1−a −r −r a =
                                                                               2 2
            2          2   2                          −1 0
−(ra + sc) − 1 − a − r , contradiction. Thus           0 −1
                                                             is not a commutator.
    8. By Proposition 7.8 the constructed group is a quotient of the group given by
generators and relations. We actually have an isomorphism if each element of the
group given by generators and relations is of the form b p a q with 0 ≤ p ≤ 2 and
0 ≤ q ≤ 8 because the group given by generators and relations then has order
≤ 27. Right multiplication by a carries this set to itself. Right multiplication by b
has b p a q b = b p b(b−1 a q b) = b p+1 (b−1 ab)q = b p (a 4 )q = b p a 4q , and this equals
                          0  0
a suitable element b p a q with 0 ≤ p0 ≤ 2 and 0 ≤ q 0 ≤ 8. Hence the group
defined by generators and relations has at most 27 elements, and we have the desired
isomorphism.
    9. Let Fn be free on x1 , y1 , . . . , xn , yn , let ϕ : Fn → Fn /Fn0 be the homomorphism
of Corollary 7.5, and let 9 : Fn → G n be the given quotient homomorphism. Then
ker ϕ ⊆ ker 9, and Proposition 4.11 shows that there exists a group homomorphism
√ : G n → Fn /Fn0 such that √ ◦ 9 = ϕ. Since Fn /Fn0 is abelian, √ factors as
√¯ ◦ q, where q : G n → G n /G 0n is the quotient and √¯ : G n /G 0n → Fn /Fn0 is a
homomorphism. Thus √¯ ◦ q ◦ 9 = ϕ. Since ϕ is onto, √¯ is onto; thus the image
of √¯ is isomorphic with Fn /Fn0 , which is free abelian of rank 2n. The group G n /G 0n
is abelian and has a generating set of 2n generators, thus is a homomorphic image
ξ : An → G n /G 0n , where An is free abelian with 2n generators. The composition
√¯ ◦ ξ is a homomorphism from a free abelian group of rank 2n onto a free abelian
group of rank 2n. Taking into account the proof of Theorem 4.46, we see that √¯ ◦ ξ
is one-one. Since ξ is onto G n /G 0n , √¯ is one-one. Therefore G n /G 0n is free abelian
of rank 2n.
    10. Let F be a free group of rank n, let q : F → F/F 0 be the quotient homo-
morphism, let x1 , . . . , xk with k < n be generators of F, let F        e = F({x1 , . . . , xk }),
and let 8 : F   e → F be the quotient homomorphism. The composition q ◦ 8
is a homomorphism of F      e onto the abelian group F/F 0 , and it factors through to a
666                             Hints for Solutions of Problems

homomorphism of F/  e Fe0 onto F/F 0 . Here the domain is abelian with k generators, and
the image is free abelian with n generators, and there can be no such homomorphism.
   11. For (a), we can use 1 and a. For (b), the proof of Theorem 7.10 says that we are
to multiply each of these by a, b, c on the right and take the H part of the result. The H
parts that are not 1 form a free basis. We have 1a = a and 1aρ(a)−1 = 1, 1b = ba −1 a
and 1bρ(b)−1 = ba −1 , 1c = c = ca −1 a and 1cρ(c)−1 = ca −1 , aa = a 2 1 and
aaρ(a 2 )−1 = a 2 , ab = ab1 and abρ(ab)−1 = ab, and ac = ac1 and acρ(ac)−1 =
ac. Thus a free basis of the generated subgroup is {ba −1 , ca −1 , a 2 , ab, ac}.
    12. The thing to prove, by induction on n, is that if a1 a2 · · · an is a reduced word
in variables u 0 , u 1 , u 2 , . . . and their inverses, and if we then substitute x k yx −k for
u k and reduce in terms of x, y, then the reduced form involves a total of n factors of
y or y −1 , the factor to the left of the first y or y −1 is x p if a1 = u ±1
                                                                            p , and the factor
to the right of the last y or y −1 is x −q if an = u q±1 .
     13. The remarks with Proposition 7.15 show that the reduced words in C2 ∗ C2 are
 all words whose terms are alternately x and y. Let H be a normal subgroup 6= {1}.
Then H contains a conjugate of a nontrivial such word. Form the shortest such word
6= 1 in H . If the word begins and ends with x and has length > 1, we can conjugate
 by x and reduce the length by 2; similarly if it begins and ends with y and has length
> 1, we can conjugate by y and reduce the length by 2. We conclude that the word
has length 1. Then H contains x or y and is a quotient of either hy; y 2 i or hx; x 2 i,
 which give C2 and {1}.
     Thus we may assume that a shortest nontrivial reduced word in H is a product
 x y · · · x y with 2n factors or a product yx · · · yx with 2n factors. Then G/H is a
 quotient of ha, b; a 2 , b2 , (ab)n i, and we saw in an example in Section 2 that this
 group is Dn . We readily check that all quotients of Dn are of the form {1}, C2 ,
C2 × C2 , and Dm for certain values of m ∏ 3.
    14. Argument #1: When the irreducible representations are all 1-dimensional,
Corollary 7.25 shows that the number of irreducible representations must be |G|, and
Corollary 7.28 shows that the number of conjugacy classes must be |G|. Therefore
each conjugacy class contains just one element, and G is abelian.
    Argument #2: Theorem 7.24 shows that the irreducible representations separate
points in G in the sense that for any pair x, y in the group, there is some irreducible
R with R(x) 6= R(y). When the irreducible representations are all 1-dimensional,
the multiplicative characters separate points. Since every multiplicative character is
trivial on the commutator subgroup, the commutator subgroup must be {1}. Then
every pair x, y has x yx −1 y −1 = 1 and x y = yx.
   15. This is immediate from Lemma 7.11.
    16. For (a), every cochain f has the property that m f = 0. Hence the same thing
is true of cocycles and of cohomology elements.
                                                Chapter VII                                                 667

   For (b), the cocycle condition for f says that

    (−1)n f (g1 , . . . , gn ) = g1 ( f (g2 , . . . , gn+1 ))
                                      n−1
                                      P
                                  +         (−1)i f (g1 , . . . , gi−1 , gi gi+1 , gi+2 , . . . , gn+1 )
                                      i=1
                                 + (−1)n f (g1 , . . . , gn−1 , gn gn+1 ).

Summing over gn+1 in G gives

   (−1)n |G| f (g1 , . . . , gn ) = g1 (F(g2 , . . . , gn ))
                                          n−1
                                          P
                                      +         (−1)i F(g1 , . . . , gi−1 , gi gi+1 , gi+2 , . . . , gn )
                                          i=1
                                      + (−1)n F(g1 , . . . , gn−1 ).

The right side we recognize as (δn−1 F)(g1 , . . . , gn ), which is the value of a cobound-
ary at (g1 , . . . , gn ). Therefore |G| f is a coboundary and becomes the 0 element in
H 2 (G, N ). Thus f , when regarded as an element of H 2 (G, N ) has order dividing
|G|.
    17. The two parts of the previous problem show that every element of H 2 (G, N )
is of finite order dividing both |G| and |G/N |. Since GCD(|G|, |G/N |) = 1, every
element of H 2 (G, N ) has order 1. Thus H 2 (G, N ) = 0, and the only extension is
the semidirect product.
    18. The only automorphism of C2 is the trivial automorphism, and therefore τ is
trivial. The two possibilities for G are C2 × C2 and C4 . With G = C2 × C2 , the
group E can be C2 × C2 × C2 or H8 , and with G = C4 , E can be C2 × C4 or C8 .
For the cases E = C2 × C2 × C2 and E = C2 × C4 , the extension is the direct
product, and no further discussion is necessary. For the cases E = H8 and E = C8 ,
the embedding of N = C2 is unique, and we therefore get only one extension in each
case. Thus there are exactly two inequivalent extensions for each choice of G.
     19. If N embeds as a summand C2 , then the quotient E/N has one fewer summand
C2 , is still the countable direct sum of copies of C2 and C4 , and is therefore isomorphic
to E. If N embeds as a 2-element subgroup of a summand C4 , then the quotient E/N
has one fewer summand C4 and one more summand C2 , is still the countable direct
sum of copies of C2 and C4 , and is therefore isomorphic to E.
     The action τ has to be trivial because C2 has only the trivial automorphism.
     If an equivalence 8 of extensions were to exist, it would have to satisfy 8 i 1 (x) =
i 2 (x) for the nontrivial element x of N = C2 . But i 1 (x) is an element of order 2 that
is not the square of an element of order 4, while i 2 (x) is an element of order 2 that
is the square of an element of order 4. Since 8 is an isomorphism, it has to carry
nonsquares to nonsquares, and we cannot have 8 i 1 (x) = i 2 (x).
668                              Hints for Solutions of Problems

     20. Let us write i 1 and i 2 for the inclusions of N into E 1 and E 2 . For (i 1 (x), 1)
to be in Q, i 1 (x) must be 1; hence x must be 1. Thus x 7→ (i 1 (x), 1)Q is one-one.
The image of ϕ is the same as the image of ϕ1 , which is G. Suppose that (e1 , e2 )
is in (E 1 , E 2 ) ∩ Q. Then ϕ1 (e1 ) = ϕ2 (e2 ) and (e1 , e2 ) = (i 1 (x), i 2 (x)−1 ) for some
x ∈ N . Then ϕ(e1 , e2 ) = ϕ1 (i 1 (x)) = 1, and ϕ descends to the quotient.
     If (e1 , e2 )Q is in the kernel of the descended ϕ, then (e1 , e2 ) is in the kernel
of the original ϕ, and e1 is in the kernel of ϕ1 . Therefore e1 = i 1 (x) for some
x ∈ N . Since ϕ2 (e2 ) = ϕ1 (e1 ), e2 is in the kernel of ϕ2 and e2 = i 2 (y) for some
y ∈ N . The element (i 1 (y), i 2 (y)−1 ) is in Q, and we therefore have (i 1 (x), i 2 (y))Q =
(i 1 (x), i 2 (y))(i 1 (y), i 2 (y)−1 )Q = (i 1 (x y), 1)Q. Thus (i 1 (x), i 2 (y))Q is exhibited
as in the image of the embedded copy of N .
   21. Since Q is normal, we have (ū, e          v )Q = (a(u, v)uv, b(u, v)f
                                           u )(v̄,e                             u v)Q =
(b(u, v), b(u, v)−1 )(a(u, v)uv, b(u, v)f
                                        u v)Q = (b(u, v)a(u, v)uv, 1f         u v)Q =
(b(u, v)a(u, v), 1)(uv, ufv)Q). Thus the cocycle for (E 1 , E 2 )Q is {b(u, v)a(u, v)} =
{a(u, v)b(u, v)}.
   22. Let 81 : E 1 → E 10 and 82 : E 2 → E 20 be isomorphisms exhibiting the
equivalences of the extensions. Define 8(e1 , e2 ) = (8(e1 ), 8(e2 ))Q 0 , and check
that this descends to the required isomorphism 8 : (E 1 , E 2 )/Q → (E 10 , E 20 )/Q 0 .
                P                   P       P               . .     P               . . .
   23. b f (χ) = t∈G f (t)χ(t) = t∈G/H.
                                               h∈H f (t +h)χ(t) =
                                                                      .
                                                                      t∈G/H
                                                                                F(t)χ(t)
       .
   b χ).
= F(
                                                                  P
   24. Fourier inversion and Problem 23 give F(x) = |G/H |−1 χ∈     .        b . .
                                                                       [ F(χ)χ(x)
                                                                       G/H        Ø
              P
= |G/H |−1 χ∈   .       b .                      .
                                                                      b
                  [ f (χ)χ(x). Pulling back χ to the member χ of G with χ H = 1
                                                                                  Ø
                  G/H
and substituting the definition of F, we obtain the desired result.
   25. For (a), if C = 0, then all a ∈ Fn have (a, 0) = 0, and hence C ⊥ = Fn .
For (b), the repetition code has C = {0, (1, . . . , 1)}. The members a of Fn with
(a, (1, . . . , 1)) = 0 are the members of even weight, hence the members of the
parity-check code. For (c), it is enough to check that (a, c) = 0 for each pair of
members a, c of a basis of C, and this one can do by hand.
   For (d), Proposition 6.3 shows that n = dim C + dim C ⊥ . Since C = C ⊥ ,
dim C = n/2.
   For (e), every member c of C is in C ⊥ and must in particular have (c, c) = 0.
Therefore c has even weight.
   For (f), let c and c0 be in C, and write cc0 for the entry-by-entry product (logical
“and”). Then wt(c + c0 ) = wt(c) + wt(c0 ) − 2wt(cc0 ), and hence 12 wt(c + c0 ) =
1              1    0         0
2 wt(c) + 2 wt(c ) − wt(cc ). Considering this equality modulo 2 shows that it is
enough to prove that C ⊆ C ⊥ implies that wt(cc0 ) is even whenever c and c0 are in
C. Modulo 2, we have wt(cc0 ) ≡ (c, c0 ), and (c, c0 ) = 0 since C ⊆ C ⊥ .
   26. In (a), every element of Fn has order at most 2, and thus χ takes only the
values ±1. Define (aχ )i to be 0 if χ(ei ) = +1 and to be 1 if χ(ei ) = −1. Then
χ(ei ) = (−1)(a,ei ) for each i. The two sides extend uniquely as homomorphisms of
                                                     Chapter VII                                           669

Fn to {±1}, and it follows that χ(c) = (−1)(a,c) for all c ∈ Fn . The remainder of (a)
is routine.                                                       P
    In (b), let χ correspond to a. Then b    f (a) = b f (χ) =      c∈Fn f (c)χ(c) =
P                    (a,c) .
   c∈Fn     f (c)(−1)
    In (c), we have
  Q               QP
     bf i (ai ) =         f i (ci )(−1)ai ci
  i              i ci ∈F
                P                                    P
            =           f 1 (c1 )(−1)a1 c1 · · ·             f n (cn )(−1)an cn
                c1 ∈F                                cn ∈F
                P                                                                 P
            =           f 1 (c1 )(−1)a1 c1 · · · f n (cn )(−1)an cn =                 f (c)(−1)(a,c) = b
                                                                                                       f (a).
                c∈Fn                                                          c∈Fn
                         P
                 f (0) = c0 ∈F f 0 (c0 )(−1)0c0 = f 0 (0)(+1) + f 0 (1)(+1) = x + y and
     27. In (a), b
           P 0
f 0 (1) = c0 ∈F f 0 (c0 )(−1)1c0 = f 0 (0)(+1) + f 0 (1)(−1) = x − y.
b
     In (b), Problem 26c gives
                           n
                           Q                   °       Q               ¢°         Q            ¢
              b
              f (a) =            b
                                 f 0 (ai ) =                    (x + y)                 (x − y)
                           i=1                     i with ai =0             i with ai =1
                                     n−wt(a)                  wt(a)
                        = (x + y)                  (x − y)            .
                                                                                        Ø
    28. In (a), the members of G/H  [ lift exactly to the members ω of G      b with ωØ = 1.
                                                                                          H
Under the mapping of Problem 26a, any member χ of G             b yields a unique member aχ
of Fn with χ(c) =     Ø (−1)
                             (aχ ,c) for all c ∈ Fn . If a is in C ⊥ , then this formula gives
                                                          χ
χ(c)   =   1, i.e., χ Ø = 1. If aχ is not in C ⊥ , then χ(c0 ) 6= 1 for some c0 ∈ C, i.e.,
  Ø                    H
χ Ø H 6= 1.
    In (b), we apply the special case of Problem 24 mentioned in the educational note.
Then the result is immediate, in view of (a).
    In (c), we let f (c) = x n−wt(c) y wt(c) . Problem 27b says that b                 f (a) =
(x  +  y) n−wt(a) (x − y)wt(a) . Substituting into the formula of the previous part gives
P         n−wt(c) y wt(c) = |C ⊥ |−1
                                      P                n−wt(a) (x − y)wt(a) , and this says that
    c∈C x                                a∈C ⊥ (x + y)
                    ⊥  −1
WC (x, y) = |C | WC ⊥ (x + y, x − y).
    In (e), parts (d) and (e) of Problem 25 show that the only monomials X k Y l in
WC (X, Y ) with nonzero coefficients are those with k and l even. Therefore WC (X, Y )
is invariant under the transformations X 7→ −X and Y 7→ −Y . The MacWilliams
identity shows that WC (X, Y ), apart from a constant, is the same polynomial in X +Y
and X − Y . Therefore WC (X, Y ) is invariant also under (X + Y ) 7→ −(X + Y ) and
under (X − Y ) 7→ −(X − Y ). Thus WC (X, Y ) is invariant under the group of
symmetries of a regular octagon centered at 0 with one of its sides centered at (1, 0).
This symmetry group is D8 .
    29. The characters of G are the ones with χn (1) = ≥mn for 0 ≤ n < m. Such a
                                                                                   nq
character is trivial on H if and only if χn (q) = 1, i.e., if and only if ≥m = 1; this
means that nq is a multiple of m, hence that n is a multiple of p.
670                                Hints for Solutions of Problems

     The element 1 of H is the element q of G. Thus the question about the identification
of the descended characters asks the value of χn (1) when n is a multiple j p of p. The
                              jp        j
value is χn (1) = ≥mn = ≥ pq = ≥q .
     If we have computed F on G/H and want to compute F                 b from the definition of
Fourier coefficients, we have to multiply each of the q values of F by the values of
each of the q characters of G/H and then add. The number of multiplications is q 2 .
The actual computation of F from f involves p additions for each of the q values of
.
t, hence pq additions.
                  j p+k   Pm−1            −( j p+k)i     Pm−1                − jp
     30. b f (≥m ) = i=0          f (i)≥m            = i=0    ( f (i)≥m−ki )≥m . The variant of
                                               −ki
 f for the number k is then i 7→ f (i)≥m . Handling each value of k involves m = pq
steps to compute the variant of f and then the q 2 + pq steps of Problem 29. Thus we
have q 2 + 2 pq steps for each k, which we regard as of order q 2 + pq. This means
p(q 2 + pq) steps when all k’s are counted, hence pq( p + q) steps.
     32. By inspection, (`v1 , `v2 )V 0 = (v1 , v2 )V has the properties of an inner product.
The definition is set up so that the linear mapping `v 7→ v of V 0 into V preserves
inner products.
     33. The contragredient has (R c (x)`v )(v 0 ) = `v (R(x −1 )v 0 ) = (R(x −1 )v 0 , v)V =
(v , R(x)v)V = ` R(x)v (v 0 ). Hence R c (x)`v = ` R(x)v , and (R c (x)`v , R c (x)`0v )V 0 =
   0

(R(x)v, R(x)v 0 )V = (R(x)v 0 , R(x)v)V = (v 0 , v)V = (v, v 0 )V = (`v , `v0 )V 0 .
     34. If {v j } is an orthonormal basis of V , then {`vj } is an orthonormal basis of
V 0 by Problem 32, and (R c (x)`vj , `vj )V 0 = (` R(x)vj , `vj )V 0 = (v j , R(x)v j )V =
(R(x)v j , v j )V . Summing on j gives the desired equality of group characters.
     35. In view of Problem 34 a necessary condition on a 1-dimensional representation
for it to be equivalent to its contragredient is that it be real-valued. Hence the two
nontrivial multiplicative characters of C3 are not equivalent to their contragredients.
     36. Following the notation in the discussion before Theorem 7.23, let ρi j (x) =
(R(x)u j , u i ), let l be the left-regular representation, and let `v (u) = (u, v)V be as
above. Consider, for fixed j0 , the image of R c (g)`u i under the linear extension ° P to V¢ of
                                                                                                    0
               0                                           0 P
the map E (`u k )(x) = (R(x)u j0 , u k )V . This is E (` ck u k )(x) = E          0
                                                                                         k c̄k `u k (x)
     P           0
                             P                                   k       P
=       k c̄k E (`u k )(x) =     k c̄k (R(x)u j0 , u k )V = (R(x)u j0 ,     k ck u k )V , and hence
E 0 (`v )(x) = (R(x)u j0 , v)V . Then the image of interest is

                E 0 (R c (g)`u i )(x) = E 0 (` R(g)u i )(x) = (R(x)u j0 , R(g)u i )V
                                     = (R(g −1 x)u j0 , u i )V = (l(g)ρi j0 )(x).

Therefore l carries a column of matrix coefficients to itself and is equivalent on such
a column to R c . ≥      ¥          ≥       ¥
                       0 −1                      0 1
   37. Let x =         1 0
                               and y =         −1 −1 ≥
                                                      , and let 0 be     the subgroup generated
                                                               ¥               ≥     ¥
                                                         −1 −1                    10
by x and y. Observe that −I =            x 2 , y −1 =     1 0
                                                                , and    yx = −1 1 are in 0.
                                       Chapter VII                                     671
                                                                              ≥ ¥
Arguing by contradiction, suppose that 0 6= SL(2, Z). Choose a matrix z = ac db
in SL(2, Z) but not 0 such that max(|a|, |b|) is as small as possible. If ab = 0,
then one of |a| and |b| is 1 and the other is 0 because the matrix has determinant
1. If |a| = 0, then zy −1 has top row ( ±1 0 ); ≥so in¥either event we see that some
member of SL(2, Z) outside 0 is of the form ± 1t 01 . Since x 2 = −I is in 0 and
       ≥     ¥
          10
yx = −1 1 is in 0, this is a contradiction.
   Thus the matrix z cannot have ab = 0. Suppose that ab > 0. Then zy has
top row ( −b a − b ), and zy −1 has top row ( −a + b −a ). The minimality of
max(|a|, |b|) for z says that

max(|a|, |b|) ≤ max(| − b|, |a − b|) and        max(|a|, |b|) ≤ max(| − a + b|, | − a|).

Now |a − b| < max(|a|, |b|) since ab > 0, and the only way that we can have the
above inequalities is if a = b. In this case, zy is a member of SL(2, Z) outside 0
whose top-row entries have product 0, and we have seen that this is a contradiction.
   Thus we must have ab < 0. Then zx has top row ( b −a ). The product of these
entries is positive and the maximum of their absolute values is the same as that for z.
So we are reduced to the situation in the previous paragraph, which we saw leads to
a contradiction. We conclude that 0 = SL(2, Z).
   38. In PSL(2, Z), we have x 2 = y 3 = 1, and Problem 37 shows that x and
y generate PSL(2, Z). Proposition 7.8 therefore produces a homomorphism car-
rying hX, Y ; X 2 , Y 3 i onto PSL(2, Z). Proposition 7.16 shows that C2 ∗ C3 ∼       =
hX, Y ; X 2 , Y 3 i, and the composition of these two maps yields the desired homo-
morphism 8.
≥ 39. Let¥ ≥ us drop    ≥ “mod
                    ¥ the     ¥ ± I ” in order
                                           ≥ to simplify
                                                   ¥≥      ¥the notation.
                                                                 ≥       ¥ In (a), yx =
   0 1        0 −1         10       −1       −1 −1    0 −1         −1 1
                      = −1 1 and y x =                        =           . Then zyx =
≥ −1 −1 ¥ 1 0                                  1 0    1 0           0 −1
  a−b b
  c−d d
          , and µ(zyx) = max(|a − b|, |b|). If ab ≤ 0, then |a − b| ∏ |a| and hence
                                       ≥        ¥
                                         −a a−b
µ(zyx) ∏ µ(z). Similarly zy −1 x = −c c−d , and µ(zy −1 x) = max(|a|, |a − b|).
If ab ≤ 0, then |a − b| ∏ |b| and hence µ(zy −1 x) ∏ µ(z). The arguments with ∫
are similar.                 ≥       ¥
                               b −a
    In (b), we have zx = d −c . Then µ(zx) = max(|b|, |a|) = µ(z) and ∫(zx) =
max(|d|, |c|) = ∫(z).
    In (c), the entries of z are limited to ±1 and 0. We may take the first nonzero entry
in the first column to be ≥+1 by      ≥ ¥ ≥ by −I
                                   ¥ adjusting      ¥ if≥ necessary.
                                                             ¥ ≥      ¥ ≥ the possibilities
                                                                     Then      ¥ ≥       ¥
                               10      11      1 −1       10     1 −1       10       11
with determinant 1 are 0 1 , 0 1 , 0 1 , 1 1 , 1 0 , −1 1 , −1 0 ,
≥       ¥ ≥       ¥      ≥       ¥
  0 −1       0 −1           0 −1
  1 0
         ,   1 1
                   , and    1 −1
                                   .
    In (d), let us prove≥ by ¥induction on n that if Z = a1 · · · an is reduced and ends
in X, then 8(Z ) = ac db has ab ≤ 0. The base cases of the induction are n = 1
672                            Hints for Solutions of Problems

and
≥ n ¥=≥2, where ¥ we≥ have ¥     Z = X, Z = Y X, and Z = Y −1 X; since 8(Z ) is
  0 −1       10           −1 1
  1 0
        , −1 1 , and       0 −1
                                   in the three cases, we have ab ≤ 0 for each. For the
inductive step we pass from Z , which ends in X, to anything obtained by adjoining
factors at the right in such a way that the new word is still reduced and has X at
the right end.                                                      −1
          ≥ ¥This means that Z is replaced by Z Y X or by Z Y X. Suppose that
8(Z ) = ac db . We are assuming that ab ≤ 0. According to the calculation in the
solution of (a), the entries in the first row of 8(Z Y X) are a − b and b, with product
(a − b)b = ab − b2 ≤ ab ≤ 0, and the entries in the first row of 8(Z Y −1 X) are −a
and a − b, with product −a(a − b) = −a 2 + ab ≤ ab ≤ 0. Thus the induction goes
forward, and our assertion follows.
   Now we can prove by induction that

                         µ(8(a1 · · · an )) ∏ µ(8(a1 · · · an−1 ))                       (∗)

if Z = a1 · · · an = Z 0 an is reduced. The result is trivial for n = 1, and we let n ∏ 2 be
given and assume the inequality for words of length < n. Let a word of length n ∏ 2
be given. If an = X, then (∗) is immediate from (b). If an 6= X, then an−1 = X and
an is Y or Y −1 . Also, Z X is a reduced word. From the previous paragraph we know
that the product of the entries in the first row of µ(8(Z 0 )) is ≤ 0. Applying (b) and
then (a), we obtain µ(8(Z )) = µ(8(Z X)) = µ(8(Z 0 an X)) ∏ µ(8(Z 0 )), and this
proves (∗). Similar arguments apply to ∫.
    For (e), we are to prove that if W is a nonempty reduced word, then 8(W ) is
not the identity of PSL(2, Z). Assuming the contrary, we may assume without loss
of generality that W is as short as possible with this property. If W = a1 · · · an ,
and 8(W ) is the identity, then µ(8(W )) = µ(I ) = 1 and similarly ∫(8(W )) = 1.
By (d), we must have µ(8(a1 · · · ak )) = ∫(8(a1 · · · ak )) = 1 for 1 ≤ k ≤ n.
Then, for each k with 1 ≤ k < n, 8(a1 · · · ak ) lies in the set of 10 matrices in
(c) but is not the identity. The 10 matrices in (c) are obtained by applying 8 to
the elements 1, XY , Y −1 X, XY −1 , XY X, Y X, Y −1 , X, Y , and XY −1 X. The
remaining words W of length 3 are Y XY , Y XY −1 , Y −1 XY , Y −1 XY −1 , and the
ones of length 4 are XY XY , XY XY −1 , XY −1 XY , XY −1 XY −1 , Y XY X, Y XY −1 X,
Y −1 X Y≥X, Y −1¥ X≥Y −1 X. ¥ We        ¥ ≥ 8 directly
                                 ≥ compute       ¥ ≥ ¥on≥these     ¥ ≥    ¥ ≥ ¥words
                                                                      12 reduced     ≥ and ¥
           0 1        −1 −1       −1 −2      21       12       21      11     10       10
obtain −1 −2 ,                 ,
                       2 1 ¥ 1 1
                                         ,  −1 0
                                                   ,  01
                                                           ,   11
                                                                    ,  12
                                                                            , 21
                                                                                   ,  −2 1
                                                                                            ,
≥        ¥ ≥         ¥ ≥
    1 −1        2 −1      1 −2
  −1 2
          , −1 1 , 0 1 . Consequently 8(W ) is not the identity for W of positive
length ≤ 4. The inequality of (d) shows that µ(8(W )) ∏ 2 if W has length > 4, and
therefore 8(W ) is the identity only if W is the empty word.
                                      ≥ ¥ ≥                     ¥
    40. The definition of e σm is eσm ac db = a+mZ       b+mZ
                                                  c+mZ d+mZ
                                                                 . We readily check that e
                                                                                         σm
respects multiplication and hence is a homomorphism into some group of matrices.
Since (a + mZ)(d + mZ) − (b + mZ)(c + mZ) = (ad − bc) + mZ = 1 + mZ,                  ≥ the¥
image group is contained in SL(2, Z/mZ). The kernel is the set of matrices ac db
                                       Chapter VII                                    673

in SL(2, Z) with a + mZ = 1 + mZ, b + mZ = 0 + mZ, c + mZ = 0 + mZ,
d + mZ = 1 + mZ, and these are exactly the matrices M in SL(2, Z) with every entry
of M − I divisible by m. Therefore ker e σm = 0(m). This proves (a).
   In (b), let ∞ = GCD(α, m), so that α∞ −1 and m∞ −1 are relatively prime. Applying
Dirichlet’s theorem on primes in arithmetic progressions, take p > |β| to be a prime of
the form p = α∞ −1 +rm∞ −1 for some r. Then α+rm = p∞ , and GCD(α+rm, β) =
GCD( p∞ , β) = GCD(∞ , β) = GCD(GCD(α, m), β) = GCD(α, β, m) =≥ 1. ¥
   For (c), corresponding to any member of SL(2, Z/mZ) is a matrix ac db with
integer entries with ad − bc ≡ 1 mod m. If p is a prime dividing a − b and
c − d, then ad − bc ≡ bd − bd ≡ 0 mod p, and hence p does not divide m.
Therefore GCD(a − b, c − d, m)=1. Applying (b), we obtain ≥an integer    ¥ r such that
                                                                  a+rm b
GCD(a + rm − b, c − d) = 1. Let us then work instead with           c d
                                                                           . Adjusting
                            ≥ ¥
                              a b
notation to call this matrix c d , we may assume that GCD(a − b, c − d) = 1.
Since m divides ad − bc − 1, there exist integers C and A with
                                                     1−(ad−bc)
                       (a − b)C + (d − c)A =             m     .
        ≥             ¥
          a+m A b+m A
Then det c+mC d+mC is equal to
                                                                    °         ¢
     (ad − bc) + (d − c)m A + (a − b)mC = (ad − bc) + m 1−(ad−bc)       m       = 1,
     ≥              ¥
       a+m A b+m A
and c+mC d+mC is a member of SL(2, Z) whose image under e                 σm is the given
matrix in SL(2, Z/mZ).
   41. For the remainder of the problems in this set, it will be convenient to regard
the isomorphism C2 ∗ C3 ∼      = hX, Y ; X 2 , Y 3 i of Proposition 7.16 as an equality:
C2 ∗ C3 = hX, Y ; X 2 , Y 3 i.
   In (a), 8m is well defined as a consequence of the second conclusion of Proposition
7.8.
   In (b), it is immediate from Proposition 7.8 that the kernel of 8m is the smallest
normal subgroup of C2 ∗ C3 containing the element (XY )m . Under the isomorphism
8 : C2 ∗ C3 → PSL(2, Z), we have 8((XY )m ) = (x y)m mod ±I . Since the
smallest normal subgroup Hm of PSL(2, Z) containing (x y)m mod ±I = 8((XY )m )
is 8 of the smallest normal subgroup of C2 ∗ C3 containing (XY )m , we have Hm =
8(ker 8m ).
   In (c), if passage to the quotient is denoted by qm , Proposition 4.11 shows that the
                                            ¥ matrices in SL(2, Z) lie in the kernel of
point needing verification is that≥the scalar
                                     −1 0
qm ◦ eσm , and this follows since      0 −1
                                             maps under e  σm to the matrix with entries
taken modulo m and then maps to the identity under qm .
   In (d), K m is a normal subgroup of PSL(2, Z), and            thus
                                                         ≥ it is ¥        ¥ to≥show
                                                                   ≥ enough         ¥ that
                                                           0 −1       0 1        11
the element (x y)m mod ±I of Hm is in K m . Since 1 0               −1 −1
                                                                            =    01
                                                                                      and
since the m th power of this matrix is in 0(m), (x y)m mod ±I is indeed in K m .
674                              Hints for Solutions of Problems
                                  ≥         ¥                                  ≥     ¥
                                      1m
   For (e), part (d) shows that         mod ±I is in K m , and its t th power 10 tm
                                      0 1                                          1
                                                                                       mod
                                               ≥      ¥       ≥        ¥
                                                  1 0           1 tm      −1
±I , for t an integer, has to be in K m . Then −tm 1 = x 0 1 x mod ±I is in
                                     ≥          ¥         ≥      ¥         ≥           ¥
                                       1+tm tm                               1−tm tm
K m since K m is normal, and so are −tm 1−tm = y −1 10 tm     1
                                                                    y  and    −tm 1+tm
                                                                                         =
      ≥      ¥
x y −1 10 tm
           1
               yx −1 , for the same reason.
    42. Let x and y be the listed images in the stated permutation groups of X and Y .
The homomorphisms in this problem come from Proposition 7.8 since in each case
x 2 = 1, y 2 = 1, and (x y)m can be verified to be 1. What needs to be verified in each
case is that x and y generate the stated permutation group.
    In (a), the image group has a subgroup of order 2 and a subgroup of order 3 and
hence must be the whole 6-element S3 .
    In (b), Lemma 4.41 shows that (1 2 3)(1 2)(3 4)(1 2 3)−1 = (2 3)(1 4), and
hence the image group has a subgroup of 4 even permutations and a subgroup of 3
even permutations, therefore must be all of A4 .
    In (c), we have (1 2)(2 3 4) = (1 2 3 4). Thus the image group contains
(1 2 3 4)2 = (1 3)(2 4), (2 3 4)(1 3)(2 4)(2 3 4)−1 = (1 4)(2 3), and
(2 3 4)(1 2)(2 3 4)−1 = (1 3), hence a subgroup of order 8 and a subgroup of
order 3. Therefore it is all of S4 .
    In (d), we have (1 2)(3 4)(1 3 5) = (1 4 3 5 2). Thus the image group
contains a subgroup of order 5, a subgroup of order 3, and a subgroup of order 2, all
contained in A5 . The image group is not of order 30 because A5 has no nontrivial
normal subgroups, and hence it must be all of A5 .
    43. As with Problem 39, let   ≥ us¥drop the “≥mod ±I              ≥ to simplify
                                                            ¥ ” in order        ¥     the notation.
                                                                                           ≥     ¥
                                    10                1 −1                0 1                 01
In (a), we can take g1 = 0 1 , g2 = 0 1 , g3 = −1 −1 , g4 = −1 0 ,
      ≥         ¥          ≥      ¥
        −1 −1                 10
g5 =       1 0
                 , g 6  =    −1 1
                                    .
                                                                                 ≥      ¥
                                                                                     01
    For (b), first we compute the six values of gi b1 as g1 b1 = −1 0 , g2 b1 =
≥      ¥              ≥        ¥              ≥        ¥            ≥       ¥            ≥       ¥
    11                  −1 0                    −1 0                  1 −1                   0 1
  −1 0
        ,  g  b
            3 1   =       1 −1
                                ,  g   b
                                     4 1   =      0 −1
                                                         , g b
                                                            5 1  =           ,  g  b
                                                                                 6 1  =           ,
                                                                   ≥ 0 1 ¥               ≥ −1 −1 ¥
                                                                       0 1                   1 2
and then we compute the six values of gi b2 as g1 b2 = −1 −1 , g2 b2 = −1 −1 ,
          ≥        ¥              ≥          ¥             ≥ ¥                ≥       ¥
                                                                                  0 1
                                              , g5 b2 = 10 01 , g6 b2 = −1 −2 . Next we
             −1 −1                    −1 −1
g3 b2 =       1 0
                      , g4 b2 =         0 −1
locate each of these products in a coset, writing them with some gi on the right.
We find that, up to mod ± I , the results are g1 b1 = g4 , ≥g2 b1 =¥ g5 , g3 b1 = g6 ,
                                                                           3 2
g4 b1 = g1 , g5 b1 = g2 , g6 b1 = g3 , g1 b2 = g3 , g2 b2 = −2 −1 g6 , g3 b2 = g5 ,
          ≥ ¥                                   ≥      ¥
                                                    10
g4 b2 = 10 21 g2 , g5 b2 = g1 , g6 b2 = −2 1 g4 . The conclusion is that generators
                                   ≥         ¥ ≥ ¥ ≥            ¥
                                        3 2                  10
of K 2 are the three matrices −2 −1 , 10 21 , −2 1 .
    For (c), the≥ second¥ and third≥ of ¥the  ≥ generators
                                                    ¥         in (b) are in H2 by Problem 41e.
                    3 2                1 −2     10
The equality −2 −1 = − 0 1                      21
                                                      exhibits the first of the generators as in
                                               Chapter VII                                              675

H2 . Hence all the generators are in H2 and K 2 ⊆ H2 . Therefore K 2 = H                       ≥2 . ¥
    For (d) with m = 3, we can take the 12 coset representatives to be g1 = 10 01 , g2 =
≥       ¥           ≥ ¥              ≥         ¥          ≥      ¥              ≥       ¥           ≥      ¥
  1 −1                11                  01                  01                    0 1               1 −1
  0 1≥
          , g3   =         , g4  =              ,  g5  =           ,  g6   =             .  g7  =           ,
                ¥     01 ≥       ¥ −1 0 ≥                  ¥−1 1        ≥ ¥ −1 −1 ≥                ¥1 0
           11                 11                     1 −1                  10                  10
g8 = 1 2 , g9 = −1 0 , g10 = −1 2 , g11 = 1 1 , g12 = −1 1 . Then
                                 ≥         ¥                     ≥         ¥                    ≥       ¥
                                      01                              11                           −1 1
we compute that g1 b1 = −1 0 = g4 , g2 b1 = −1 0 = g9 , g3 b1 = −1 0 =
                  ≥        ¥                         ≥        ¥                         ≥         ¥
                    −1 0                               −1 0                                −1 0
g7 , g4 b1 =                 =   g 1 ,  g5  b1  =                =    g 11 ,  g 6 b1  =              = g12 ,
            ≥ ¥ 0 −1                       ≥        ¥ −1 −1≥         ¥                   ≥ 1 −1 ¥
               11                             −1 1           −1 0                           −1 1
g7 b1 = 0 1 = g3 , g8 b1 = −2 1 = −3 −1 g10 , g9 b1 =                                        0 −1
                                                                                                      = g2 ,
            ≥         ¥ ≥          ¥                    ≥      ¥                       ≥        ¥
                 1 1           10                           01                             0 1
g10 b1 = −2 −1 = −3 1 g8 , g11 b1 = −1 1 = g5 , g12 b1 = −1 −1 = g6 .
                      ≥        ¥                      ≥        ¥ ≥                 ¥               ≥       ¥
                          0 1                             1 2              4 3                       −1 0
    Also, g1 b2 = −1 −1 = g6 , g2 b2 = −1 −1 = −3 −2 g10 , g3 b2 = −1 −1
                      ≥        ¥                        ≥        ¥                        ≥        ¥
                        −1 −1                             −1 −1                             −1 −1
= g11 , g4 b2 =                   =    g3 ,  g5 b 2  =               =   g  8 , g 6 b2 =              = g9 ,
            ≥ ¥ ≥ 0 −1¥                          ≥         ¥ −2 ≥
                                                          −1                 ¥                1≥ 0      ¥
             12           13                        −1 0           −1 0                          −1 0
g7 b2 = 0 1 = 0 1 g2 , g8 b2 = −2 −1 = −3 −1 g12 , g9 b2 =                                                 =
                  ≥       ¥ ≥               ¥                 ≥          ¥                     ≥ 0 −1 ¥
                     1 2          −2 3                           0 1                               0 1
g1 , g10 b2 = −2 −3 =               3 −5
                                              g7 , g11 b2 = −1 0 = g4 , g12 b2 = −1 −2 =
≥       ¥
    10
  −3 1
           g5 .
                                    ≥           ¥ ≥        ¥ ≥           ¥ ≥ ¥ ≥               ¥ ≥         ¥
                                       −1 0            10        4 3                    −1 0        −2 3
    Thus generators of K 3 are −3 −1 , −3 1 , −3 −2 , 10 31 , −3 −1 , 3 −5 ,
≥       ¥            ≥         ¥       ≥          ¥                                                ≥       ¥
    10                   4 3             −2 3                                                          4 3
  −3  1
          . All but −3 −2 and 3 −5 are certainly in H3 . The expressions −3 −2
   ≥             ¥      ≥        ¥ ≥                  ¥≥       ¥
      1+3 3               −2 3             1−3 −3         1 −3
= −3 1−3 and 3 −5 = 3 1+3                                 0 1
                                                                 show that these two generators are
in H3 . Therefore K 3 = H3 .
    44. Problem 41 produces a homomorphism σm of G m onto PSL(2, Z/mZ) with
kernel isomorphic to K m /Hm . The given fact Hm = K m for 2 ≤ m ≤ 5 implies
that σm is an isomorphism for these values of m. This proves the first isomorphism
in each part. Problem 42 gives us homomorphisms of G m for these m’s onto the
third group listed in each part. Composition with σm−1 then gives a homomorphism of
PSL(2, Z/mZ) onto the third group. In each case the statement of Problem 43 gives
the number of elements in PSL(2, Z/mZ), and this matches the number of elements
in the third group. It follows that these homomorphisms are isomorphisms.
    45. For (a), linearity gives Rθ T(a,b) Rθ−1 (x, y) = Rθ (Rθ−1 (x, y) + (a, b)) =
Rθ Rθ−1 (x, y) + Rθ (a, b) = (x, y) + Rθ (a, b) = TRθ (a,b) (x, y).
    For (b), the result of (a) says that we get a semidirect product. Let us show that
the two sets—the elements of the semidirect product and the union of the translations
and rotations—coincide. In one direction a rotation about (x0 , y0 ) is of the form
(x, y) 7→ Rθ (x − x0 , y − y0 ) + (x0 , y0 ) = Rθ (x, y) + (a, b) = T(a,b) Rθ (x, y), where
(a, b) = −Rθ (x0 , y0 ) + (x0 , y0 ). Hence it is in the semidirect product. In the reverse
direction suppose that T(a,b) Rθ is in the semidirect product and is not a translation.
Then θ is not a multiple of 2π, and we can put (x0 , y0 ) = (1 − Rθ )−1 (a, b). Then we
676                                   Hints for Solutions of Problems

have T(a,b) Rθ (x, y) = Rθ (x, y)+(a, b) = Rθ (x −x0 , y− y0 )+ Rθ (x0 , y0 )+(a, b) =
Rθ (x − x0 , y − y0 ) + Rθ (1 − Rθ )−1 (a, b) + (a, b) = Rθ (x − x0 , y − y0 )−
(1− Rθ )(1− Rθ )−1 (a, b)+(a, b)+(1− Rθ )−1 (a, b) = Rθ (x −x0 , y − y0 )+(x0 , y0 ).
Hence T(a,b) Rθ is a rotation about (x0 , y0 ).
    46. In (a), we need to show only that rc = ra rb . In (b), we need to show that
rb ra rb ra rb is a translation but not the identity. Then it follows from (b) that the
group G generated by ra≠ and rb is infinite. Since  Æ   (a) and Proposition 7.8 yield a
homomorphism of G 6 = X, Y ; X 2 , Y 3 , (XY )6 onto the infinite group G, it follows
that G 6 is infinite. Since PSL(Z/6Z) is finite, (c) follows.
    To establish the two facts that need checking, we may, without loss ofpgenerality,
take T to be the triangle with vertices a = (0, 0), b = (0, −1), and c = ( 3, 0). The
formulas for ra , rb , and rc are ra (x, y) = (−x, −y),

       rb (x, y) = (x cos 2π               2π        2π              2π
                           3 − (y + 1) sin 3 , x sin 3 + (y + 1) cos 3 − 1)
                   °           p        p        p                      ¢
                 = − x/2 − y 3/2 − 3/2, x 3/2 − y/2 − 1/2 − 1 ,

and
                       p                      p        p
      rc (x, y) = ((x − 3) cos π3 + y sin π3 + 3, −(x − 3) sin π3 + y cos π3 )
                  °    p          p         p        p p             ¢
                = (x − 3)/2 + y 3/2 + 3, −(x − 3) 3/2 + y/2 .

Then ra rb (x, y) = −rb (x, y) = rc (x, y) by inspection.
    To verify that rb ra rb ra rb is a translation, we write rb ra rb ra rb (x, y) = rb rc2 (x, y).
The formula above for rc gives
                        p                            p            p
     rc2 (x, y) = ((x − 3) cos 2π               2π                           2π
                                    3 + y sin 3 + 3, −(x − 3) sin 3 + y cos 3 )
                                                                                           2π
                   °        p             p        p            p p                 ¢
                = − (x − 3)/2 + y 3/2 + 3, −(x − 3) 3/2 − y/2 .
                                                                 p              p           p
Then the first coordinate of rb rc2 (x, y) is − 12 (−(x − 3)/2 + y 3/2 + 3)+
         p p                 p           p               p
((x − 3)p 3/2 + y/2)    p 3/2p− p3/2 = x − p            2 3,p while the second coordinate
is (−(x − 3)/2 + yp 3/2 + 3) 3/2 + ((x − 3) 3/2 + y/2)/2 − 3/2 = y. So
rb rc2 (x, y) = (x − 2 3, y) is a translation.
    47. We may suppose that the representations are unitary. Let {v1,i } and {v2, j } be
orthonormal bases of V1 and V2 . Then
                          P
     (χ R1 ∗ χ R2 )(x) = χ R1 (x y −1 )χ R2 (y)
                           y
                           P
                      =            (R1 (x y −1 )v1,i , v1,i )(R2 (y)v2, j , v2, j )
                          y,i, j
                            P
                      =              (R1 (x)(R1 (y −1 )v1,i , v1,k )v1,k , v1,i )(R2 (y)v2, j , v2, j )
                          y,i, j,k
                          P                               P
                      =            (R1 (x)v1,k , v1,i )       (R1 (y)v1,k , v1,i )(R2 (y)v2, j , v2, j )
                          i, j,k                          y
                                                    Chapter VII                                             677

For (a), the inside sum is 0, and the argument is complete. For (b), let R1 = R2 and
v2, j = v1, j . Then the right side of the display continues as
                      P
                  =            (R1 (x)v1,k , v1,i )|G|d R−1
                                                          1
                                                            (v1, j , v1,k )(v1, j , v1,i )
                      i, j,k
                                    P
                  = |G|d R−1
                           1
                                            (R1 (x)v1,k , v1,i )δ jk δ ji
                                   i, j,k
                                   P
                  = |G|d R−1
                           1
                                    (R1 (x)v1,i , v1,i ) = |G|d R−1
                                                                  1
                                                                    χ R1 (x).
                                     i

   48. We have E α E β = |G|−2 dα dβ R(χα )R(χβ ) = |G|−2 dα dβ R(χα ∗ χβ ). Prob-
lem 47a shows that this is 0 if Rα and Rβ are inequivalent; this proves (b). Problem 47b
shows that the computation with Rα = Rβ continues as = |G|−1 dα R(χα ) = E α ; this
proves (a).
   49. Let S be the set of all finite-dimensional irreducible
                                                           P       invariant subspaces Vs of
V . Call a subset T of S “independent” if the sum t∈T Vt is direct. This condition
means that for every finite subset {t1 , . . . , tn } of T and every set of elements vi ∈ Vti ,
the equation
                                    v1 + · · · + vn = 0
implies that each vi is 0. From this formulation it follows that the union of any
increasing chain of independent subsets of S is itself independent. By Zorn’sPLemma
there is a maximal independent subset T0 of S. By definition the sum V0 = t∈T0 Vt
is direct. Consequently the problem is to show that V0 is all of V . Since every
member of V lies in a finite direct sum of finite-dimensional irreducible invariant
subspaces of V , it suffices to show that each Vs is contained in V0 . If s is in T0 ,
this conclusion is obvious. Thus suppose s is not in T0 . By the maximality of T0 ,
T0 ∪{s} is not independent. Consequently the sum V0 + Vs is not direct, and it follows
that V0 ∩ Vs 6= 0. But this intersection is an invariant subspace of Vs . Since Vs is
irreducible, a nonzero invariant subspace must be all of Vs . Thus Vs is contained in
V0 , as we wished to show.
                                                            Ø
    50. Let us impose an inner product on V0 that makes R ØV0 unitary. Let {v1 , . . . , vn }
                                                         Pn
be an orthonormal basis of V0 . If we write R(x)v j = i=1 Ri j (x)vi , then Ri j (x) =
                                                    Ø                       P
(R(x)v j , vi ). Consequently the character χα of R ØV0 is given by χα (x) = i Rii (x).
Then we have
                           P                                           PP
    E α v j = |G|−1 dα            χα (x)R(x)v j = |G|−1 dα                      Rkk (x)Ri j (x)vi = v j ,
                          x∈G                                         x∈G i,k

and E α is the identity on V0 .
                                                          Lpossibly infinitely many
    51. Problem 49 allows us to write V as the direct sum of
finite-dimensional irreducible invariant subspaces V =       ∞ V∞ . If any v in V is
                           P
given, we can write v = ∞ v∞ with only finitely many terms nonzero. Applying
678                           Hints for Solutions of Problems
                                                                                  Ø
E α and using Problem 50, we see that E α v is the sum of those v∞ such that R ØV∞ is
equivalent to Rα . Thus each nonzero v∞ has the property that E α v∞ = v∞ for some α.
    On the other hand, this equality cannot hold for two distinct α’s. In fact, if Rα
and Rβ are inequivalent and we have E α v∞ = v∞ and E β v∞ = v∞ , then application
of E α to the second equality gives E α E β v∞ = E α v∞ = v∞ . But E α E β = 0 by
Problem 48b, and hence v∞ = 0.
    The conclusion is that for each nonzero v∞ , there is P one and only one P E α such
that E α v∞ 6= 0, and that α has E α v∞ = v∞ . Applying α E α to v = ∞ v∞ , we
         P           P                P                     P
obtain α E α v = ∞ ,α E α v∞ = ∞ v∞ = v. Thus α E α = I . Problem 50
shows that E α is the identity on any finite sum of vectors lying in finite-dimensional
irreducible invariant subspaces equivalent to Rα . The direct-sum decomposition just
proved shows that E α is 0 on any vector in the direct sum of the images of the other
E β ’s. Thus the image of E α is as asserted.
                                                                    P
    52. For α as given and for any v in V , we have E α v = |G|−1 x∈G ω(x)R(x)v.
The members of the image of E α are exactly
                                       P       the vectors v for which E α v = v, hence
exactly the vectors v for which |G| −1
                                          x∈G ω(x)R(x)v = v. Applying R(y) to both
                              P                               P
sides gives R(y)v = |G|−1 x∈G ω(x)R(yx)v = |G|−1 x∈G ω(y −1 x)R(x)v =
               P
ω(y −1 )|G|−1 x∈G ω(x)R(x)v = ω(y −1 )v = ω(y)v.


                                    Chapter VIII

   1. In (a), ϕ fixes 1 and must therefore fix the subfield generated by 1; this is Q.
For (b), ϕ(a 2 ) = ϕ(a)2 . For (c), if a ≤ b, then b − a = c2 for some c. Hence
ϕ(b) − ϕ(a) = ϕ(c)2 , and ϕ(a) ≤ ϕ(b). For (d), let r be any real, let ≤ > 0 be
given, and choose rationals q1 and q2 with q1 ≤ r ≤ q2 and q2 − q1 < ≤. Then
q1 = ϕ(q1 ) ≤ ϕ(r) ≤ ϕ(q2 ) = q2 by (a) and (c). Hence |ϕ(r) − r| < ≤. Since ≤ is
arbitrary, ϕ(r) = r.
   2. (1 + r)−1 = 1 − r + r 2 − r 3 + · · · ± r n−1 if r n = 0.
   3. This follows from the universal mapping property of the field of fractions.
   4. Suppose that X divides A(X)B(X), i.e., A(X)B(X) = XC(X). If a0 and b0
are the constant terms of A(X) and B(X), we then have a0 b0 = 0. If a0 = 0, then X
divides A(X); if b0 = 0, then X divides B(X). Hence X is prime.
   5. In (a), take (X) as the ideal. It is prime by Problem 4. Suppose that a is a
member of R with no inverse in R. then (X) is not maximal since (a, X) strictly
contains it and does not contain 1. For (b), we can use (a, X).
    6. In (a), Ix0 is certainly an ideal. Suppose J is an ideal with I x0 $ J . Choose f
in J that is not in Ix0 . The function x − x0 is in Ix0 . Therefore g = f 2 + (x − x0 )2
is in J . This function is everywhere > 0, and consequently 1/g is in R. Hence
1 = (1/g)g is in J , and J cannot be proper. So Ix0 is maximal.
                                       Chapter VIII                                   679

   Part (b) uses the Heine–Borel Theorem. For each point p in [0, 1], choose a
function f p in I with f p ( p) 6= 0. By continuity, f p is nonvanishing on some open
set N p containing p. As p varies, these open sets N p cover [0, 1]. The Heine–
Borel Theorem produces finitely many N p1 , . . . , N pk that cover [0, 1]. Then f pj is
nonvanishing on N pj . If x is a member of [0, 1], then x is in some N pj , and f pj does
not vanish at x. Thus the functions f p1 , . . . , f pk have no common zero.
   For (c), suppose that the maximal ideal I is not some Ix0 . Using (b), we form the
function g = f p21 + · · · + f p2k . This is in I and is everywhere positive. The function
1/g is therefore in R, and 1 = (1/g)g is in I . Hence I = R, in contradiction to the
fact that I is proper.
    7. In (a), I∞ is an ideal, and it is properly contained in the proper ideal of all
members of R vanishing at −∞. Part (b) follows from Proposition 8.8. The reason
for (c) is that for each x0 in R, there is a member of R that is nonzero at x0 and
vanishes at infinity; this function has to be in I , and thus I cannot equal Ix0 .
                                                                   p             p
    8. For (a), let a + bi be a nonzero member of I . Then (a + b −5)(a − b −5) =
a 2 + 5b2 is a positive integer in I .
                                                  p
    For (b), I is an additive subgroup of Z + Z −5, which is free abelian of rank 2.
Therefore I is free abelian of rank 1 or 2. We can rule outp   rank 1 because I contains
a nonzero integer and also the product of that integer and −5.
                                                       p                       p
    For (c), a Z basis of I consists of x1 = a1 +pb1 −5 and x2 = a2 + b2 −5. Put
y1 = r x1 + sx2 = (ra1 + sa2 ) + (rb1 + sb2 ) −5     p and y2 = t x1 + ux2 , and aim to
have y1 , y2 form a Z basis with y1 not involving −5. We thus want rb1 + sb2 = 0,
and the most economical way of achieving this equality is to put d = GCD(b1 , b2 )
and to take r = b2 d −1 and s = −b1 d −1 . Then GCD(r, ≥s) =¥ 1, and we≥ can¥ choose
                                                               y1     ° r s ¢ x1
t and u with ru − st = 1. With these choices we have y2 = t u                 x2 . Since
    °r s ¢
det t u = 1, this change is invertible. In other words, y1 and y2 form a Z basis
in which y1 is some nonzero integer n. We may assume that n > 0. Let m be the
smallest positive integer in I . Then n must be a multiple of m by an application of
the division algorithm. Since y1 and y2 form a Z basis of I , we see that n equals m.
  9. It is straightforward to see that P is an ideal and that x y ∈ P implies x ∈ P or
y ∈ P. The ideal P is proper since the presence of 1 in ϕ −1 (P 0 ) would mean that
ϕ(1) = 1 is in P 0 . But P 0 is proper, and thus 1 is not in P 0 .
   10. (a) {(r, 0) | r ∈ R} and {(0, r) | r ∈ R}.
   (b) (X).
   (c) (X − 1) and (X − 2).
   (d) (0).
   11. For (a), Q[X]/I is a field and hence is ap unique factorization domain. For
(b), one can give a counterexample. The ring Z[ −5] is an integral domain and is
               Z[X] by the ideal (X 2 + 5); therefore I = (X 2 + 5) is prime. On the
the quotient ofp
other hand, Z[ −5] is not a unique factorization domain.
680                              Hints for Solutions of Problems

   12. For (a), choose x and y with xd + yc = 1. Dividing by n gives xc−1 + yd −1 =
n −1 .
     Then (a) follows by multiplying through by m. Part (b) uses an induction. Group
                   kr−1 kr                                                kr−1 −1
n as ( p1k1 · · · pr−1 ) pr and apply (a) to write mn −1 = a( p1k1 · · · pr−1 ) + bpr−kr .
                                         k
Repeat the process with a( p1k1 · · · pr−1
                                        r−1 −1
                                           ) , and continue.

                                                p 4 until near the end, obtaining x
   13. For (a), proceed as in the argument in Section
and y just as in that construction. Then δ(x + y −2) = x 2 + 2y 2 ≤ 14 + 2 · 14 = 34 .
                        p                p
Then we have δ(r + s −2) < δ(cp+ d −2), and the argument goes through.
   For (b), we would get δ(x + y −3) = x 2 + 3y 2 ≤ 14 + 3 · 14 = 1, and then the
             p                 p
step δ(r + s −3) < δ(c + d −3) fails.
     14. The map extends to an R module homomorphism by the universal mapping
property of RG, and it is one-one onto by inspection. To check that it respects
multiplication, it is enough to show that the product g1 g2 in RG maps to f g1 ∗ f g2 , i.e.,
                                                                    P
that f g1 ∗ f g2 = f g1 g2 . The computation is ( f g1 ∗ f g2 )(x) = y∈G f g1 (x y −1 ) f g2 (y) =
 f g1 (xg2−1 ), and this is 1 if and only if xg2−1 = g1 , i.e., if and only if x = g1 g2 . For
other values of x, it is 0. Therefore ( f g1 ∗ f g2 )(x) = f g1 g2 (x) for all x.
   15. Let the monic polynomial in question be P(X). We prove by induction on
m that any polynomial A(X) in I of degree m is a multiple of P(X). The base case
of the induction is all polynomials of degree < n in I ; only 0 fits this description.
Assume the result for all degrees < m, and let A(X) be any polynomial in I , say
with leading term am X m , am 6= 0. Then am X m−n P(X) is in I , and so is B(X) =
A(X) − am X m−n P(X). The coefficient of X m in B(X) is 0, and hence B(X) = 0
or else deg B(X) < m. If B(X) = 0, then A(X) = am X m−n P(X), and A(X) is a
multiple of P(X). If deg B(X) < m, then induction gives B(X) = C(X)P(X), and
therefore A(X) = (am X m−n + C(X))P(X). So again A(X) is a multiple of P(X).
   16. Let p1 , . . . , pn be n distinct positive primes in Z, put qk = p1 · · · pk for
0 ≤ k ≤ n, and take In+1 = (qn , qn−1 X, qn−2 X 2 , . . . , q0 X n ). This can be written
with n + 1 generators but not with fewer than that.
   17. In (a), certainly ker ϕ ⊇ (y 2 − x 3 ). In the reverse direction, suppose that
PN              n                       2     3        2    3
   n=0 Pn (x)y is in ker ϕ. Since y ≡ x mod (y − x ), we can reduce this element
of ker ϕ to the form Q 0 (x)+Q 1 (x)y. Substituting with t gives Q 0 (t 2 )+Q 1 (t 2 )t 3 = 0.
The first term involves only even powers of t, and the second term involves only odd
powers. Thus each is 0 separately. We are thus to determine what members Q 0 (x)
and Q 1 (x)y of K[x, y] are in ker ϕ. For Q 0 (t 2 ) to be 0, every coefficient of Q 0 must
be 0. For Q 1 (t 2 )t 3 to be 0, every coefficient of Q 1 must be 0. Therefore only 0 is of
the stated form, and every member of ker ϕ lies in (y 2 − x 3 ).
   For (b), image ϕ contains t 2 , t 3 , and every power t n such that n = 2a + 3b with a
and b nonnegative integers. It follows that image ϕ consists of all linear combinations
of powers t n for n ∏ 2.
  19. Write A(X) = B(X)Q(X) in F[X], and let A(X) = c(A)(c(A)−1 A(X)),
B(X) = c(B)(c(B)−1 B(X)), and Q(X) = c(Q)(c(Q)−1 Q(X)) be the decomposi-
                                         Chapter VIII                                      681

tions of Proposition 8.19. Then we have
                                      °                          ¢
          c(A)(c(A)−1 A(X)) = c(B)c(Q) (c(B)−1 B(X))(c(Q)−1 Q(X)) .

By Gauss’s Lemma and the uniqueness in Proposition 8.19, we obtain c(A)−1 A(X) =
(c(B)−1 B(X))(c(Q)−1 Q(X)), apart from unit factors. Therefore the member B0 (X)
= c(B)−1 B(X) of R[X] is exhibited as dividing A0 (X) = c(A)−1 A(X) with a
quotient c(Q)−1 Q(X) in R[X].
    20. Let R be a finite integral domain, and let a 6= 0. Multiplication by a is one-one
since R is an integral domain, and it must be onto R by the finiteness. Therefore there
is some b with ab = 1, and we have produced an inverse for a.
    21. Let R 0 = R/( p). Suppose that A(X) = B(X)C(X) nontrivially in R[X] with
B(X) = bk X k + · · · + b0 , C(X) = bl X l + · · · + c0 , and k + l = N . Since p divides
a0 but p2 does not, p divides exactly one of b0 and c0 , say the former. In R 0 [X],
we have A(X) ≡ a N X N , C(X) ≡ cl X l + · · · + c0 , and A(X) = B(X)C(X). Now
X is prime in R 0 [X] by Problem 4, and X N divides B(X)C(X) in R 0 [X]. Using the
defining property of a prime, one power at a time, we find that X N divides B(X).
Since deg B < N , we must have B(X) ≡ 0 in R 0 [X]. Thus p divides bk in R, and p
divides a N , contradiction.
    22. In (a), we regard W Z − XY as a first-degree polynomial in W , with Z being a
prime in the ring of coefficients.
                        °           A nontrivial factorization
                                                          ¢         of W Z − XY must be of
the form A(X, Y, Z ) B(X, Y, Z )W + C(X, Y, Z ) with Z = A(X, Y, Z )B(X, Y, Z ).
Since Z is prime, one of these factors must be a unit, hence a scalar. If A(X, Y, Z )
is a scalar, then the factorization of W Z° − XY is trivial. Otherwise we may assume
that the factorization is W Z − XY = Z W + C(X, Y, Z )). Then Z divides XY , and
we arrive at a contradiction since Z does not appear in XY .
    In (b), we expand in cofactors about the top row. Using induction, we see that
we can regard the determinant det[X i j ] as a first-degree polynomial in X 11 with an
irreducible coefficient P(X 22 , X 23 , . . . , X nn ). A nontrivial factorization must be of
the form det[X i j ] = P X 11 + Q = A(B X 11 + C), where Q, A, B, C are polynomials
in the remaining indeterminates. Then AB = P and P irreducible implies that
A or B is a unit, hence a scalar. If A is a scalar, our factorization of det[X i j ] is
trivial. Otherwise we may assume that the factorization is det[X i j ] = P X 11 + Q =
P(X 11 +C). Then P must divide Q. Taking the degrees of homogeneity into account,
we see that Q must be the product  Qnof P and a homogeneous polynomial of degree 1.
Every term of P is of the form i=2          X 2,σ (2) for some permutation σ of {2, . . . , n},
and thus such a factor must appear
                                 Qn     in  every term of Q. However, the only terms of
det[X i j ] that contain a factor i=2  X 2,σ (2) also contain the factor X 11 , and this factor
is absent in Q. Thus the assumed reducibility has led to a contradiction.
  23. The ideal of Z[X] generated by A(X) and B(X) consists of all polynomials
A(X)C(X)+ B(X)D(X) with C(X) and D(X) in Z[X]. If such an expression equals
some integer n, then a GCD within Q[X] of A(X) and B(X) divides A(X) and B(X)
682                           Hints for Solutions of Problems

and hence must divide n. It is therefore of degree 0 and is a unit in Q[X]. Thus A(X)
and B(X) are relatively prime in Q[X].
    Conversely if A(X) and B(X) are members of Z[X] that are relatively prime in
Q[X], we can find P(X) and Q(X) in Q[X] with A(X)P(X) + B(X)Q(X) = 1.
Multiplying by a common denominator of the coefficients of P(X) and Q(X), we
obtain a relation A(X)C(X) + B(X)D(X) = n with all polynomials in Z[X]. Thus
n is in the ideal of Z[X] generated by A(X) and B(X).
                       ≥         ¥≥u ¥ ≥ ¥                                   ≥         ¥
    24. We are given 1+i 3 5i
                             2−i     1
                                    u2  =   0
                                            0
                                                with coefficient matrix C  =   1+i 2−i
                                                                                3 5i
                                                                                        .
Left multiplication on C≥by ¥a matrix with determinant a unit does not change the
                              u
total set of conditions on u 12 , and right multiplication by such a matrix changes the
generators but not the module they generate. In the first column of C, we observe
that GCD(1 + i, 3) = 1 because 1 + i divides 2 and GCD(2,                   ¥ we have
                                                               ≥ 3) = 1. Then
                                                                         1
−(1 −i)(1 +i) + 1 · 3 = 1, and we are led to the matrix A = −(1−i)           , which has
                                                     ≥         ¥ −3 1+i
                                                       1 −1+8i
determinant 1. We can thus replace C by AC = 0 −11+8i . An invertible column
operation
≥           replaces
            ¥        the upper right entry by 0. Thus we are led to the diagonal matrix
  1    0
  0 −11+8i
              . In other words, we may assume that the Z[i] module was given to us
with generators t1 , t2 satisfying t1 = 0 and (−11 + 8i)t2 = 0. Therefore the given
Z[i] module is cyclic and is Z[i] isomorphic to Z[i]/(−11 + 8i).
   25. In (a), δ(z) = z z̄. Then δ(zw) = zwz̄ w̄ = z z̄ww̄ = δ(z)δ(w).
   In (b), we start with two nonzero members α and β of R. We are to find ∞ and
ρ in R with α = β∞ + ρ and δ(ρ) < δ(β). It is the same to find ∞ and ρ with
α/β = ∞ + ρ/β and δ(ρ/β) < 1. Apply the hypothesis with z = α/β, and let ∞ be
the element r such that δ(z − r) < 1. Then ρ may be defined as β(z − r), and all the
conditions are satisfied.
                          p                                p            p
   26. Given z = x + y −m, define r = a + 12 b(1 + −m) in Z[ 12 (1 + −m)] by
choosing b to be an integer with |2y − b| ≤ 12 and then choosing a to be an integer
with |x − a − 12 b| ≤ 12 . Since |y − 12 b| ≤ 14 , we then have

       δ(z − r) = (x − a − 12 b)2 + m(y − 12 b)2 ≤       1
                                                         4   +m   1
                                                                  16   ≤   1
                                                                           4   +   11
                                                                                   16   < 1.

   27. In (a), complex conjugation is an automorphism of Z[i] and must therefore
carry primes to primes.
   In (b), we know that (a + bi)(a − bi) is the integer N (a + bi). Suppose that
N (a + bi) = mn nontrivially with GCD(m, n) = 1. Since a + bi is prime, it
divides one of m and n. Say that m = (a + bi)(c + di). Then m 2 = N (m) =
N (a + bi)N (c + di) = mn N (c + di). Any prime number dividing n must divide
the left side m 2 , and hence there can be no such prime. We conclude that N (a + bi)
does not have nontrivial relatively prime divisors. Hence it is a power of some prime
number p.
                                       Chapter VIII                                    683

    In (c), let N (a + bi) = pk . The left side is the product of two primes of Z[i]. If p
is the product of l primes of Z[i], then pk is the product of kl primes. Then we must
have kl = 2, and k must divide 2.
    In (d), suppose N (a + bi) = p 2 , so that k = 2 in (c). Then l = 1, and p is prime
in Z[i].
    28. The equation N (a + bi) = p says that a 2 + b2 = p. The right side is
≡ 3 mod 4, but 3 is not the sum of two squares modulo 4. Hence N (a + bi) = p
is impossible when p ≡ 3 mod 4. Problem 27c then forces N (a + bi) = p 2 , and
Problem 27d says that p is prime in Z[i].
    29. If N (a + bi) = 2, then |a| = |b| = 1, and we obtain 1 + i and its associates.
If N (a + bi) = 4, then a = ±2 with b = 0 or else a = 0 with b = ±2; in these cases
a + bi is an associate of 2, which is (1 + i)(1 − i) and is not prime in Z[i].
    30. The multiplicative group of F p is cyclic of order p − 1. If p is of the form
4n + 1, then F×                          th
                 p has order 4n. The n power of a generator then has to be an integer
whose square is ≡ −1 mod p.
    31. For (a), we obtain ϕ1 by mapping Z[X] to F p [X] with a substitution homo-
morphism and following this with a passage to the quotient. Similarly ϕ2 is obtained
from the substitution homomorphism Z[X] → Z[i] followed by the passage to the
quotient.
    For (b), the kernel of ϕ1 consists of all polynomials that are multiples of X 2 +1 when
their coefficients are taken modulo p. This is pZ[X] + (X 2 + 1)Z[X] = ( p, X 2 + 1).
The kernel of ϕ2 consists of all polynomials with the property that when taken modulo
X 2 + 1, they are multiples of p. This too is the ideal ( p, X 2 + 1).
    For (c), Problem 30 shows that the polynomial X 2 +1 factors nontrivially in F p [X].
Therefore X 2 + 1 is not prime, the ideal (X 2 + 1) is not prime, and Fp [X]/(X 2 + 1)
is not an integral domain. By (b), Z[i]/( p) is not an integral domain, and the ideal
( p) is not prime. Hence p is not prime in Z[i]. By (c) and (d) in Problem 27, p is of
the form N (a + bi) for some prime a + bi in Z[i].
    For (d), if we have p = N (a + bi) = N (a 0 + b0 i), we obtain two prime
factorizations of p in Z[i] as p = (a + bi)(a − bi) = (a 0 + b0 i)(a 0 − b0 i), and
unique factorization in Z[i] implies that a 0 + b0 i is an associate of a + bi or a − bi.
    32. For (a), multiply C on the left by the matrix A that is the identity except in the
first column, where the i th entry is Cii .
    For (b) and (c), the step of row reduction leads to a first column that is 0 in all
entries but the first, where it is GCD(C11 , . . . , Cnn ). In other words, the new entry
in position (1, 1) divides all entries in the new C. Therefore one step of column
reduction leaves the entry unchanged in position (1, 1), leaves the remainder of the
first column equal to 0, and makes the remainder of the first row equal to 0. What
is left in the rows and columns other than the first is a matrix whose entries are all
divisible by GCD(C11 , . . . , Cnn ). Hence we can induct on the size.
    33. In (a), changing notation slightly from Lemma 8.26, write AE = D B with
det A and det B in R × . Over the field of fractions of R, the m-by-n matrices E and D
684                            Hints for Solutions of Problems

must have the same rank since A and B are invertible, and consequently D and E have
the same number of nonzero diagonal entries. Thus for some l with 0 ≤ l ≤ k, we
are given that D j j divides D j+1, j+1 and E j j divides E j+1, j+1 whenever 1 ≤ j < l.
                                              ° ¢ i-by-i determinants that can be formed
Fix i with 1 ≤ i ≤ l, and consider all possible
using the first i rows of B and one of the ni sets of i columns. Since det B is in R × , it
follows from the expansion-by-cofactors formula that these determinants have GCD
equal to 1. Each corresponding determinant for D B equals D11 · · · Dii times such a
determinant, and hence the GCD for D B is D11 · · · Dii .
   Meanwhile, the GCD of the determinants for A is also 1, and, because of the
divisibility property of the diagonal entries of E, E 11 · · · E ii divides each of the
determinants for AE. Hence E 11 · · · E ii divides the GCD of the determinants for
AE, which equals the GCD of the determinants for D B, which equals D11 · · · Dii .
Thus E 11 · · · E ii divides D11 · · · Dii .
   Arguing similarly with the determinants formed from the first i columns of A, AE,
B, and B D, we see that D11 · · · Dii divides E 11 · · · E ii . Therefore D11 · · · Dii and
E 11 · · · E ii are associates for 1 ≤ i ≤ l. Since none of the factors in question is 0, we
see that each of the first l diagonal entries of D is an associate of the corresponding
diagonal entry of E. This proves the desired uniqueness.
   34. For (a), we have seen in this setting that the decomposition of V as a direct
sum of cyclic K[X] modules means a decomposition of V as a direct sum of vector
subspaces, each of which is invariant under L. Also, if V0 is one of these vector
subspaces, the cyclic nature of the module means that there is some vector v0 in
V0 such that K[X]v0 = V0 , and the diagonal entry of the matrix D in the proof
of Theorem 8.25 is a polynomial M[X] such that V0 ∼     = K [X]/(M(X)) as a K [X]
module. Referring to Problems 26–31 of Chapter V, we see that v0 is a cyclic vector
for the cyclic subspace V0 , and M[X] is the minimal polynomial of L on this subspace.
   The divisibility property of the minimal polynomials and also the uniqueness
assertion now follow from what has been proved in Problems 32–33. We know
from Problem 28 in Chapter V that the data of a cyclic subspace and the minimal
polynomial yield a particular matrix for the linear mapping and hence determine
the linear mapping on that subspace up to similarity. Consequently the uniqueness
statement that has just been observed says that L is determined up to similarity by
the integer r and the sequence of minimal polynomials.
    35. Let A and B be members of Mn (K). Form the data for each from the rational
canonical form in Problem 34. Now consider everything as involving vector spaces
over the larger field L. We are given that the two matrices are similar over L, i.e., are
conjugate via GL(n, L). Problem 34 shows that the respective decompositions have
the same data. The two matrices still have the same data when we again consider the
field to be K. Hence they are similar over K, i.e., are conjugate via GL(n, K).
    36. The fact that the homomorphisms are isomorphisms follows from the compo-
sition rule.
                                             Chapter VIII                                           685

   38. In (b), we can write any member of F[X 1 , . . . , X n , X] as

         An (X 1 , . . . , X n )X n + · · · + A1 (X 1 , . . . , X n )X + A0 (X 1 , . . . , X n ),

and σ ∗∗ acts by having σ ∗ act on each coefficient. Invariance under all σ ∗∗ ’s therefore
means that each coefficient is invariant under all σ ∗ ’s and hence is a symmetric
polynomial.
   39. In (a), if, for example i < j and ki < k j , then the monomial a X 1k1 · · · X nkn is
                                                                     k         k
increased in the ordering by replacing the factors X iki X j j by X i j X kj i .
   For (b), we need only take the largest monomial in each E i , raise it to the ci power,
and multiply the results.
   For (c), let the largest monomial in A be a X 1k1 · · · X nkn . To define M, choose r = a
and define c j = k j − k j+1 for 1 ≤ j < n and cn = kn .
   For (d), the construction in (c) yields 0 coefficient for X 1k1 · · · X nkn , and A − r M
has no larger monomials. So if A − r M = 0, the largest monomial is below that
monomial X 1k1 · · · X nkn .
   For (e), iteration of the construction in (c) and (d) shows that any homogeneous
symmetric polynomial equals a homogeneous polynomial in the elementary sym-
metric polynomials. Problem 37 shows that any symmetric polynomial is a linear
combination of homogeneous symmetric polynomials, and hence every symmetric
polynomial is a polynomial in the elementary symmetric polynomials.
    40. Suppose that z 0 and w0 in Cm have P(z 0 ) 6= 0 and P(w0 ) 6= 0. As a function
of t ∈ C, P(z 0 + t (w0 − z 0 )) is a polynomial function nonvanishing at t = 0 and
t = 1. The subset of t ∈ C where it vanishes is finite, and its complement in C
is necessarily pathwise connected and therefore connected. Thus z 0 and w0 lie in a
connected subset of Cm where P is nonvanishing. Taking the union of these connected
sets with z 0 fixed and w0 varying, we see that the set of w0 ∈ Cm where P(w0 ) 6= 0
is connected.
    41. For (a), two applications of the formula relating Pfaffians and determinants
gives us Pfaff(At X A)2 = det(At X A) = (det A)2 det X = (det A)2 Pfaff(X)2 . Tak-
ing the square root gives the desired result.
    For (b), we fix X with Pfaff(X) 6= 0 and allow A to vary. On the set where
det A 6= 0, the function A 7→ Pfaff(At X A)/ det A is a continuous function with image
in the two-point set {±Pfaff(X)}, by (a). The domain of the function is connected by
Problem 40, and therefore the image has to be connected. Hence the function has to
be constant. Checking the value of the function at A = I , we see that the function
has to be constantly equal to Pfaff(X).
    42. Form the ring S = Z[{Ai j }, {X i j }]. We can then regard Pfaff(At X A) and
(det A)Pfaff(X) as two polynomials with entries in S. If we fix arbitrary elements
ai j ∈ Z for all i and j and also xi j ∈ Z for i < j, then Proposition 4.30 gives us
a unique substitution homomorphism 9 → Z such that 9(1) = 1, 9(Ai j ) = ai j ,
686                             Hints for Solutions of Problems

and 9(X i j ) = xi j . Assemble the ai j and xi j into matrices a = [ai j ] and x = [xi j ]
with x alternating. Problem 41b shows that the identity in question holds when the
entries are in C, and in particular it holds when the entries are in Z. Therefore
Pfaff(a t xa) = (det a)Pfaff(x). Since Z is an integral domain and since a and x are
arbitrary with x alternating, Corollary 4.32 allows us to conclude that Pfaff(At X A) =
(det A)Pfaff(X) as an equality in S.
    To pass from S to K, let 1K be the identity of K, and let ϕ1 : Z → K be the
unique homomorphism of rings such that ϕ1 (1) = 1K . If we fix arbitrary elements
ai j of K for all i and j, as well as arbitrary elements xi j of K for i < j, then
Proposition 4.30 gives us a unique substitution homomorphism 8 : S → K such
that 8(1) = ϕ1 (1) = 1K , 8(Ai j ) = ai j for all i and j, and 8(X i j ) = xi j whenever
i < j. Applying 8 to our identity in S, we obtain Pfaff(a t xa) = (det a)Pfaff(x) as
an equality in K.
   43. From Problem 42 and the hypothesis on g, we have 1 = Pfaff(J ) =
Pfaff(g t J g) = (det g)Pfaff(J ) = det g. Hence det g = 1.
   45. For (a), if ϕ : R → R/P k is the quotient homomorphism, then ϕ −1 of any
ideal of R/P k is an ideal I of R containing P k . If Q is a prime ideal dividing I , then
Q divides P k , and it follows that Q = P. Thus the only possibilities for I are the
powers P i of P, necessarily stopping with i = k.
   For (b), we know that π i lies in P i but not P i+1 . For 1 ≤ i ≤ k − 1, it follows that
the principal ideal (π i + P k )/P k is contained in the ideal P i /P k but not in P i+1 /P k .
Since the ideals P j /P k for j ≤ k are nested and there are no other ideals in R/P k ,
we must have (π i + P k )/P k = P i /P k . Thus P i /P k is principal.
   46. Corollary 8.63 and Problem 44 together show that every ideal of R/I is
principal if it can be shown that every ideal of R/P k is principal when P is a nonzero
prime ideal. The two parts of Problem 45 together show that every ideal of R/P k is
principal.
    47. We may assume that (a) $ I since otherwise the result follows with b = 0.
Since a 6= 0, the ideal I /(a) in °R/(a) ¢is a principal ideal by Problem 46c. If b0
is a generator of this ideal, then R/(a) b0 = I /(a). Since b0 is in I /(a), we can
write
°                   ¢ (a) for some b in I . Every member of I /(a) is then of the form
      it as¢°b0 = b +
 r + (a) b + (a) = rb + (a), and we conclude that every member of I is of the
form rb + sa with r and s in R.
   48. Any R submodule of R is an ideal.
                                                                           of the form
     49. Write M = Rx1 + · · · + Rxn with x1 , . . . , xn in F. Each xi is Q
                                                                             n
ri si−1 with ri and si in R and with si 6= 0. Then a M lies in R for a = i=1    si . So
a M is an ideal in R, by Problem 48. If N is a second fractional ideal, choose b 6= 0
such that bN is an ideal in R. Then (a M)(bN ) is an ideal in R, and the formula
M N = (ab)−1 (a M)(bN ) shows that M N is a fractional ideal.
   50. Since I is a finitely generated R module, we can write I = Ra1 + · · · + Ran
with all ai in R. The condition for x ∈ F to be in I −1 is that x I ⊆ R, and it is
                                       Chapter IX                                   687

necessary and sufficient that xai be in R for all i. Thus it is necessary that x be
in (a1 · · · an )−1 R. Consequently I −1 is an R submodule of the singly generated R
module (a1 · · · an )−1 R. Since R is Noetherian, I −1 is finitely generated.
   51. If I is maximal among the nonzero ideals of R for which there is no fractional
ideal M of F with I M = R, then Lemma 8.58 shows that I is not prime. Choose
a nonzero prime ideal P with I $ P. Then Lemma 8.58 and the definitions give
I ⊆ I P −1 ⊆ I I −1 ⊆ R. We cannot have I = I P −1 since otherwise I P =
(I P −1 )P = I (P −1 P) = I and Proposition 8.52 gives I = 0. By maximality of I ,
we can find some fractional ideal N with (I P −1 )N = R. Then I (P −1 N ) = R, and
we can take M = P −1 N , by Problem 49.
   52. Every member x of M has x I ⊆ R, and thus M ⊆ I −1 . On the other hand, if
x is in I −1 , then x I ⊆ R, x = x I M ⊆ R M = M, and x is in M.
   53. If M is a fractional ideal, then Problem 49 produces c 6= 0 in F with cM ⊆ R,
and Problem 48 shows that cM is an ideal of R. Using Problem 52, we can write
M = (c)−1 (c)M = (c)−1 (cM). This proves that M = I J −1 for ideals I and J . Then
(a) follows from Theorem 8.55 and Problem 52, and (b) follows from Problem 52.


                                     Chapter IX

     1. The equation for r gives r 3 = 3r − 4 and r 4 = 3r 2 − 4r. Therefore the inverse
has 1 = (r 2 +r +1)(ar 2 +br +c) = ar 4 +(a +b)r 3 +(a +b +c)r 2 +(b +c)r +c =
r 2 (4a + b + c) + r(−a + 4b + c) + 1(−4a − 4b + c), and we are led to the system
of linear equations

                                    4a + b + c = 0,
                                  −a + 4b + c = 0,
                                 −4a − 4b + c = 1.

                      3     5 17                               3 2     5
Then (a, b, c) = (− 49  , − 49 , 49 ), and (r 2 + r + 1)−1 = − 49 r − 49 r + 17
                                                                             49 .
    2. Multiplication by a nonzero r is a one-one F linear mapping from the F vector
space R onto itself. Since dim F R < ∞, this linear mapping must be onto. The
element s such that rs = 1 is a multiplicative inverse of r.
    3. Let z 0 be a nonreal element of K. Then the closure of the Q vector space
Q + Qz 0 contains R + Rz 0 = C.
    4. If y = F(x)/G(x), then G(x)y = F(x). Arranging the terms as powers of x
with coefficients of the form ay + b with a and b in k, we see that x is a root of a
polynomial in one indeterminate over k(y). Therefore x is algebraic over k(y).
    5. The condition is that N be                                                 2
                                  p the square of an integer. For any other N , X p− N
is irreducible over Q, and p [Q( N ) : Q] = 2. Since 2 does not divide 3, Q( N )
cannot be a subfield of Q( 3 2 ).
688                             Hints for Solutions of Problems

   6. X 7→ Y + 1.
   7. No, since 8 is not a power of 4. See Corollary 9.19.
   8. Let g be a generator of the cyclic group K× , and let q be the order of K. Then
                                                                    1
                  g · g 2 · g 3 · · · g q−1 = g 1+2+3+···+(q−1) = g 2 q(q−1) .
                                                                                 1
If q is even, then this is (g q−1 )q/2 = 1q/2 = 1 = −1. If q is odd, it is (g 2 (q−1) )q =
(−1)q = −1.
    9. Proof 1: Let F(X) = X n + cn−1 X n−1 + · · · + c0 be the minimal polynomial
of r. We are given that n is odd. Write the equation F(r) = 0 as

        r(r n−1 + cn−2r n−3 + · · · + c1 ) = −cn−1r n−1 − cn−3r n−3 − · · · − c0 .

Then r is expressed as an element of k(r 2 ) unless r n−1 + cn−2r n−3 + · · · + c1 = 0.
But this expression cannot be 0 because this polynomial has degree n − 1 and the
minimal polynomial for r has degree n.
   Proof 2: The element r of K is a root of the polynomial X 2 − r 2 in k(r 2 )[X], and
hence [k(r) : k(r 2 )] ≤ 2. Since [k(r) : k] = [k(r) : k(r 2 )] [k(r 2 ) : k] with the left
side odd by assumption, [k(r) : k(r 2 )] has to be odd. Thus it is 1.
   10. Let dr = [k(r) : k] and ds = [k(s) : k]. Since K contains k(r) and k(s), we
see that dr and ds divide [K : k]. Since GCD(dr , ds ) = 1, dr ds divides [K : k]. The
minimal polynomial M(X) of r over k is a polynomial over k(s) such that M(r) = 0.
Thus the minimal polynomial N (X) of r over k(s) divides M(X). If c is the degree
of N (X), we then have c ≤ dr . Since dr ds divides [K : k], we obtain

      dr ds ≤ [K : k] = [k(r, s) : k] = [k(s)(r) : k] = c[k(s) : k] = cds ≤ dr ds .

Equality must hold throughout. Equality at the right end says that c = dr , and this
proves (a). Equality at the left end says that dr ds = [K : k], and this proves (b).
                                                                                  p
    11. In (a), we have ∞ = β + cα = β(1 + cω). Here r = 1 + cω lies in Q( −3 ),
and so does r 3 . Therefore r 3 is a root of a quadratic polynomial Y 2 + pY + q. Then
∞ 6 + a∞ 3 + b = r 6 β 6 + ar 3 β 3 + b = 4r 6 + 2ar 3 + b = 4(r 6 + 12 ar 3 + 14 b), and
the right side is 0 if a and b are chosen such that p = 12 a and q = 14 b.
                                                              p
    In (b), ∞ = β + α = β(1 + ω), and ∞ 3 = β 3 ( 12 (1 + −3 ))3 = 2(−1) = −2.
Then ∞ satisfies ∞ 3 + 2 = 0, and this is irreducible since
                                                          p −2 is not a cubepin Q.
    In (c), the field Q(∞ ) contains ∞ 3 = β 3 ( 12 (3 − −3 ))3 = 14 (3 − −3 )3 =
1
          p                           p            p              p
4 (27 − 9 −3 + 3 · 3(−3) − (−3) −3 ) =          − 3 −3. Thus Q( −3 ) is a subfield of
                                              p 2
Q(∞ ), and 2 divides
                  p    [Q(∞ ) : Q]. Since Q( −3 ) is a subfield, β = ∞ (1−ω)−1 lies in
Q(∞ ). Thus Q( 2 ) is a subfield of Q(∞ ), and 3 divides [Q(∞ ) : Q]. Consequently
                   3


6 divides [Q(∞ ) : Q], and the minimal polynomial of ∞ has degree ∏ 6. By (a), it
has degree exactly 6.
                                         Chapter IX                                      689

     12. Let the characteristic be p. If F(X) has F 0 (X) = 0, then all the exponents
of X appearing in F(X) are multiples of p. Let F(X) = an X np + an−1 X (n−1) p +
· · · + a1 X p + a0 . Since the Frobenius map is onto in the case of a finite field, we
                                                   p         p                     p
can choose members cn , . . . , c0 of k such that cn = an , cn−1 = an−1 , . . . , c0 = a0 .
Put G(X) = cn X n + cn−1 X n−1 + · · · + c0 . Then F(X) = G(X) p , and F(X) is
reducible.
     13. In (a), if F(X) = G(X)H (X) is reducible and r1 is a root of G(X), then σ (r1 )
is a root of G(X) for any σ ∈ Gal(K/k). Consequently the orbit of r1 under Gal(K/k)
is a proper subset of the set of roots of F(X). Conversely if F(X) is irreducible and r j
is given, then the uniqueness of simple extensions gives us a k isomorphism of k(r1 )
onto k(r j ). Theorem 9.130 shows that this isomorphism extends to a k automorphism
of K, and hence Gal(K/k) is transitive on the set of roots of F(X).
     In (b), the transitivity follows from (a) and the irreducibility of 88 (X) over Q. Let
≥ = e2πi/8 . The roots of 88 (X) = X 4 + 1 are ≥, ≥ 3 , ≥ 5 , ≥ 7 . So if σ is in Gal(K/Q),
                                                                                    2
then σ (≥ ) = ≥ k with k odd. Then σ 2 (≥ ) = σ (≥ k ) = σ (≥ )k = (≥ k )k = ≥ k . Since
the square of any odd integer is congruent to 1 modulo 8, σ 2 (≥ ) = ≥ . Thus each σ
has σ 2 = 1, and Gal(K/Q) cannot contain a 4-cycle.
     In (c), the irreducibility of F(X) implies that F(X) is the minimal polynomial of
r1 . Hence [k(r1 ) : k] = n. Since k(r1 ) ⊆ K, [k(r1 ) : k] must divide [K : k], and
n divides [K : k]. Therefore n divides the equal integer Gal(K/k). If n is prime,
then the fact that n divides the order of Gal(K/k) implies that Gal(K/k) contains an
element of order n, by Sylow’s Theorems. The only elements of order n in Sn are
the n-cycles, and hence Gal(K/k) contains at least one n-cycle.
                                        p
     14. In (a), we have Lk+1 = Lk ( ak+1 ), and hence [Lk+1 : Lk ] equals 1 or 2. By
induction, [Lk : Q] is a power of 2, and the power is at most the number of steps in
the induction, namely k.
                                                                                Q p
     In (b), associate to each subset S of {1, . . . , k} the element v S = j∈S a j in
Lk . The product of any two such elements is an integer multiple of a third such
element, and hence the elements v S span Lk linearly over Q. Since there are 2k
such elements, they form a vector-space basis. The extension Lk /Q is separable,
                                                                           Q
being in characteristic 0, and it is normal as the splitting field of kj=1 (X 2 − a j ).
So it is a finite Galois extension. Any member σ of Gal(Lk /Q) must permute the
                                                  p           p
roots of each X 2 − a j and hence must send a j to ± a j . On the other hand, σ is
                                      p
determined by its effect on each a j . Since Gal(Lk /Q) has order 2k , there exists for
                                                                   p          p
each subset S of {1, . . . , k} one and only one σ such that σ ( a j ) = − a j for j ∈ S
         p             p
and σ ( a j ) = + a j for j ∈      / S. The group Gal(Lk /Q) consists exactly of these
elements.
                                                               p         p
     In (c), let σ j be the member of Gal(Lk /Q) with σ j ( ai ) = − ai for i = j and
     p            p
σ j ( ai ) = + ai for i 6= j. Then σ j (v S ) = −v S if j is in S, and σ j (v S ) = +v S if j
is not in S.
                                         p            P
     Arguing by contradiction, let ak+1 =                 c S v S with each c S in Q. If
690                               Hints for Solutions of Problems

     p        p
σ j ( ak+1 ) = ak+1 , then we have
P                 p            p         P                    P                         P
        cS vS =    ak+1 = σ j ( ak+1 ) =   c S σ j (v S ) = −               cS vS +             cS vS ,
all S                                      all S               S with j∈S         S with j ∈S
                                                                                           /

                                                                           p
and it follows that c S = 0 whenever j is in S. On the other hand, if σ j ( ak+1 ) =
  p
− ak+1 , then we have
P                 p             p           P                          P                 P
        cS vS =    ak+1 = −σ j ( ak+1 ) = −   c S σ j (v S ) =               cS vS −            cS vS ,
all S                                              all S            S with j∈S        S with j ∈S
                                                                                               /


and it follows that c S = 0 whenever j is not in S.
                            p                p
    Define S0 = { j | σ j ( ak+1 ) = − ak+1 }. From the above it follows that
c S = 0 whenever some member of S0 is not in S, and that c S = 0 whenever some
member of the complement of S0 is in  qQ S. In other words, c S = 0 except for c S0 . We
              p                                                                  2
                                                                                   Q
conclude that ak+1 = c S0 v S0 = c S0        j∈S0 a j and hence that ak+1 = cs0      j∈S0 a j .
This contradicts the hypothesis that {a1 , . . . , an } are relatively prime and square free.
        p
Hence ak+1 does not lie in Lk . This proves (c), and we obtain [Lk+1 : Lk ] = 2. By
induction we see that [L : Q] = 2n . This proves (d).
    15. For (a) and (b), Lemma 9.45 shows that X p − a is irreducible over Q. Hence
[Q(r) : Q] = p. Let ≥ be a primitive p th root of 1. Then [Q(≥ ) : Q] = p − 1
is relatively prime to [Q(r) : Q] = p. Problem 10a shows that 8 p (X) is irre-
ducible in Q(r). Since ≥ and r generate K, Problem 10b shows that [K : Q] =
[Q(r) : Q] [Q(≥ ) : Q] = p( p − 1).
    In (c), the Galois correspondence between intermediate fields and subgroups of
G = Gal(K/Q) associates Q(≥ ) to the subgroup N = Gal(K/Q(≥ )), and it associates
Q(r) to the subgroup H = Gal(K/Q(r)). Since Q(≥ )/Q is a normal extension, N
is a normal subgroup of G. Any member of H ∩ N fixes r and ≥ , hence fixes all of
K; thus H ∩ N = {1}. The order of N is [K : Q(≥ )] = p, and the order of H is
[K : Q(r)] = p − 1. Therefore |G| = |H ||N |, and G is a semidirect product with N
normal.
    Proposition 4.44 says that the action of an internal semidirect product is given by
τh (n) = hnh −1 . Let us identify τh . Let h ∈ H = Gal(K/Q(r)) have h(r) = r and
h(≥ ) = ≥ k , and let n in N = Gal(K/Q(≥ )) have n(≥ ) = ≥ and n(r) = r≥ l . Then
                                                                0         0
hnh −1 (r) = hn(r) = h(r≥ l ) = r≥ kl , and hnh −1 (≥ ) = hn(≥ k ) = h(≥ k ) = ≥ . So if
n sends r to r≥ l and h(≥ ) = ≥ k , then hnh −1 is the member of N sending r to ≥ kl .
    This n is the member of N corresponding to l ∈ F p , and this h is the member of
H corresponding to k ∈ F×    p . We have just shown that hnh
                                                               −1 is the member of N

corresponding to kl ∈ F p . Hence the action corresponds to multiplication of F×   p on
additive F p .
    16. [K : k] = Gal(K/k), and Gal(K/k) is a subgroup of Sn . Being a subgroup,
its order divides the order of Sn , which is n!.
                                        Chapter IX                                      691

    17. In (a), the most general element of K is of the form x + yr with x and y in k,
and its square is (x 2 + y 2r 2 ) + 2x yr. This is in k if and only if x y = 0, i.e., if and
only if x + yr is in k or in rk. In other words, the only squares in K that lie in k are
the obvious ones.
    In (b), the same remarks apply unless the characteristic is 2. If the characteristic
is 2, then (x + yr)2 = x 2 + y 2r 2 and this is in k for all x and y. Hence every element
of K is a square.
  18. The finite group G may be regarded as a subgroup of the symmetric group
Sn for n = |G|. It was shown in Example 3 of Section 17 that there exists a finite
Galois extension K of Q with Galois group Sn . Let k be the fixed field of G within
Sn . Then Gal(K/k) = G.
    19. The polynomial in question in fixed by every element of the Galois group.
Hence its coefficients are in the subfield of K fixed by all elements of Gal(K/k). This
is k.
                                    Q
    20. For (a), define F(X) = nj=1 (X − x j ). For ϕ in H , we have F ϕ (X) =
Qn                      Qn                                             H
   j=1 (X − ϕ(x j )) =    j=1 (X − x j ) = F(X). Thus F(X) is in K [X]. Let M(X)
                                          H
be the minimal polynomial of x1 over K . Since F(x1 ) = 0, M(X) divides F(X). On
the other hand, the equalities M ϕ (X) = M(X) and M(x1 ) = 0 imply that M(x j ) = 0
for each j. Thus M(X) has degree at least n, and we conclude that F(X) = M(X).
    In (b), n is the number of elements in an orbit of H and hence divides |H |.
    In (c), when the isotropy subgroup of H at x1 is trivial, n = |H |. Therefore
[K H (x1 ) : K H ] = n = |H | = [K : K H ], the last equality following from Corollary
9.37. Since K H (x1 ) ⊆ K, it follows that K H (x1 ) = K.
    21. For (a), let ϕ(z) = az+b
                               cz+d with ad − bc 6= 0. Then we have a substitution
homomorphism of C[X] into C(z) fixing C and sending X into z. Since the range
is a field, this factors through the field of fractions of C[X] to give a field mapping
C(X) → C(z). We can regard the result as a map of C(z) into itself, and we write the
map of C(z) into itself as 8ϕ −1 . The formula is 8ϕ −1 (r) = r ◦ ϕ for r = r(z) in C(z).
Then 8√ϕ (r) = r ◦ (√ϕ)−1 = (r ◦ ϕ −1 ) ◦ √ −1 = 8√ (r ◦ ϕ −1 ) = 8ϕ (8√ (r)), and
hence 8√ϕ = 8√ ◦ 8ϕ . From this it follows that 8ϕ −1 is a two-sided inverse of 8ϕ .
Hence 8ϕ is an automorphism.
    For (b), 8σ (w(z)) = w(σ −1 (z)) = (−z)2 + (−z)−2 = z 2 + z −2 = w(z),
and 8τ (w(z)) = w(τ −1 (z)) = (1/z)2 + (1/z)−2 = z 2 + z −2 = w(z). Since
8ϕ√ = 8ϕ 8√ by (a), it follows that every element of H fixes w. Since each 8ϕ is
a field automorphism, C(w) lies in K H .
    For (c), we know from (b) that C(w) ⊆ K H . The orbit of z under H has 4 elements,
and Problem 20a shows that the minimal polynomial of z over K H has degree 4 and
is equal to

F(X) = (X −z)(X +z)(X −z −1 )(X +z −1 ) = (X 2 −z 2 )(X 2 −z −2 ) = X 4 −w(z)X 2 +1.
692                          Hints for Solutions of Problems

The polynomial F(X) is irreducible over K H , and its formula shows that its coef-
ficients are in the smaller field C(w). Hence it is irreducible over C(w) and is the
minimal polynomial of z over C(w).
    For (d), (c) shows that [K H (z) : C(w)] = 4. Problem 20c shows that K = K H (z),
and hence [K : C(w)] = 4. Since [K : C(w)] = [K : K H ] [K H : C(w)] and since
[K : K H ] = 4 by Corollary 9.37, K H = C(w).
                              p                  p                                p
    22. For (a), let L = K( u ) and K = k( v ). The minimal polynomialp        of u
over K is X 2 − u, and this must divide thep    minimal polynomial of u over k. The
degree of the latter polynomial equals [k( u ) : k], which must divide 4. Hence
it must be 2 or 4. If it is 2, then X 2 − u lies in k[X], and u is inpk. We return
to this case in a moment. Suppose      pthat the minimal polynomial of p u over k has
degree 4. Letp   us write u  =p r +  s  v for some r and s in k. Then u is a root of
(X 2 −r −s v )(X 2 −r +s v) = (X 2 −r)2 −s 2 v = X 4 −2r X 2 +(r 2 −s 2 v), which
is a quartic polynomial in k[X]. Since the minimal polynomial over k has p   degree 4,
this is the minimal polynomial and is irreducible.
                                                p    Thus (a) holds with r =p u. p
    The remaining case is that u is in k but u is not in k. Consider ± u ± v.
None of these is in k. The computation
        p    p         p     p        p    p        p p
 (X +    u + v))(X + u − v)(X − u + v)(X − u − v)
                    p             p
           = ((X + u)2 − v)((X − u)2 − v)
                              p                   p
           = (X 2 + u − v + 2X u)(X 2 + u − v − 2X u)
            = (X 2 + u − v)2 − 4u X 2 = X 4 + 2u X 2 − 2v X 2 + (u − v)2 − 4u X 2
            = X 4 − 2(u + v)X 2 + (u − v)2 = X 4 + bX 2 + c

shows that these are all roots of a quartic polynomial in k[X] of the correct kind,
and the question concerns its irreducibility over k. As in the previous paragraph,
reducibility implies that it is the product of two irreducible quadratic members of
k[X]. Then the product of two of the first-order factors is in k[X], and the sum p
ofpthose two roots must bep  in k. Thep six possible sums of pairs of roots are ± u,
± v, andp   0 twice.
                  p Since u and     p v arep not in k, the irreducible quadratic
                                                                         p       must
be X 2 −p( u + v)2 or X 2 − ( p       u − pv)2 . However, the fact that u is not in
K = k( v) implies that neither of u ± v is in k. Thus the quartic polynomial is
indeed irreducible. This completes (a).
   In (b), we have 4 = [L : k] = [L : k(r)] [k(r) : k] = 4[L : k(r)]. Thus
[L : k(r)] = 1, and L = K(r).
   In (c), suppose that c = t 2 for the given F(X). Find members u and v of k with
−2(u
  p +p   v) = b and u − v = t. Then the displayed computation in (a) shows that
± u ± v are the roots of X 4 − 2(u + v)X     2          2      4      2
                                                 p− v) = X + bX + c. The given
                                            p + (u
root r must be one of these. Say that r = u + v without loss of generality. Since
[k(r) : k] =    p [L : k] = p
            p 4 and            4 andpk(r) ⊆ L, we have
                                                   p p  L = k(r).pOn thep other hand,
k(r) ⊆ k( u, v ), and [k( u, v ) : k] = [k( u, v) : k( u)] [k( u) : k] ≤
                                         Chapter IX                                       693
                       p p                                                 p      p
2 · 2 = 4. Hence k( u, v ) = k(r) = L. Then all four roots ± u ± v of F(X)
                                              p k, and
lie in L, L is the splitting field of F(X) over          p L/k is normal.
                                                                        p The Galois group
is generated by one   p element  that psends    pu to −    u and  fixes    v, and by a second
element that fixes u and sends v to − v. Hence it is C2 × C2 .
    Conversely suppose that L/k is normal with Galois group G = Gal(L/k) =
C2 × C2 . Let an irreducible polynomial X 4 + bX 2 + c in k[X] with a root r in L be
given. Since L/k is normal, X 4 + bX 2 + c splits in L. Let the four roots be ±r and
±s. The square u of any of these roots satisfies u 2 + bu + c = 0 and therefore lies
in a quadratic extension within K, the same quadratic p         extension for each root. Let
us define K to be this extension. Then K = k( b2 − 4c ). Because of the structure
of G, there exists exactly one element σ p      in G whose fixed field is K. The minimal
polynomial of ±r over K is X 2 + 12 b ± 12 b − 4c for one of the two choices of sign,
and the minimal polynomial of ±s over K is the one for the other choice of sign. The
element σ must then permute the roots of each of these polynomials, and it follows
that σ (r) = ±r and σ (s) = ±s. Since neither r nor s is in K, we must in fact have
σ (r) = −r and σ (s) = −s. Therefore σ (rs) = rs. One of the other two nontrivial
members τ of G has τ (r) = s. Since τ 2 = τ , we have τ (s) = r. Thus τ (rs) = rs,
and we see that every member of G fixes rs. Consequently rs is in k. Since rs is
equal for some choice of signs to
     q             p          q            p                 q                          p
   ± − 12 b + 12 b2 − 4c − 12 b − 12 b2 − 4c = ± 14 b2 − 14 (b2 − 4c) = ± c,
p
   c is in k. In other words, c is the square of a member of k, as asserted.
    In (d), suppose that c−1 (b2 − 4c) p    for the given F(X) is a square in k. Arguing
        2
with r as in (c), we see that K = k( b2 −         p4c ). Making     the same computation as
                                                                −1 (b2 − 4c) is a square in k,
in
p  the  display  just above,  we  see that rs  =    c.  Since c                  p
   c lies in K. One of the roots, say r, lies in L, and the product rs = c lies in K,
hence in L. We conclude that ±r and ±s all lie in L. In other words, L is the splitting
field of F(X) over k and is normal. Thus L/k is normal. The          p Galois group must be
either C2 × C2 or C4 . If it is C2 × C2 , then  p   (c) shows   that   c lies in k. Under our
assumption that c−1 (b2 − 4c) is a square, b2 − 4c lies in k. Consequently F(X) is
reducible, contradiction. We conclude that the Galois group is C4 .
    Conversely suppose that L/k is normal with Galois group G = Gal(L/k) = C4 .
Let an irreducible polynomial X 4 + bX 2 + c in        p k[X] with a root r in L be given.
Arguing with r 2 , we see that r 2 lies in K = k( b2 − 4c ). Since L is generated by
k and r, a generator of G cannot send r into ±r. On the other hand, some element
of G has to send r into −r since −r is a root of the given polynomial. Therefore
σ 2 (r) = −r. Then we have σ (rσ (r)) = σ (r)σ 2 (r) = −rσ (r), and we see that
  2
p (rσ (r)) = rσ (r). Consequently rσ (r) lies in K. Computing as in (c), we find that
σ                                                                                          p
   c lies in K. This member of K p     has its square in k, and Problem 17 shows that c
                                                            p
          or in the set of products k b2 − 4c. By (c), c cannot lie in k, and therefore
lies in k p
p
   c = d b2 − 4c for some d in k. Hence c−1 (b2 − 4c) = d −2 for an element d of
k.
694                           Hints for Solutions of Problems
                                    p                  p
    For (e), one can take L = K( 4 2 ) and K = Q( 2 ). We can easily see directly
that L is not normal. But let us use (c) and (d). The minimal polynomial F(X) in
question is X 4 + 2, with b = 0 and c = 2. The conditions in (c) and (d) say that
L/k is normal if and only if either 2 is a square in Q or −1 is a square in Q. Neither
condition is satisfied, and hence L/k is not normal.
    23. A cubic will be irreducible if it is divisible by no degree-one factor over Q,
hence if it has no root in Q. Since these cubics are monic and are in Z[X], they will
be irreducible if they have no integer root. An integer root must divide the constant
term, and we check that neither of ±1 is a root in either case. Hence both cubics are
irreducible. By Problem 13 the Galois group in each case is a transitive subgroup of
S3 , hence is S3 or A3 . The discriminant −4 p3 − 27q 2 is 81 in the first case and −31
in the second case; this is a square in the first case but not in the second case. Thus
X 3 − 3X + 1 has Galois group A3 , and X 3 + X + 1 has Galois group S3 .
    24. The extensionp field is either K itself, in which case the Galois group remains
S3 , or it is L = K[ −3 ]. Since K/Q is normal, Gal(L/K) is a normal subgroup of
Gal(L/Q) of order 2 with quotient isomorphic to Gal(K/Q) = S3 . The groups of
order 12 are classified in Problems 45–48 at the end of Chapter IV. Two such groups
are abelian, one is A4 , and one is D6 ∼= Cp2 × S3 .               p              p
    Write a general element of L as a + b −3. Define τ (a + b −3 ) = a − b −3.
This is the nontrivial member of the 2-element group Gal(L/K). If σ is in Gal(K/Q),
                                                                               p
then σ extends p  to a  member    σ of Gal(L/Q)     by the definition σ (a + b   −3) =
σ (a) + σ (b) −3. In fact, σ respects addition. To see that it respects multiplication,
we compute
              p            p        °            p ¢°                p ¢
      σ (a + b −3 )σ (c + d −3 ) = σ (a) + σ (b) −3 σ (c) + σ (d) −3
                       °                      ¢ °                       ¢p
                    = σ (a)σ (c) − 3σ (b)σ (d) + σ (b)σ (c) + σ (a)σ (d) −3
                                                p
                    = σ (ac − 3bd) + σ (bc + ad) −3
                         °                      p ¢
                    = σ (ac − 3bd) + (bc + ad) −3
                         °     p            p    ¢
                    = σ (a + b −3 )(c + d −3 ) .

It follows that Gal(L/Q) is the direct product C2 × S3 , the subgroup C2 being
Gal(L/K).
    25. Yes. Let L be the intermediate field corresponding to the subgroup {(1), (1 2)}.
Since the subgroup is not normal, L/k is not normal. Let r be any element of L not
in k. Then the minimal polynomial of r over k has degree 3, and it does not split in
L since L/k is not normal. Its splitting field has to be something between L and K,
and the only choice is K.
    26. Yes, substitute and check it.
    28. In (a), direct expansion of the right side gives (X − r)(X 2 + r X + (r 2 + p)) =
X 3 + p X − r 3 − pr. Since −r 3 − pr = q, the assertion follows.
                                           Chapter IX                                         695

  For (b), let us check that r 2 (−4 p3 − 27q 2 ) = (−3r 2 − 4 p)(3q + 2 pr)2 , from
which the assertion follows. In fact, the right side equals
    −(3r 2 + 4 p)(9q 2 + 12 pqr + 4 p2r 2 )
       = −(36 pq 2 + 48 p2 qr + 27q 2r 2 + 16 p3r 2 + 36 pqr 3 + 12 p2r 4 )
       = −r 2 (4 p3 + 27q 2 ) − 12 p3r 2 − 36 pq 2 − 48 p2 qr − 36 pqr 3 − 12 p2r 4
       = −r 2 (4 p3 + 27q 2 ) − 12 p3r 2 − 36 pq 2 − 48 p2 qr
          − 36 pq(− pr − q) − 12 p2r(− pr − q)
       = −r 2 (4 p3 + 27q 2 ).
    29. No. For example, F(X) could have three real roots, and then K would be a
subfield of R. A concrete example is X 3 − 12X + 1, which is < 0 at −4, is > 0 at
0, is < 0 at 1, and is > 0 at 4; the Intermediate Value Theorem shows that F(X) has
three real roots.
    30. The group in question is a subgroup of S5 . It is transitive because of the
irreducibility, and it is a subgroup of A5 since the discriminant is a square. Problem
13c shows that it contains a 5-cycle. The other cycle structures in A5 are the 3-cycles
and the pairs of 2-cycles. If a 3-cycle is present, then the group is all of A5 because
15 divides its order, all groups of order 15 are cyclic, and A5 contains no subgroup
of order 30, being simple.
    Suppose there are no 3-cycles. A Sylow 2-subgroup may be taken to be a subgroup
of H = {(1), (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)}, and it acts on the group of powers
of a 5-cycle. The only nontrivial action of a 2-element group on a 5-element group
carries elements to their inverses. Since no nontrivial element of H commutes with a
5-cycle (because S5 has no elements of order 10), the Sylow 2-subgroup contains at
most two elements. If it is trivial, then the group in question is of order 5, consisting
of the powers of a 5-cycle. If the Sylow 2-subgroup has 2 elements, we obtain a
semidirect product of a 2-element group with the powers of the 5-cycle, and the result
has to be isomorphic to the dihedral group D5 .
    Thus the only possibilities are C5 , D5 , and A5 .
    31. Computation shows that the discriminant is 212 72 192 , which is a square.
By Proposition 9.63 the Galois group is a subgroup of A5 . Modulo 3, the given
polynomial is 2 + 2x + x 5 and is irreducible. By Theorem 9.64 the Galois group
contains a 5-cycle. The given polynomial factors as (7 + x)(7 + 10x + 7x 2 + x 3 )
modulo 11, and Theorem 9.64 shows that the Galois group contains a 3-cycle. The
5-cycle and 3-cycle generate all of A5 , and thus the Galois group is A5 .
    32. Write e and f for e1 and f 1 . The proof of Theorem 9.64 showed that f 0 = f .
Then e0 f 0 = |G P | = |G|/g = e f g/g = e f = e f 0 , and e0 = e.
                    Q e(P |p)                   Q e(Q |P )                  Q       e(P |p)
    33. If pT = i Pi i and Pi U = j Q i j i j i , then pU = i (Pi i U ) =
Q           e(Pi |p) =
                       Q    Q    e(Q i j |Pi ) e(Pi |p)
   i (Pi U )             i ( j Qi j           )         . Hence e(Pi |p) = e(Q i j |Pi ))e(Pi |p).
The formula for the f ’s follows from Corollary 9.7.
696                              Hints for Solutions of Problems

    34.pCorollary 9.58pshows that the norm and the    trace are the product and sum of
a + b m and a − b m. Hence they are a 2 − bp       2 m and 2a. This proves (a).

    In (b), the minimal polynomial of r = a + b m has degree 2 if b 6= 0, and this
is the same as the degree of the field polynomial. Hence the two polynomials are
equal, and the minimal polynomial is X 2 − (Tr r)X + N (r). An algebraic integer is
an algebraic element whose minimal polynomial over Q has integer coefficients, and
(b) follows.              p
    In (c), if r = a + b m is a unit with inverse s, then N (r)N (s) = N (rs) =
N (1) = 1 shows that N (r) pis a unit with inverse N p(s). Conversely if r is in T with
N (r) = ±1,pthen r(a − b m) = ±1, and ±(a − b m) is          pan inverse elementp in T .
    For (d), 2 − 1 is a unit in the algebraic integers of Q[ 2]. Its inverse is 2 + 1.
                                          ° p p ¢
    35. With respect to the ordered basis 1, 3 2, ( 3 2)2 , the matrix of multiplication
           p       p
by a + b 3 2 + c( 3 2)2 is             µ        ∂
                                            a 2c 2b
                                            b a 2c      .
                                            c b a
The trace and norm are the trace and determinant of this matrix, namely 3a and
a 3 + 2b3 + 4c3 − 6abc.
    36. In (a), if ξ is any number algebraic over Q of degree r, then the norm relative
to Q(r)/Q of ξ is (−1)r M(0), where M(X) is the minimal polynomial of ξ over
Q. Since M(1 − (1 − ξ )) = 0, the minimal polynomial of 1 − ξ is the polynomial
M(1 − X) adjusted so as to be monic. That is, it is P(X) = (−1)r M(1 − X). Hence
the norm of 1 − ξ is (−1)r P(0) = (−1)2r M(1) = M(1). In the case of the given ≥ ,
the minimal polynomial of ≥ is 8n (X), and therefore  Q the norm of 1 − ≥ is 8n (1).
    For (b), division of both sides of the identity d|n 8d (X) = X n − 1 by X − 1
        Q                                                          Q
gives d|n, d>1 8d (X) = X n−1 + X n−2 + · · · + 1. Therefore d|n, d>1 8d (1) = n.
    If n is a prime power, say with n = pk , let us see by induction on k that 8n (1) = p.
The base case of the induction is k = 1, and the result of the previous paragraph
                                                                          Qk+1
applies. Assuming that 8n (1) = p for n = pk , we have pk+1 = l=1               8 pl (1) =
            Qk
8 pk+1 (1) l=1 p. Therefore 8 pk+1 (1) = p, and the induction is complete.
    Inducting on n, let us now show that 8n (1) = 1 if n is divisible by more than one
positive prime. The base case of the induction is n = 2. Assume that n = p1k1 · · · prkr
and that the result is known for integers less than n. We may assume that n is divisible
by at least two positive primes. Then
                        Q                      ks
                                            r °Q
                                            Q                    ¢ Q
                n=               8d (1) =               8 psl (1)    8d (1),
                      d|n, d>1              s=1   l=1          other d

where the “other d” are the divisors of n that are divisible by at least two primes.
These include n itself. So one of the corresponding factors is 8n (1), and the others
are 1 by the inductive hypothesis. The factor in parentheses is plkl by the result of
the previous paragraph, and the product of the factors in parentheses is n. Therefore
8n (1) = 1, and the induction is complete.
                                            Chapter IX                                           697
                                                       p
   37. Forp(a), the imaginary part of p−1 x ± p−1 −1 is not an integer, and therefore
p−1 (x
     p ± −1 ) is not a Gaussian       integer. Consequently ppdoes not divide either of
x±     −1.    Since  p divides x 2 + 1 in Z and hence in Z[ −1 ], p is not prime in
   p
Z[ −1 ].                                                                         p
   For (b), it follows since p is not prime that p = αβ nontrivially in Z[ −1 ]. Then
p 2 = N ( p) = N (α)N (β). Problem 34c shows that nontrivial factorization      p     implies
that N (α) and N (β) are not units. Thus they are both p. If α = a + b −1, then the
equation p = N (α) says that p = a 2 + b2 .
                                                   p                                2
   38.p Let N be   p the norm function in Q(−1 −2).p Since p divides x + p               2 =
(x + −2 )(x − −2 ) and since neither of       p  p    (x ±    −2  ) is of the form a + b  −2
with a and b in Z, p is not prime in Z[ −2]. Write p = αβ nontrivially.            p     Then
p2 = N ( p) = N (α)N (β) and N (α) = N (β) = p. If α = a + b −2, then
p = N (α) says that p = a 2 + 2b2 .
   39. Thisp is similar to Problem 38 except that the members of the ring are of the
form a +b −3 with a, b in Z or a, b in Z+ 12 . Thus p = N (α) says that p = a 2 +3b2
                                                                                      p
either with a, b in Z or with a, b in Z+ 12 . In the latter case, let ω±1 = 12 (−1− −3 ).
These have N (ω±1 ) = 1. Therefore
                                              °        p                  p      ¢
              p = N (α) = N (αω±1 ) = N (a + b −3 )(− 12 ± 12 −3 )
                     °                              p ¢
                = N 12 (−a ∓ 3b) + 12 (±a − b) −3
                   °             ¢2     °              p ¢2
                = 12 (−a ∓ 3b) + 3 12 (±a − b) −3 .

Since a, b are in Z + 12 , one of a + b and a − b is even, and the other is odd, the sum
2a being odd. If a + b is even, then a − 3b is even since their difference 4b is even,
and vice versa. Hence one of the two choices of sign exhibits p as c2 + 3d 2 with c, d
in Z.
    40. Write L0 = k(x) by the Theorem of the Primitive Element, and let K be
a splitting field of the minimal polynomial of x over k. Then K is a finite Galois
extension of k by Corollary 9.30, and we Q       have k ⊆ L ⊆ L0 ⊆ K. For Q          a in L0 and b in
L, Corollary 9.58 says that NL0 /k (a) = σ ∈G/H 0 σ (a), NL/k (b) = σ ∈G/H 0 σ (b),
                   Q                                                     Q
and NL0 /L (a) = τ ∈H/H 0 τ (a). Hence NL/k (NL0 /L (a)) = σ ∈G/H σ (NL0 /L (a)) =
Q            °Q                ¢      Q              Q                           Q
   σ ∈G/H σ      τ ∈H/H 0 τ (a) =        σ ∈G/H          τ ∈H/H 0 σ τ (a) =        σ ∈G/H 0 σ (a) =
NL0 /k (a). The formula for traces follows similarly by replacing the products by
sums in the above computation.
    41. Since P is symmetric, P(X σ (1) , . . . , X σ (n) ) = P(X 1 , . . . , X n ) for every
permutation σ . Therefore P(rσ (1) , . . . , rσ (n) ) = P(r1 , . . . , rn ) for every σ . Problem
39e at the end of Chapter VIII implies that P(r1 , . . . , rn ) = Q(s1 , . . . , sn ) for a
polynomial Q(X 1 , . . . , X n ) in k[X 1 , . . . , X n ], where s1 , . . . , sn are the elementary
symmetric polynomials in r1 , . . . , rn . The elements s1 , . . . , sn are the coefficients of
F(X), up to sign, and hence are in k. Therefore P(r1 , . . . , rn ) = Q(s1 , . . . , sn ) is in
k.
698                                       Hints for Solutions of Problems
                                                                          Qm
   42. Inspection of the formula gives H1 (X) =                                 i=1       G(X − ri ). For each i, we
can expand G(X − ri ) in powers of X as

             G(X − ri ) = X n + bn−1 (ri )X n−1 + · · · + b1 (ri )X + b0 (ri ),

and each of bn−1 , . . . , b0 is a member of k[X]. When we multiply these for 1 ≤ i ≤ m,
each power of X in the product has a coefficient that is unchanged if we permute
r1 , . . . , rn . Problem 41 says that the coefficient of each power of X is therefore in k.
Thus H1 (X) is in k[X]. A similar argument shows that H2 (X) is in k[X].
    43.pFor (a),                        2                    2
                   p we use F(X) = X − 2 and G(X) = X − 3 in the previous problem.
Then 2 + 3 is a root of
              p      p            p      p              p     p             p      p
  (X − ( 2 + 3 ))(X − ( 2 − 3 ))(X − (− 2 + 3 ))(X − (− 2 − 3 )),

which must have coefficients in Q.
    44. Proposition 4.40 extends the action by an element σ in Sn uniquely from the
set {r1 , . . . , rn } to k[r1 , . . . , rn ] fixing k. The extended σ is a one-one homomor-
phism of k[r1 , . . . , rn ] into itself, hence into k(r1 , . . . , rn ). It extends uniquely to a
field mapping of k(r1 , . . . , rn ) into itself by Proposition 8.6. The homomorphism
corresponding to a composition is the composition of the homomorphisms, and
consequently the homomorphism corresponding to σ −1 is a two-sided inverse of the
homomorphism corresponding to σ . Thus the extension of σ is an automorphism, as
required.
    Conclusion (a) is immediate from Problem 20a. For (b), since K is generated by
k and r1 , . . . , rn , K is certainly generated by KSn and r1 , . . . , rn . We have arranged
that F(X) splits over K, and hence K is the splitting field. Conclusion (d) follows
from Corollary 9.37 once (c) is proved. Thus we are to prove (c).
    The argument for (c) is similar to that in Problem 21. Since F(X) is in KSn , its
coefficients are in KSn . Thus k(u 1 , . . . , u n ) ⊆ KSn . Consequently Corollary 9.37
gives n! = [K : KSn ] ≤ [K : k(u 1 , . . . , u n )]. Problem 16 shows that the right
side divides n!. Therefore equality holds throughout, and we see that [K : KSn ] =
[K : k(u 1 , . . . , u n )]. Since k(u 1 , . . . , u n ) ⊆ KSn , we must have k(u 1 , . . . , u n ) =
KSn .
    45. For (a), we have
                   P            P
          c1 = θi = 2                 si s j = 2 p,
                 i            i< j
                P                 P                      P
         c2 =          θi θ j =          si2 s j2 + 3            si s j sk (si + s j + sk ) + 6s1 s2 s3 s4 ,
                i< j              i< j                  i< j<k
                                  P                                     °P            ¢
         c3 = θ1 θ2 θ3 =                   si3 s j2 sk + 2s1 s2 s3 s4           si2
                               i, j,k                                      i
                              unequal
                                          P                                     °P              ¢
                              +2                  si2 s j2 sk2 + 4s1 s2 s3 s4             si s j .
                                         i< j<k                                  i< j
                                       Chapter IX                                     699

   Part (b) is a calculation with symmetric polynomials and is omitted. For (c), we
have

                            θ1 − θ2 = −(s1 − s4 )(s2 − s3 ),
                            θ1 − θ3 = −(s1 − s3 )(s2 − s4 ),
                            θ2 − θ3 = −(s1 − s2 )(s3 − s4 ).

The square of the product of the left sides is the discriminant of the cubic resolvent,
and the square of the product of the right sides is the discriminant of the given quartic.
   46. In (a), the subgroups in question are

                   H = {(1), (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)}

and A4 . In (b), one considers the possibilities for a Sylow 2-subgroup and is led to
conclude that the only possibilities for the subgroups in question are the powers of a
4-cycle, the dihedral group (generated by H and (1 2 3 4)), and S4 . (The group H
and any 2-cycle generate S4 , and thus the dihedral group cannot be generated by H
and a 2-cycle.)
    47. In (a), the discriminant reduces when q = 0 to 16 p 4r − 128 p2r 2 + 256r 3 =
16r( p4 − 8 p2r + 16r 2 ) = 16r( p2 − 4r)2 . This is 0 if r = 0 or r = p2 /4. If it is
nonzero, it is a square if and only if r is a square. Hence in all cases it is a square if
and only if r is a square.
    In (b), let Y = X 2 . The equation is Y 2 + pY + r = 0, which can be solved with
a square root. For each of the two solutions, we can then solve for X with a square
root. Hence all the roots lie in an extension obtained by adjoining at most three square
roots. Thus [K : Q] divides 8, and |G| divides 8. Consequently G cannot have any
element of order 3.
    In (c), the irreducibility shows that the possibilities for G are as in Problem 46.
Since r is a square, the discriminant is a square, by (a). Proposition 9.63 shows that
the possibilities are as in Problem 46a. Part (b) rules out A4 , and then (c) follows.
    In (d), r nonsquare and F(X) irreducible implies that G is a transitive subgroup of
S4 but not a subgroup of A4 , by (a). Problem 46b shows that G is S4 , or the powers
of a 4-cycle, or the dihedral group D4 . By (b), there is no element of order 3, and S4
is therefore ruled out.
    48. The polynomial remains irreducible when reduced modulo 2, and a prime
factorization modulo 3 is (X + 2)(X 3 + X 2 + X + 2). Thus G is a transitive subgroup
of S4 containing a 3-cycle. The discriminant is 257, not square. By Problem 46b,
G = S4 .
    49. Part (a) is just a computation; the answer is 212 34 . The factorization in (b)
is routine to check, and the only issue is the irreducibility of the cubic factor. For
a cubic polynomial, irreducibility follows if the polynomial has no root in the field.
Thus we need only verify that none of 0, 1, 2, 3, 4 is a root modulo 5.
700                            Hints for Solutions of Problems

   For (c), the conclusion of (b) shows that the only possible reducibility over Q is
into a degree-one factor and a cubic factor. For X 4 + 8X + 12 to have a degree-one
factor, it must have a rational root, and this root must be an integer dividing 12. Let
r be an integer dividing 12. If r is even, then r 4 + 8r is divisible by 16, but 12 is not;
so an even r cannot be a root. We are left with ±1 and ±3 as the possibilities, and
we check that none of these is a root.
   In (d), F(X) is irreducible, and G is transitive. It is a subgroup of A4 since the
discriminant is a square. By (b) and Theorem 9.64, G contains a 3-cycle. Problem
46a shows therefore that G = A4 .
   50. We saw in Problem 49 that G = A4 for X 4 + 8X + 12, in Problem 47c that
G = {(1), (1 2)(3 4), (1 3)(2 4), (1 4)(2 3)} for X 4 + 1, in Problem 48 that
G = S4 for X 4 + X + 1, andpvia Eisenstein’s criterion that G = C4 for 85 (X). Since
X 4 − 2 does not split in Q( 4 2 ), the Galois group in this case cannot be of order 4,
and Problem 47d shows that G must be D4 in this case.
    51. For (a), let C correspond to a set of polynomials I of degree at most n −1. If C
is cyclic, then I is at least a vector space over F. If F(X) = c0 +c1 X +· · ·+cn−1 X n−1
is in I , then X F(X) = c0 X + c1 X 2 + · · · + cn−1 X n is congruent modulo (X n − 1)
to cn−1 + c0 X + · · · + cn−2 X n−1 , which is in I since C is cyclic. Hence I is closed
under multiplication by X mod (X n − 1) and hence under arbitrary multiplications
modulo (X n − 1). Therefore I is an ideal in F[X]/(X n − 1).
    Conversely if I is an ideal in F[X]/(X n − 1), then it is a vector space and is
closed under multiplication by X mod (X n − 1) in F[X]/(X n − 1). If F(X) =
c0 +c1 X +· · ·+cn−1 X n−1 is in I , then X F(X) = c0 X +c1 X 2 +· · ·+cn−1 X n mod I
has to be in I , and the corresponding member of C is (cn−1 , c0 , c1 , . . . , cn−2 ). Hence
C is cyclic.
    For the remaining parts, we identify the cyclic code C with the corresponding ideal
I in F[X]/(X n −1). In (b), let the lowest degree of a member of I be n−k, and let G(X)
be a member of I of this degree. If there is a second member of this same degree,
then their difference has lower degree since both polynomials are monic, and the
difference must be in I , contradiction. Thus G(X) is uniquely defined. Regard G(X)
as a member of F[X] of degree n − k, and let M(X) = GCD(G(X), X n − 1). Then
we can choose A(X) and B(X) in F[X] with A(X)(X n − 1) + B(X)G(X) = M(X).
Passing to F[X]/(X n − 1), we have B(X)G(X) ≡ M(X) mod (X n − 1). Therefore
M(X) is in the ideal I . Since the degree of M(X) is at most deg G(X) and since
G(X) has the minimum degree among the nonzero members of I , either M(X) = 0
or M(X) = G(X). The conclusion M(X) = 0 is ruled out since M(X) is a greatest
common divisor of nonzero polynomials, and thus M(X) = G(X). Therefore G(X)
divides X n − 1.
    Let e I be the inverse image of I in F[X]. This is an ideal, it contains G(X), and
it contains no nonzero element of degree < deg G(X). Since e        I has to be principal,
I = (G(X)). In other words, e
e                                    I consists of all products of G(X) by a member of
F[X]. If F(X)G(X) is such a product, then the division algorithm gives F(X)G(X) =
                                         Chapter IX                                      701

B(X)(X n − 1) + R(X) with R(X) = 0 or deg R < n. Since G(X) divides X n − 1,
G(X) divides R(X). Therefore every member of e         I is congruent modulo X n − 1 to a
product G(X)S(X) that is 0 or has degree < n. Then (c) is clear.
   For (d), (b) showed that G(X) divides X n −1 in F[X]. Write X n −1 = G(X)H (X).
If B(X) in F[X]/(X n −1) corresponds to a member of C, then (b) shows that B(X) =
F(X)G(X) for some F(X) in F[X]. Multiplying by H (X) gives B(X)H (X) =
F(X)G(X)H (X) = F(X)(X n − 1). Hence B(X)H (X) ≡ 0 mod (X n − 1). Con-
versely if B(X)H (X) = A(X)(X n − 1), then B(X)H (X) = A(X)G(X)H (X), and
B(X) = A(X)G(X).
   52. In (a), if r1 , r2 , r3 denote the rows and if v1 = r1 + r3 , v2 = r2 , and v3 = r3 ,
then v1 , v2 , v3 form a basis for the row space, and they cycle into one another when
the columns are shifted in cyclic fashion. Consequently the code is cyclic. Part (b)
involves looking at the 7 nonzero members of the space, and one can just do that
directly.
   In (c), one such matrix is
                                                       
                                          0001101
                                  H = 0 0 1 1 0 1 0.
                                          0110100
                                          1101000

A little check shows that the matrix product HG t is the 4-by-3 zero matrix, and hence
Hv = 0 for each v in C. Thus C is contained in the null space of H. The rank of H
is 4 since the rows are certainly linearly independent. Since the sum of the rank and
the dimension of the null space is the number of columns, namely 7, the dimension
of the null space is 3. Therefore the null space is C and is no larger.
     For (d), the general matrix H is to have n columns and n − k rows. The entries
of the top row are the coefficients of H (X) with the constant term at the right, the
coefficient of X in the next-to-last position, and so on. In each successive row these
coefficients are shifted one position to the left.
     Let G(X) = g0 + g1 X + · · · + gn−k X n−k and H (X) = h 0 + h 1 X + · · · + X k .
We know that {0, X G(X), X 2 G(X), . . . , X k−1 G(X)} is a basis of C. In terms of
members of Fn the l th such vector has the entries g0 , g1 , . . . , gn−k beginning in the
l th position. The (1, j)th entry of H is h n− j with 0’s elsewhere in the row, and the
(i, j)th entry is h n− j−i+1 with 0’s elsewhere in the row. The product of the i th row of
                                    P
H and the l th basis vector of C is n−i+1
                                        j=n−k−i+1 h n− j−i+1 g j−l , which is the coefficient
of X  n−i+1−l  in G(X)H (X). Here 1 ≤ i ≤ n −k and 1 ≤ l ≤ k, so that 2 ≤ i +l ≤ n.
Thus the power of X in question varies from 1 to n − 1. Since G(X)H (X) = X n − 1,
the coefficient is 0. Thus C lies in the null space of H. The same argument with rank
as in the previous paragraph shows that C is exactly the null space.
     53. Since X n − 1 has derivative n X n−1 , we have GCD(X n − 1, n X n−1 ) = 1 when
n is odd. Lemma 9.26 then shows that X n−1 is separable. If n is even, write n = 2k.
Then X n − 1 = (X k − 1)2 in characteristic 2 by Lemma 9.18, and hence every root
has multiplicity at least 2.
702                              Hints for Solutions of Problems

   54. In (a), we have 0 = P(α j ) = c0 + c1 α j + c2 α 2 j + · · · + cn−1 α (n−1) j for
r ≤ j ≤ r + s, and therefore the column vector (c0 , c1 , . . . , cn−1 ) satisfies
                                           c0   0 
                         αr      α 2r ··· α (n−1)r           c1
                       αr+1   α 2(r+1) ··· α (n−1)(r+1)   c2 0
                                      ..                  
                                                          .  = 0.
                                        .                   ..   .. 
                                                                    .
                        αr+s α 2(r+s) ··· α   (n−1)(r+s)
                                                           cn−1    0

    In (b), since s + 1 ≤ n, the number s + 1 of rows is ≤ the number n of columns.
Any square submatrix of size s + 1 is a Vandermonde matrix after factoring a power
of α from each column and transposing, and the determinant of the square submatrix
is therefore the product of a power of α and the differences αr+ j − αr+i with j > i.
Since α is nonzero and since two powers of α can be equal only when the exponents
differ by a multiple of n, the determinant of the square submatrix is nonzero.
    In (c), suppose that s + 1 or fewer of the coefficients c0 , c1 , . . . , cn−1 are nonzero.
Choose s + 1 of them, say ci j for 1 ≤ j ≤ s + 1, such that the remaining ones are 0.
If we discard the others from the matrix equation in (a) and discard the corresponding
columns of the coefficient matrix, then the matrix equation is still valid since we have
discarded only 0’s from the given equations. The resulting system is square with an
invertible coefficient matrix, and hence the unique solution has ci j = 0 for all j. But
then P(X) = 0, in contradiction to the assumption that F(X) 6= 0.
    In (d), if some nonzero member P(X) of C has weight less than s + 2, then (c)
leads to a contradiction. Hence every nonzero weight is ∏ s + 2, and δ(C) ∏ s + 2.
    55. Since α is a root of X n − 1, so is every α j . Since Fj is the minimal polynomial
of α j , Fj divides X n − 1. Also, 1 + X = X − 1 divides X n−1 , and no Fj equals
X − 1, since α j 6= 1 for 1 ≤ j ≤ 2e when 2e < n. Therefore G(X) divides X n − 1.
Applying Problem 54 with r = 0 and s = 2e, we see that the code C generated by
G(X) has δ(C) ∏ s + 2 = 2e + 2.
    56. In (a), if an irreducible polynomial F(X) of degree d has a root β in K, then
K ⊇ F(β) ⊇ F, and [F(β) : F] = d must divide [K : F] = m. In the previous
problem it follows that each Fj (X) has degree dividing m, hence degree ≤ m. The
worst case for the degree of G(X) is that the LCM equals the product, and then the
degree of G(X) is the sum of 1 (from 1 + X) and the sum of the degrees of the
Fj (X)’s. Hence deg G ≤ 2em + 1 in all cases.
    In (b), let nr = 2r − 1, and let K be a field with 2r elements. Theorem 9.14 shows
                                   r
that K is a splitting field for X 2 − X over F. Hence it is a splitting field for X nr − 1
over F. Let e = r, so that e < nr /2 as soon as r ∏ 3. Using this e in the previous
problem, we obtain a cyclic code Cr in Fnr with δ(Cr ) ∏ 2r + 2. According to (a),
the generating polynomial G r (X) has degree at most 2er + 1 = 2r 2 + 1. Therefore
kr = dim Cr = nr − deg G r ∏ nr − 2r 2 − 1 = 2r − 2r 2 − 2. Then kr /nr tends to 1,
and δ(Cr ) tends to infinity, as required.
    57. In (a), the polynomial F1 (X) splits over K because every finite extension of
a finite field is Galois. The Galois group Gal(K/F) consists of the powers of the
                                           Chapter IX                                          703

Frobenius isomorphism x 7→ x 2 , by Proposition 9.40, and is transitive on the roots
                                                                                    k
of F1 (X), by Problem 13a. Hence all the roots are of the form α 2 , and all these
elements are roots. Taking k = 0, 1, 2, 3, we get distinct roots, which is necessary
since K/F is separable.
   For (b), we start from 1 + α + α 4 = 0 and compute the powers of α in terms of
1, α, α 2 , α 3 . The interest is in only the powers α 0 , α 3 , α 6 , α 9 , α 12 , but some of the
intermediate powers help in the computation. We have

                    α3 = α3,
                    α 4 = 1 + α,
                    α5 = α + α2,
                    α6 = α2 + α3,
                    α 9 = α 3 α 6 = α 3 (α 2 + α 3 ) = α 5 + α 6 = α + α 3 ,
                   α 12 = (α 2 + α 3 )2 = α 4 + α 6 = 1 + α + α 2 + α 3 .

Then we form the equation a + bα 3 + cα 6 + dα 9 + α 12 = 0, substitute from above,
and equate coefficients. The result is a homogeneous system of four linear equations
with five unknowns in F. Solving, we find that the space of solutions is 1-dimensional
with a = b = c = d = e. Therefore the minimal polynomial of α 3 has degree 4 and
is 1 + α + α 2 + α 3 + α 4 .
    In (c), we apply Problem 55 with n = 15 and e = 2. Part (a) shows that F1 =
F2 = F4 , and part (b) computed F3 as something else of degree 4. Therefore G(X) =
(1 + X)LCM(F1 , F2 , F3 , F4 ) = (1 + X)LCM(F1 F3 ) = (1 + X)F1 (X)F3 (X), which
has degree 9. Then dim C = 15 − 9 = 6, and Problem 55 gives δ(C) ∏ 2e + 2 = 6.
    59. In (a), Problems 12–13 are applicable when the scalars are extended to K
because the minimal polynomial becomes a product of first-degree factors. The
existence in the conclusion is immediate by applying (a) through (d) in Problem 12
to L ⊗ 1, and the uniqueness is immediate from Problem 13.
                                                          K
P In (b), fix a basis {vi } of V over k. Any member of V has a unique expansion as
   i (vi ⊗ ci ) with each ci in K. Since ϕ(1) = 1, application of the given identity to
v ⊗ 1 gives
                   T (v ⊗ 1) = T (1 ⊗ ϕ)(v ⊗ 1) = (1 ⊗ ϕ)T (v ⊗ 1).
                             P
If we expand T (v ⊗ 1) as i (vi ⊗ ci ), the displayed equation says that
                P                             P                   P
                   i (vi   ⊗ ci ) = (1 ⊗ ϕ)    i (vi   ⊗ ci ) =   i (vi   ⊗ ϕ(ci )).

Hence ϕ(ci ) =P ci for all i. Since ϕ is arbitrary, Theorem 9.38 implies that ci is in k
for all i. Thus i (vi ⊗ ci ) is in V . If we write T v for this element of V , then T is a
k linear map of V to itself such that T = T ⊗ 1.
704                            Hints for Solutions of Problems

   In (c), we multiply the identity L ⊗ 1 = S + N on the left by 1 ⊗ ϕ −1 and on the
right by 1 ⊗ ϕ to obtain

 L ⊗ 1 = (1 ⊗ ϕ −1 )(L ⊗ 1)(1 ⊗ ϕ) = (1 ⊗ ϕ −1 )S(1 ⊗ ϕ) + (1 ⊗ ϕ −1 )N (1 ⊗ ϕ).
                °                      ¢n
The equation (1⊗ϕ −1 )N (1⊗ϕ) = (1⊗ϕ −1 )N n (1⊗ϕ) shows that N is nilpotent.
         °                       ¢°                         ¢
Since (1 ⊗ ϕ −1 )S (1 ⊗ ϕ) (1 ⊗ ϕ −1 )N (1 ⊗ ϕ) = (1 ⊗ ϕ −1 )SN (1 ⊗ ϕ) =
                               °                         ¢°                     ¢
(1 ⊗ ϕ −1 )N S(1 ⊗ ϕ) = (1 ⊗ ϕ −1 )N (1 ⊗ ϕ) (1 ⊗ ϕ −1 )S(1 ⊗ ϕ) , the maps
                                                                             P
(1 ⊗ ϕ −1 )S(1 ⊗ ϕ) and (1 ⊗ ϕ −1 )N (1 ⊗ ϕ) commute.       ° P Finally¢if P   i (vi ⊗ ci ) is
an eigenvector of S with eigenvalue ∏, we have S                 v
                                                                i i ⊗ ci )  =      v ⊗ ∏ci .
                    −1
                                    °P            −1
                                                          ¢           −1
                                                                            P i i
Therefore (1 ⊗ ϕ )S(1 ⊗ ϕ)                  (v ⊗ ϕ (ci )) = (1 ⊗ ϕ ) i (vi ⊗ ∏ci ) =
P           −1 (∏)ϕ −1 (c )), and
                                  P i i −1
   i (vi ⊗ϕ              i           i (v i ⊗ϕ (ci )) is an eigenvector of (1⊗ϕ −1 )S(1⊗ϕ)
                     −1
with eigenvalue ϕ (∏). Then it follows that (1 ⊗ ϕ −1 )S(1 ⊗ ϕ) has a basis of
eigenvectors. By uniqueness of the decomposition L ⊗ 1 = S + N, we must have
(1 ⊗ ϕ −1 )S(1 ⊗ ϕ) = S and (1 ⊗ ϕ −1 )N (1 ⊗ ϕ) = N. Since ϕ is arbitrary in
Gal(K/k), (b) shows that S = S ⊗ 1 and N = N ⊗ 1.
    In (d), (N n ⊗1) = (N ⊗1)n = N n , and N nilpotent implies N nilpotent. Similarly
SN = N S implies S N = N S. Then the fact that S K = S ⊗ 1 = S has a basis of
eigenvectors implies that S is semisimple.
    In (e), S ⊗ 1 and N ⊗ 1 can be expressed uniquely as polynomials in L ⊗ 1 that
are 0 or have degree less than the degree of the minimal polynomial of L ⊗ 1; the
coefficients of these polynomials are in K. Application of a member ϕ to a polynomial
expression S⊗1 = P(L ⊗1) just affects the coefficients and gives another polynomial
expression for S unless ϕ fixes each coefficient. By uniqueness and Theorem 9.38,
we see that the coefficients are in k. A similar argument applies to N ⊗ 1.
    60. This is proved by the same argument as for Problem 13 in Chapter V.
    61. The splitting field for the minimal polynomial is C. According to the procedure
in the solution of Problem 59, we are first supposed to find a decomposition over C.
In a suitable basis we know that A is the sum of a diagonal matrix and a strictly upper
triangular matrix, and this is the Jordan–Chevalley decomposition. Section V.6 shows
how to find the Jordan form and the basis over C in which it is realized. We transform
the D and N back separately to find the semisimple and nilpotent components of A
relative to the standard basis. The result is that
                                                                           
                   0 −1    0  1/2                                0   0    0 −1/2
                 1    0   1/2  0                          0         0   1/2  0 
            S = 0    0    0 −1
                                            and       N = 0         0    0   0
                                                                                  .
                   0 0     1   0                                 0   0    0   0

   62. In (a), if there were a basis of eigenvectors over K, then the fact that the
eigenvalues are equal would mean that A is similar to a scalar matrix. This is
                    ≥ A
manifestly not so. Thus  ¥ is not semisimple.                               ° a cx ¢
   In (b), a matrix ac db that commutes with A is necessarily of the form c a
and has characteristic polynomial X 2 + a 2 + c2 x since the characteristic is 2. If the
                                          Chapter IX                                       705

characteristic polynomial reduces to 0, and then a = c = 0. In this case, A is the 0
matrix.
   In (c), suppose that A = S + N is a Jordan–Chevalley decomposition. Then (a)
says that A is not semisimple and hence cannot be S. On the other hand, (b) says
that N has to be 0 and therefore that A = S is the only possibility. The result is a
contradiction, and thus there is no Jordan–Chevalley decomposition.
   63. This comes down to what is happening in Problem 12 in Chapter V. In terms
of matrices, the problem reduces to the case that a square matrix A is upper triangular
with a certain nonzero scalar c in every diagonal entry. Then D = cI , and U is taken
to be D −1 A.
   64. In (e), for characteristic p > 0, −1 is the sum of p − 1 copies of 1. Hence k
cannot be formally real.
   65. In (e), we have (b−1 − a −1 )ab = a − b. The right side is in P, and so are
a and b. Thus the remaining factor, b−1 − a −1 , has to be in P. In (f), the sum of
a(c−d) > 0 and (a −b)d > 0 is ac−bd > 0. In (g), expansion of (a −b)(c−d) > 0
gives ac + bd > ad + bc.
                                     m
    66. The definition is that abmn xx n +···+b
                                         +···+a0
                                               0
                                                 is positive if am bn−1 is in P. It is routine
to check that the set P of positive elements of k(x) is closed under addition and
                           0

multiplication, and certainly every nonzero element is in exactly one of P 0 and −P 0 .
                                            p                 p
                         p has a + b 2pin P if a + b 2 > 0 in the ordinary sense,
    67. In (a), one ordering
and the other has a + 2 in P if a − b 2 > 0 in the ordinary sense.
                                    p                                    p
    In (b), for any element a + b c with a 2p> b2 c, define a + b c to be in p       P 0 if and
                                                            2        2
only if a is in P. For any element a + b c with b c > a , define a + b c to be
in P 0 if and only if b is in P. The only element    p left undecided by this process is 0,
which is not to be in P 0 . The elements a + b c in P 0 with a 2 > b2 c will be said to
be of type I, while those with a 2 < b2 c will be said to be of type II. It is clear that
each nonzero element x of K is in exactly one of P 0 and −P 0 , and we have to verify
that P 0 is closed under addition and multiplication.
    The verification is a little complicated. It uses parts (f) and (g)     p of Problemp    65
repeatedly. Consider addition. There are cases. Case 1 is that a + b c and a 0 + b0 c
are in P 0 with both of type I. If the sum is of type II, then addition of a 2 > b2 c,
a 02 > b02 c, and (b + b0 )2 c > (a + a 0 )2 gives bb0 c > aa 0 upon cancellation. Squaring
and taking into account that aa 0 > 0, we obtain (b2 c)(b02 c) > a 2 a 02 . On the other
hand, a 2 > b2 c and a 02 > b02 c together imply a 2 a 02 > (b2 c)(b02 c), contradiction.
Thus the sum is of type I. Since a and a 0 are in P, so is a + a 0 . Thus the sum is in P 0 .
                          p                  p
    Case 2 is that a + b c and a 0 + b0 c are in P 0 with both of type II. If the sum is
of type I, then addition of a 2 < b2 c, a 02 < b02 c, and (b + b0 )2 c < (a + a 0 )2 gives
bb0 c < aa 0 upon cancellation. Squaring and taking into account that bb0 > 0, we
obtain (b2 c)(b02 c) < a 2 a 02 . On the other hand, a 2 < b2 c and a 02 < b02 c together
imply a 2 a 02 < (b2 c)(b02 c), contradiction. Thus the sum is of type II. Since b and b0
are in P, so is b + b0 . Thus the sum is in P 0 .
706                              Hints for Solutions of Problems
                            p                               p
    Case 3 is that a + b c is of type I and a 0 + b0 c is of type II (or vice versa).
The argument now depends on the type of the sum. Case 3A is that the sum is of
type I. Adding a 2 > b2 c, b02 c > a 02 , and (a + a 0 )2 > (b + b0 )2 c and canceling gives
a(a + a 0 ) > b(b + b0 )c. We want to see that a + a 0 > 0. If a + a 0 < 0, then the left
side is negative, and hence both sides are negative. Thus the squares of the two sides
are related in the opposite order: a 2 (a + a 0 )2 < b2 (b + b0 )2 c2 . Here the right side is
< a 2 (b + b0 )2 c, and we get (a + a 0 )2 < (b + b0 )2 c, in contradiction to the fact that
the sum is of type I. So a + a 0 is > 0, and the sum is in P 0 . Case 3B is that the sum
is of type II. Adding a 2 > b2 c, b02 c > a 02 , and (b + b0 )2 c > (a + a 0 )2 and canceling
gives b0 (b + b0 )c > a 0 (a + a 0 ). We want to see that b + b0 > 0. If b + b0 < 0, then
both sides are negative. Thus the squares of the two sides are related in the opposite
order: b02 (b + b0 )2 c2 < a 02 (a + a 0 )2 . Here the right side is < b02 c(a + a 0 )2 , and
thus (b + b0 )2 c < (a + a 0 )2 , in contradiction to the fact that the sum is of type II. So
b + b0 is > 0, and the sum is in P 0 .
    This completes the verification that P 0 is closed under addition. p       We now consider p
multiplication, again dividing matters into cases. Case 1 is that a +b c and a 0 +b0 c
are in P 0 with both of type I. Applying Problem 65g to the inequalities a 2 > b2 c and
a 02 > b02 c, we obtain a 2 a 02 +b2 b02 c2 > a 2 b02 c +a 02 b2 c, which says that the product
is of type I. We are to show that aa 0 + bb0 c is > 0. From a 2 > b2 c and a 02 > b02 c,
we obtain 0 < a 2 a 02 − b2 b02 c2 = (aa 0 + bb0 c)(aa 0 − bb0 c). Thus aa 0 + bb0 c and
aa 0 − bb0 c are both > 0 or both < 0, and they are the same as their sum, which is
2aa 0 . Since a and a 0 are in P, we have aa 0 > 0, we conclude that the product is in
P 0.
                            p                p
    Case 2 is that a + b c and a 0 + b0 c are in P 0 with both of type II. Applying
Problem 65g to the inequalities b2 c > a 2 and b02 c > a 02 , we obtain a 2 a 02 +b2 b02 c2 >
a 2 b02 c+a 02 b2 c, which says that the product is of type I. We are to show that aa 0 +bb0 c
is > 0. From a 2 < b2 c and a 02 < b02 c, we see that 0 < b2 b02 c2 − a 2 a 02 =
(bb0 c + aa 0 )(bb0 c − aa 0 ). Thus bb0 c + aa 0 and bb0 c − aa 0 are both > 0 or both < 0,
and they are the same as their sum, which is 2bb0 . Since b and b0 are in P, we have
bb0 > 0, we conclude that the product is in P 0 .
                           p                            p
    Case 3 is that a + b c is of type I and a 0 + b0 c is of type II (or vice versa). From
(a 2 − b2 c)(b02 c − a 02 ) > 0, we obtain c(a 02 b2 + a 2 b02 ) > a 2 a 02 + b2 b02 c2 . Addition
of 2aa 0 bb0 c to both sides yields (ab0 + a 0 b)2 c > (aa 0 + bb0 c)2 , an inequality that
shows the product to be of type II. To show that the product is in P 0 , we are to show
that ab0 + a 0 b > 0. The product of a 2 > b2 c and b02 c > a 02 gives a 2 b02 > b2 a 02
upon cancellation of c, c being positive. Then (ab0 + ba 0 )(ab0 − ba 0 ) > 0, and the
two factors have the same sign. Now a > 0 and b0 > 0 since the given elements
are in P 0 . Thus ab0 > 0. Arguing by contradiction, suppose that ab0 < ba 0 . Then
ab0 > 0 implies (ab0 )2 < (ba 0 )2 , in contradiction to a 2 b02 > b2 a 02 . We conclude
that ab0 > ba 0 , hence that ab0 − ba 0 > 0. Thus ab0 + ba 0 > 0, as required.
    These steps complete all the verifications that P 0 , as we have defined it, is a positive
system. It remains to define a second version of P 0 and to carry out the verifications        p
for it. For the definition, there is no change if a 2 > b2 c, but if b2 c > a, then a + b c
                                          Chapter IX                                       707

is to be in P 0 if and only if −b is in P. The verifications are essentially unchanged
except that the roles of b and −b are interchanged throughout.
     68. In (a), the integer n, if it exists, cannot be 0 because k is formally real by
                                                            p
Problem 64d. So n ∏ 1. We write ξ j = a j + b j cn with each a j and b j in
   p            p
k( c1 , . . . , cn−1 ) and expand out the squares.
                           p         p                                    p
     In (b), let k0 = k( c1 , . . . , cn−1 ). If the coefficient of cn is 0, then (∗)
becomes an equality in k0 that exhibits k0 as not formally real, in contradiction to the
                                       p                                  p
definition of n. If the coefficient of cn is not 0, then (∗) exhibits cn as a member of
k0 , again in contradiction to the definition of n. The conclusion is that K is formally
real.
     69. Order the formally real subfields of k by inclusion upward. The set of such
subfields is nonempty since k is one. The union of a chain of such subfields is again
such a subfield because any expression of a sum equal to −1 has to be valid in a finite
such union. By Zorn’s Lemma, there is a maximal element K. By maximality, K is
a real closed field.
                                                     p
     70. In (a), if c is not a square in k, then k( c ) is a proper algebraic extension
                                                                          p
of k. Since k is maximal among formally real     p subfields of k, k( c ) is not formally
real. Therefore −1 is a sum of squares
                                     P         k( c ), as
                                            in P        p indicated.
                                                            P
     In (b), expansion gives −1 = a j2 +c b2j +2 c             a j b j . Equating coefficients
            p                       P 2        P                           P
of 1 and c shows that −1 =            a j + c b2j . We cannot have b2j = 0 because
                                       P 2
otherwise we would have −1 =              a j and k would not be formally real. Thus
               P 2 P 2
−c = (1 + a j )/ b j , and −c is exhibited as a sum of squares, hence a member
of P. Thus if c is not a square in k, then c cannot be a sum of squares in k. The
contrapositive is: every sum of squaresP in kP   is a square in k.
     In (c), the equality −c = (1 + a j2 )/ b2j , in view of (b), exhibits −c as the
quotient of two squares, hence as a square.
     In (d), let P be the set of nonzero squares. We see from (a) through (c) that every
nonzero element is in P or in −P. By (b), every sum of squares is a square; thus
P is closed under addition. It is clear that P is closed under multiplication. Thus F
becomes an ordered field. Problem 64b shows that every nonzero square has to be in
P, and thus P is the only possibility for the set of positive elements.
     71. In (a), let n be the least odd positive integer such that some polynomial over
k of degree n has no root in k. If this polynomial were reducible, some factor of it
would have smaller odd degree and would have a root. So the polynomial in question
has to be irreducible.                                                          Pk
     In (b), if −1 is a sum of squares in k(α), then we have −1 =                             2
                                                                                   j=1 R j (α)
for suitable polynomials R j (X) in k[X], necessarily of degree ≤ n − 1. In other
          P
words, kj=1 R j (X)2 + 1 is a member of k[X] that vanishes at α. Since Q(X)
                                                       P
is the minimal polynomial of α, Q(X) divides kj=1 R j (X)2 + 1, and we obtain
         Pk
−1 = j=1 R j (X)2 + Q(X)A(X) for a suitable polynomal A(X) in k[X] of degree
≤ n − 2.
708                          Hints for Solutions of Problems

   In (c), the equality of the coefficients of X 2n−2 in the polynomial identity of (b)
shows that the −1 equals the sum of squares of the leading coefficients of the R j ’s
plus the coefficient of X n−2 in A(X). The coefficient of X n−2 in A(X) cannot be 0, or
else −1 would be exhibited as a sum of squares in k. Thus A(X) has degree exactly
n − 2, which is odd. The inductive hypothesis applies to A(X) and says that A(X)
has a root r. We evaluate the polynomial identity from (b) at r, take into account that
                               P
A(r) = 0, and obtain −1 = kj=1 R j (r)2 . Again we have a contradiction to the fact
that k is formally real, and thus the minimal integer n in (a) cannot exist.
   72. The indicated proof goes through without essential change.
    73. Problem 68 shows that k is contained in a certain formally real subfield L of k,
Problem 69 shows that L is contained in a real closed subfield K of k, and Problem 70
shows that K becomes an ordered field. The set of positive elements for K includes
all squares by Problem 64b, and all the members of k in P have become squares in
L by definition
        p       of L. Therefore all members of P are squares in K. The fact that
k = K( −1 ) follows from Problem 72.


                                     Chapter X

    1. If R is a field, then the only ideals are 0 and R, and they certainly satisfy the
descending chain condition. Conversely if the ideals satisfy the descending chain
condition, then there is a minimal nonzero ideal I . Fix m 6= 0 in I . For any nonzero
element a ∈ I , Ra = I since I is a simple module. If x 6= 0 is in R, we apply this
observation to xm, which is nonzero since R is an integral domain. Since Rxm = I ,
there exists y in R with yxm = m. Then (1 − yx)m = 0. Since R is an integral
domain and m 6= 0, we obtain 1 − yx = 0. Therefore y = x −1 .
                                                  ≥ ¥                   ≥ ¥
    2. In (a), let C2 = {±1}. Define r(1) = 10 01 and r(−1) = 10 11 . Then r
                                                                      ≥ ¥
is a representation since 1 + 1 = 0 in F. The subspace U = F 10 is invariant.
If there were a complementary invariant subspace, there would be an eigenvector of
r(−1) not in U . However, the roots of the characteristic polynomial are both 1, and a
second eigenvector would mean that r(−1) is the identity, which it is not. For (b), the
representation in (a) makes F2 into a unital left R module, the R submodules being
the invariant subspaces. There is no complementary R submodule to U , and hence
F2 is not semisimple as an R module.
   3. If {as } is a set of generators of M as a right R module and {bt } is a set of
generators of N as a left R module, then {as ⊗ bt } is a set of generators of M ⊗ R N
as an abelian group. Then (a) follows from this fact and the fact that 1 generates both
Z/kZ and Z/lZ.
   In (b), if l = dk for some d and if b has b = qk + r with 0 ≤ r < |k|, then
a1 ⊗ b1 = aqk(1 ⊗ 1) + (a1 ⊗ r1) = aq(k1 ⊗ 1) + (a1 ⊗ r1) = a1 ⊗ r1, and it
                                        Chapter X                                     709

follows that the map a1⊗b1 7→ a1⊗(b mod k)1 is a well-defined group isomorphism
of (Z/kZ) ⊗Z (Z/lZ) onto (Z/kZ) ⊗Z (Z/kZ).
   In (c), let b(x1, y1) = x y mod k for x, y ∈ Z/kZ. This is Z bilinear from
Z/kZ×Z/kZ into Z/kZ and extends to a group homomorphism L : Z/kZ⊗Z Z/kZ
→ Z/kZ with L(x1 ⊗ y1) = x y mod k. In particular, L(1 ⊗ 1) = 1 mod k.
Therefore k divides the order of 1 ⊗ 1, and Z/kZ × Z/kZ has at least |k| elements.
   In (d), we have 0 = k1 ⊗ 1 = k(1 ⊗ 1) and 0 = 1 ⊗ l1 = l(1 ⊗ 1). If xk + yl = d,
then d(1 ⊗ 1) = x(k(1 ⊗ 1)) + y(l(1 ⊗ 1)) = 0. Hence 1 ⊗ 1 has order dividing d.
By (c), 1 ⊗ 1 has order at least |d|. The result follows.
    4. In (a), each ker ϕ n is an R submodule of M, and these R submodules form
an ascending chain. Hence they are the same from some point on. Similarly each
image ϕ n is an R submodule of M, and these form a descending chain. Hence they
are the same from some point on.
    In (b), if x is in K ∩ I, then ϕ N x = 0 and x = ϕ N y for some y. Then 0 = ϕ N x =
  2N
ϕ y. Since y is in ker ϕ 2N = ker ϕ N , we obtain 0 = ϕ N y = x, and x = 0.
    In (c), if x is in M, then ϕ N x is in image ϕ N = image ϕ 2N . Hence ϕ N x = ϕ 2N z =
  N
ϕ (ϕ N z) for some z ∈ M, and ϕ N x = ϕ N y with y = ϕ N z.
    For (d), if x is in M, let y be as in (c), and write x = (x − y) + y. Then
ϕ N (x − y) = ϕ N x − ϕ N y = 0 and y = ϕ N z show that x − y is in K and y is in I.
Thus M = K + I. Since K ∩ I = 0 by (b), M = K ⊕ I.
    In (e), we know that ϕ(image ϕ       n            n+1 for all n. Taking n > N , we see
                                       Ø ) = image ϕ
                                       Ø
that ϕ(I ) = I. From (b), ker(ϕ I ) ⊆ K ∩ I = 0. Therefore ϕ is one-one from I
onto itself. In addition, ϕ(ker ϕ n ) ⊆ ker ϕ n−1 for all n.Ø Taking n > N shows that
ϕ(K) ⊆ K. For x in K, we have ϕ N x = 0. Therefore (ϕ ØK ) N = 0.
                             Ø
    5. If (i) holds, then √ Ø 0 is one-one from N 0 onto P. Let σ be its inverse. Then
                           N
σ : P → N 0 is one-one with √σ = 1 P . So (ii) holds.
   If (ii) holds, then any n in N has the property that n − σ √(n) has √(n − σ √(n)) =
√(n) − 1 P √(n) = 0 and is therefore in image ϕ. Write n − σ √(n) = ϕ(m) for
some m depending on n; m is unique since ϕ is one-one. If τ : N → M is defined
by τ (n) = m, then τ is an R homomorphism by the uniqueness of m. Consider
τ (ϕ(m)) for m in M. The element n = ϕ(m) has n − σ √(n) = ϕ(m) − σ √ϕ(m) =
ϕ(m) − σ (0) = ϕ(m), and the definition of τ says that τ (ϕ(m)) = m. Hence
τ ϕ = 1 M , and (iii) holds.
   If (iii) holds, then N 0 = ker τ is an R submodule of N . If n is in N 0 ∩ image ϕ,
then n = ϕ(m) for some m ∈ M and also 0 = τ (n) = τ ϕ(m) = 1 M (m) = m. So
n = 0, and N 0 ∩ image ϕ = 0. If n ∈ N is given, write n = (n − ϕτ (n)) + ϕτ (n).
Then ϕτ (n) is certainly in image ϕ, and τ (n − ϕτ (n)) = τ (n) − 1 M τ (n) = 0 shows
that n − ϕτ (n) is in N 0 . Therefore N = N 0 ⊕ image ϕ. Since image ϕ = ker √, we
see that N = N 0 ⊕ ker √ and that (i) holds.
    6. For (a), the conjugation mapping C on R, carrying 1 to itself and carrying i, j,
and k to their negatives, respects addition and satisfies C(x y) = C(y)C(x). Hence
it exhibits R and R o as isomorphic. Then the result follows from Proposition 10.14.
710                           Hints for Solutions of Problems

    For (b), again by Proposition 10.14, we need a noncommutative ring R with identity
such that R is not isomorphic to R o . Let F be a ≥field ¥with two elements, and let R
be the 8-element ring consisting of all matrices a0 bc with a, b, c in F. Define x
to be the matrix with a = 1 and b = c = 0, and define y to be the matrix with
b = 1 and a = c = 0. Computation shows that x 2 = x, y 2 = 0, x y = y, and
yx = 0. A ring isomorphism of R with R o is the same as an additive isomorphism
that reverses the order of multiplication, and we call this an “antiautomorphism” of
R. Suppose that an antiautomorphism ϕ of R exists. We must have ϕ(1) = 1.
Suppose that ϕ(x) = u and ϕ(y) = v. Then u = ϕ(x) = ϕ(x 2 ) = ϕ(x)2 = u 2 and
0 = ϕ(y 2 ) = ϕ(y)2 = v 2 . Expanding u and v in terms of the basis {1, x, y} and
computing, we find that u = k1 + lx and v = my with k, l, m in F. Since ϕ reverses
the order of multiplication, we have uv = ϕ(x)ϕ(y) = ϕ(yx) = ϕ(0) = 0. Thus
0 = (k1 + lx)(my) = km1 + lmx y = (km)1 + (lm)y, and km = lm = 0. Therefore
either m = 0 or k = l = 0. In the first case, ϕ(y) = v = my = 0; in the second case
ϕ(x) = u = k1 + lx = 0. In either case, ϕ fails to be one-one. We conclude that no
antiautomorphism ϕ of R exists.
    7. Take the sum of all simple R submodules of M.
    8. Example 4 in Section 5 shows that A ⊗F K is a vector space over K in such a
way that k0 (a ⊗ k) = a ⊗ k0 k. It is therefore enough to show that the multiplication
is K linear in° each variable of the
                                  ¢ product.
                                      °       Additivity
                                                   ¢      is known, and it° is enough to¢
check that k0 (a1 ⊗k1 )(a2 ⊗k2 ) = k0 (a1 ⊗k1 ) (a2 ⊗k2 ) = (a1 ⊗k1 ) k0 (a2 ⊗k2 ) .
Since scalar multiplication by k0 equals left multiplication by 1 ⊗ k0 , the left equality
is immediate from associativity of multiplication, and the right equality follows from
associativity and from the formula (a1 ⊗ k1 )(1 ⊗ k0 ) = a1 ⊗ k1 k0 = a1 ⊗ k0 k1 =
(1 ⊗ k0 )(a1 ⊗ k1 ).
    9. Define µ(x)(y) = [x, y] for x and y in g, and let ∫(c)(d) = cd for c and d in L.
Then µ(x) : g → g and ∫(c) : L → L are K linear. Therefore b(x, c) = µ(x) ⊗ ∫(c)
is K bilinear from g × L into the K vector space EndK (g ⊗K L), and it extends to a
K linear mapping L : g ⊗K L → EndK (g ⊗K L). Define [X, Y ] = L(X)(Y ).
    With the Lie algebra multiplication now well defined in g⊗K L, one readily checks
the two required properties. Therefore g ⊗K L is a Lie algebra over K satisfying the
two required identities.
    Meanwhile, we know that g ⊗K L is a vector space over L because of a change of
rings. To complete the proof, we need to show that the multiplication is L linear, not
just K linear. It is enough to check L linearity in the second variable because of the
alternating property. Let s be in L, and let x ⊗ c and y ⊗ d be elements of g ⊗K L.
Then we have [x ⊗ c, s(y ⊗ d)] = [x ⊗ c, y ⊗ sd] = [x, y] ⊗ csd = s([x, y] ⊗ cd) =
s[x ⊗ c, y ⊗ d]. Forming K linear combinations, we obtain the desired L linearity
in the second variable of the Lie algebra product.
    10. This problem will follow from the uniqueness of the tensor product as given in
Theorem 10.18 if it is shown that ((A ⊗Z B)/H, qb2 ) is a tensor product of A and B
over R. Thus let β : A × B → G be an R bilinear function from A × B into an abelian
                                           Chapter X                                          711

group G. Since β is automatically Z bilinear, there exists a group homomorphism
ϕ : A ⊗Z B → G such that ϕ(a ⊗ b) = β(a, b) for all a ∈ A and b ∈ B. Then
ϕ(ar ⊗ b − a ⊗ rb) = ϕ(ar ⊗ b) − ϕ(a ⊗ rb) = β(ar, b) − β(a, rb). The right side
is in H , and hence ϕ descends to a group homomorphism ϕ : (A ⊗Z B)/H → G
such that ϕq = ϕ. Then β(a, b) = ϕ(a ⊗ b) = ϕqb2 (a, b) shows that ϕ(qb2 ) = β.
Thus ϕ is the required additive extension of β. For uniqueness, suppose ϕ 0 is a second
additive extension of β. Then ϕ 0 qb2 (a, b) = ϕqb2 (a, b) for all a ∈ A and b ∈ B,
and hence ϕ 0 q(a ⊗ b) = ϕq(a ⊗ b). The elements a ⊗ b generate A ⊗Z B, and hence
ϕ 0 q = ϕq on A ⊗Z B. Since q maps onto (A ⊗Z B)/H , ϕ 0 = ϕ on (A ⊗Z B)/H .
    11. We are to show that if C is a commutative associative R algebra with
identity and if ϕ1 : A1 → C and ϕ2 : A2 → C are homomorphisms of commu-
tative associative R algebras with identity, then there exists a unique homomorphism
ϕ : A1 ⊗ R A2 → C of R algebras with identity such that ϕi 1 = ϕ1 and ϕi 2 = ϕ2 .
Define b(a1 , a2 ) = ϕ1 (a1 )ϕ2 (a2 ). This is R bilinear into C because b(a1r, a2 ) =
ϕ1 (a1r)ϕ2 (a2 ) = ϕ1 (a1 )rϕ2 (a2 ) = ϕ1 (a1 )ϕ2 (ra2 ) = b(a2 , ra2 ), and hence there
exists a unique homomorphism ϕ : A1 ⊗ R A2 → C of abelian groups such that
ϕ(a1 ⊗a2 ) = b(a1 , a2 ) = ϕ1 (a1 )ϕ2 (a2 ). Then ϕi 1 (a1 ) = ϕ(a1 ⊗1) = ϕ1 (a1 )ϕ2 (1) =
ϕ1 (a1 )1 = ϕ1 , and ϕi 1 = ϕ1 . Similarly ϕi 2 = ϕ2 . To complete the proof, it is enough
to show that the homomorphism ϕ of abelian groups is a homomorphism of R algebras.
The fact that ϕ is a homomorphism of R modules is immediate from Corollary 10.19.
Also, ϕ(1 ⊗ 1) = °ϕ1 (1)ϕ2 (1) = 1 shows      ¢ that ϕ carries identity to identity. Finally
the computation ϕ (a1 ⊗ a2 )(a10 ⊗ a20 ) = ϕ(a1 a10 ⊗ a2 a20 ) = ϕ1 (a1 a10 )ϕ2 (a2 a20 ) =
ϕ1 (a1 )ϕ1 (a10 )ϕ2 (a2 )ϕ2 (a20 ) = ϕ1 (a1 )ϕ2 (a2 )ϕ1 (a10 )ϕ2 (a20 ) = ϕ(a1 ⊗ a2 )ϕ(a10 ⊗ a20 )
shows that ϕ respects multiplication on a set of additive generators of A1 ⊗ R A2 .
   12. Part (a) is immediate from Proposition 10.1. If √ is a nonzero map in M E , then
√(E) is a submodule of M isomorphic to E. Hence √(E) ⊆ M E by construction,
and (b) follows. Part (c) is immediate from (b).
   13. With d ∈ D E = Hom R (E, E), we can form √d = √ ◦ d if √ is in
Hom R (E, M E ), and we can form de = d(e) if e is in E. These definitions give
the required unital D E module structures for (a) and (b). The members of D E =
Hom R (E, E) commute with the left R action on E by definition, and this is (c).
   14. In view of (c) in the previous problem, the left action of R on E can be regarded
as a right R o action on E in such a way that it commutes with the left D E action on
E. In other words, E is a unital (D E , R o ) bimodule. Corollary 10.19b shows that
M E ⊗ D E E becomes a unital right R o module, hence a unital left R module.
  15. Define a map b : M E × E → M, additive in each variable, by b(√, e) =
√(e). For d in D E , this has b(√ ◦ d, e) = (√ ◦ d)(e) = √(d(e)) = b(√, d(e)).
Hence b is D E bilinear and has an additive extension 8 : M E ⊗ D E E → M with
8(√ ⊗ e) = √(e).
  The map 8 is R linear since 8(r(√ ⊗ e)) = 8(√ ⊗ re) = √(re) = r(√(e)) =
r(8(√ ⊗ e)). Since √ is in M E , √(e) is in M E ; thus 8 has image in M E .
712                          Hints for Solutions of Problems
                                               L
   To see that 8 is onto M E , write M E = s∈T Ms with each Ms simple, and fix
an isomorphism αs ∈ Hom R (E, Ms ) for each s P   ∈ T . For any element m ∈ M E , we
can find a finite subset° T 0 of T such that
                                          ¢   m =   s∈T 0 m s with m s ∈ Ms . If we let
                          P
es = αs−1 (m s ), then 8          α
                             s∈T 0 s ⊗ es   = m. Thus 8 maps onto M E .
   To see that 8 is one-one, we observe from Problem 12 and Lemma 10.3 that
                                                  ° M ¢ M
 M E = Hom R (E, M) = Hom R (E, M E ) = Hom R E,            Ms =       Hom R (E, Ms ).
                                                         s∈T      s∈T

Each summand on the right side is isomorphic to D E . That is, the collection of
isomorphisms {αs }s∈T from the previous paragraph is a basis of M E as a right D E
vector                                               E
      Pspace. Consequently every element of M ⊗ D E EP                    P as a finite
                                                             may be written
sum αs ⊗ es with es ∈ E. The image of the element αs ⊗ es is αs (es ). If
                                                                 the Ms ’s. Since αs is
this is 0, then each αs (es ) is 0 because of the independence ofP
an isomorphism, it follows that es = 0 for each s. Therefore αs ⊗ es = 0. Thus
8 is one-one.
   16. The composition in one order is

                   N 7→ Hom R (E, N ) 7→ Hom R (E, N ) ⊗ D E E.                    (∗)

For N = M E , the map 8, when applied to the composition, recovers M E , since
Problem 15 says that 8 is onto. For general N , we can write M E = N ⊕ N 0 . When
we apply 8 to (∗) for N and N 0 separately, we recover R submodules of N and N 0 ,
respectively. To have a match for all of M E , we must recover all of N and N 0 .
   The composition in the other order is

                    W 7→ W ⊗ D E E 7→ Hom R (E, W ⊗ D E E).                       (∗∗)

For W = M E , the image corresponds under the map Hom(1, 8) to Hom R (E, M E ) =
M E . For general W , we can write M E = W ⊕ W 0 . When we apply Hom(1, 8)
to (∗∗) for W , we get an R submodule of M E that contains W . In fact, for any
w ∈ W , Hom R (E, E ⊗ D E E) contains the map e 7→ w ⊗ e. Composing with 8
gives e 7→ w(e). Thus the members of W are in the image. Similarly the members
of W 0 are in the image for W 0 . The direct sum of the images must be M E , and thus
the images must be exactly W and W 0 .
   17. The computation

ϕ(8 M (√ ⊗ e)) = ϕ(√(e)) = (ϕ ◦ √)(e)) = 8 N ((ϕ ◦ √) ⊗ e) = 8 N (ϕ E (√) ⊗ e)

proves the formula in the last line of the statement of the problem. For the inverse,
suppose we are given a map τ ∈ Hom D E (M E , N E ). Then τ induces an R linear map

                          τ E0 : M E ⊗ D E E → N E ⊗ D E E
                                           Chapter X                                         713

defined by
                                  τ E0 (√ ⊗ e) = τ (√) ⊗ e.
Composition with the isomorphism of Problem 15 gives an R homomorphism

                           τ E = 8 N ◦ τ E0 ◦ 8−1
                                               M : ME → NE .

We show that ϕ 7→ ϕ E and τ 7→ τ E are inverses. If a map ϕ in Hom R (M E , N E ) is
given, we are to calculate (ϕ E ) E ∈ Hom R (M E , N E ). It is enough to find the effect
of (ϕ E ) E on elements 8 M (√ ⊗ e) with √ ∈ M E and e ∈ E. For such an element,

          (ϕ E ) E (8 M (√ ⊗ e)) = 8 N ((ϕ E )0 (√ ⊗ e)) = 8 N (ϕ E (√) ⊗ e)
                                   = ϕ E (√)(e) = ϕ(√(e)) = ϕ(8 M (√ ⊗ e)).

Thus (ϕ E ) E = ϕ. Similarly for τ ∈ Hom D E (M E , N E ), we find that (τ E ) E = τ .
Thus ϕ 7→ ϕ E and τ 7→ τ E are inverses.
                           L
   18. Let us write M = s∈S MP     s with each Ms semisimple. Each Ms is contained
in some M E , and hence M =          E∈E M E . Let us see that the sum is direct. If
M E has nonzero intersection with M E1 + · · · + M En , where E 1 , . . . , E n are simple
R modules with no two isomorphic, then there is a nonzero R linear map from E
into M E1 + · · · + M En . We can write each M E j as a sum of simple R submodules
isomorphic to E j , and Proposition 10.1 shows that
                                                        M
                               M E1 + · · · + M En =          Ms0
                                                        s∈T

with each Ms0 isomorphic to one of E 1 , . . . , E n . If all of E 1 , . . . , E n are nonisomor-
phic with E, then Lemma 10.3 and Proposition 10.4a show that

                            Hom R (E, M E1 + · · · + M En ) = 0,
                                                  P
contradiction. We conclude that the sum M = E∈E M E is direct. This proves the
equality at the left in the displayed formula of the problem, and the isomorphism on
the right in that display follows from Problem 15.
   19. If N is a left R submodule of M, then N E ⊆ M E for every E. Conversely the
previous problem shows that a system of N E ’s defines an R submodule N . Thus this
problem is a restatement of Problem 16.
   20. We have
                             Q                    Q
             Hom R (M, N ) ∼
                           =   Hom R (M E , N ) =   Hom R (M E , N E ),
                                E∈E                        E∈E

and the rest follows from Problem 17.
                    SELECTED REFERENCES




Artin, E., Geometric Algebra, Interscience Publishers, Inc., New York, 1957;
        reprinted, John Wiley & Sons, Inc., New York, 1988.
Artin, M., Algebra, Prentice–Hall, Englewood Cliffs, NJ, 1991.
Baez, J. C., The octonions, Bull. Amer. Math. Soc. 39 (2002), 145–205.
Berlekamp, E. R., Algebraic Coding Theory, McGraw–Hill Book Company, 1968.
Berlekamp, E. R. (ed.), Key Papers in the Development of Coding Theory, IEEE Press
        Selected Reprint Series, IEEE Press [Institute of Electrical and Electronics
        Engineers, Inc.], New York, 1974.
Brown, K. S., Cohmology of Groups, Springer-Verlag, New York, 1982; reprinted
        with corrections, 1994.
Dunford, N., and J. T. Schwartz, Linear Operators, Part I, Interscience Publishers,
        Inc., New York, 1958; reprinted, John Wiley & Sons, Inc., New York, 1988.
Elkies, N. D., Lattices, linear codes, and invariants II, Notices of the American
        Mathematical Society 47 (2000), 1382–1391.
Farb, B., and R. K. Dennis, Noncommutative Algebra, Springer-Verlag, New York,
        1993.
Hall, M., The Theory of Groups, The Macmillan Company, New York, 1959; reprinted,
        Chelsea Publishing Company, New York, 1976.
Halmos, P. R., Naive Set Theory, D. Van Nostrand Company, Inc., Princeton, 1960;
        reprinted, Springer-Verlag, New York, 1974.
Hasse, H., Number Theory, English translation of the original German, Springer-
        Verlag, Berlin, 1980; reprinted, 2002.
Hayden, S., and J. F. Kennison, Zermelo–Fraenkel Set Theory, Charles E. Merrill
        Publishing Company, Columbus, 1968.
Hecke, E., Lectures on the Theory of Algebraic Numbers, English translation of the
        original German, Springer-Verlag, New York, 1981.
Hermite, C., Sur quelques approximations algébriques, J. Reine Angew. Math. 76
        (1873), 342–344.
Hilton, P. J., and U. Stammbach, P., A Course in Homological Algebra, Springer-
        Verlag, New York, 1971; second edition, 1997.
Hoffman, K., and R. Kunze, Linear Algebra, Prentice–Hall, Englewoord Cliffs, NJ,
        1961; second edition, 1971.
Hua, L.-K., Introduction to Number Theory, English translation of the original Chi-
        nese, Springer-Verlag, Berlin, 1982.
                                        715
716                              Selected References

Ireland, K., and M. Rosen A Classical Introduction to Modern Number Theory,
        Springer-Verlag, New York, 1982; second edition, 1990.
Jacobson, N., Basic Algebra, Volume I, W. H. Freeman and Company, San Fran-
        cisco, 1974; second edition, New York, 1985. Volume II, W. H. Freeman and
        Company, San Francisco, 1980; second edition, New York, 1989.
Jacobson, N., Lectures in Abstract Algebra, Volume I, D. Van Nostrand Company,
        Inc., Princeton, 1951; reprinted, Springer-Verlag, New York, 1975. Volume II,
        D. Van Nostrand Company, Inc., Princeton, 1953; reprinted, Springer-Verlag,
        New York, 1975. Volume III, D. Van Nostrand Company, Inc., Princeton,
        1964; reprinted with corrections, Springer-Verlag, New York, 1975.
Kelley, J. L., General Topology, D. Van Nostrand Company, Inc., Princeton, 1955;
        reprinted, Springer-Verlag, New York, 1975.
Knapp, A. W., Basic Real Analysis, Birkhäuser, Boston, 2005.
Lam, T. Y., A First Course in Noncommutative Rings, Springer-Verlag, New York,
        1991; second edition, 2001.
Lang, S., Algebra, Addison-Wesley, Reading, MA, 1965; second edition 1984; revised
        third edition, Springer, New York, 2002.
Lang, S., Algebraic Number Theory, Springer-Verlag, New York, 1986; second edi-
        tion, Springer-Verlag, New York, 1994.
Lindemann, F., Über die Zahl π, Math. Annalen 20 (1882), 213–225.
Mac Lane, S., Categories for the Working Mathematician, Springer, New York, 1971;
        second edition, 1998
Morgan, S. P., Richard Wesley Hamming (1915–1998), Notices of the American
        Mathematical Society 45 (1998), 972–977.
Pollard, H., The Theory of Algebraic Numbers, Carus Monographs, Mathematical
        Association of America, 1950.
Rotman, J., Galois Theory, Springer-Verlag, New York, 1990; second edition, 1998.
Sah, C.-H., Abstract Algebra, Academic Press, New York, 1967.
St. Andrews, School of Mathematics and Statistics, University of St. Andrews, Scot-
        land, MacTutor History of Mathematics Archive, Biographies of Mathemati-
        cians, updated as of 2015, http://www-groups.dcs.st-and.ac.uk
        for background, http://www-history.mcs.st-andrews.ac.uk/
        history/index.html for official entry point, http://www-groups
        .dcs.st-and.ac.uk/~ history/Mathematicians for direct access
        to list of mathematicians.
Van der Waerden, B. L., Modern Algebra, English translation of the original German,
        Volume I, Frederick Ungar Publishing Company, New York, 1949; multiple
        later translated editions. Volume II, Frederick Ungar Publishing Company,
        New York, 1950; multiple later translated editions.
Zariski, O., and P. Samuel, Commutative Algebra, Volume I, D. Van Nostrand Com-
        pany, Inc., Princeton, 1958; reprinted, Springer-Verlag, New York, 1975.
                         INDEX OF NOTATION



This list indexes recurring symbols introduced in Chapters I through X (pages
1–591). For other recurring symbols, including set-theoretic notation introduced
in the appendix (pages 593–613), see the list of Standard Notation on page xx.
   In the list below, each piece of notation is regarded as having a key symbol.
The first group consists of those items for which the key symbol is a fixed Latin
letter, and the items are arranged roughly alphabetically by that key symbol. The
next group consists of those items for which the key symbol is a Greek letter.
The final group consists of those items for which the key symbol is a variable or
a nonletter, and these are arranged by type.

An , 121                                 F p , 142, 148
a(u, v), 348                             Fq , 461
Aadj , 72                                F(S), 307, 377
Ann(U ), 52                              F(S), 159
Aut H, 167                               Gal(K/k), 474
B n (G, N ), 356                         GCD, 2, 394
Cm , 126                                 GL(V ), 122
C(G, C), 330                             GL(n, F), 122
C(G, R), 381                             H, 128
c(A), 395                                H8 , 128
C n (G, N ), 356                         H (V ), 302
C`(x), 165                               H n (G, N ), 356
Cliff(E, h · · i), 302                   Hom(√, ϕ), 568
D, 511, 532                              HomF (U, V ), 43, 44
Dn , 122                                 HomK (U, V ), 266
deg, 150, 156                            Hom R (M, N ), 554
det, 67, 215                             i, j, k, 128
dim V, 37                                K, k, K/k, 453
EndK (V ), 372                           ker L , 46
End R (M), 554                           ker ϕ, 131
e1 , . . . , en , 36                     l, 332
ei , f i , g, 527                        LCM, 32
F, 9, 34, 158                            lrad, 250
F4 , 143                                 Mn (R), 215
                                      717
718                    Index of Notation


                                   p
Mkn (F), 25                     Z[ −1 ], 392
Mmn (R), 376                    Z n (G, N ), 356
Morph(A, B), 189                ZG, 373
NK/k (a), 519
N (H ), 188                     Greek
O, 304                          °0,u ¢1, 44
O, 343                                 , 45
                                ≥0 ¥
                                     L
O(V ), O(n), 122                   10
                                         , 45
Obj(C ), 189                     δn , 356
C opp , 191, 210                 δ(C), 206
Pfaff(X), 299, 449               ∂ : V → V 00 , 54
PSL(2, Z), 366                   6, 48
PSL(2, Z/mZ), 366                χ R , 339
PSL(n, F), 205                   ϕ, 7
Q[θ], 122, 143                   ϕx , 454
r, 332                           8n (X), 490
rrad, 250
Sn , 121                        Operations on sets given
S(E), 284                       by superscripts
S n (E), 284                    V 0 , 50
S 0 = S ∪ S −1 , 307            G 0 , 313
SL(V ), 122                     M ⊥ , 96
SL(n, F), 122                   U ⊥ , 251
SO(V ), SO(n), 122              L ∗ , 100
SU(V ), SU(n), 122              A∗ , 101
sgn, 17                         V , 115
span{vα }, 35                   b 329
                                G,
T (E), 281                      At , 41
T n (E), 281                    L t , 53
Tr A, 74                        M o , R o , 555
TrK/k (a), 519                  C× , Q× , R× , Z× , 120
U (g), 301                      (Z/mZ)× , 142
U(V ), U(n), 122                R × , 143
W (S 0 ), 307                   P −1 , 439
W (V ), 302
                                Specific functions
wt(c), 206
                                ( · , · ), 90
Z G , 165
                                k · k, 91
Z G (x), 165
                                [ · , · ], 301
Z/mZ, 120
                                h · , · i, 249
Z/(m), 120
                                [K : k], 456
                                 Index of Notation                              719



Isolated symbols                          E ⊗K F, 265
∼
=, 48, 119, 144                           e ⊗ f, 265
≡, 120                                    M ⊗ R N , 574
1, 118                                    m ⊗ n, 574
{1}, 118                                  ϕ ⊗ √, 575
                                          V
1 A , 190                                 V(E),     291
                                            n
                                              (E), 291
Operations on sets and classes            E C , 274
G/H, G\H, 130                             E L , 275
                                          L
g H, Hg, 129                                      , 62, 138, 376
                                          Q s∈S
G 1 × G 2 , 126                                  , 62, 136, 198, 376
G ×τ H, 169                               `s∈S
                                            s∈S , 199
G 1 ∗ G 2 , 324
G p , 163
G = hS; Ri, 314
                                          *K , 474
                                            s∈S , 323
                                            H

RG, 380                                     −1
                                          S R, 428
F[X], 9                                   R S , 428
R[X], 149                                 R P , 430
R[X 1 , . . . , X n ], 155                G P , 534
k[x1 , . . . , xn ], 454
k(x1 , . . . , xn ), 454                  Miscellaneous
                                          µ              ∂
K(X), 384                                     12345
                                                            , permutation, 15
C S , 196                                     43512
V /U, 55                                  (5 2 3), cycle, 16
M/N , 378                                  f 1 ∗ f 2 , convolution, 339
I + J, 405                                (a), principal ideal, 390
I J, 405, 435                             (a1 , . . . , an ), ideal, 390
U ⊕ V, 59
                                    INDEX




Abel, 494                                 uniqueness, 467–468
abelian group, 119                     algebraic curve, 411
   direct sum for, 138, 139            algebraic element, 454
   finitely generated, 176             algebraic extension, 456
   free, 176                              finite 456
   tensor product for, 578                simple 457
absolute value, 604                    algebraic integer, 342, 411, 421, 515
addition in abelian group, 119         algebraic number, 123, 387, 457. 465, 515
addition in ring, 141                  algebraic number field, 123, 373, 387, 457
addition in vector space, 34           algebraically closed, 464
addition of cardinal numbers, 613      algebraically closed field, 212
addition of matrices, 25               alternating, 67
additive extension, 574                alternating bilinear form, 253
additive functor, 585                  alternating group, 121, 171
additive in a variable, 574            alternating matrix, 257
adjoin, 454                            alternative algebra, 304
adjoint, 100, 101                      annihilator, 52, 85
   classical, 72                       antisymmetrized tensor, 294
algebra, 280                           antisymmetrizer, 294
   alternative, 304                    area, 86
   associative, 280, 372               Artin–Schreier Theorem, 550, 552
   associative R, 380                  ascending chain condition, 421, 565
   Clifford, 302                       associate, 393
   division, 373                       associated graded map, 300
   exterior, 291                       associated graded vector space, 300
   filtered associative, 301           associated primitive polynomial, 396
   graded associative, 301             associative algebra, 280, 372
   group, 380, 445                        filtered, 301
   Heisenberg Lie, 302                    graded, 301
   Jordan, 303                            tensor product for, 582
   Lie, 281, 301                       associative law, 25, 34, 82, 118, 141
   polynomial, 289                     associative R algebra, 380
   symmetric, 284                      associativity formula, 580, 581
   tensor, 282                         associator, 304
   tensor product for, 582             automorphism, 453
   universal enveloping, 301              inner, 201
   Weyl, 302                              of group, 167
algebraic closure, 465                    of number field, 124
   existence, 466                      Axiom of Choice, 597
                                     721
722                                     Index



                                           Cayley number, 304
Baer multiplication, 355, 361              Cayley–Dickson construction, 304
basis, 36, 176                             Cayley–Hamilton Theorem, 221
   dual, 51                                Cayley’s Theorem, 125
   free, 312                               center, 372, 380, 554
   standard, 36                               of group, 165
   standard ordered, 48                    centralizer of element, 165
   vector space, 36                        chain, 583, 605
   Weyl, 296                               chain condition
BCH code, 548                                 ascending, 417, 565
Bessel’s inequality, 94                       descending, 565
Bezout’s identity, 3                       change of rings, 573, 578
bilinear, 90                               character, 339
bilinear form, 249                            multiplicative, 329
   alternating, 253                        characteristic of a field, 148
   invariant, 260                          characteristic polynomial, 74, 218
   nondegenerate, 251                      characteristic subgroup, 360
   skew-symmetric, 253                     check matrix, 548
   symmetric, 253                          Chinese Remainder Theorem, 6, 405
bilinear function, 263, 574                class, 594, 595
bilinear map, 263                             equivalence, 600
bilinear mapping, 263                      class equation of group, 187
bimodule, 573                              class function, 340
block, 232                                 classical adjoint, 72
block multiplication, 86                   Clifford algebra, 302
Bolzano–Weierstrass Theorem, 603           closed, 583
boundary map, 583                          closed form, 584
Burnside’s Theorem, 345                    coboundary, 356
                                           coboundary map, 356
cancellation law, 118                      cochain, 356
canonical form, 212                        cocycle, 356
  Jordan, 232, 409                         code, 207
  of rectangular matrix, 242                  BCH, 548
  rational, 245, 447, 448                     cyclic, 547
canonical map into double dual, 54            cyclic redundancy, 209
canonical-form problem, 214                   dual, 363
Cantor, 612                                   error-correcting, 206, 363, 547
Cardan’s formula, 492, 510, 513               Hamming, 207
Cardano, 493                                  linear, 207
cardinal number, 610                          parity-check, 207
  addition of, 613                            repetition, 207
cardinality, 610                              self-dual linear, 363
Cartan matrix, 86                          codomain, 596
Cartesian product, 595                     coefficient, 9, 149
  indexed, 597                                Fourier, 330, 362
category, 53, 135, 189                        leading, 150
  opposite, 191, 210                          matrix, 336
Cauchy’s Theorem in group theory, 185      cofactor, 70, 217
                                               Index                                     723



cohomology group, 356                             covariant functor, 192
cohomology of groups, 355, 584                    Cramer’s rule, 24, 72, 217
collection, 595                                   CRC-8, 209
column space, 38                                  crossed homomorphism, 357
column vector, 25                                 cubic polynomial, 542
  in an ordered basis, 45                         cubic resolvent, 545
common multiple, 32                               cut, 602
commutative diagram, 194                          cycle, 15
commutative law, 25, 34, 83, 119                  cycle structure, 166
commutative ring, 141                             cycles, disjoint, 16
commutator, 360                                   cyclic
commutator subgroup, 313                            code, 547
complement, 595                                     group, 125
completely reducible, 555                            R module, 401
complex, 583, 585                                   redundancy code, 209
complex conjugate of vector space, 115              subspace, 244
complex conjugation, 604                            vector, 244
complex number, 604                               cyclotomic field, 490, 500
complexification, 274                             cyclotomic polynomial, 399, 490, 540
composition, 598
composition factor, 173, 561                      dal Ferro, 493
composition series, 172, 560                      de Rham cohomology, 584
congruent modulo, 120                             decomposition group, 534
conjugacy class, 165                              Dedekind domain, 416, 437, 450, 525
conjugate, 165                                    degree, 10, 150, 154, 456
conjugate linear, 90                              dependent, integrally, 421
conjugates of an element, 523                     derivative of polynomial, 461
conjugation, complex, 604                         descend to, 57, 133, 147, 375
consecutive quotient, 172, 560                    descending chain condition, 565
constant polynomial, 10, 150, 155                 determinant, 65, 86, 215
constructible coordinates, 470                       Gram, 114
  field of, 470–471                                  of linear map, 66
constructible regular polygon, 473, 489, 499         of matrix, 66
contraction of ideal, 432                            of square matrix, 67
contragredient, 53                                   properties of, 68, 216
  matrix of, 53                                      Vandermonde, 71, 217
contragredient representation, 365                diagonal entry, 24, 180, 447
contravariant functor, 193                        diagonal matrix, 24, 447
convolution, 339, 372, 381                        diagram, 194
coproduct functor, 199, 376, 589                     commutative, 194
  in a category, 198                                 square, 194
corner variable, 21                               difference, 595
correspondence, one-one, 598                      difference product, 511
coset                                             differential equations, system, 246
  left, 129                                       differential form, 584
  right 129                                       differentiation, 461
countable, xx                                     dihedral group, 121, 170, 316
counting formula, 164                             dimension, 564
724                                           Index



   of vector space, 37, 78                       Eisenstein’s irreducibility criterion, 398
direct image, 599                                element, 593, 594
direct product                                   elementary divisor, 179, 447
   of groups, 126, 127, 136, 137                 elementary matrix, 28
   of R modules, 376                             elementary row operation, 20
   of rings, 374                                 elementary symmetric polynomial, 448
   of vector spaces, 62, 63                      entity, 593
direct sum                                       entry, 20, 24
   of abelian groups, 138, 139                      diagonal, 24, 180, 447
   of R modules, 376                             enveloping algebra, universal, 301
   of vector spaces, 59, 60, 61, 62, 64          equality of matrices, 24
Dirichlet’s theorem on primes in arithmetic      equation, linear, 23
      progressions, 330, 367                     equivalence class, 600
discriminant, 511, 532, 533                      equivalence relation, 599–600
disjoint cycles, 16                              equivalent
disjoint union, 198                                 factor set, 352
distributive law, 26, 34, 141                       finite filtrations, 561
divide, 1, 10, 388, 438                             group extensions, 352
division algebra, 373                               normal series, 174
division algorithm, 2, 11                           words, 307
division ring, 144, 373                          equivariant mapping, 191
divisor, 1                                       error-correcting code, 206, 363, 547
   elementary, 179, 447                          Euclid’s Lemma, 5
   greatest common, 2, 8, 12, 393                Euclidean algorithm, 2, 13
   zero, 144                                     Euclidean domain, 392, 444, 446
Dixmier, 559                                     Euler ϕ function, 7
domain, 596                                      evaluate, 10
   Dedekind, 416, 437, 450, 525                  evaluation, 151, 157
   Euclidean, 392, 444, 446                      even permutation, 121
   integral, 144                                 exact, 583, 584
   principal ideal, 390, 442                     exact form, 584
   unique factorization, 389                     exact sequence, 584, 585
dot product, 90                                     short, 585
double a cube, 469, 471                             split, 588
double dual, 54                                  expansion
dual                                                homogeneous-polynomial, 155
   double, 54                                       in cofactors, 70, 217
   of vector space, 50                              monomial, 155
dual basis, 51                                   expressible in terms of k and radicals, 495
dual code, 363                                   extension
duality in category theory, 210                     additive, 574
                                                    algebraic, 456
eigenspace of linear function, 76                   field, 453
eigenspace of matrix, 73                            finite, 456
eigenvalue of linear function, 76                   finite algebraic, 456
eigenvalue of matrix, 73                            finite Galois, 485
eigenvector of linear function, 76                  group, 348
eigenvector of matrix, 73                           linear, 44, 264
                                             Index                                        725



  normal, 481                                       splitting, 458
  of ideal, 432                                 field isomorphism, 453
  of scalars, 275, 573, 578                     field map, 453
  separable, 476                                field mapping, 453
  simple algebraic, 457                         field polynomial, 519
exterior algebra, 291                           filtered associative algebra, 301
external direct product                         filtered vector space, 300
  of groups, 126, 136                           filtration, finite, 560
  of R modules, 376                             finite
external direct sum                                 algebraic extension, 456
  of abelian groups, 138                            basis condition, 417, 565
  of R modules, 376                                 extension, 456
  of vector spaces, 59, 61                          field, 143, 153, 159, 373, 461, 488
external semidirect product of groups, 169          filtration, 560
                                                    Galois extension, 485
factor, 1, 10, 136, 388, 438                        length, 563
factor group, 132                                   linear combination, 35
factor ring, 146                                    order, 130
factor set, 348                                     rank, 178
Factor Theorem, 11                                  rank of free R module, 401
factor through, 57, 133, 147, 378                   support, 381
factorization, 1, 10                            finite-dimensional vector space, 37
   nontrivial, 2, 10                            finitely generated abelian group, 176
   prime, 5                                         fundamental theorem for, 179
   unique, 5, 14                                finitely generated group, 315
family, 595                                     finitely generated R module, 400
fast Fourier transform, 331, 364                finitely presented group, 315
Fermat number, 472                              First Isomorphism Theorem, 57, 133, 379
Fermat prime, 472                               Fitting’s Lemma, 588
Fermat’s Little Theorem, 142                    fixed field, 474
Ferrari, 493                                    forgetful functor, 192
field, 142                                      form, 263
   algebraically closed, 212, 464                   bilinear, see bilinear form
   characteristic of, 148                           Hermitian, 258
   cyclotomic, 490, 500                             sesquilinear, see sesquilinear form
   extension, 453                                   skew-Hermitian, 258
   finite, 143, 153, 159, 373, 461, 488         formally real field, 550
   fixed, 474                                   Fourier coefficient, 330, 362
   formally real, 550                           Fourier inversion formula
   Galois, 461                                      for class functions, 341
   number, 123, 373, 387, 457                       for finite abelian group, 330
   obtained by adjoining, 454                       for finite group, 338
   of constructible coordinates, 470–471        Fourier inversion problem, 330
   of fractions, 383, 601                       Fourier series 330
   ordered, 550                                 fractional ideal, 450
   prime, 148                                       unique factorization of, 451
   quadratic number, 422, 543                   fractions
   real closed, 550                                 field of, 383, 601
726                                            Index



   partial, 444                                   graded associative algebra, 301
free abelian group, 176                           graded vector space, 300
free basis, 312                                   Gram determinant, 114
free group, 308                                   Gram matrix, 114
   rank, 314                                      Gram–Schmidt orthogonalization process, 95
free product, 199, 323                            greatest common divisor, 2, 8, 12, 393
free R module, 377                                greatest lower bound, 603
free subset, 312                                  group, 118
Frobenius map, 462                                  abelian, 119
function, 595                                       alternating, 121, 171
   bilinear, 263                                    automorphism of, 167
   class, 340                                       center of, 165
   k-linear, 263                                    cohomology, 356
   k-multilinear, 263                               cyclic, 125
   linear, 42, 44                                   decomposition, 534
   multilinear, 263                                 dihedral, 121, 170, 316
   polynomial, 153, 158                             direct product for, 126, 127, 136, 137
functional, linear, 50                              finitely generated, 315
functional, multilinear, 66                         finitely presented, 315
functor, 53, 135                                    free, 308
   additive, 585                                    free abelian, 176
   contravariant, 193                               free product for, 323
   coproduct, 199, 376, 589                         Galois, 474
   covariant, 192                                   general linear, 122
   forgetful, 192                                   homomorphism of, 131
   product, 196, 376                                icosahedral, 368
Fundamental Theorem                                 octahedral, 368
   of Algebra, 14, 465, 492                         of units, 143
   of Arithmetic, 5                                 order of, 129
   of Finitely Generated Abelian Groups, 179        orthogonal, 122
   of Finitely Generated Modules, 402, 447          quaternion, 128
   of Galois Theory, 345, 490                       quotient of, 132
                                                    rotation, 122
Galois, 494                                         semidirect product for, 169
Galois extension, finite, 485                       simple, 171
Galois field, 461                                   solvable, 494
Galois group, 474                                   special linear, 122
Galois theory, 123, 484                             special unitary, 122
Gauss, 473, 489, 500                                symmetric, 121
Gauss’s Lemma, 395                                  tetrahedral, 368
Gaussian integer, 392, 446                          trivial, 118
general linear group, 122                           unitary, 122
generated by, 125                                 group action, 124, 159
generated submodule, 377–378                        transitive, 163
generating polynomial, 209, 547                     trivial, 161
generator, 125, 176, 399                          group algebra, 380, 445
  monic, 244                                      group extension, 348
generators, 314                                   group ring, integral, 373
                                        Index                                           727



                                           imaginary part, 604
Hamming code, 207                          independent variable, 21
Hamming distance, 206                      indeterminate, 9, 149, 154, 155
Hamming space, 206                         index of subgroup, 164
harmonic analysis, 506                     indexed Cartesian product, 597
harmonic polynomial, 116                   indexed intersection, 597
Heisenberg Lie algebra, 302                indexed union, 597
heptadecagon, 503                          infimum, 603
Hermite, 515                               infinite order, 130
Hermitian, 101                             infinite-dimensional vector space, 78
Hermitian form, 258                        inhomogeneous system, 23
Hermitian matrix, 259                      injection, 59, 62
Hermitian sesquilinear form, 258           inner automorphism, 201
Hermitian symmetric, 90                    inner product, 90
Hilbert Basis Theorem, 416, 418            inner-product space, 90
Hilbert–Schmidt norm, 112                  integer, algebraic, 342, 411, 421, 515
homogeneous element, 281                   integer, Gaussian, 392, 446
homogeneous ideal, 284                     integers modulo, 120
homogeneous polynomial, 116, 155           integral, 421
homogeneous system, 23                     integral closure, 416, 421
homogeneous-polynomial expansion, 155      integral domain, 144
homomorphism                               integral group ring, 373
  crossed, 357                             integrally closed, 425
  of groups, 131                           integrally dependent, 421
  of R modules, 375                        Intermediate Value Theorem, 603
  of rings, 144                            internal direct product
  substitution, 151, 156                      of groups, 127, 137
                                              of R modules, 376
icosahedral group, 368                        of vector spaces, 63
ideal, 145                                 internal direct sum
   contraction of, 432                        of abelian groups, 139
   extension of, 432                          of R modules, 377
   fractional, 450                            of vector spaces, 60, 61, 64
   left, 378                               internal semidirect product of groups, 169
   maximal, 385                            intersection, 595
   prime, 384                                 indexed, 597
   principal, 390                          intertwining operator, 333
   right, 378                              invariant
   two-sided, 145                             leave a bilinear form, 260
   unique factorization of, 438               of group action, 357
identity element, 118                      invariant subspace, 73, 333
identity in a ring, 142                    invariant vector subspace, 218
identity matrix, 27                        inverse, 192
identity morphism, 190                        multiplicative, 143
image, 596                                 inverse element, 118
   direct, 599                             inverse function, 598
   inverse, 599                            inverse image, 599
   of homomorphism, 131                    inverse matrix, 27
728                                        Index



invertible matrix, 27                         leave a bilinear form invariant, 260
involution, 242                               left coset, 129
irreducible element, 388                      left ideal, 378
irreducible left R module, 555                left R module, 374
irreducible representation, 333               left radical, 250
isometry, 159                                 left regular representation, 332, 338, 365
isomorphic, 48, 119, 144, 164, 192, 352,      left vector space, 556
     378                                      left-coset space, 130
isomorphism, 48, 119, 144, 192, 378, 453      Legendre polynomial, 114
   natural, 268                               length of module, 563
isotropic subspace, 296                       length of word, 307
isotropy subgroup, 163                        letter, 121
isotypic submodule, 589                       Lie algebra, 281, 301
Iwasawa decomposition, 113                       Heisenberg, 302
                                              Lie bracket, 301
Jacobi identity, 301                          Lindemann, 515
Jordan algebra, 303                           linear, 42, 44
Jordan block, 231, 409                        linear code, 207
Jordan canonical form, 232, 409                  self-dual, 363
Jordan form, 231                              linear combination, 35
Jordan normal form, 231                       linear equation, 23
Jordan–Chevalley decomposition, 243, 549      linear extension, 44, 264
Jordan–Hölder Theorem, 176, 562              linear fractional transformation, 160
                                              linear function, see linear map
k automorphism, 453                           linear functional, 50
k isomorphism, 453                            linear map, 42, 44
k-linear, 66                                     determinant of, 66
   function, 263                                 eigenspace of, 76
   map, 263                                      eigenvalue of, 76
   mapping, 263                                  eigenvector of, 76
k-multilinear                                    kernel of, 46
   function, 263                                 normal, 110
   map, 263                                      orthogonal, 103
   mapping, 263                                  positive definite, 107
kernel of homomorphism, 131                      positive semidefinite, 107
kernel of linear map, 46                         unitary, 103
Kronecker delta, xx, 27                       linear mapping, see linear map
Kronecker product, 297                        linear operator, 42
                                              linear transformation, see linear map
Lagrange resolvents, 506                      linearly independent set, 36, 176
Lagrange’s Theorem, 130                       local ring, 434
law of composition, 190                       localization, 416
law of cosines, 91                               of R at the prime P, 430
law of quadratic reciprocity, 499, 544           of R with respect to S, 429
leading coefficient, 150                      lower bound, 603
leading term, 150
least common multiple, 32                     MacWilliams identity, 364
least upper bound, 603, 606                   map, 596
                                            Index                                  729



 bilinear, 263                                 matrix representation, 332
 coboundary, 356                               matrix ring, 371
 field, 453                                    maximal element, 605
 k-linear, 263                                 maximal ideal, 385
 k-multilinear, 263                            maximum condition, 417, 565
 linear, 42, 44                                member, 595
 multilinear, 263                              minimal distance, 207
mapping, see map                               minimal polynomial, 221, 223, 455
matrix, 24                                     minimum condition, 565
 addition for, 25                              module
 alternating, 257                                cyclic, 401
 Cartan, 86                                      direct product for, 376
 check, 548                                      direct sum for, 376
 coefficient, 336                                finitely generated, 400
 column space of, 38                             free R, 377
 determinant of, 66, 67                          homomorphism of, 375
 diagonal, 24, 447                               irreducible, 555
 eigenspace of, 73                               left R, 374
 eigenvalue of, 73                               of finite rank, 401
 eigenvector of, 73                              quotient, 378
 elementary, 28                                  rank of, 402
 equality for, 24                                right R, 375
 Gram, 114                                       semisimple, 555
 Hermitian, 259                                  simple, 555
 identity, 27                                    tensor product for, 574
 inverse, 27                                   modulo, 120
 invertible, 27                                monic generator, 244
 multiplication for, 26                        monic polynomial, 150
 nilpotent, 232                                monomial, 155
 nonsingular, 212, 217                         monomial expansion, 155
 null space of, 38                             morphism, 189
 of a linear map in two ordered bases, 45        identity, 190
 orthogonal, 103                               multilinear form
 positive definite, 107                          symmetric, 283
 positive semidefinite, 107                      function, 263
 rank of, 41                                     functional, 66
 row space of, 38                                map, 263
 scalar multiplication for, 25                   mapping, 263
 singular, 212, 217                            multiple, 1, 10
 skew-symmetric, 257                             least common, 32
 square, 24                                    multiplication
 symmetric, 253                                  Baer, 355, 361
 symplectic, 450                                 in a group, 118
 trace of, 74                                    in a ring, 141
 transpose of, 41                                in an algebra, 280
 unitary, 103                                    of matrices, 26
 Vandermonde, 71, 217                          multiplicative character, 329
 zero, 25                                      multiplicative inverse, 143
730                                     Index



multiplicative system, 428                 opposite category, 191, 210
multiplicity of a root, 14                 opposite ring, 555
                                           orbit, 163
n-fold tensor product, 280                 order
Nakayama’s Lemma, 436                         finite, 130
natural isomorphism, 268                      infinite, 130
natural transformation, 268                   of group, 129
negative, xx, 119                          ordered field, 550
nicely normed, 305                         ordered pair, 595
Nielsen–Schreier Theorem, 318              ordering
nilpotent, 549                                partial, 605
   element, 443                               simple, 286, 605
   matrix, 232                                total, 605
Noetherian ring, 418                          well, 605
nondegenerate bilinear form, 251           ordinary differential equations, system, 246
nonsingular, 212, 217                      orthogonal complement, 97
nontrivial factorization, 2, 10            orthogonal group, 122, 262
norm, 91, 519, 544                         orthogonal linear map, 103
   Hilbert–Schmidt, 112                    orthogonal matrix, 103
normal extension, 481                      orthogonal projection, 97
normal linear map, 110                     orthogonal set, 93
normal series, equivalent, 174             orthogonal vectors, 93
normal series of groups, 172               orthonormal basis, 93
normal subgroup, 131                       orthonormal set, 93
normalizer of subgroup, 188
null space, 38                             pair
Nullstellensatz, 412                         ordered, 595
number                                       unordered, 595
   algebraic, 123, 387, 457, 465, 515      parallelogram law, 91
   complex, 604                            parity-check code, 207
   rational, 601                           Parseval’s equality, 98
   real, 602                               partial fractions, 444
number field, 123, 373, 387, 457           partial ordering, 605
   automorphism of, 124                    pentagon, 501
   quadratic, 422, 543                     period of cyclotomic field, 500
                                           permanence of identities, 215
object, 189                                permutation, 15, 121
octahedral group, 368                        even, 121
octonion, 304                                odd, 121
odd permutation, 121                       Pfaffian, 299, 449
one-one, 598                               Plancherel formula, 338
one-one correspondence, 598                Poincaré–Birkhoff–Witt Theorem, 301
onto, 598                                  point, 595
operation, elementary row, 20              Poisson summation formula, 362
operator                                   polar decomposition, 111
  intertwining, 333                        polarization, 92
  linear, 42                               polynomial, 9, 149, 154
  projection, 226                            associated primitive, 396
                                        Index                                 731



   characteristic, 74, 218                   difference, 511
   constant, 10, 150, 155                    dot, 90
   cubic, 542                                free, 199, 323
   cyclotomic, 399, 490, 540                 functor, 198, 376
   elementary symmetric, 448                 in a category, 196
   field, 519                                in a group, 118
   generating, 209, 547                      in an algebra, 280
   harmonic, 116                             indexed Cartesian, 597
   homogeneous, 116, 155                     inner, 90
   Legendre, 114                             Kronecker, 297
   minimal, 221, 223, 455                    n-fold tensor, 280
   monic, 150                                of matrices, 26
   primitive, 394                            of permutations, 15
   quartic, 541, 546                         set-theoretic, 595
   separable, 476                            tensor, 263
   split, 458                                triple tensor, 277
   symmetric, 448, 544                       vector, 281
   weight enumerator, 209                  projection, 59, 62, 226
   zero, 10, 150                             orthogonal, 97
polynomial algebra, 289                    Projection Theorem, 96
polynomial function, 153, 158              proper subset, 595
polynomial ring, 371                       properly contained, 595
positive, xx                               pure tensor, 265
positive definite linear map, 107          Pythagorean Theorem, 91
positive definite matrix, 107
positive semidefinite linear map, 107      quadratic number field, 422, 543
positive semidefinite matrix, 107          quadratic reciprocity, 499, 544
power, 125                                 quartic polynomial, 541, 546
presentation, 314                          quaternion, 128
primary block, 232                         quaternion group, 128
primary decomposition, 229                 quotient
Primary Decomposition Theorem, 229           group, 132
primary subspace, 229                        homomorphism, 132, 146
prime, 2, 10                                 map, 55
   relatively, 6                             module, 378
prime element, 389                           ring, 146, 374
prime factorization, 5                       space, 55, 130
prime field, 148
prime ideal, 384                           R homomorphism, 375
primitive element, 480                     R module, 375
primitive polynomial, 394                  R submodule, 377
   associated, 396                         radical, 250, 253, 257, 495
primitive root, 490                        ramification index, 527, 543
Principal Axis Theorem, 254                range, 596
principal ideal, 390                       rank
principal ideal domain, 390, 442             of free abelian group, 178
product                                      of free group, 314
   Cartesian, 595                            of free R module, 402
732                                           Index



   of matrix, 41                                    matrix, 371
rational canonical form, 245, 447, 448              Noetherian, 418
rational number, 601                                opposite, 555
real closed field, 550                              polynomial, 371
real number, 602                                    quotient of, 146
real part, 604                                      with identity, 142
reduced row-echelon form, 20                        zero, 142
reduced word, 325                                Rodrigues’s formula, 114
reducible element, 389                           root, 10, 152
refinement, 174, 561                                multiplicity of, 14
reflexive, 600, 605                                 primitive, 490
regular                                             tower, 495
   17-gon, 503                                   rotation, 43
   heptadecagon, 503                             rotation group, 122
   pentagon, 501                                 row operation, elementary, 20
   polygon, 473, 489, 499                        row reduction, 21
   representation, 332, 337, 338, 365            row space, 38
relation, 314, 595                               row vector, 25
   equivalence, 599–600                          row-echelon form, 20
   function as, 595                              Russell’s paradox, 593
   partial ordering as, 605
relatively prime, 6                              S-tuple, 196
repetition code, 207                             scalar, 9, 19, 34, 89, 211
representation, 161                              scalar multiplication
   contragredient, 365                              in vector space, 34
   irreducible, 333                                 of matrices, 25
   left regular, 332, 338, 365                   scalars, extension of, 275, 573, 578
   matrix, 332                                   scalars, restriction of, 277
   right regular, 332, 337, 338                  Schreier, 175, 348, 562
   unitary, 332                                  Schreier set, 319
residue class degree, 527, 543                   Schroeder–Bernstein Theorem, 79, 610
restriction, 598                                 Schur orthogonality, 335
restriction of scalars, 277                      Schur’s Lemma, 333, 559
Riemann sphere, 160                              Schwarz inequality, 92
Riesz Representation Theorem, 99                 Second Isomorphism Theorem, 58, 135, 379
right coset, 129                                 self-adjoint, 101
right ideal, 378                                 self-dual linear code, 363
right R module, 375                              semidirect product of groups, 169
right radical, 250                               semisimple, 549
right regular representation, 332, 337, 338      semisimple left R module, 555
rigid motion, 159                                separable element, 476
ring, 141                                        separable extension, 476
   commutative, 141                              separable polynomial, 476
   direct product for, 374                       sesquilinear, 90
   division, 144, 373                            sesquilinear form, 258
   group, 373                                       Hermitian, 258
   homomorphism of, 144                             skew-Hermitian, 258
   local, 434                                    set, 593, 594
                                        Index                                           733



set theory, von Neumann, 594                 characteristic, 360
set theory, Zermelo–Fraenkel, 593            commutator, 313
set-theoretic product, 595                   index of, 164
short exact sequence, 585                    isotropy, 163
sign of permutation, 17                      normal, 131
signature, 255, 260                          normalizer of, 188
significant factor, 321                    submodule, 377
similar matrices, 48, 213                    generated, 377–378
simple algebraic extension, 457              isotypic, 589
   existence, 457                          subring, 144
   uniqueness, 458                         subset, 595
simple group, 171                          subspace, 35
simple left R module, 555                    cyclic, 244
simple ordering, 286, 605                    invariant, 73, 333
simplicial complex, 583                      isotropic, 296
simplicial homology, 583                     primary, 229
simply transitive group action, 163          stable, 73
singleton, 595                             substitution homomorphism, 151, 156
singular, 212, 217                         sum of two cardinal numbers, 613
size, 24                                   sum of vector subspaces, 58
skew-Hermitian form, 258                   superset, 595
skew-Hermitian sesquilinear form, 258      support, finite, 381
skew-symmetric bilinear form, 253          supremum, 603
skew-symmetric matrix, 257                 Sylow p-subgroup, 185
socle, 589                                 Sylow Theorems, 185
solvable group, 494                        Sylvester’s Law, 255, 260
span, 35, 36                               symmetric, 90, 101, 600
spanning set, 36                             Hermitian, 90
special linear group, 122                  symmetric algebra, 284
special unitary group, 122                 symmetric bilinear form, 253
Spectral Theorem, 105                      symmetric group, 121, 159
split exact sequence, 588                  symmetric matrix, 253
split polynomial, 458                      symmetric multilinear form, 283
splitting field, 458                       symmetric polynomial, 448, 544
   existence, 458                            elementary, 448
   uniqueness, 459                         symmetrized tensor, 290
square a circle, 469, 472                  symmetrizer, 290
square diagram, 194                        symplectic group, 262
square matrix, 24                          symplectic matrix, 450
stabilizer, 163                            system of linear equations, 23
stable subspace, 73                        system of ordinary differential equations, 246
standard basis, 36
standard ordered basis, 48                 Tartaglia, 493
Steinitz, 466                              tensor algebra, 282
straightedge and compass, 468              tensor product, 263
subcategory, 190                              n-fold, 280
subfield, 144                                 of abelian groups, 578
subgroup, 119                                 of modules, 574
734                                           Index



   of R algebras, 582                              abstract, 200, 298
   triple, 277                                     of Clifford algebra, 302
tetrahedral group, 368                             of coproduct in a category, 198
Theorem of the Primitive Element, 123, 457,        of direct product of groups, 136, 137
      480, 524                                     of direct product of vector spaces, 63–64
total ordering, 605                                of direct sum of abelian groups, 138–139,
trace, 519, 544                                       139–140
   of matrix, 74                                   of direct sum of vector spaces, 60, 64–65
transcendental element, 454                        of exterior algebra, 292
transcendental π, 472, 515                         of field of fractions, 383
transformation                                     of free group, 308
   linear, 42, 44                                  of free R module, 377
   linear fractional, 160                          of group algebra, 381
   natural 268                                     of integral group ring, 374
transitive, 600, 605                               of localization, 431
transitive group action, 163                       of product in a category, 196
transpose of matrix, 41                            of ring of polynomials, 150, 156–157
transposition, 16                                  of S n (E), 285
triangle inequality, 605                           of symmetric algebra, 285
triangular form, 219                               of tensor algebra, 282
triple tensor product, 277                         of tensor product of modules, 575
trisect an angle, 469, 472                         of tensor product of vector spaces, 263–264
trivial group, 118                                 of universal enveloping algebra, 301
                                                      V
trivial group action, 161                          of n (E), 292
tuple 196                                          of Weyl algebra, 303
two-sided ideal, 145                             unknown, 19
                                                 unordered pair, 595
UFD1, 389, 419                                   upper bound, 603, 605
UFD2, 389
union, 595                                       Van Kampen Theorem, 323
  disjoint, 198                                  Vandermonde determinant, 71, 217
  indexed, 597                                   Vandermonde matrix, 71, 217
unipotent, 550                                   variable, 19
unique factorization, 5, 14                        corner, 21
  of fractional ideal, 451                         independent, 21
  of ideal, 438                                  vector, 34
unique factorization domain, 389                   addition for, 34
unit, 1, 10                                        column, 25
  in a ring, 143                                   cyclic, 244
unit vector, 93                                    row, 25
unital, 375                                        scalar multiplication for, 34
unitary group, 122                                 unit, 93
unitary linear map, 103                          vector product, 281
unitary matrix, 103                              vector space, 34, 158
unitary matrix representation, 332                 associated graded, 300
unitary representation, 332                        basis of, 36
universal enveloping algebra, 301                  complex conjugate of, 115
universal mapping property                         dimension of, 37, 78
                                       Index                                      735



  direct product for, 62, 63              Wentzel, 473
  direct sum for, 59, 60, 61, 62, 64      Weyl algebra, 302
  dual of, 50                             Weyl basis, 296
  filtered, 300                           Wilson’s Theorem, 201, 539
  finite-dimensional, 37                  word, 307
  graded, 300                             word problem, 310
  infinite-dimensional, 78                  for finitely presented groups, 316
  left, 556                                 for free groups, 310
  quotient of, 55                           for free products, 325, 326
vector subspace, 35
  invariant, 218                          Zassenhaus, 174, 561
  sum for, 58                             Zermelo–Fraenkel set theory, 593
volume, 86                                Zermelo’s Well-Ordering Theorem, 466, 609
von Neumann set theory, 594               zero divisor, 144
                                          zero matrix, 25
weight, 206                               zero polynomial, 10, 150
weight enumerator polynomial, 209         zero ring, 142
well ordering, 605                        Zorn’s Lemma, 79, 385, 466, 468, 555, 605
